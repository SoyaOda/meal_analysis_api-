================================================================================
MEAL ANALYSIS API v2.0 - æˆ¦ç•¥çš„ãƒãƒ«ãƒDBæ „é¤Šæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ
================================================================================
ç”Ÿæˆæ—¥æ™‚: 2025-06-10 17:11:57
åˆ†æå¯¾è±¡: test_multi_db_nutrition_search.pyå®Ÿè¡Œæ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«
================================================================================

ğŸš€ STRATEGIC MULTI-DB ELASTICSEARCH SEARCH ARCHITECTURE OVERVIEW
------------------------------------------------------------

ğŸ”„ STRATEGIC SEARCH EXECUTION FLOW:
1. test_multi_db_nutrition_search.py â†’ FastAPI /complete endpoint
2. Phase1: Gemini AIç”»åƒåˆ†æ â†’ æ–™ç†ãƒ»é£Ÿæè­˜åˆ¥
3. æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢:
   ğŸ“ DISHæˆ¦ç•¥: EatThisMuch dish â†’ EatThisMuch branded (fallback)
   ğŸ¥• INGREDIENTæˆ¦ç•¥: EatThisMuch ingredient â†’ Multi-DB (MyNetDiary/YAZIO/branded)
4. æ „é¤Šãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»çµæœä¿å­˜

ğŸ—ï¸ COMPONENT-BASED ARCHITECTURE v2.0:
â”œâ”€â”€ Test Layer
â”‚   â””â”€â”€ test_multi_db_nutrition_search.py (æˆ¦ç•¥çš„æ¤œç´¢ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ)
â”œâ”€â”€ FastAPI Application Layer (app_v2)
â”‚   â”œâ”€â”€ main/app.py (Server, CORS, routing)
â”‚   â””â”€â”€ api/v1/endpoints/meal_analysis.py (Complete analysis endpoint)
â”œâ”€â”€ Pipeline Management Layer
â”‚   â”œâ”€â”€ orchestrator.py (MealAnalysisPipeline - å…¨ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆ¶)
â”‚   â””â”€â”€ result_manager.py (ResultManager - çµæœä¿å­˜ãƒ»ç®¡ç†)
â”œâ”€â”€ Component Layer
â”‚   â”œâ”€â”€ base.py (BaseComponent - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŸºåº•ã‚¯ãƒ©ã‚¹)
â”‚   â”œâ”€â”€ phase1_component.py (Phase1Component - Gemini AIåˆ†æ)
â”‚   â””â”€â”€ elasticsearch_nutrition_search_component.py (æˆ¦ç•¥çš„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³)
â”œâ”€â”€ Data Model Layer
â”‚   â”œâ”€â”€ nutrition_search_models.py (NutritionMatch, NutritionQueryInput/Output)
â”‚   â””â”€â”€ phase1_models.py (Phase1Input/Output, Dish, Ingredient)
â”œâ”€â”€ Elasticsearch Infrastructure
â”‚   â”œâ”€â”€ create_elasticsearch_index.py (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãƒ»ç®¡ç†)
â”‚   â””â”€â”€ elasticsearch-8.10.4/ (Elasticsearchã‚µãƒ¼ãƒãƒ¼)
â””â”€â”€ Data Layer
    â”œâ”€â”€ yazio_db.json (1,825é …ç›® - ãƒãƒ©ãƒ³ã‚¹é£Ÿå“)
    â”œâ”€â”€ mynetdiary_db.json (1,142é …ç›® - ç§‘å­¦çš„ãƒ‡ãƒ¼ã‚¿)
    â””â”€â”€ eatthismuch_db.json (8,878é …ç›® - æœ€å¤§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹)

ğŸ¯ STRATEGIC SEARCH FEATURES:
- ğŸ”¥ Dishæ¤œç´¢æˆ¦ç•¥: EatThisMuch dish (ãƒ¡ã‚¤ãƒ³) + branded (è£œåŠ©)
- ğŸ¥• Ingredientæ¤œç´¢æˆ¦ç•¥: EatThisMuch ingredient (ãƒ¡ã‚¤ãƒ³) + Multi-DB (è£œåŠ©)
- âš¡ é«˜é€ŸåŒ–: 677ms â†’ 381ms (44%å‘ä¸Š)
- ğŸ“Š æœ€é©åŒ–çµæœ: 144ä»¶ â†’ 50ä»¶ (é–¢é€£æ€§é‡è¦–)
- ğŸ¯ æˆ¦ç•¥çš„åˆ†æ•£: EatThisMuch 72%, MyNetDiary/YAZIO å„14%
- ğŸ“ˆ å“è³ªå‘ä¸Š: ã‚¹ã‚³ã‚¢é–¾å€¤20.0ã«ã‚ˆã‚‹å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ğŸ’¾ è©³ç´°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: æˆ¦ç•¥ãƒ•ã‚§ãƒ¼ã‚ºãƒ»ã‚½ãƒ¼ã‚¹æƒ…å ±è¿½è·¡

ğŸ”§ TECHNICAL SPECIFICATIONS:
- Search Engine: Elasticsearch 8.10.4 (11,845ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ)
- AI Service: Google Vertex AI (Gemini 2.5 Flash)
- Web Framework: FastAPI 0.104+ (async/await)
- Architecture: Component-based Pipeline Pattern
- Data Format: JSON (100gæ­£è¦åŒ–æ „é¤Šãƒ‡ãƒ¼ã‚¿)
- Search Strategy: BM25F + Multi-Signal Boosting + Strategic Filtering
- Performance: 90.9% match rate, sub-second response times

ğŸš€ STRATEGIC IMPROVEMENTS vs LEGACY:
- æˆ¦ç•¥çš„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸æŠ (EatThisMuchã‚’ä¸­å¿ƒã¨ã—ãŸæœ€é©åŒ–)
- ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ä½å“è³ªæ™‚ã®è‡ªå‹•è£œå®Œ)
- åŠ¹ç‡çš„ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ (å¿…è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—)
- æ§‹é€ åŒ–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (æ¤œç´¢ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡å¯èƒ½)
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«è¨­è¨ˆ (æ–°æˆ¦ç•¥ãƒ»DBã®å®¹æ˜“è¿½åŠ )

================================================================================

ğŸ“ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«
============================================================

ğŸ“„ FILE: test_multi_db_nutrition_search.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 19,963 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 14:55:21
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Multi-Database Nutrition Search Test v3.0 - Multi-DB Elasticsearch Edition

ElasticsearchNutritionSearchComponentã®ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦
3ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆyazio, mynetdiary, eatthismuchï¼‰ã‹ã‚‰å„3ã¤ãšã¤
æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœã‚’å–å¾—ã—ã¦ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import requests
import json
import time
import os
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional

# Elasticsearch Nutrition Search Component
from app_v2.components.elasticsearch_nutrition_search_component import ElasticsearchNutritionSearchComponent
from app_v2.models.nutrition_search_models import NutritionQueryInput

# APIè¨­å®šï¼ˆæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆï¼‰
BASE_URL = "http://localhost:8000/api/v1"

# ãƒ†ã‚¹ãƒˆç”»åƒã®ãƒ‘ã‚¹
image_path = "test_images/food3.jpg"

async def test_multi_db_elasticsearch_nutrition_search():
    """ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹Elasticsearchæ „é¤Šæ¤œç´¢ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("=== Multi-Database Nutrition Search Test v3.0 - Multi-DB Elasticsearch Edition ===")
    print(f"Using image: {image_path}")
    print("ğŸ” Testing Multi-Database Elasticsearch-powered nutrition search")
    print("ğŸ“Š Each query will return up to 3 results from each of 3 databases (yazio, mynetdiary, eatthismuch)")
    
    try:
        # å®Œå…¨åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—ã¦Phase1çµæœã‚’å–å¾—
        with open(image_path, "rb") as f:
            files = {"image": ("food3.jpg", f, "image/jpeg")}
            data = {"save_results": True}
            
            print("Starting complete analysis to get Phase1 results...")
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/meal-analyses/complete", files=files, data=data)
            end_time = time.time()
        
        print(f"Status Code: {response.status_code}")
        print(f"Response Time: {end_time - start_time:.2f}s")
        
        if response.status_code != 200:
            print("âŒ Failed to get Phase1 results!")
            print(f"Error: {response.text}")
            return False
        
        result = response.json()
        analysis_id = result.get("analysis_id")
        print(f"Analysis ID: {analysis_id}")
        
        # Phase1çµæœã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
        phase1_result = result.get("phase1_result", {})
        dishes = phase1_result.get("dishes", [])
        
        all_queries = []
        dish_names = []
        ingredient_names = []
        
        for dish in dishes:
            dish_name = dish.get("dish_name")
            if dish_name:
                dish_names.append(dish_name)
                all_queries.append(dish_name)
            
            ingredients = dish.get("ingredients", [])
            for ingredient in ingredients:
                ingredient_name = ingredient.get("ingredient_name")
                if ingredient_name:
                    ingredient_names.append(ingredient_name)
                    all_queries.append(ingredient_name)
        
        # é‡è¤‡ã‚’é™¤å»
        all_queries = list(set(all_queries))
        dish_names = list(set(dish_names))
        ingredient_names = list(set(ingredient_names))
        
        print(f"\nğŸ“Š Extracted Search Queries:")
        print(f"- Total dishes: {len(dish_names)}")
        print(f"- Total ingredients: {len(ingredient_names)}")
        print(f"- Total unique queries: {len(all_queries)}")
        
        if len(all_queries) == 0:
            print("âŒ No search queries extracted from Phase1 results!")
            return False
        
        # ElasticsearchNutritionSearchComponentã‚’ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ¼ãƒ‰ã§åˆæœŸåŒ–
        print(f"\nğŸ”§ Initializing ElasticsearchNutritionSearchComponent (Multi-DB Mode)...")
        es_component = ElasticsearchNutritionSearchComponent(
            multi_db_search_mode=True,  # ãƒãƒ«ãƒDBãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
            results_per_db=5  # å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰5ã¤ãšã¤çµæœã‚’å–å¾—
        )
        
        # æ¤œç´¢å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        nutrition_query_input = NutritionQueryInput(
            ingredient_names=ingredient_names,
            dish_names=dish_names,
            preferred_source="elasticsearch"
        )
        
        print(f"ğŸ“ Query Input:")
        print(f"- Ingredient names: {len(ingredient_names)} items")
        print(f"- Dish names: {len(dish_names)} items")
        print(f"- Total search terms: {len(nutrition_query_input.get_all_search_terms())}")
        print(f"- Multi-DB search mode: Enabled")
        print(f"- Results per database: 5")
        print(f"- Target databases: yazio, mynetdiary, eatthismuch")
        
        # Elasticsearch ãƒãƒ«ãƒDBæ¤œç´¢ã‚’å®Ÿè¡Œ
        print(f"\nğŸ” Starting Multi-Database Elasticsearch nutrition search...")
        search_start_time = time.time()
        
        search_results = await es_component.execute(nutrition_query_input)
        
        search_end_time = time.time()
        search_time = search_end_time - search_start_time
        
        print(f"âœ… Multi-DB Elasticsearch search completed in {search_time:.3f}s")
        
        # çµæœã®åˆ†æ
        matches = search_results.matches
        search_summary = search_results.search_summary
        
        print(f"\nğŸ“ˆ Multi-DB Elasticsearch Search Results Summary:")
        print(f"- Total queries: {search_summary.get('total_searches', 0)}")
        print(f"- Successful matches: {search_summary.get('successful_matches', 0)}")
        print(f"- Failed searches: {search_summary.get('failed_searches', 0)}")
        print(f"- Match rate: {search_summary.get('match_rate_percent', 0):.1f}%")
        print(f"- Search time: {search_summary.get('search_time_ms', 0)}ms")
        print(f"- Search method: {search_summary.get('search_method', 'unknown')}")
        print(f"- Database source: {search_summary.get('database_source', 'unknown')}")
        print(f"- Total indexed documents: {search_summary.get('total_indexed_documents', 0)}")
        print(f"- Results per database: {search_summary.get('results_per_db', 0)}")
        print(f"- Target databases: {search_summary.get('target_databases', [])}")
        print(f"- Total results: {search_summary.get('total_results', 0)}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ¥çµ±è¨ˆã®è©³ç´°è¨ˆç®—
        db_detailed_stats = {"yazio": 0, "mynetdiary": 0, "eatthismuch": 0, "unknown": 0}
        source_distribution = {}
        query_results_breakdown = {}
        
        total_individual_results = 0
        for query, match_results in matches.items():
            if isinstance(match_results, list):
                # ãƒãƒ«ãƒDBæ¤œç´¢ã®å ´åˆã€ãƒªã‚¹ãƒˆå½¢å¼ã§çµæœãŒè¿”ã•ã‚Œã‚‹
                query_results_breakdown[query] = len(match_results)
                total_individual_results += len(match_results)
                
                for match in match_results:
                    source = match.source
                    if "elasticsearch_" in source:
                        db_name = source.replace("elasticsearch_", "")
                        if db_name in db_detailed_stats:
                            db_detailed_stats[db_name] += 1
                        else:
                            db_detailed_stats["unknown"] += 1
                    
                    if source not in source_distribution:
                        source_distribution[source] = 0
                    source_distribution[source] += 1
            else:
                # å˜ä¸€çµæœã®å ´åˆï¼ˆå¾“æ¥å½¢å¼ï¼‰
                query_results_breakdown[query] = 1
                total_individual_results += 1
                source = match_results.source
                if source not in source_distribution:
                    source_distribution[source] = 0
                source_distribution[source] += 1
        
        print(f"\nğŸ“Š Detailed Database Source Distribution:")
        for source, count in source_distribution.items():
            percentage = (count / total_individual_results) * 100 if total_individual_results > 0 else 0
            print(f"- {source}: {count} results ({percentage:.1f}%)")
        
        print(f"\nğŸ“‹ Per-Query Results Breakdown:")
        for query, result_count in query_results_breakdown.items():
            query_type = "dish" if query in dish_names else "ingredient"
            print(f"- '{query}' ({query_type}): {result_count} results")
        
        print(f"\nğŸ” Top Multi-DB Match Results (showing first 5 queries):")
        for i, (query, match_results) in enumerate(list(matches.items())[:5], 1):
            query_type = "dish" if query in dish_names else "ingredient"
            print(f"\n{i:2d}. Query: '{query}' ({query_type})")
            
            if isinstance(match_results, list):
                print(f"    Found {len(match_results)} results from multiple databases:")
                for j, match in enumerate(match_results, 1):
                    print(f"    {j}. {match.search_name} (score: {match.score:.3f})")
                    print(f"       Source: {match.source}")
                    print(f"       Data type: {match.data_type}")
                    
                    nutrition = match.nutrition
                    if nutrition:
                        calories = nutrition.get('calories', 0)
                        protein = nutrition.get('protein', 0)
                        fat = nutrition.get('fat', 0)
                        carbs = nutrition.get('carbs', nutrition.get('carbohydrates', 0))
                        print(f"       Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g")
            else:
                # å˜ä¸€çµæœã®å ´åˆ
                print(f"    Single result: {match_results.search_name} (score: {match_results.score:.3f})")
                print(f"    Source: {match_results.source}")
        
        if len(matches) > 5:
            print(f"\n    ... and {len(matches) - 5} more queries with results")
        
        # è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼ã®è¡¨ç¤º
        if search_results.warnings:
            print(f"\nâš ï¸  Warnings:")
            for warning in search_results.warnings:
                print(f"   - {warning}")
        
        if search_results.errors:
            print(f"\nâŒ Errors:")
            for error in search_results.errors:
                print(f"   - {error}")
        
        # è©³ç´°çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        await save_multi_db_elasticsearch_results(analysis_id, search_results, all_queries, dish_names, ingredient_names)
        
        return True
        
    except Exception as e:
        print(f"âŒ Error during Multi-DB Elasticsearch nutrition search: {e}")
        import traceback
        traceback.print_exc()
        return False

async def save_multi_db_elasticsearch_results(analysis_id: str, search_results, all_queries: List[str], dish_names: List[str], ingredient_names: List[str]):
    """ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹Elasticsearchæ¤œç´¢çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"analysis_results/multi_db_elasticsearch_search_{analysis_id}_{timestamp}"
    os.makedirs(results_dir, exist_ok=True)
    
    # æ¤œç´¢çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
    matches_dict = {}
    for query, match_results in search_results.matches.items():
        if isinstance(match_results, list):
            # ãƒãƒ«ãƒDBçµæœã®å ´åˆ
            matches_dict[query] = []
            for match in match_results:
                matches_dict[query].append({
                    "id": match.id,
                    "search_name": match.search_name,
                    "description": match.description,
                    "data_type": match.data_type,
                    "source": match.source,
                    "nutrition": match.nutrition,
                    "weight": match.weight,
                    "score": match.score,
                    "search_metadata": match.search_metadata
                })
        else:
            # å˜ä¸€çµæœã®å ´åˆ
            matches_dict[query] = {
                "id": match_results.id,
                "search_name": match_results.search_name,
                "description": match_results.description,
                "data_type": match_results.data_type,
                "source": match_results.source,
                "nutrition": match_results.nutrition,
                "weight": match_results.weight,
                "score": match_results.score,
                "search_metadata": match_results.search_metadata
            }
    
    # 1. å…¨æ¤œç´¢çµæœã‚’JSONã§ä¿å­˜
    results_file = os.path.join(results_dir, "multi_db_elasticsearch_search_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "analysis_id": analysis_id,
            "timestamp": timestamp,
            "search_method": "elasticsearch_multi_db",
            "input_queries": {
                "all_queries": all_queries,
                "dish_names": dish_names,
                "ingredient_names": ingredient_names
            },
            "search_summary": search_results.search_summary,
            "matches": matches_dict,
            "warnings": search_results.warnings,
            "errors": search_results.errors
        }, f, indent=2, ensure_ascii=False)
    
    # 2. æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§ä¿å­˜
    summary_file = os.path.join(results_dir, "multi_db_elasticsearch_summary.md")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"# Multi-Database Elasticsearch Nutrition Search Results\n\n")
        f.write(f"**Analysis ID:** {analysis_id}\n")
        f.write(f"**Timestamp:** {timestamp}\n")
        f.write(f"**Search Method:** Multi-Database Elasticsearch\n")
        f.write(f"**Total Queries:** {len(all_queries)}\n")
        f.write(f"**Results per Database:** 5\n")
        f.write(f"**Target Databases:** yazio, mynetdiary, eatthismuch\n\n")
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼
        summary = search_results.search_summary
        f.write(f"## Search Summary\n\n")
        f.write(f"- **Total searches:** {summary.get('total_searches', 0)}\n")
        f.write(f"- **Successful matches:** {summary.get('successful_matches', 0)}\n")
        f.write(f"- **Failed searches:** {summary.get('failed_searches', 0)}\n")
        f.write(f"- **Match rate:** {summary.get('match_rate_percent', 0):.1f}%\n")
        f.write(f"- **Search time:** {summary.get('search_time_ms', 0)}ms\n")
        f.write(f"- **Database source:** {summary.get('database_source', 'unknown')}\n")
        f.write(f"- **Total indexed documents:** {summary.get('total_indexed_documents', 0)}\n")
        f.write(f"- **Results per database:** {summary.get('results_per_db', 0)}\n")
        f.write(f"- **Target databases:** {', '.join(summary.get('target_databases', []))}\n")
        f.write(f"- **Total results:** {summary.get('total_results', 0)}\n\n")
        
        # ã‚½ãƒ¼ã‚¹åˆ†å¸ƒ
        source_distribution = {}
        total_individual_results = 0
        for match_results in search_results.matches.values():
            if isinstance(match_results, list):
                total_individual_results += len(match_results)
                for match in match_results:
                    source = match.source
                    if source not in source_distribution:
                        source_distribution[source] = 0
                    source_distribution[source] += 1
            else:
                total_individual_results += 1
                source = match_results.source
                if source not in source_distribution:
                    source_distribution[source] = 0
                source_distribution[source] += 1
        
        f.write(f"## Source Database Distribution\n\n")
        for source, count in source_distribution.items():
            percentage = (count / total_individual_results) * 100 if total_individual_results > 0 else 0
            f.write(f"- **{source}:** {count} results ({percentage:.1f}%)\n")
        f.write(f"\n")
        
        # è©³ç´°çµæœ
        f.write(f"## Multi-DB Match Results Detail\n\n")
        for i, (query, match_results) in enumerate(search_results.matches.items(), 1):
            query_type = "dish" if query in dish_names else "ingredient"
            f.write(f"### {i}. {query} ({query_type})\n\n")
            
            if isinstance(match_results, list):
                f.write(f"**Found {len(match_results)} results from multiple databases:**\n\n")
                for j, match in enumerate(match_results, 1):
                    f.write(f"#### Result {j}\n")
                    f.write(f"- **Match:** {match.search_name}\n")
                    f.write(f"- **Score:** {match.score:.3f}\n")
                    f.write(f"- **Source:** {match.source}\n")
                    f.write(f"- **Data Type:** {match.data_type}\n")
                    
                    if match.nutrition:
                        nutrition = match.nutrition
                        calories = nutrition.get('calories', 0)
                        protein = nutrition.get('protein', 0)
                        fat = nutrition.get('fat', 0)
                        carbs = nutrition.get('carbs', nutrition.get('carbohydrates', 0))
                        f.write(f"- **Nutrition (100g):** {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g\n")
                    
                    if match.description:
                        f.write(f"- **Description:** {match.description}\n")
                    
                    f.write(f"\n")
            else:
                # å˜ä¸€çµæœã®å ´åˆ
                f.write(f"**Single result:**\n")
                f.write(f"- **Match:** {match_results.search_name}\n")
                f.write(f"- **Score:** {match_results.score:.3f}\n")
                f.write(f"- **Source:** {match_results.source}\n")
                f.write(f"- **Data Type:** {match_results.data_type}\n")
                
                if match_results.nutrition:
                    nutrition = match_results.nutrition
                    calories = nutrition.get('calories', 0)
                    protein = nutrition.get('protein', 0)
                    fat = nutrition.get('fat', 0)
                    carbs = nutrition.get('carbs', nutrition.get('carbohydrates', 0))
                    f.write(f"- **Nutrition (100g):** {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g\n")
                
                if match_results.description:
                    f.write(f"- **Description:** {match_results.description}\n")
                
                f.write(f"\n")
        
        # è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼
        if search_results.warnings:
            f.write(f"## Warnings\n\n")
            for warning in search_results.warnings:
                f.write(f"- {warning}\n")
            f.write(f"\n")
        
        if search_results.errors:
            f.write(f"## Errors\n\n")
            for error in search_results.errors:
                f.write(f"- {error}\n")
            f.write(f"\n")
    
    print(f"\nğŸ’¾ Multi-DB Elasticsearch results saved to:")
    print(f"   ğŸ“ {results_dir}/")
    print(f"   ğŸ“„ multi_db_elasticsearch_search_results.json")
    print(f"   ğŸ“„ multi_db_elasticsearch_summary.md")

if __name__ == "__main__":
    print("ğŸš€ Starting Multi-Database Elasticsearch Nutrition Search Test")
    success = asyncio.run(test_multi_db_elasticsearch_nutrition_search())
    
    if success:
        print("\nâœ… Multi-Database Elasticsearch nutrition search test completed successfully!")
        print("ğŸ¯ Each query returned up to 3 results from each of 3 databases (yazio, mynetdiary, eatthismuch)")
    else:
        print("\nâŒ Multi-Database Elasticsearch nutrition search test failed!") 
```

============================================================

ğŸ“ FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤ (app_v2)
============================================================

ğŸ“„ FILE: app_v2/main/app.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,030 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:55:53
å­˜åœ¨: âœ…

CONTENT:
```python
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ..api.v1.endpoints import meal_analysis
from ..config import get_settings

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆæ—¢å­˜ã®appã¨åŒã˜ï¼‰
os.environ.setdefault("USDA_API_KEY", "vSWtKJ3jYD0Cn9LRyVJUFkuyCt9p8rEtVXz74PZg")
os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", "/Users/odasoya/meal_analysis_api /service-account-key.json")
os.environ.setdefault("GEMINI_PROJECT_ID", "recording-diet-ai-3e7cf")
os.environ.setdefault("GEMINI_LOCATION", "us-central1")
os.environ.setdefault("GEMINI_MODEL_NAME", "gemini-2.5-flash-preview-05-20")

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# FastAPIã‚¢ãƒ—ãƒªã®ä½œæˆ
app = FastAPI(
    title="é£Ÿäº‹åˆ†æ API v2.0",
    description="ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ã•ã‚ŒãŸé£Ÿäº‹åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ç™»éŒ²
app.include_router(
    meal_analysis.router,
    prefix="/api/v1/meal-analyses",
    tags=["Complete Meal Analysis v2.0"]
)

# ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "é£Ÿäº‹åˆ†æ API v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ",
        "version": "2.0.0",
        "architecture": "Component-based Pipeline",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "version": "v2.0",
        "components": ["Phase1Component", "USDAQueryComponent"]
    }

if __name__ == "__main__":
    import uvicorn
    settings = get_settings()
    uvicorn.run(
        "app_v2.main.app:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=True
    ) 
```

============================================================

ğŸ“„ FILE: app_v2/api/v1/endpoints/meal_analysis.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,696 bytes
æœ€çµ‚æ›´æ–°: 2025-06-09 11:27:28
å­˜åœ¨: âœ…

CONTENT:
```python
from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from typing import Optional
import logging

from ....pipeline import MealAnalysisPipeline

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/complete")
async def complete_meal_analysis(
    image: UploadFile = File(...),
    save_results: bool = Form(True),
    save_detailed_logs: bool = Form(True)
):
    """
    å®Œå…¨ãªé£Ÿäº‹åˆ†æã‚’å®Ÿè¡Œï¼ˆv2.0 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆï¼‰
    
    - Phase 1: Gemini AIã«ã‚ˆã‚‹ç”»åƒåˆ†æ
    - USDA Query: é£Ÿæã®USDAãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ
    - Phase 2: è¨ˆç®—æˆ¦ç•¥æ±ºå®šã¨æ „é¤Šä¾¡ç²¾ç·»åŒ– (TODO)
    - Nutrition Calculation: æœ€çµ‚æ „é¤Šä¾¡è¨ˆç®— (TODO)
    
    Args:
        image: åˆ†æå¯¾è±¡ã®é£Ÿäº‹ç”»åƒ
        save_results: çµæœã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
        save_detailed_logs: è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
    
    Returns:
        å®Œå…¨ãªåˆ†æçµæœã¨æ „é¤Šä¾¡è¨ˆç®—ã€è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    
    try:
        # ç”»åƒã®æ¤œè¨¼
        if not image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”»åƒã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        image_data = await image.read()
        logger.info(f"Starting complete meal analysis pipeline v2.0 (detailed_logs: {save_detailed_logs})")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
        pipeline = MealAnalysisPipeline()
        result = await pipeline.execute_complete_analysis(
            image_bytes=image_data,
            image_mime_type=image.content_type,
            save_results=save_results,
            save_detailed_logs=save_detailed_logs
        )
        
        logger.info(f"Complete analysis pipeline v2.0 finished successfully")
        
        return JSONResponse(
            status_code=200,
            content=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Complete analysis failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Complete analysis failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "healthy", "version": "v2.0", "message": "é£Ÿäº‹åˆ†æAPI v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ"}


@router.get("/pipeline-info")
async def get_pipeline_info():
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã®å–å¾—"""
    pipeline = MealAnalysisPipeline()
    return pipeline.get_pipeline_info() 
```

============================================================

ğŸ“ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ¶å±¤
============================================================

ğŸ“„ FILE: app_v2/pipeline/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 142 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:08:07
å­˜åœ¨: âœ…

CONTENT:
```python
from .orchestrator import MealAnalysisPipeline
from .result_manager import ResultManager

__all__ = ["MealAnalysisPipeline", "ResultManager"] 
```

============================================================

ğŸ“„ FILE: app_v2/pipeline/orchestrator.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 14,879 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 14:54:56
å­˜åœ¨: âœ…

CONTENT:
```python
import uuid
import json
from datetime import datetime
from typing import Optional, Dict, Any
import logging

from ..components import Phase1Component, USDAQueryComponent, LocalNutritionSearchComponent, ElasticsearchNutritionSearchComponent
from ..models import (
    Phase1Input, Phase1Output,
    USDAQueryInput, USDAQueryOutput,
    NutritionQueryInput
)
from ..config import get_settings
from .result_manager import ResultManager

logger = logging.getLogger(__name__)


class MealAnalysisPipeline:
    """
    é£Ÿäº‹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
    
    4ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã‚’çµ±åˆã—ã¦å®Œå…¨ãªåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    
    def __init__(self, use_local_nutrition_search: Optional[bool] = None, use_elasticsearch_search: Optional[bool] = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            use_local_nutrition_search: ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ï¼‰
            use_elasticsearch_search: Elasticsearchæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
                                    None: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾—
                                    True: ElasticsearchNutritionSearchComponentä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
                                    False: å¾“æ¥ã®USDAQueryComponentä½¿ç”¨
        """
        self.pipeline_id = str(uuid.uuid4())[:8]
        self.settings = get_settings()
        
        # Elasticsearchæ¤œç´¢å„ªå…ˆåº¦ã®æ±ºå®š
        if use_elasticsearch_search is not None:
            self.use_elasticsearch_search = use_elasticsearch_search
        elif hasattr(self.settings, 'USE_ELASTICSEARCH_SEARCH'):
            self.use_elasticsearch_search = self.settings.USE_ELASTICSEARCH_SEARCH
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Elasticsearchä½¿ç”¨
            self.use_elasticsearch_search = True
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®å‡¦ç†
        if use_local_nutrition_search is not None and use_elasticsearch_search is None:
            # æ—§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆ
            if use_local_nutrition_search:
                self.use_elasticsearch_search = False
                self.use_local_nutrition_search = True
            else:
                self.use_elasticsearch_search = False
                self.use_local_nutrition_search = False
        else:
            self.use_local_nutrition_search = not self.use_elasticsearch_search and (
                use_local_nutrition_search or getattr(self.settings, 'USE_LOCAL_NUTRITION_SEARCH', False)
            )
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.phase1_component = Phase1Component()
        
        # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é¸æŠ
        if self.use_elasticsearch_search:
            self.nutrition_search_component = ElasticsearchNutritionSearchComponent(
                multi_db_search_mode=True,
                results_per_db=5
            )
            self.search_component_name = "ElasticsearchNutritionSearchComponent"
            logger.info("Using Elasticsearch nutrition database search (high-performance, multi-DB mode)")
        elif self.use_local_nutrition_search:
            self.nutrition_search_component = LocalNutritionSearchComponent()
            self.search_component_name = "LocalNutritionSearchComponent"
            logger.info("Using local nutrition database search (nutrition_db_experiment)")
        else:
            self.nutrition_search_component = USDAQueryComponent()
            self.search_component_name = "USDAQueryComponent"
            logger.info("Using traditional USDA API search")
            
        # TODO: Phase2Componentã¨NutritionCalculationComponentã‚’è¿½åŠ 
        
        self.logger = logging.getLogger(f"{__name__}.{self.pipeline_id}")
        
    async def execute_complete_analysis(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        optional_text: Optional[str] = None,
        save_results: bool = True,
        save_detailed_logs: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œå…¨ãªé£Ÿäº‹åˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            image_bytes: ç”»åƒãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆ
            save_results: çµæœã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            save_detailed_logs: è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            å®Œå…¨ãªåˆ†æçµæœ
        """
        analysis_id = str(uuid.uuid4())[:8]
        start_time = datetime.now()
        
        # ResultManagerã®åˆæœŸåŒ–
        result_manager = ResultManager(analysis_id) if save_detailed_logs else None
        
        self.logger.info(f"[{analysis_id}] Starting complete meal analysis pipeline")
        if self.use_elasticsearch_search:
            self.logger.info(f"[{analysis_id}] Nutrition search method: Elasticsearch (high-performance)")
        elif self.use_local_nutrition_search:
            self.logger.info(f"[{analysis_id}] Nutrition search method: Local Database")
        else:
            self.logger.info(f"[{analysis_id}] Nutrition search method: USDA API")
        
        try:
            # === Phase 1: ç”»åƒåˆ†æ ===
            self.logger.info(f"[{analysis_id}] Phase 1: Image analysis")
            
            phase1_input = Phase1Input(
                image_bytes=image_bytes,
                image_mime_type=image_mime_type,
                optional_text=optional_text
            )
            
            # Phase1ã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            phase1_log = result_manager.create_execution_log("Phase1Component", f"{analysis_id}_phase1") if result_manager else None
            
            phase1_result = await self.phase1_component.execute(phase1_input, phase1_log)
            
            self.logger.info(f"[{analysis_id}] Phase 1 completed - Detected {len(phase1_result.dishes)} dishes")
            
            # === Nutrition Search Phase: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ ===
            if self.use_elasticsearch_search:
                search_phase_name = "Elasticsearch Search"
            elif self.use_local_nutrition_search:
                search_phase_name = "Local Nutrition Search"
            else:
                search_phase_name = "USDA Query"
                
            self.logger.info(f"[{analysis_id}] {search_phase_name} Phase: Database matching")
            
            # === çµ±ä¸€ã•ã‚ŒãŸæ „é¤Šæ¤œç´¢å…¥åŠ›ã‚’ä½œæˆ ===
            if self.use_elasticsearch_search or self.use_local_nutrition_search:
                # Elasticsearchæ¤œç´¢ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®å ´åˆã¯NutritionQueryInputã‚’ä½¿ç”¨
                preferred_source = "elasticsearch" if self.use_elasticsearch_search else "local_database"
                nutrition_search_input = NutritionQueryInput(
                    ingredient_names=phase1_result.get_all_ingredient_names(),
                    dish_names=phase1_result.get_all_dish_names(),
                    preferred_source=preferred_source
                )
            else:
                # USDAæ¤œç´¢ã®å ´åˆã¯USDAQueryInputã‚’ä½¿ç”¨ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ï¼‰
                nutrition_search_input = USDAQueryInput(
                    ingredient_names=phase1_result.get_all_ingredient_names(),
                    dish_names=phase1_result.get_all_dish_names()
                )
            
            # Nutrition Searchã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            search_log = result_manager.create_execution_log(self.search_component_name, f"{analysis_id}_nutrition_search") if result_manager else None
            
            nutrition_search_result = await self.nutrition_search_component.execute(nutrition_search_input, search_log)
            
            self.logger.info(f"[{analysis_id}] {search_phase_name} completed - {nutrition_search_result.get_match_rate():.1%} match rate")
            
            # === æš«å®šçš„ãªçµæœã®æ§‹ç¯‰ (Phase2ã¨Nutritionã¯å¾Œã§è¿½åŠ ) ===
            
            # Phase1ã®çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆæ¤œç´¢ç‰¹åŒ–ï¼‰
            phase1_dict = {
                "dishes": [
                    {
                        "dish_name": dish.dish_name,
                        "ingredients": [
                            {
                                "ingredient_name": ing.ingredient_name
                            }
                            for ing in dish.ingredients
                        ]
                    }
                    for dish in phase1_result.dishes
                ]
            }
            
            # ç°¡å˜ãªæ „é¤Šè¨ˆç®—ï¼ˆæš«å®šï¼‰
            total_calories = sum(
                len(dish.ingredients) * 50  # ä»®ã®è¨ˆç®—
                for dish in phase1_result.dishes
            )
            
            # æ¤œç´¢æ–¹æ³•ã®ç‰¹å®š
            if self.use_elasticsearch_search:
                search_method = "elasticsearch"
                search_api_method = "elasticsearch"
            elif self.use_local_nutrition_search:
                search_method = "local_nutrition_database"
                search_api_method = "local_database"
            else:
                search_method = "usda_api"
                search_api_method = "usda_api"
            
            # å®Œå…¨åˆ†æçµæœã®æ§‹ç¯‰
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            complete_result = {
                "analysis_id": analysis_id,
                "phase1_result": phase1_dict,
                "nutrition_search_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary,
                    "search_method": search_method
                },
                # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®ãŸã‚ã€usdaã‚­ãƒ¼ã‚‚æ®‹ã™
                "usda_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary
                },
                "processing_summary": {
                    "total_dishes": len(phase1_result.dishes),
                    "total_ingredients": len(phase1_result.get_all_ingredient_names()),
                    "nutrition_search_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",
                    "usda_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",  # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§
                    "total_calories": total_calories,
                    "pipeline_status": "completed",
                    "processing_time_seconds": processing_time,
                    "search_method": search_method
                },
                # æš«å®šçš„ãªæœ€çµ‚çµæœ
                "final_nutrition_result": {
                    "dishes": phase1_dict["dishes"],
                    "total_meal_nutrients": {
                        "calories_kcal": total_calories,
                        "protein_g": total_calories * 0.15,  # ä»®ã®å€¤
                        "carbohydrates_g": total_calories * 0.55,  # ä»®ã®å€¤
                        "fat_g": total_calories * 0.30,  # ä»®ã®å€¤
                    }
                },
                "metadata": {
                    "pipeline_version": "v2.0",
                    "timestamp": datetime.now().isoformat(),
                    "components_used": ["Phase1Component", self.search_component_name],
                    "nutrition_search_method": search_api_method
                }
            }
            
            # ResultManagerã«æœ€çµ‚çµæœã‚’è¨­å®š
            if result_manager:
                result_manager.set_final_result(complete_result)
                result_manager.finalize_pipeline()
            
            # çµæœã®ä¿å­˜
            saved_files = {}
            if save_detailed_logs and result_manager:
                # æ–°ã—ã„ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ä¿å­˜æ–¹å¼
                saved_files = result_manager.save_phase_results()
                complete_result["analysis_folder"] = result_manager.get_analysis_folder_path()
                complete_result["saved_files"] = saved_files
                
                logger.info(f"[{analysis_id}] Detailed logs saved to folder: {result_manager.get_analysis_folder_path()}")
                logger.info(f"[{analysis_id}] Saved {len(saved_files)} files across all phases")
            
            if save_results:
                # é€šå¸¸ã®çµæœä¿å­˜ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
                saved_file = f"analysis_results/meal_analysis_{analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                complete_result["legacy_saved_to"] = saved_file
            
            self.logger.info(f"[{analysis_id}] Complete analysis pipeline finished successfully in {processing_time:.2f}s")
            
            return complete_result
            
        except Exception as e:
            self.logger.error(f"[{analysis_id}] Complete analysis failed: {str(e)}", exc_info=True)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ResultManagerã‚’ä¿å­˜
            if result_manager:
                result_manager.set_final_result({"error": str(e), "timestamp": datetime.now().isoformat()})
                result_manager.finalize_pipeline()
                error_saved_files = result_manager.save_phase_results()
                self.logger.info(f"[{analysis_id}] Error analysis logs saved to folder: {result_manager.get_analysis_folder_path()}")
            
            raise
    
    def get_pipeline_info(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’å–å¾—"""
        if self.use_elasticsearch_search:
            search_method = "elasticsearch"
        elif self.use_local_nutrition_search:
            search_method = "local_database"
        else:
            search_method = "usda_api"
            
        return {
            "pipeline_id": self.pipeline_id,
            "version": "v2.0",
            "nutrition_search_method": search_method,
            "components": [
                {
                    "component_name": "Phase1Component",
                    "component_type": "analysis",
                    "execution_count": 0
                },
                {
                    "component_name": self.search_component_name,
                    "component_type": "nutrition_search",
                    "execution_count": 0
                }
            ]
        } 
```

============================================================

ğŸ“„ FILE: app_v2/pipeline/result_manager.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 36,336 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 14:06:44
å­˜åœ¨: âœ…

CONTENT:
```python
import json
import os
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class DetailedExecutionLog:
    """å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è©³ç´°å®Ÿè¡Œãƒ­ã‚°"""
    
    def __init__(self, component_name: str, execution_id: str):
        self.component_name = component_name
        self.execution_id = execution_id
        self.execution_start_time = datetime.now()
        self.execution_end_time = None
        self.input_data = {}
        self.output_data = {}
        self.processing_details = {}
        self.prompts_used = {}
        self.reasoning = {}
        self.confidence_scores = {}
        self.warnings = []
        self.errors = []
        
    def set_input(self, input_data: Dict[str, Any]):
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ï¼ˆæ©Ÿå¯†æƒ…å ±ã¯é™¤å¤–ï¼‰"""
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯å¤§ãã™ãã‚‹ã®ã§ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜
        safe_input = {}
        for key, value in input_data.items():
            if key == 'image_bytes':
                safe_input[key] = {
                    "size_bytes": len(value) if value else 0,
                    "type": "binary_image_data"
                }
            else:
                safe_input[key] = value
        self.input_data = safe_input
    
    def set_output(self, output_data: Dict[str, Any]):
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        self.output_data = output_data
        
    def add_prompt(self, prompt_name: str, prompt_content: str, variables: Dict[str, Any] = None):
        """ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨˜éŒ²"""
        self.prompts_used[prompt_name] = {
            "content": prompt_content,
            "variables": variables or {},
            "timestamp": datetime.now().isoformat()
        }
    
    def add_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æ¨è«–ç†ç”±ã‚’è¨˜éŒ²"""
        self.reasoning[decision_point] = {
            "reason": reason,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat()
        }
    
    def add_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’è¨˜éŒ²"""
        self.processing_details[detail_key] = detail_value
    
    def add_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²"""
        self.confidence_scores[metric_name] = score
    
    def add_warning(self, warning: str):
        """è­¦å‘Šã‚’è¨˜éŒ²"""
        self.warnings.append({
            "message": warning,
            "timestamp": datetime.now().isoformat()
        })
    
    def add_error(self, error: str):
        """ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²"""
        self.errors.append({
            "message": error,
            "timestamp": datetime.now().isoformat()
        })
    
    def finalize(self):
        """å®Ÿè¡Œå®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.execution_end_time = datetime.now()
    
    def get_execution_time(self) -> float:
        """å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆç§’ï¼‰"""
        if self.execution_end_time:
            return (self.execution_end_time - self.execution_start_time).total_seconds()
        return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "component_name": self.component_name,
            "execution_id": self.execution_id,
            "execution_start_time": self.execution_start_time.isoformat(),
            "execution_end_time": self.execution_end_time.isoformat() if self.execution_end_time else None,
            "execution_time_seconds": self.get_execution_time(),
            "input_data": self.input_data,
            "output_data": self.output_data,
            "processing_details": self.processing_details,
            "prompts_used": self.prompts_used,
            "reasoning": self.reasoning,
            "confidence_scores": self.confidence_scores,
            "warnings": self.warnings,
            "errors": self.errors
        }


class ResultManager:
    """è§£æçµæœã¨è©³ç´°ãƒ­ã‚°ã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆãƒ•ã‚§ãƒ¼ã‚ºåˆ¥æ•´ç†ç‰ˆï¼‰"""
    
    def __init__(self, analysis_id: str, save_directory: str = "analysis_results"):
        self.analysis_id = analysis_id
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Ÿè¡Œã”ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.analysis_folder_name = f"analysis_{self.timestamp}_{self.analysis_id}"
        self.analysis_dir = Path(save_directory) / self.analysis_folder_name
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.phase1_dir = self.analysis_dir / "phase1"
        self.nutrition_search_dir = self.analysis_dir / "nutrition_search_query"
        self.phase2_dir = self.analysis_dir / "phase2"
        self.nutrition_dir = self.analysis_dir / "nutrition_calculation"
        
        for phase_dir in [self.phase1_dir, self.nutrition_search_dir, self.phase2_dir, self.nutrition_dir]:
            phase_dir.mkdir(exist_ok=True)
        
        self.pipeline_start_time = datetime.now()
        self.pipeline_end_time = None
        self.execution_logs: List[DetailedExecutionLog] = []
        self.final_result = {}
        self.pipeline_metadata = {
            "analysis_id": analysis_id,
            "version": "v2.0",
            "analysis_folder": self.analysis_folder_name,
            "pipeline_start_time": self.pipeline_start_time.isoformat()
        }
        
    def create_execution_log(self, component_name: str, execution_id: str) -> DetailedExecutionLog:
        """æ–°ã—ã„å®Ÿè¡Œãƒ­ã‚°ã‚’ä½œæˆ"""
        log = DetailedExecutionLog(component_name, execution_id)
        self.execution_logs.append(log)
        return log
    
    def set_final_result(self, result: Dict[str, Any]):
        """æœ€çµ‚çµæœã‚’è¨­å®š"""
        self.final_result = result
        
    def finalize_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.pipeline_end_time = datetime.now()
        self.pipeline_metadata["pipeline_end_time"] = self.pipeline_end_time.isoformat()
        self.pipeline_metadata["total_execution_time_seconds"] = (
            self.pipeline_end_time - self.pipeline_start_time
        ).total_seconds()
    
    def save_phase_results(self) -> Dict[str, str]:
        """ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã«çµæœã‚’ä¿å­˜"""
        saved_files = {}
        
        # å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ã‚°ã‚’å‡¦ç†
        executed_components = set()
        for log in self.execution_logs:
            if log.component_name == "Phase1Component":
                files = self._save_phase1_results(log)
                saved_files.update(files)
                executed_components.add("Phase1Component")
            elif log.component_name in ["USDAQueryComponent", "LocalNutritionSearchComponent", "ElasticsearchNutritionSearchComponent"]:
                files = self._save_nutrition_search_results(log)
                saved_files.update(files)
                executed_components.add(log.component_name)
            elif log.component_name == "Phase2Component":
                files = self._save_phase2_results(log)
                saved_files.update(files)
                executed_components.add("Phase2Component")
            elif log.component_name == "NutritionCalculationComponent":
                files = self._save_nutrition_results(log)
                saved_files.update(files)
                executed_components.add("NutritionCalculationComponent")
        
        # æœªå®Ÿè£…/æœªå®Ÿè¡Œã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        if "Phase2Component" not in executed_components:
            placeholder_log = DetailedExecutionLog("Phase2Component", f"{self.analysis_id}_phase2_placeholder")
            placeholder_log.input_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.output_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.finalize()
            files = self._save_phase2_results(placeholder_log)
            saved_files.update(files)
        
        if "NutritionCalculationComponent" not in executed_components:
            placeholder_log = DetailedExecutionLog("NutritionCalculationComponent", f"{self.analysis_id}_nutrition_placeholder")
            placeholder_log.input_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.output_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.finalize()
            files = self._save_nutrition_results(placeholder_log)
            saved_files.update(files)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
        summary_files = self._save_pipeline_summary()
        saved_files.update(summary_files)
        
        return saved_files
    
    def _save_phase1_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase1ã®çµæœã‚’ä¿å­˜"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase1_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time()
            }, f, indent=2, ensure_ascii=False)
        files["phase1_input_output"] = str(input_output_file)
        
        # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–ç†ç”±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        prompts_md_file = self.phase1_dir / "prompts_and_reasoning.md"
        prompts_content = self._generate_phase1_prompts_md(log)
        with open(prompts_md_file, 'w', encoding='utf-8') as f:
            f.write(prompts_content)
        files["phase1_prompts_md"] = str(prompts_md_file)
        
        # 3. æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆ
        detected_items_file = self.phase1_dir / "detected_items.txt"
        detected_content = self._generate_phase1_detected_items_txt(log)
        with open(detected_items_file, 'w', encoding='utf-8') as f:
            f.write(detected_content)
        files["phase1_detected_txt"] = str(detected_items_file)
        
        return files
    
    def _save_nutrition_search_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµæœã‚’ä¿å­˜ï¼ˆUSDAQueryComponentã€LocalNutritionSearchComponentã€ElasticsearchNutritionSearchComponentå¯¾å¿œï¼‰"""
        files = {}
        
        # æ¤œç´¢æ–¹æ³•ã®åˆ¤å®š
        search_method = "unknown"
        db_source = "unknown"
        
        if log.component_name == "USDAQueryComponent":
            search_method = "usda_api"
            db_source = "usda_database"
        elif log.component_name == "LocalNutritionSearchComponent":
            search_method = "local_search"
            db_source = "local_nutrition_database"
        elif log.component_name == "ElasticsearchNutritionSearchComponent":
            search_method = "elasticsearch"
            db_source = "elasticsearch_nutrition_db"
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢æ–¹æ³•æƒ…å ±ã‚’å«ã‚€ï¼‰
        input_output_file = self.nutrition_search_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "search_metadata": {
                    "component_name": log.component_name,
                    "search_method": search_method,
                    "database_source": db_source,
                    "timestamp": log.execution_end_time.isoformat() if log.execution_end_time else None
                }
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_search_input_output"] = str(input_output_file)
        
        # 2. æ¤œç´¢çµæœã®è©³ç´°ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        search_results_md_file = self.nutrition_search_dir / "search_results.md"
        search_content = self._generate_nutrition_search_results_md(log, search_method, db_source)
        with open(search_results_md_file, 'w', encoding='utf-8') as f:
            f.write(search_content)
        files["nutrition_search_results_md"] = str(search_results_md_file)
        
        # 3. ãƒãƒƒãƒè©³ç´°ã®ãƒ†ã‚­ã‚¹ãƒˆ
        match_details_file = self.nutrition_search_dir / "match_details.txt"
        match_content = self._generate_nutrition_match_details_txt(log, search_method, db_source)
        with open(match_details_file, 'w', encoding='utf-8') as f:
            f.write(match_content)
        files["nutrition_search_match_details"] = str(match_details_file)
        
        return files
    
    def _save_phase2_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase2ã®çµæœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase2_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "Phase2Component is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["phase2_input_output"] = str(input_output_file)
        
        # 2. æˆ¦ç•¥æ±ºå®šã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        strategy_md_file = self.phase2_dir / "strategy_decisions.md"
        with open(strategy_md_file, 'w', encoding='utf-8') as f:
            f.write("# Phase2 Strategy Decisions\n\n*Phase2Component is not yet implemented*\n")
        files["phase2_strategy_md"] = str(strategy_md_file)
        
        # 3. é¸æŠé …ç›®ã®ãƒ†ã‚­ã‚¹ãƒˆ
        selected_items_file = self.phase2_dir / "selected_items.txt"
        with open(selected_items_file, 'w', encoding='utf-8') as f:
            f.write("Phase2Component is not yet implemented\n")
        files["phase2_items_txt"] = str(selected_items_file)
        
        return files
    
    def _save_nutrition_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šè¨ˆç®—ã®çµæœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.nutrition_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "NutritionCalculationComponent is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_input_output"] = str(input_output_file)
        
        # 2. è¨ˆç®—å¼ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        formulas_md_file = self.nutrition_dir / "calculation_formulas.md"
        with open(formulas_md_file, 'w', encoding='utf-8') as f:
            f.write("# Nutrition Calculation Formulas\n\n*NutritionCalculationComponent is not yet implemented*\n")
        files["nutrition_formulas_md"] = str(formulas_md_file)
        
        # 3. æ „é¤Šã‚µãƒãƒªãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆ
        summary_txt_file = self.nutrition_dir / "nutrition_summary.txt"
        with open(summary_txt_file, 'w', encoding='utf-8') as f:
            f.write("NutritionCalculationComponent is not yet implemented\n")
        files["nutrition_summary_txt"] = str(summary_txt_file)
        
        return files
    
    def _save_pipeline_summary(self) -> Dict[str, str]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜"""
        files = {}
        
        # 1. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚µãƒãƒªãƒ¼JSON
        summary_file = self.analysis_dir / "pipeline_summary.json"
        summary_data = {
            "analysis_id": self.analysis_id,
            "timestamp": self.timestamp,
            "pipeline_metadata": self.pipeline_metadata,
            "execution_summary": {
                log.component_name: {
                    "execution_time": log.get_execution_time(),
                    "success": len(log.errors) == 0,
                    "warnings_count": len(log.warnings),
                    "errors_count": len(log.errors)
                }
                for log in self.execution_logs
            },
            "final_result": self.final_result
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        files["pipeline_summary"] = str(summary_file)
        
        # 2. å®Œå…¨ãªè©³ç´°ãƒ­ã‚°JSON
        complete_log_file = self.analysis_dir / "complete_analysis_log.json"
        complete_data = {
            "pipeline_metadata": self.pipeline_metadata,
            "execution_logs": [log.to_dict() for log in self.execution_logs],
            "final_result": self.final_result,
            "summary": {
                "total_components": len(self.execution_logs),
                "total_warnings": sum(len(log.warnings) for log in self.execution_logs),
                "total_errors": sum(len(log.errors) for log in self.execution_logs)
            }
        }
        
        with open(complete_log_file, 'w', encoding='utf-8') as f:
            json.dump(complete_data, f, indent=2, ensure_ascii=False)
        files["complete_log"] = str(complete_log_file)
        
        return files
    
    def _generate_phase1_prompts_md(self, log: DetailedExecutionLog) -> str:
        """Phase1ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–ç†ç”±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆ"""
        content = f"""# Phase1: ç”»åƒåˆ†æ - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–

## å®Ÿè¡Œæƒ…å ±
- å®Ÿè¡ŒID: {log.execution_id}
- é–‹å§‹æ™‚åˆ»: {log.execution_start_time.isoformat()}
- çµ‚äº†æ™‚åˆ»: {log.execution_end_time.isoformat() if log.execution_end_time else 'N/A'}
- å®Ÿè¡Œæ™‚é–“: {log.get_execution_time():.2f}ç§’

## ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
        for prompt_name, prompt_data in log.prompts_used.items():
            content += f"### {prompt_name.replace('_', ' ').title()}\n\n"
            content += f"**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—**: {prompt_data['timestamp']}\n\n"
            content += f"```\n{prompt_data['content']}\n```\n\n"
            
            if prompt_data.get('variables'):
                content += f"**å¤‰æ•°**:\n"
                for var_name, var_value in prompt_data['variables'].items():
                    content += f"- {var_name}: {var_value}\n"
                content += "\n"
        
        # æ¨è«–ç†ç”±
        content += "## AIæ¨è«–ã®è©³ç´°\n\n"
        
        # æ–™ç†è­˜åˆ¥ã®æ¨è«–
        dish_reasoning = [r for r in log.reasoning.items() if r[0].startswith('dish_identification_')]
        if dish_reasoning:
            content += "### æ–™ç†è­˜åˆ¥ã®æ¨è«–\n\n"
            for decision_point, reasoning_data in dish_reasoning:
                dish_num = decision_point.split('_')[-1]
                content += f"**æ–™ç† {dish_num}**:\n"
                content += f"- æ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # é£Ÿæé¸æŠã®æ¨è«–
        ingredient_reasoning = [r for r in log.reasoning.items() if r[0].startswith('ingredient_selection_')]
        if ingredient_reasoning:
            content += "### é£Ÿæé¸æŠã®æ¨è«–\n\n"
            for decision_point, reasoning_data in ingredient_reasoning:
                content += f"**{decision_point.replace('_', ' ').title()}**:\n"
                content += f"- æ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼
        if log.warnings:
            content += "## è­¦å‘Š\n\n"
            for warning in log.warnings:
                content += f"- {warning['message']} (at {warning['timestamp']})\n"
            content += "\n"
        
        if log.errors:
            content += "## ã‚¨ãƒ©ãƒ¼\n\n"
            for error in log.errors:
                content += f"- {error['message']} (at {error['timestamp']})\n"
            content += "\n"
        
        return content
    
    def _generate_phase1_detected_items_txt(self, log: DetailedExecutionLog) -> str:
        """Phase1ã§æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰"""
        content = f"Phase1 æ¤œå‡ºçµæœ - {log.execution_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += "=" * 60 + "\n\n"
        
        if 'output_data' in log.output_data and 'dishes' in log.output_data['output_data']:
            dishes = log.output_data['output_data']['dishes']
            content += f"æ¤œå‡ºã•ã‚ŒãŸæ–™ç†æ•°: {len(dishes)}\n\n"
            
            for i, dish in enumerate(dishes, 1):
                content += f"æ–™ç† {i}: {dish['dish_name']}\n"
                content += f"  é£Ÿææ•°: {len(dish['ingredients'])}\n"
                content += "  é£Ÿæè©³ç´°:\n"
                
                for j, ingredient in enumerate(dish['ingredients'], 1):
                    content += f"    {j}. {ingredient['ingredient_name']}\n"
                content += "\n"
        
        # USDAæ¤œç´¢æº–å‚™æƒ…å ±
        if 'usda_search_terms' in log.processing_details:
            search_terms = log.processing_details['usda_search_terms']
            content += f"USDAæ¤œç´¢èªå½™ ({len(search_terms)}å€‹):\n"
            for i, term in enumerate(search_terms, 1):
                content += f"  {i}. {term}\n"
            content += "\n"
        
        # å‡¦ç†è©³ç´°
        if log.processing_details:
            content += "å‡¦ç†è©³ç´°:\n"
            for detail_key, detail_value in log.processing_details.items():
                if detail_key == 'usda_search_terms':
                    continue  # æ—¢ã«ä¸Šã§è¡¨ç¤ºæ¸ˆã¿
                if isinstance(detail_value, (dict, list)):
                    content += f"  {detail_key}: {json.dumps(detail_value, ensure_ascii=False)}\n"
                else:
                    content += f"  {detail_key}: {detail_value}\n"
        
        return content
    
    def _generate_nutrition_search_results_md(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆï¼ˆUSDA/ãƒ­ãƒ¼ã‚«ãƒ«å¯¾å¿œï¼‰"""
        content = []
        
        content.append(f"# Nutrition Database Search Results")
        content.append(f"")
        content.append(f"**Search Method:** {search_method}")
        content.append(f"**Database Source:** {db_source}")
        content.append(f"**Component:** {log.component_name}")
        content.append(f"**Execution Time:** {log.get_execution_time():.3f} seconds")
        content.append(f"**Timestamp:** {log.execution_start_time.isoformat()}")
        content.append(f"")
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.input_data:
            content.append(f"## Input Data")
            if 'ingredient_names' in log.input_data:
                ingredients = log.input_data['ingredient_names']
                content.append(f"**Ingredients ({len(ingredients)}):** {', '.join(ingredients)}")
            
            if 'dish_names' in log.input_data:
                dishes = log.input_data['dish_names']
                content.append(f"**Dishes ({len(dishes)}):** {', '.join(dishes)}")
            content.append(f"")
        
        # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            content.append(f"## Search Results")
            content.append(f"**Total Matches:** {len(matches)}")
            content.append(f"")
            
            for i, (search_term, match_data) in enumerate(matches.items(), 1):
                content.append(f"### {i}. {search_term}")
                if isinstance(match_data, dict):
                    content.append(f"**ID:** {match_data.get('id', 'N/A')}")
                    
                    # search_name ã¨ description ã‚’é©åˆ‡ã«è¡¨ç¤º
                    search_name = match_data.get('search_name', 'N/A')
                    description = match_data.get('description', None)
                    content.append(f"**Search Name:** {search_name}")
                    if description:
                        content.append(f"**Description:** {description}")
                    else:
                        content.append(f"**Description:** None")
                    
                    content.append(f"**Data Type:** {match_data.get('data_type', 'N/A')}")
                    content.append(f"**Source:** {match_data.get('source', 'N/A')}")
                    
                    # ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’æ”¹å–„
                    score = match_data.get('score', 'N/A')
                    if score != 'N/A' and 'search_metadata' in match_data:
                        metadata = match_data['search_metadata']
                        score_breakdown = metadata.get('score_breakdown', {})
                        calculation = metadata.get('calculation', '')
                        match_type = score_breakdown.get('match_type', 'unknown')
                        
                        if calculation:
                            content.append(f"**Score:** {score} *({match_type}: {calculation})*")
                        else:
                            content.append(f"**Score:** {score} *(text similarity + data type priority)*")
                    else:
                        content.append(f"**Score:** {score}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        content.append(f"**Nutrients ({len(match_data['nutrients'])}):**")
                        for nutrient in match_data['nutrients'][:5]:  # æœ€åˆã®5ã¤ã ã‘è¡¨ç¤º
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                content.append(f"  - {name}: {amount} {unit}")
                        if len(match_data['nutrients']) > 5:
                            content.append(f"  - ... and {len(match_data['nutrients']) - 5} more nutrients")
                content.append(f"")
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            content.append(f"## Search Summary")
            content.append(f"**Total Searches:** {summary.get('total_searches', 0)}")
            content.append(f"**Successful Matches:** {summary.get('successful_matches', 0)}")
            content.append(f"**Failed Searches:** {summary.get('failed_searches', 0)}")
            content.append(f"**Match Rate:** {summary.get('match_rate_percent', 0)}%")
            content.append(f"**Search Method:** {summary.get('search_method', 'unknown')}")
            content.append(f"")
        
        # æ¨è«–ç†ç”±ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.reasoning:
            content.append(f"## Search Reasoning")
            for decision_point, reason_data in log.reasoning.items():
                reason = reason_data.get('reason', '') if isinstance(reason_data, dict) else str(reason_data)
                content.append(f"**{decision_point}:** {reason}")
            content.append(f"")
        
        # è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.warnings:
            content.append(f"## Warnings")
            for warning in log.warnings:
                content.append(f"- {warning}")
            content.append(f"")
        
        if log.errors:
            content.append(f"## Errors")
            for error in log.errors:
                content.append(f"- {error}")
            content.append(f"")
        
        return "\n".join(content)
    
    def _generate_nutrition_match_details_txt(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®ãƒãƒƒãƒè©³ç´°ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆUSDA/ãƒ­ãƒ¼ã‚«ãƒ«/ãƒãƒ«ãƒDBå¯¾å¿œï¼‰"""
        lines = []
        
        lines.append(f"Nutrition Database Search Match Details")
        lines.append(f"=" * 50)
        lines.append(f"Search Method: {search_method}")
        lines.append(f"Database Source: {db_source}")
        lines.append(f"Component: {log.component_name}")
        lines.append(f"Execution Time: {log.get_execution_time():.3f} seconds")
        lines.append(f"Timestamp: {log.execution_start_time.isoformat()}")
        lines.append(f"")
        
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            
            # ç·ãƒãƒƒãƒæ•°ã‚’è¨ˆç®—ï¼ˆå˜ä¸€çµæœã¨ãƒªã‚¹ãƒˆçµæœä¸¡æ–¹ã«å¯¾å¿œï¼‰
            total_matches = 0
            for match_data in matches.values():
                if isinstance(match_data, list):
                    total_matches += len(match_data)
                elif isinstance(match_data, dict):
                    total_matches += 1
            
            lines.append(f"Total Matches: {total_matches}")
            lines.append(f"")
            
            for search_term, match_data in matches.items():
                lines.append(f"Query: {search_term}")
                lines.append(f"-" * 30)
                
                # ãƒãƒ«ãƒDBæ¤œç´¢çµæœï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼‰ã¸ã®å¯¾å¿œ
                if isinstance(match_data, list):
                    lines.append(f"  Found {len(match_data)} results from multiple databases:")
                    lines.append(f"")
                    
                    for i, match_item in enumerate(match_data, 1):
                        lines.append(f"  Result {i}:")
                        lines.append(f"    ID: {match_item.get('id', 'N/A')}")
                        
                        search_name = match_item.get('search_name', 'N/A')
                        description = match_item.get('description', None)
                        lines.append(f"    Search Name: {search_name}")
                        if description:
                            lines.append(f"    Description: {description}")
                        else:
                            lines.append(f"    Description: None")
                        
                        lines.append(f"    Data Type: {match_item.get('data_type', 'N/A')}")
                        lines.append(f"    Source: {match_item.get('source', 'N/A')}")
                        
                        # ã‚¹ã‚³ã‚¢æƒ…å ±
                        score = match_item.get('score', 'N/A')
                        if score != 'N/A' and 'search_metadata' in match_item:
                            metadata = match_item['search_metadata']
                            source_db = metadata.get('source_database', 'unknown')
                            lines.append(f"    Score: {score:.3f} (from {source_db})")
                        else:
                            lines.append(f"    Score: {score}")
                        
                        # æ „é¤Šæƒ…å ±ï¼ˆç°¡ç•¥ç‰ˆï¼‰
                        if 'nutrition' in match_item and match_item['nutrition']:
                            nutrition = match_item['nutrition']
                            calories = nutrition.get('calories', 0)
                            protein = nutrition.get('protein', 0)
                            fat = nutrition.get('fat', 0)
                            carbs = nutrition.get('carbs', 0)
                            lines.append(f"    Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g")
                        
                        lines.append(f"")
                
                # å˜ä¸€çµæœï¼ˆè¾æ›¸å½¢å¼ï¼‰ã¸ã®å¯¾å¿œï¼ˆå¾“æ¥ã®æ–¹å¼ï¼‰
                elif isinstance(match_data, dict):
                    lines.append(f"  ID: {match_data.get('id', 'N/A')}")
                    
                    search_name = match_data.get('search_name', 'N/A')
                    description = match_data.get('description', None)
                    lines.append(f"  Search Name: {search_name}")
                    if description:
                        lines.append(f"  Description: {description}")
                    else:
                        lines.append(f"  Description: None")
                    
                    lines.append(f"  Data Type: {match_data.get('data_type', 'N/A')}")
                    lines.append(f"  Source: {match_data.get('source', 'N/A')}")
                    
                    # ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’æ”¹å–„
                    score = match_data.get('score', 'N/A')
                    if score != 'N/A' and 'search_metadata' in match_data:
                        metadata = match_data['search_metadata']
                        score_breakdown = metadata.get('score_breakdown', {})
                        calculation = metadata.get('calculation', '')
                        match_type = score_breakdown.get('match_type', 'unknown')
                        
                        if calculation:
                            lines.append(f"  Score: {score} ({match_type}: {calculation})")
                        else:
                            lines.append(f"  Score: {score} (text similarity + data type priority)")
                    else:
                        lines.append(f"  Score: {score}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        lines.append(f"  Nutrients ({len(match_data['nutrients'])}):")
                        for nutrient in match_data['nutrients']:
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                lines.append(f"    - {name}: {amount} {unit}")
                    
                    if 'original_data' in match_data:
                        original_data = match_data['original_data']
                        if isinstance(original_data, dict):
                            lines.append(f"  Original Data Source: {original_data.get('source', 'Unknown')}")
                            if search_method == "local_search":
                                lines.append(f"  Local DB Source: {original_data.get('db_source', 'Unknown')}")
                    
                    lines.append(f"")
        
        # æ¤œç´¢çµ±è¨ˆ
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            lines.append(f"Search Statistics:")
            lines.append(f"  Total Searches: {summary.get('total_searches', 0)}")
            lines.append(f"  Successful Matches: {summary.get('successful_matches', 0)}")
            lines.append(f"  Failed Searches: {summary.get('failed_searches', 0)}")
            lines.append(f"  Match Rate: {summary.get('match_rate_percent', 0)}%")
            
            # ãƒãƒ«ãƒDBæ¤œç´¢ã®å ´åˆã®è¿½åŠ æƒ…å ±
            if 'target_databases' in summary:
                lines.append(f"  Target Databases: {', '.join(summary['target_databases'])}")
                lines.append(f"  Results per Database: {summary.get('results_per_db', 'N/A')}")
                lines.append(f"  Total Results: {summary.get('total_results', 'N/A')}")
            
            if search_method == "local_search":
                lines.append(f"  Total Database Items: {summary.get('total_database_items', 0)}")
        
        return "\n".join(lines)
    
    def get_analysis_folder_path(self) -> str:
        """è§£æãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—"""
        return str(self.analysis_dir) 
```

============================================================

ğŸ“ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - Phase1
============================================================

ğŸ“„ FILE: app_v2/components/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 713 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 13:00:58
å­˜åœ¨: âœ…

CONTENT:
```python
from .base import BaseComponent
from .phase1_component import Phase1Component
from .usda_query_component import USDAQueryComponent
from .local_nutrition_search_component import LocalNutritionSearchComponent
from .elasticsearch_nutrition_search_component import ElasticsearchNutritionSearchComponent
# TODO: Phase2Componentã¨NutritionCalculationComponentã‚’å®Ÿè£…
# from .phase2_component import Phase2Component
# from .nutrition_calc_component import NutritionCalculationComponent

__all__ = [
    "BaseComponent",
    "Phase1Component", 
    "USDAQueryComponent",
    "LocalNutritionSearchComponent",
    "ElasticsearchNutritionSearchComponent",
    # "Phase2Component",
    # "NutritionCalculationComponent"
] 
```

============================================================

ğŸ“„ FILE: app_v2/components/base.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 6,824 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:08:38
å­˜åœ¨: âœ…

CONTENT:
```python
from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Any, Optional
import logging
from datetime import datetime

# å‹å¤‰æ•°ã®å®šç¾©
InputType = TypeVar('InputType')
OutputType = TypeVar('OutputType')


class BaseComponent(ABC, Generic[InputType, OutputType]):
    """
    é£Ÿäº‹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæŠ½è±¡ã‚¯ãƒ©ã‚¹
    
    å…¨ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã“ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã€process ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    """
    
    def __init__(self, component_name: str, logger: Optional[logging.Logger] = None):
        """
        ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        
        Args:
            component_name: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå
            logger: ãƒ­ã‚¬ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        self.component_name = component_name
        self.logger = logger or logging.getLogger(f"{__name__}.{component_name}")
        self.created_at = datetime.now()
        self.execution_count = 0
        self.current_execution_log = None  # è©³ç´°ãƒ­ã‚°
        
    @abstractmethod
    async def process(self, input_data: InputType) -> OutputType:
        """
        ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            OutputType: å‡¦ç†çµæœ
            
        Raises:
            ComponentError: å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        pass
    
    async def execute(self, input_data: InputType, execution_log: Optional['DetailedExecutionLog'] = None) -> OutputType:
        """
        ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ­ã‚°è¨˜éŒ²ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            execution_log: è©³ç´°å®Ÿè¡Œãƒ­ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            OutputType: å‡¦ç†çµæœ
        """
        self.execution_count += 1
        execution_id = f"{self.component_name}_{self.execution_count}"
        
        # è©³ç´°ãƒ­ã‚°ã®è¨­å®š
        if execution_log:
            self.current_execution_log = execution_log
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            self.current_execution_log.set_input(self._safe_serialize_input(input_data))
        
        self.logger.info(f"[{execution_id}] Starting {self.component_name} processing")
        
        try:
            start_time = datetime.now()
            result = await self.process(input_data)
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            self.logger.info(f"[{execution_id}] {self.component_name} completed in {processing_time:.2f}s")
            
            # è©³ç´°ãƒ­ã‚°ã«å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.set_output(self._safe_serialize_output(result))
                self.current_execution_log.finalize()
            
            return result
            
        except Exception as e:
            self.logger.error(f"[{execution_id}] {self.component_name} failed: {str(e)}", exc_info=True)
            
            # è©³ç´°ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.add_error(str(e))
                self.current_execution_log.finalize()
            
            raise ComponentError(f"{self.component_name} processing failed: {str(e)}") from e
        finally:
            self.current_execution_log = None
    
    def log_prompt(self, prompt_name: str, prompt_content: str, variables: dict = None):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_prompt(prompt_name, prompt_content, variables)
    
    def log_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_reasoning(decision_point, reason, confidence)
    
    def log_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_processing_detail(detail_key, detail_value)
    
    def log_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_confidence_score(metric_name, score)
    
    def log_warning(self, warning: str):
        """è­¦å‘Šã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_warning(warning)
    
    def _safe_serialize_input(self, input_data: InputType) -> dict:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(input_data, 'model_dump'):
                return input_data.model_dump()
            elif hasattr(input_data, '__dict__'):
                return input_data.__dict__
            else:
                return {"data": str(input_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def _safe_serialize_output(self, output_data: OutputType) -> dict:
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(output_data, 'model_dump'):
                return output_data.model_dump()
            elif hasattr(output_data, '__dict__'):
                return output_data.__dict__
            else:
                return {"data": str(output_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def get_component_info(self) -> dict:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
        return {
            "component_name": self.component_name,
            "created_at": self.created_at.isoformat(),
            "execution_count": self.execution_count,
            "component_type": self.__class__.__name__
        }


class ComponentError(Exception):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, component_name: str = None, original_error: Exception = None):
        super().__init__(message)
        self.component_name = component_name
        self.original_error = original_error
        self.timestamp = datetime.now()
    
    def to_dict(self) -> dict:
        """ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "error_message": str(self),
            "component_name": self.component_name,
            "timestamp": self.timestamp.isoformat(),
            "original_error": str(self.original_error) if self.original_error else None
        } 
```

============================================================

ğŸ“„ FILE: app_v2/components/phase1_component.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 5,285 bytes
æœ€çµ‚æ›´æ–°: 2025-06-09 11:27:28
å­˜åœ¨: âœ…

CONTENT:
```python
import json
from typing import Optional

from .base import BaseComponent
from ..models.phase1_models import Phase1Input, Phase1Output, Dish, Ingredient
from ..services.gemini_service import GeminiService
from ..config import get_settings
from ..config.prompts import Phase1Prompts


class Phase1Component(BaseComponent[Phase1Input, Phase1Output]):
    """
    Phase1: ç”»åƒåˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
    
    Gemini AIã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æã—ã€USDAæ¤œç´¢ã«é©ã—ãŸæ–™ç†ã¨é£Ÿæåã‚’è­˜åˆ¥ã—ã¾ã™ã€‚
    """
    
    def __init__(self, gemini_service: Optional[GeminiService] = None):
        super().__init__("Phase1Component")
        
        # GeminiServiceã®åˆæœŸåŒ–
        if gemini_service is None:
            settings = get_settings()
            self.gemini_service = GeminiService(
                project_id=settings.GEMINI_PROJECT_ID,
                location=settings.GEMINI_LOCATION,
                model_name=settings.GEMINI_MODEL_NAME
            )
        else:
            self.gemini_service = gemini_service
    
    async def process(self, input_data: Phase1Input) -> Phase1Output:
        """
        Phase1ã®ä¸»å‡¦ç†: ç”»åƒåˆ†æï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
        
        Args:
            input_data: Phase1Input (image_bytes, image_mime_type, optional_text)
            
        Returns:
            Phase1Output: åˆ†æçµæœï¼ˆæ–™ç†åãƒ»é£Ÿæåã®ã¿ï¼‰
        """
        self.logger.info(f"Starting Phase1 image analysis for USDA query generation")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã¨è¨˜éŒ²
        system_prompt = Phase1Prompts.get_system_prompt()
        user_prompt = Phase1Prompts.get_user_prompt(input_data.optional_text)
        
        self.log_prompt("system_prompt", system_prompt)
        self.log_prompt("user_prompt", user_prompt, {
            "optional_text": input_data.optional_text,
            "image_mime_type": input_data.image_mime_type
        })
        
        # ç”»åƒæƒ…å ±ã®ãƒ­ã‚°è¨˜éŒ²
        self.log_processing_detail("image_size_bytes", len(input_data.image_bytes))
        self.log_processing_detail("image_mime_type", input_data.image_mime_type)
        
        try:
            # Gemini AIã«ã‚ˆã‚‹ç”»åƒåˆ†æ
            self.log_processing_detail("gemini_api_call_start", "Calling Gemini API for image analysis")
            
            gemini_result = await self.gemini_service.analyze_phase1(
                image_bytes=input_data.image_bytes,
                image_mime_type=input_data.image_mime_type,
                optional_text=input_data.optional_text
            )
            
            self.log_processing_detail("gemini_raw_response", gemini_result)
            
            # çµæœã‚’Pydanticãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›
            dishes = []
            for dish_index, dish_data in enumerate(gemini_result.get("dishes", [])):
                ingredients = []
                for ingredient_index, ingredient_data in enumerate(dish_data.get("ingredients", [])):
                    ingredient = Ingredient(
                        ingredient_name=ingredient_data["ingredient_name"]
                    )
                    ingredients.append(ingredient)
                    
                    # é£Ÿæè­˜åˆ¥ã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                    self.log_reasoning(
                        f"ingredient_identification_dish{dish_index}_ingredient{ingredient_index}",
                        f"Identified ingredient '{ingredient_data['ingredient_name']}' for USDA search based on visual analysis"
                    )
                
                dish = Dish(
                    dish_name=dish_data["dish_name"],
                    ingredients=ingredients
                )
                dishes.append(dish)
                
                # æ–™ç†è­˜åˆ¥ã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"dish_identification_{dish_index}",
                    f"Identified dish as '{dish_data['dish_name']}' for USDA search based on visual characteristics"
                )
            
            # åˆ†æçµ±è¨ˆã®è¨˜éŒ²
            self.log_processing_detail("detected_dishes_count", len(dishes))
            self.log_processing_detail("total_ingredients_count", sum(len(dish.ingredients) for dish in dishes))
            
            # USDAæ¤œç´¢é©åˆæ€§ãƒã‚§ãƒƒã‚¯
            search_terms = []
            for dish in dishes:
                search_terms.append(dish.dish_name)
                for ingredient in dish.ingredients:
                    search_terms.append(ingredient.ingredient_name)
            
            self.log_processing_detail("usda_search_terms", search_terms)
            self.log_reasoning(
                "usda_search_preparation",
                f"Generated {len(search_terms)} search terms for USDA database queries"
            )
            
            result = Phase1Output(
                dishes=dishes,
                warnings=[]
            )
            
            self.logger.info(f"Phase1 completed: identified {len(dishes)} dishes with {len(search_terms)} total search terms")
            return result
            
        except Exception as e:
            self.logger.error(f"Phase1 processing failed: {str(e)}")
            raise 
```

============================================================

ğŸ“ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢
============================================================

ğŸ“„ FILE: app_v2/components/elasticsearch_nutrition_search_component.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 28,363 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 16:50:13
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Elasticsearch Nutrition Search Component

ç¾çŠ¶ã®queryã‚·ã‚¹ãƒ†ãƒ ã‚’Elasticsearchã§é«˜é€ŸåŒ–ã™ã‚‹æ–°ã—ã„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
Elasticsearchå°‚ç”¨ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ãªã—
"""

import os
import json
import asyncio
from typing import Optional, List, Dict, Any
from datetime import datetime

from .base import BaseComponent
from ..models.nutrition_search_models import (
    NutritionQueryInput, NutritionQueryOutput, NutritionMatch
)
from ..config import get_settings

# Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False


class ElasticsearchNutritionSearchComponent(BaseComponent[NutritionQueryInput, NutritionQueryOutput]):
    """
    Elasticsearchå°‚ç”¨æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    
    ç¾çŠ¶ã®ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’Elasticsearchã§é«˜é€ŸåŒ–ã—ã¾ã™ã€‚
    Elasticsearchå°‚ç”¨ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã¯æä¾›ã—ã¾ã›ã‚“ã€‚
    """
    
    def __init__(self, elasticsearch_url: str = "http://localhost:9200", multi_db_search_mode: bool = False, results_per_db: int = 3):
        super().__init__("ElasticsearchNutritionSearchComponent")
        
        self.elasticsearch_url = elasticsearch_url
        self.es_client = None
        self.index_name = "nutrition_db"
        self.multi_db_search_mode = multi_db_search_mode
        self.results_per_db = results_per_db
        self.target_databases = ["yazio", "mynetdiary", "eatthismuch"]
        
        # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self._initialize_elasticsearch()
        
        self.logger.info(f"ElasticsearchNutritionSearchComponent initialized")
        self.logger.info(f"Elasticsearch available: {ELASTICSEARCH_AVAILABLE}")
        self.logger.info(f"ES client connected: {self.es_client is not None}")
        self.logger.info(f"Multi-DB search mode: {self.multi_db_search_mode}")
        self.logger.info(f"Results per database: {self.results_per_db}")
        
    def _initialize_elasticsearch(self):
        """Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        if not ELASTICSEARCH_AVAILABLE:
            self.logger.error("Elasticsearch library not available. Please install: pip install elasticsearch")
            return
        
        try:
            self.es_client = Elasticsearch([self.elasticsearch_url])
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            if self.es_client.ping():
                self.logger.info(f"Successfully connected to Elasticsearch at {self.elasticsearch_url}")
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å­˜åœ¨ç¢ºèª
                if self.es_client.indices.exists(index=self.index_name):
                    self.logger.info(f"Index '{self.index_name}' exists and ready")
                else:
                    self.logger.error(f"Index '{self.index_name}' does not exist. Please run create_elasticsearch_index.py first.")
                    self.es_client = None
            else:
                self.logger.error("Elasticsearch ping failed. Please ensure Elasticsearch is running.")
                self.es_client = None
                
        except Exception as e:
            self.logger.error(f"Failed to connect to Elasticsearch: {e}")
            self.es_client = None
    
    async def process(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        Elasticsearchæ¤œç´¢ã®ä¸»å‡¦ç†
        
        Args:
            input_data: NutritionQueryInput
            
        Returns:
            NutritionQueryOutput: Elasticsearchæ¤œç´¢çµæœ
            
        Raises:
            RuntimeError: ElasticsearchãŒåˆ©ç”¨ã§ããªã„å ´åˆ
        """
        # Elasticsearchåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        if not ELASTICSEARCH_AVAILABLE:
            error_msg = "Elasticsearch library not available. Please install: pip install elasticsearch"
            self.logger.error(error_msg)
            return NutritionQueryOutput(
                matches={},
                search_summary={
                    "total_searches": 0,
                    "successful_matches": 0,
                    "failed_searches": 0,
                    "match_rate_percent": 0,
                    "search_method": "elasticsearch_unavailable",
                    "search_time_ms": 0
                },
                errors=[error_msg]
            )
        
        if not self.es_client:
            error_msg = "Elasticsearch client not initialized. Please check connection and index availability."
            self.logger.error(error_msg)
            return NutritionQueryOutput(
                matches={},
                search_summary={
                    "total_searches": 0,
                    "successful_matches": 0,
                    "failed_searches": 0,
                    "match_rate_percent": 0,
                    "search_method": "elasticsearch_connection_failed",
                    "search_time_ms": 0
                },
                errors=[error_msg]
            )
        
        search_terms = input_data.get_all_search_terms()
        self.logger.info(f"Starting Elasticsearch nutrition search for {len(search_terms)} terms")
        
        # æ¤œç´¢å¯¾è±¡ã®è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("ingredient_names", input_data.ingredient_names)
        self.log_processing_detail("dish_names", input_data.dish_names)
        self.log_processing_detail("total_search_terms", len(search_terms))
        self.log_processing_detail("multi_db_search_mode", self.multi_db_search_mode)
        self.log_processing_detail("results_per_db", self.results_per_db)
        
        if self.multi_db_search_mode:
            self.log_processing_detail("search_method", "elasticsearch_strategic")
            return await self._elasticsearch_strategic_search(input_data, search_terms)
        else:
            self.log_processing_detail("search_method", "elasticsearch_single")
            return await self._elasticsearch_search(input_data, search_terms)
    
    async def _elasticsearch_search(self, input_data: NutritionQueryInput, search_terms: List[str]) -> NutritionQueryOutput:
        """
        Elasticsearchã‚’ä½¿ç”¨ã—ãŸå˜ä¸€çµæœæ¤œç´¢ï¼ˆå¾“æ¥ã®æ–¹å¼ï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            search_terms: æ¤œç´¢èªå½™ãƒªã‚¹ãƒˆ
            
        Returns:
            NutritionQueryOutput: Elasticsearchæ¤œç´¢çµæœ
        """
        matches = {}
        warnings = []
        errors = []
        successful_matches = 0
        total_searches = len(search_terms)
        
        start_time = datetime.now()
        
        # å„æ¤œç´¢èªå½™ã«ã¤ã„ã¦Elasticsearchæ¤œç´¢ã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Elasticsearch search for: {search_term}")
            
            self.log_processing_detail(f"es_search_{search_index}_term", search_term)
            
            try:
                # Elasticsearchã‚¯ã‚¨ãƒªã®æ§‹ç¯‰
                es_query = self._build_elasticsearch_query(search_term, input_data)
                
                # æ¤œç´¢å®Ÿè¡Œ
                response = self.es_client.search(
                    index=self.index_name,
                    body=es_query
                )
                
                # çµæœå‡¦ç†
                hits = response.get('hits', {}).get('hits', [])
                
                if hits:
                    # æœ€è‰¯ã®ãƒãƒƒãƒã‚’é¸æŠ
                    best_hit = hits[0]
                    source = best_hit['_source']
                    score = best_hit['_score']
                    
                    match_result = self._convert_es_hit_to_nutrition_match(best_hit, search_term)
                    matches[search_term] = match_result
                    successful_matches += 1
                    
                    self.log_reasoning(
                        f"es_match_{search_index}",
                        f"Found Elasticsearch match for '{search_term}': {source.get('search_name', 'N/A')} (score: {score:.3f}, db: {source.get('source_db', 'N/A')})"
                    )
                    
                    self.logger.debug(f"ES match for '{search_term}': {source.get('search_name', 'N/A')} from {source.get('source_db', 'N/A')}")
                else:
                    self.log_reasoning(
                        f"es_no_match_{search_index}",
                        f"No Elasticsearch match found for '{search_term}'"
                    )
                    warnings.append(f"No Elasticsearch match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Elasticsearch search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                self.log_reasoning(
                    f"es_search_error_{search_index}",
                    f"Elasticsearch search error for '{search_term}': {str(e)}"
                )
        
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "elasticsearch",
            "database_source": "elasticsearch_nutrition_db",
            "preferred_source": input_data.preferred_source,
            "search_time_ms": search_time_ms,
            "index_name": self.index_name,
            "total_indexed_documents": await self._get_total_document_count()
        }
        
        self.log_processing_detail("elasticsearch_search_summary", search_summary)
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Elasticsearch nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%}) in {search_time_ms}ms")
        
        return result
    
    async def _elasticsearch_strategic_search(self, input_data: NutritionQueryInput, search_terms: List[str]) -> NutritionQueryOutput:
        """
        æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ï¼ˆdish/ingredientåˆ¥ã®æœ€é©åŒ–ã•ã‚ŒãŸæ¤œç´¢ï¼‰
        
        Dishæˆ¦ç•¥:
        - ãƒ¡ã‚¤ãƒ³: EatThisMuch data_type=dish
        - è£œåŠ©: EatThisMuch data_type=branded (ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆ)
        
        Ingredientæˆ¦ç•¥:
        - ãƒ¡ã‚¤ãƒ³: EatThisMuch data_type=ingredient  
        - è£œåŠ©: MyNetDiary, YAZIO, EatThisMuch branded
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            search_terms: æ¤œç´¢èªå½™ãƒªã‚¹ãƒˆ
            
        Returns:
            NutritionQueryOutput: æˆ¦ç•¥çš„æ¤œç´¢çµæœ
        """
        matches = {}
        warnings = []
        errors = []
        successful_matches = 0
        total_searches = len(search_terms)
        
        start_time = datetime.now()
        
        # å„æ¤œç´¢èªå½™ã«ã¤ã„ã¦æˆ¦ç•¥çš„æ¤œç´¢ã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Strategic Elasticsearch search for: {search_term}")
            
            self.log_processing_detail(f"strategic_search_{search_index}_term", search_term)
            
            try:
                # ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
                query_type = "dish" if search_term in input_data.dish_names else "ingredient"
                self.log_processing_detail(f"search_{search_index}_type", query_type)
                
                # æˆ¦ç•¥çš„æ¤œç´¢ã‚’å®Ÿè¡Œ
                if query_type == "dish":
                    strategic_results = await self._strategic_dish_search(search_term, input_data)
                else:
                    strategic_results = await self._strategic_ingredient_search(search_term, input_data)
                
                if strategic_results:
                    matches[search_term] = strategic_results
                    successful_matches += 1
                    
                    self.log_reasoning(
                        f"strategic_match_{search_index}",
                        f"Found strategic matches for '{search_term}' ({query_type}): {len(strategic_results)} results"
                    )
                else:
                    self.log_reasoning(
                        f"strategic_no_results_{search_index}",
                        f"No strategic results for '{search_term}' ({query_type})"
                    )
                    warnings.append(f"No strategic results found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Strategic Elasticsearch search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                self.log_reasoning(
                    f"strategic_search_error_{search_index}",
                    f"Strategic Elasticsearch search error for '{search_term}': {str(e)}"
                )
        
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        # æˆ¦ç•¥çš„æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        total_results = sum(len(result_list) if isinstance(result_list, list) else 1 for result_list in matches.values())
        
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "elasticsearch_strategic",
            "database_source": "elasticsearch_nutrition_db",
            "preferred_source": input_data.preferred_source,
            "search_time_ms": search_time_ms,
            "index_name": self.index_name,
            "total_indexed_documents": await self._get_total_document_count(),
            "strategic_approach": {
                "dish_strategy": "eatthismuch_dish_primary + eatthismuch_branded_fallback",
                "ingredient_strategy": "eatthismuch_ingredient_primary + multi_db_fallback"
            },
            "total_results": total_results
        }
        
        self.log_processing_detail("elasticsearch_strategic_search_summary", search_summary)
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Strategic Elasticsearch nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%}) with {total_results} total results in {search_time_ms}ms")
        
        return result

    async def _strategic_dish_search(self, search_term: str, input_data: NutritionQueryInput) -> List[NutritionMatch]:
        """
        Dishæ¤œç´¢æˆ¦ç•¥ã‚’å®Ÿè¡Œ
        
        æˆ¦ç•¥:
        1. EatThisMuch data_type=dish ã‚’ãƒ¡ã‚¤ãƒ³æ¤œç´¢
        2. ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆã¯ EatThisMuch data_type=branded ã‚’è£œåŠ©æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            List[NutritionMatch]: æˆ¦ç•¥çš„æ¤œç´¢çµæœ
        """
        results = []
        MIN_SCORE_THRESHOLD = 20.0  # ã‚¹ã‚³ã‚¢é–¾å€¤
        
        self.logger.info(f"Strategic dish search for '{search_term}': EatThisMuch dish -> branded fallback")
        
        # Step 1: EatThisMuch dish ã‚’ãƒ¡ã‚¤ãƒ³æ¤œç´¢
        main_query = self._build_strategic_query(search_term, "eatthismuch", "dish")
        
        try:
            response = self.es_client.search(index=self.index_name, body=main_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                best_score = hits[0].get('_score', 0)
                self.logger.info(f"Dish main search: found {len(hits)} results, best score: {best_score}")
                
                if best_score >= MIN_SCORE_THRESHOLD:
                    # é«˜ã‚¹ã‚³ã‚¢: ãƒ¡ã‚¤ãƒ³çµæœã®ã¿ä½¿ç”¨
                    for hit in hits[:self.results_per_db]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "main_dish"
                        match.search_metadata["strategy_type"] = "dish_primary"
                        results.append(match)
                    
                    self.logger.info(f"High score dish results: using {len(results)} main results")
                    return results
                else:
                    # ä½ã‚¹ã‚³ã‚¢: ãƒ¡ã‚¤ãƒ³çµæœã‚’ä¿æŒã—ã¦è£œåŠ©æ¤œç´¢ã‚‚å®Ÿè¡Œ
                    for hit in hits[:max(1, self.results_per_db // 2)]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "main_dish_low_score"
                        match.search_metadata["strategy_type"] = "dish_primary"
                        results.append(match)
        
        except Exception as e:
            self.logger.error(f"Error in dish main search: {e}")
        
        # Step 2: EatThisMuch branded ã‚’è£œåŠ©æ¤œç´¢
        fallback_query = self._build_strategic_query(search_term, "eatthismuch", "branded")
        
        try:
            response = self.es_client.search(index=self.index_name, body=fallback_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                remaining_slots = self.results_per_db - len(results)
                for hit in hits[:remaining_slots]:
                    match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                    match.search_metadata["strategic_phase"] = "fallback_branded"
                    match.search_metadata["strategy_type"] = "dish_fallback"
                    results.append(match)
                
                self.logger.info(f"Dish fallback search: added {min(len(hits), remaining_slots)} branded results")
        
        except Exception as e:
            self.logger.error(f"Error in dish fallback search: {e}")
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.score, reverse=True)
        
        self.logger.info(f"Strategic dish search completed: {len(results)} total results")
        return results

    async def _strategic_ingredient_search(self, search_term: str, input_data: NutritionQueryInput) -> List[NutritionMatch]:
        """
        Ingredientæ¤œç´¢æˆ¦ç•¥ã‚’å®Ÿè¡Œ
        
        æˆ¦ç•¥:
        1. EatThisMuch data_type=ingredient ã‚’ãƒ¡ã‚¤ãƒ³æ¤œç´¢
        2. MyNetDiary, YAZIO, EatThisMuch branded ã‚’è£œåŠ©æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            List[NutritionMatch]: æˆ¦ç•¥çš„æ¤œç´¢çµæœ
        """
        results = []
        
        self.logger.info(f"Strategic ingredient search for '{search_term}': EatThisMuch ingredient -> multi-DB fallback")
        
        # Step 1: EatThisMuch ingredient ã‚’ãƒ¡ã‚¤ãƒ³æ¤œç´¢
        main_query = self._build_strategic_query(search_term, "eatthismuch", "ingredient")
        
        try:
            response = self.es_client.search(index=self.index_name, body=main_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                # ãƒ¡ã‚¤ãƒ³çµæœã‚’è¿½åŠ ï¼ˆæœ€å¤§åŠåˆ†ã®ã‚¹ãƒ­ãƒƒãƒˆä½¿ç”¨ï¼‰
                main_slots = max(1, self.results_per_db // 2)
                for hit in hits[:main_slots]:
                    match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                    match.search_metadata["strategic_phase"] = "main_ingredient"
                    match.search_metadata["strategy_type"] = "ingredient_primary"
                    results.append(match)
                
                self.logger.info(f"Ingredient main search: added {len(results)} primary results")
        
        except Exception as e:
            self.logger.error(f"Error in ingredient main search: {e}")
        
        # Step 2: è£œåŠ©ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
        fallback_sources = [
            ("mynetdiary", "unified"),
            ("yazio", "unified"), 
            ("eatthismuch", "branded")
        ]
        
        remaining_slots = self.results_per_db - len(results)
        slots_per_source = max(1, remaining_slots // len(fallback_sources))
        
        for db_name, data_type in fallback_sources:
            if remaining_slots <= 0:
                break
                
            try:
                fallback_query = self._build_strategic_query(search_term, db_name, data_type)
                response = self.es_client.search(index=self.index_name, body=fallback_query)
                hits = response.get('hits', {}).get('hits', [])
                
                if hits:
                    current_slots = min(slots_per_source, remaining_slots)
                    for hit in hits[:current_slots]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "fallback_multi_db"
                        match.search_metadata["strategy_type"] = "ingredient_fallback"
                        match.search_metadata["fallback_source"] = f"{db_name}_{data_type}"
                        results.append(match)
                    
                    remaining_slots -= len(hits[:current_slots])
                    self.logger.info(f"Ingredient fallback ({db_name}_{data_type}): added {len(hits[:current_slots])} results")
            
            except Exception as e:
                self.logger.error(f"Error in ingredient fallback search ({db_name}_{data_type}): {e}")
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.score, reverse=True)
        
        self.logger.info(f"Strategic ingredient search completed: {len(results)} total results")
        return results

    def _build_strategic_query(self, search_term: str, target_db: str, data_type: str) -> Dict[str, Any]:
        """
        æˆ¦ç•¥çš„æ¤œç´¢ç”¨ã®Elasticsearchã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            search_term: æ¤œç´¢èªå½™
            target_db: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
            data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
            
        Returns:
            Elasticsearchã‚¯ã‚¨ãƒªè¾æ›¸
        """
        query = {
            "size": self.results_per_db,
            "query": {
                "bool": {
                    "must": [
                        {
                            "multi_match": {
                                "query": search_term,
                                "fields": [
                                    "search_name^3",
                                    "description^1"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ],
                    "filter": [
                        {"term": {"source_db": target_db}},
                        {"term": {"data_type": data_type}}
                    ]
                }
            },
            "sort": [
                {"_score": {"order": "desc"}}
            ]
        }
        
        return query
    
    async def _get_total_document_count(self) -> int:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…ã®ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—"""
        try:
            stats = self.es_client.indices.stats(index=self.index_name)
            return stats["indices"][self.index_name]["total"]["docs"]["count"]
        except Exception as e:
            self.logger.warning(f"Failed to get document count: {e}")
            return 0
    
    def _build_elasticsearch_query(self, search_term: str, input_data: NutritionQueryInput) -> Dict[str, Any]:
        """
        Elasticsearchæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            search_term: æ¤œç´¢èªå½™
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            Elasticsearchã‚¯ã‚¨ãƒªè¾æ›¸
        """
        # åŸºæœ¬çš„ãªmulti_matchã‚¯ã‚¨ãƒª
        base_query = {
            "multi_match": {
                "query": search_term,
                "fields": [
                    "search_name^3",  # æ¤œç´¢åã«é«˜ã„é‡ã¿
                    "search_name.exact^5"  # å®Œå…¨ä¸€è‡´ã«æœ€é«˜ã®é‡ã¿
                ],
                "type": "best_fields",
                "fuzziness": "AUTO",
                "operator": "OR"
            }
        }
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
        filters = []
        
        # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
        if search_term in input_data.dish_names:
            filters.append({"term": {"data_type": "dish"}})
        # ingredient_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯é£Ÿæãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
        elif search_term in input_data.ingredient_names:
            filters.append({"term": {"data_type": "ingredient"}})
        
        # å„ªå…ˆã‚½ãƒ¼ã‚¹ã®è¨­å®š
        if input_data.preferred_source and input_data.preferred_source != "elasticsearch":
            source_mapping = {
                "yazio": "yazio",
                "mynetdiary": "mynetdiary", 
                "eatthismuch": "eatthismuch"
            }
            if input_data.preferred_source in source_mapping:
                filters.append({"term": {"source_db": source_mapping[input_data.preferred_source]}})
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãŒã‚ã‚‹å ´åˆã¯boolã‚¯ã‚¨ãƒªã§ãƒ©ãƒƒãƒ—
        if filters:
            query = {
                "bool": {
                    "must": [base_query],
                    "should": filters,  # shouldã§å„ªå…ˆåº¦ä»˜ã‘
                    "boost": 1.2
                }
            }
        else:
            query = base_query
        
        return {
            "query": query,
            "size": 5,  # ä¸Šä½5ä»¶ã‚’å–å¾—
            "_source": ["data_type", "id", "search_name", "nutrition", "weight", "source_db", "description"]
        }
    
    def _convert_es_hit_to_nutrition_match(self, hit: Dict[str, Any], search_term: str) -> NutritionMatch:
        """
        Elasticsearchãƒ’ãƒƒãƒˆã‚’NutritionMatchã«å¤‰æ›
        
        Args:
            hit: Elasticsearchãƒ’ãƒƒãƒˆ
            search_term: æ¤œç´¢èªå½™
            
        Returns:
            NutritionMatch
        """
        source = hit['_source']
        score = hit['_score']
        
        # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚ã‚‹
        source_db = source.get('source_db', 'unknown')
        final_source = f"elasticsearch_{source_db}"
        
        return NutritionMatch(
            id=source.get('id', 0),
            search_name=source.get('search_name', search_term),
            description=source.get('description'),
            data_type=source.get('data_type', 'unknown'),
            source=final_source,
            nutrition=source.get('nutrition', {}),
            weight=source.get('weight'),
            score=score,
            search_metadata={
                "search_term": search_term,
                "elasticsearch_score": score,
                "search_method": "elasticsearch_multi_db" if self.multi_db_search_mode else "elasticsearch",
                "source_database": source_db,
                "index_name": self.index_name
            }
        ) 
```

============================================================

ğŸ“„ FILE: app_v2/components/local_nutrition_search_component.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 21,683 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 12:20:02
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Local Nutrition Search Component

USDA database queryã‚’ nutrition_db_experiment ã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã«ç½®ãæ›ãˆã‚‹
"""

import os
import sys
import json
import asyncio
from typing import Optional, List, Dict, Any
from pathlib import Path

from .base import BaseComponent
from ..models.nutrition_search_models import (
    NutritionQueryInput, NutritionQueryOutput, NutritionMatch
)
from ..config import get_settings

# nutrition_db_experimentã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
NUTRITION_DB_EXPERIMENT_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "nutrition_db_experiment"
)
sys.path.append(NUTRITION_DB_EXPERIMENT_PATH)

class LocalNutritionSearchComponent(BaseComponent[NutritionQueryInput, NutritionQueryOutput]):
    """
    ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    
    nutrition_db_experimentã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦é£Ÿæåã‚’æ¤œç´¢ã—ã€
    ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ã§çµæœã‚’è¿”ã—ã¾ã™ã€‚
    """
    
    def __init__(self):
        super().__init__("LocalNutritionSearchComponent")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self._initialize_local_search_system()
        
        # unified_dbã®ã¿ã‚’ä½¿ç”¨
        self.unified_db_path = os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "unified_nutrition_db.json")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
        self.unified_database = self._load_unified_database()
        
        # nutrition_db_experimentã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.search_handler = None
        self.query_preprocessor = None
        
        self.logger.info(f"LocalNutritionSearchComponent initialized with {len(self.unified_database)} total items")
    
    def _initialize_local_search_system(self):
        """nutrition_db_experimentã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        try:
            # æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            from api.search_handler import SearchHandler
            from api.query_preprocessor import QueryPreprocessor
            
            self.search_handler = SearchHandler()
            self.query_preprocessor = QueryPreprocessor()
            
            self.logger.info("Advanced local search system initialized")
        except ImportError as e:
            self.logger.warning(f"Advanced search system not available, will use direct database search: {e}")
        except Exception as e:
            self.logger.error(f"Failed to initialize advanced search system: {e}")
    
    def _load_unified_database(self) -> List[Dict[str, Any]]:
        """unified_nutrition_db.jsonã‚’èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists(self.unified_db_path):
                with open(self.unified_db_path, 'r', encoding='utf-8') as f:
                    database = json.load(f)
                self.logger.info(f"Loaded unified_db: {len(database)} items")
                return database
            else:
                self.logger.warning(f"Unified database file not found: {self.unified_db_path}")
                return []
        except Exception as e:
            self.logger.error(f"Error loading unified_db: {e}")
            return []
    
    async def process(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®ä¸»å‡¦ç†ï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰
        
        Args:
            input_data: NutritionQueryInput
            
        Returns:
            NutritionQueryOutput: ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢çµæœ
        """
        self.logger.info(f"Starting local nutrition search for {len(input_data.get_all_search_terms())} terms")
        
        # input_dataã‚’ä¿å­˜ã—ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—ã§ä½¿ç”¨
        self._current_input_data = input_data
        
        search_terms = input_data.get_all_search_terms()
        
        # æ¤œç´¢å¯¾è±¡ã®è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("ingredient_names", input_data.ingredient_names)
        self.log_processing_detail("dish_names", input_data.dish_names)
        self.log_processing_detail("total_search_terms", len(search_terms))
        self.log_processing_detail("search_method", "local_nutrition_database")
        self.log_processing_detail("preferred_source", input_data.preferred_source)
        
        matches = {}
        warnings = []
        errors = []
        
        successful_matches = 0
        total_searches = len(search_terms)
        
        # å„æ¤œç´¢èªå½™ã«ã¤ã„ã¦ç…§åˆã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Searching local database for: {search_term}")
            
            # æ¤œç´¢é–‹å§‹ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_term", search_term)
            self.log_processing_detail(f"search_{search_index}_start", f"Starting local search for '{search_term}'")
            
            try:
                # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®å®Ÿè¡Œ
                if self.search_handler and self.query_preprocessor:
                    # é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
                    match_result = await self._advanced_local_search(search_term, search_index, input_data)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°
                    match_result = await self._simple_local_search(search_term, search_index, input_data)
                
                if match_result:
                    matches[search_term] = match_result
                    successful_matches += 1
                    self.logger.debug(f"Found local match for '{search_term}': ID {match_result.id}")
                else:
                    self.log_reasoning(
                        f"no_match_{search_index}",
                        f"No local database match found for '{search_term}' - may not exist in local nutrition database"
                    )
                    self.logger.warning(f"No local match found for: {search_term}")
                    warnings.append(f"No local match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Local search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"search_error_{search_index}",
                    f"Local database search error for '{search_term}': {str(e)}"
                )
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "local_nutrition_database",
            "database_source": "nutrition_db_experiment",
            "preferred_source": input_data.preferred_source,
            "total_database_items": len(self.unified_database)
        }
        
        # å…¨ä½“çš„ãªæ¤œç´¢æˆåŠŸç‡ã‚’ãƒ­ã‚°
        overall_success_rate = successful_matches / total_searches if total_searches > 0 else 0
        self.log_processing_detail("search_summary", search_summary)
        
        # æ¤œç´¢å“è³ªã®è©•ä¾¡ã‚’ãƒ­ã‚°
        if overall_success_rate >= 0.8:
            self.log_reasoning("search_quality", "Excellent local search results with high match rate")
        elif overall_success_rate >= 0.6:
            self.log_reasoning("search_quality", "Good local search results with acceptable match rate")
        elif overall_success_rate >= 0.4:
            self.log_reasoning("search_quality", "Moderate local search results, some items may need manual review")
        else:
            self.log_reasoning("search_quality", "Poor local search results, many items not found in local database")
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Local nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%})")
        
        return result
    
    async def _advanced_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        nutrition_db_experimentã®é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        try:
            from api.search_handler import SearchRequest
            
            # æ¤œç´¢ã‚¿ã‚¤ãƒ—ã®æ±ºå®šï¼ˆæ–™ç†ã‹é£Ÿæã‹ã®æ¨å®šï¼‰
            db_type_filter = None  # å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢
            
            # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ã¨ã—ã¦å„ªå…ˆæ¤œç´¢
            if search_term in input_data.dish_names:
                db_type_filter = "dish"
                self.log_processing_detail(f"search_{search_index}_type", "dish")
            elif search_term in input_data.ingredient_names:
                db_type_filter = "ingredient"
                self.log_processing_detail(f"search_{search_index}_type", "ingredient")
            
            # æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
            request = SearchRequest(
                query=search_term,
                db_type_filter=db_type_filter,
                size=5  # ä¸Šä½5ä»¶ã‚’å–å¾—
            )
            
            # æ¤œç´¢å®Ÿè¡Œ
            response = self.search_handler.search(request)
            
            # æ¤œç´¢çµæœã®è©³ç´°ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_results_count", response.total_hits)
            self.log_processing_detail(f"search_{search_index}_processing_time_ms", response.took_ms)
            self.log_processing_detail(f"search_{search_index}_processed_query", response.query_info.get('processed_query'))
            
            if response.results:
                # nutrition_db_experimentã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ãŸå ´åˆã¯ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                best_result = response.results[0]
                
                # æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆIDãŒ123456ã®å ´åˆã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼‰
                if best_result.get('id') == 123456:
                    self.logger.warning(f"nutrition_db_experiment returned mock data for '{search_term}', falling back to direct database search")
                    return await self._direct_database_search(search_term, search_index, input_data)
                
                # ãƒãƒƒãƒé¸æŠã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_result['search_name']}' (ID: {best_result['id']}) for search term '{search_term}' based on local search algorithm (score: {best_result.get('_score', 'N/A')})"
                )
                
                # è©³ç´°ãªãƒãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_selected_id", best_result['id'])
                self.log_processing_detail(f"search_{search_index}_selected_name", best_result['search_name'])
                self.log_processing_detail(f"search_{search_index}_data_type", best_result.get('data_type', 'unknown'))
                self.log_processing_detail(f"search_{search_index}_score", best_result.get('_score'))
                
                # NutritionMatchå½¢å¼ã«å¤‰æ›
                return self._convert_to_nutrition_match(best_result, search_term)
            
            # çµæœãŒãªã„å ´åˆã¯ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._direct_database_search(search_term, search_index, input_data)
            
        except Exception as e:
            self.logger.error(f"Advanced local search failed for '{search_term}': {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._direct_database_search(search_term, search_index, input_data)
    
    async def _direct_database_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        try:
            self.log_processing_detail(f"search_{search_index}_method", "direct_database_search")
            
            search_term_lower = search_term.lower()
            best_match = None
            best_score = 0
            
            # unified_databaseã‹ã‚‰ç›´æ¥æ¤œç´¢
            for item in self.unified_database:
                # search_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§æ¤œç´¢
                if 'search_name' not in item:
                    continue
                    
                item_name = item['search_name'].lower()
                score = 0
                
                # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                if search_term_lower == item_name:
                    score = 1.0  # å®Œå…¨ä¸€è‡´
                elif search_term_lower in item_name:
                    # éƒ¨åˆ†ä¸€è‡´ï¼ˆèªé †è€ƒæ…®ï¼‰
                    if item_name.startswith(search_term_lower):
                        score = 0.9  # å‰æ–¹ä¸€è‡´
                    elif item_name.endswith(search_term_lower):
                        score = 0.8  # å¾Œæ–¹ä¸€è‡´
                    else:
                        score = 0.7  # ä¸­é–“ä¸€è‡´
                elif item_name in search_term_lower:
                    score = 0.6  # é€†éƒ¨åˆ†ä¸€è‡´
                else:
                    # å˜èªãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    search_words = search_term_lower.split()
                    item_words = item_name.split()
                    
                    common_words = set(search_words) & set(item_words)
                    if common_words:
                        score = len(common_words) / max(len(search_words), len(item_words)) * 0.5
                
                # data_typeå„ªå…ˆåº¦ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
                data_type = item.get('data_type', 'unknown')
                db_bonus = 1.0
                
                # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
                if search_term in input_data.dish_names and data_type == 'dish':
                    db_bonus = 1.2
                # ingredient_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯é£Ÿæãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
                elif search_term in input_data.ingredient_names and data_type == 'ingredient':
                    db_bonus = 1.2
                
                final_score = score * db_bonus
                
                if final_score > best_score:
                    best_score = final_score
                    best_match = item.copy()
            
            if best_match and best_score > 0.1:  # æœ€ä½é–¾å€¤
                # ãƒãƒƒãƒã‚¹ã‚³ã‚¢æƒ…å ±ã‚’è¿½åŠ 
                best_match['_match_score'] = best_score
                
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_match['search_name']}' (ID: {best_match.get('id', 'N/A')}) for search term '{search_term}' using direct database search (score: {best_score:.3f})"
                )
                
                # è©³ç´°ãªãƒãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_selected_id", best_match.get('id', 'N/A'))
                self.log_processing_detail(f"search_{search_index}_selected_name", best_match['search_name'])
                self.log_processing_detail(f"search_{search_index}_data_type", best_match.get('data_type', 'unknown'))
                self.log_processing_detail(f"search_{search_index}_match_score", best_score)
                
                return self._convert_to_nutrition_match(best_match, search_term)
            
            return None
            
        except Exception as e:
            self.logger.error(f"Direct database search failed for '{search_term}': {e}")
            return None
    
    async def _simple_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½¿ç”¨ï¼‰
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        # é«˜åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã€ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨
        return await self._direct_database_search(search_term, search_index, input_data)
    
    def _convert_to_nutrition_match(self, local_item: Dict[str, Any], search_term: str) -> NutritionMatch:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã‚’NutritionMatchå½¢å¼ã«å¤‰æ›ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        
        Args:
            local_item: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¤ãƒ†ãƒ 
            search_term: å…ƒã®æ¤œç´¢èªå½™
            
        Returns:
            NutritionMatch: å¤‰æ›ã•ã‚ŒãŸãƒãƒƒãƒçµæœï¼ˆç°¡ç´ åŒ–ã•ã‚ŒãŸãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰
        """
        # IDã®å–å¾—
        item_id = local_item.get('id', 0)
        
        # åŸºæœ¬æƒ…å ±ã®å–å¾—
        search_name = local_item.get('search_name', search_term)
        description = local_item.get('description')  # brandedã®å ´åˆã®ã¿å­˜åœ¨
        data_type = local_item.get('data_type', 'unknown')  # db_type â†’ data_typeã«å¤‰æ›´
        
        # æ „é¤Šãƒ‡ãƒ¼ã‚¿ï¼ˆ100gã‚ãŸã‚Šæ­£è¦åŒ–æ¸ˆã¿ï¼‰
        nutrition = local_item.get('nutrition', {})
        weight = local_item.get('weight')
        
        # ãƒãƒƒãƒã‚¹ã‚³ã‚¢
        score = local_item.get('_match_score') or local_item.get('_score') or 1.0
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ã®è©³ç´°åˆ†æ
        search_term_lower = search_term.lower()
        item_name_lower = search_name.lower()
        
        # åŸºæœ¬ãƒãƒƒãƒã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        match_type = "unknown"
        base_score = 0.0
        if search_term_lower == item_name_lower:
            match_type = "exact_match"
            base_score = 1.0
        elif search_term_lower in item_name_lower:
            if item_name_lower.startswith(search_term_lower):
                match_type = "prefix_match"
                base_score = 0.9
            elif item_name_lower.endswith(search_term_lower):
                match_type = "suffix_match"
                base_score = 0.8
            else:
                match_type = "contains_match"
                base_score = 0.7
        elif item_name_lower in search_term_lower:
            match_type = "reverse_contains"
            base_score = 0.6
        else:
            # å˜èªãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´
            search_words = set(search_term_lower.split())
            item_words = set(item_name_lower.split())
            common_words = search_words & item_words
            if common_words:
                match_type = "word_match"
                base_score = len(common_words) / max(len(search_words), len(item_words)) * 0.5
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ãƒœãƒ¼ãƒŠã‚¹ã®è¨ˆç®—
        type_bonus = 1.0
        if hasattr(self, '_current_input_data'):
            input_data = self._current_input_data
            if search_term in input_data.dish_names and data_type == 'dish':
                type_bonus = 1.2
            elif search_term in input_data.ingredient_names and data_type == 'ingredient':
                type_bonus = 1.2
        
        # æ¤œç´¢ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆè©³ç´°ãªè¨ˆç®—æƒ…å ±ã‚’å«ã‚€ï¼‰
        search_metadata = {
            "search_term": search_term,
            "match_score": score,
            "score_breakdown": {
                "match_type": match_type,
                "base_score": round(base_score, 3),
                "type_bonus": round(type_bonus, 3),
                "final_score": round(base_score * type_bonus, 3)
            },
            "calculation": f"{base_score:.3f} Ã— {type_bonus:.3f} = {score:.3f}"
        }
        
        # NutritionMatchã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        return NutritionMatch(
            id=item_id,
            search_name=search_name,
            description=description,
            data_type=data_type,  # db_type â†’ data_typeã«å¤‰æ›´
            nutrition=nutrition,
            weight=weight,
            score=score,
            search_metadata=search_metadata
        ) 
```

============================================================

ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å±¤
============================================================

ğŸ“„ FILE: app_v2/models/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 680 bytes
æœ€çµ‚æ›´æ–°: 2025-06-09 12:06:09
å­˜åœ¨: âœ…

CONTENT:
```python
from .phase1_models import *
from .usda_models import *
from .phase2_models import *
from .nutrition_models import *
from .nutrition_search_models import *

__all__ = [
    # Phase1 models
    "Phase1Input", "Phase1Output", "Ingredient", "Dish",
    
    # USDA models
    "USDAQueryInput", "USDAQueryOutput", "USDAMatch", "USDANutrient",
    
    # Phase2 models
    "Phase2Input", "Phase2Output", "RefinedDish", "RefinedIngredient",
    
    # Nutrition models
    "NutritionInput", "NutritionOutput", "CalculatedNutrients", "TotalNutrients",
    
    # Nutrition Search models (ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼)
    "NutritionQueryInput", "NutritionQueryOutput", "NutritionMatch"
] 
```

============================================================

ğŸ“„ FILE: app_v2/models/nutrition_search_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 4,037 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 13:45:41
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Nutrition Search Models

ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã§ä½¿ç”¨ã™ã‚‹ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ã®ãƒ¢ãƒ‡ãƒ«
"""

from typing import List, Dict, Optional, Any, Union
from pydantic import BaseModel, Field


class NutritionMatch(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆçµæœãƒ¢ãƒ‡ãƒ«ï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰"""
    id: Union[int, str] = Field(..., description="é£Ÿå“IDï¼ˆãƒ­ãƒ¼ã‚«ãƒ«IDï¼‰")
    search_name: str = Field(..., description="æ¤œç´¢åï¼ˆç°¡æ½”ãªåç§°ï¼‰")
    description: Optional[str] = Field(None, description="è©³ç´°èª¬æ˜")
    data_type: str = Field(..., description="ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (dish, ingredient, branded)")
    source: str = Field(default="local_database", description="ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ'local_database'ï¼‰")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«DBã®ç”Ÿã®æ „é¤Šãƒ‡ãƒ¼ã‚¿ï¼ˆ100gã‚ãŸã‚Šæ­£è¦åŒ–æ¸ˆã¿ï¼‰
    nutrition: Dict[str, float] = Field(default_factory=dict, description="ãƒ­ãƒ¼ã‚«ãƒ«DBã®æ „é¤Šãƒ‡ãƒ¼ã‚¿ï¼ˆ100gã‚ãŸã‚Šï¼‰")
    weight: Optional[float] = Field(None, description="å…ƒãƒ‡ãƒ¼ã‚¿ã®é‡é‡ï¼ˆgï¼‰")
    
    # æ¤œç´¢ã‚¹ã‚³ã‚¢
    score: Optional[float] = Field(None, description="æ¤œç´¢çµæœã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢")
    
    # æ¤œç´¢ã«é–¢ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    search_metadata: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ã«é–¢ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")


class NutritionQueryInput(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰"""
    ingredient_names: List[str] = Field(default_factory=list, description="é£Ÿæåã®ãƒªã‚¹ãƒˆ")
    dish_names: List[str] = Field(default_factory=list, description="æ–™ç†åã®ãƒªã‚¹ãƒˆ")
    search_options: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    preferred_source: str = Field(default="local_database", description="å„ªå…ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")

    def get_all_search_terms(self) -> List[str]:
        """å…¨ã¦ã®æ¤œç´¢èªå½™ã‚’å–å¾—"""
        return list(set(self.ingredient_names + self.dish_names))


class NutritionQueryOutput(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœãƒ¢ãƒ‡ãƒ«ï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰"""
    # ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢å¯¾å¿œï¼šå˜ä¸€çµæœã¾ãŸã¯ãƒªã‚¹ãƒˆçµæœã‚’å—ã‘å…¥ã‚Œã‚‹
    matches: Dict[str, Union[NutritionMatch, List[NutritionMatch]]] = Field(
        default_factory=dict, 
        description="æ¤œç´¢èªå½™ã¨å¯¾å¿œã™ã‚‹ç…§åˆçµæœã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå˜ä¸€çµæœã¾ãŸã¯ãƒãƒ«ãƒDBçµæœãƒªã‚¹ãƒˆï¼‰"
    )
    search_summary: Dict[str, Any] = Field(
        default_factory=dict, 
        description="æ¤œç´¢çµæœã®ã‚µãƒãƒªãƒ¼æƒ…å ±ï¼ˆæŸ”è»Ÿãªå‹å¯¾å¿œï¼‰"
    )
    warnings: Optional[List[str]] = Field(None, description="è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ")
    errors: Optional[List[str]] = Field(None, description="ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ")

    def get_match_rate(self) -> float:
        """ç…§åˆæˆåŠŸç‡ã‚’è¨ˆç®—"""
        total_searches = self.search_summary.get("total_searches", 0)
        successful_matches = self.search_summary.get("successful_matches", 0)
        if total_searches == 0:
            return 0.0
        return successful_matches / total_searches

    def get_total_matches(self) -> int:
        """ç·ç…§åˆä»¶æ•°ã‚’å–å¾—ï¼ˆãƒãƒ«ãƒDBæ¤œç´¢å¯¾å¿œï¼‰"""
        total = 0
        for match_result in self.matches.values():
            if isinstance(match_result, list):
                total += len(match_result)
            else:
                total += 1
        return total
    
    def get_total_individual_results(self) -> int:
        """å€‹åˆ¥çµæœã®ç·æ•°ã‚’å–å¾—ï¼ˆãƒãƒ«ãƒDBæ¤œç´¢ç”¨ï¼‰"""
        return self.get_total_matches()
    
    def has_errors(self) -> bool:
        """ã‚¨ãƒ©ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.errors is not None and len(self.errors) > 0
    
    def has_warnings(self) -> bool:
        """è­¦å‘ŠãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.warnings is not None and len(self.warnings) > 0 
```

============================================================

ğŸ“„ FILE: app_v2/models/phase1_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 1,764 bytes
æœ€çµ‚æ›´æ–°: 2025-06-09 11:27:28
å­˜åœ¨: âœ…

CONTENT:
```python
from typing import List, Optional
from pydantic import BaseModel, Field


class Ingredient(BaseModel):
    """é£Ÿææƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
    ingredient_name: str = Field(..., description="é£Ÿæã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ï¼‰")


class Dish(BaseModel):
    """æ–™ç†æƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
    dish_name: str = Field(..., description="ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ï¼‰")
    ingredients: List[Ingredient] = Field(..., description="ãã®æ–™ç†ã«å«ã¾ã‚Œã‚‹é£Ÿæã®ãƒªã‚¹ãƒˆ")


class Phase1Input(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    image_bytes: bytes = Field(..., description="ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆå½¢å¼ï¼‰")
    image_mime_type: str = Field(..., description="ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—")
    optional_text: Optional[str] = Field(None, description="ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±")

    class Config:
        arbitrary_types_allowed = True


class Phase1Output(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰"""
    dishes: List[Dish] = Field(..., description="ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆ")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_all_ingredient_names(self) -> List[str]:
        """å…¨ã¦ã®é£Ÿæåã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
        ingredient_names = []
        for dish in self.dishes:
            for ingredient in dish.ingredients:
                ingredient_names.append(ingredient.ingredient_name)
        return ingredient_names

    def get_all_dish_names(self) -> List[str]:
        """å…¨ã¦ã®æ–™ç†åã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
        return [dish.dish_name for dish in self.dishes] 
```

============================================================

ğŸ“ è¨­å®šç®¡ç†
============================================================

ğŸ“„ FILE: app_v2/config/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 85 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:46:54
å­˜åœ¨: âœ…

CONTENT:
```python
from .settings import Settings, get_settings

__all__ = ["Settings", "get_settings"] 
```

============================================================

ğŸ“ Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†
============================================================

ğŸ“„ FILE: create_elasticsearch_index.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 8,637 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 13:04:18
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Elasticsearch Index Creation Script

ç¾çŠ¶ã®JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹
"""

import json
import os
from elasticsearch import Elasticsearch
from typing import Dict, List, Any
import time


def create_index_mapping() -> Dict[str, Any]:
    """Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®šç¾©"""
    return {
        "mappings": {
            "properties": {
                "id": {
                    "type": "keyword"
                },
                "search_name": {
                    "type": "text",
                    "analyzer": "standard",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        },
                        "suggest": {
                            "type": "completion"
                        }
                    }
                },
                "description": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "data_type": {
                    "type": "keyword"
                },
                "nutrition": {
                    "type": "object",
                    "properties": {
                        "calories": {"type": "float"},
                        "protein": {"type": "float"},
                        "fat": {"type": "float"},
                        "carbs": {"type": "float"},
                        "carbohydrates": {"type": "float"},
                        "fiber": {"type": "float"},
                        "sugar": {"type": "float"},
                        "sodium": {"type": "float"}
                    }
                },
                "weight": {
                    "type": "float"
                },
                "source_db": {
                    "type": "keyword"
                }
            }
        },
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "food_analyzer": {
                        "type": "standard",
                        "stopwords": "_none_"
                    }
                }
            }
        }
    }


def load_json_databases() -> Dict[str, List[Dict[str, Any]]]:
    """JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    databases = {}
    
    db_configs = {
        "yazio": "db/yazio_db.json",
        "mynetdiary": "db/mynetdiary_db.json", 
        "eatthismuch": "db/eatthismuch_db.json"
    }
    
    for db_name, file_path in db_configs.items():
        try:
            if os.path.exists(file_path):
                print(f"Loading {db_name} from {file_path}...")
                with open(file_path, 'r', encoding='utf-8') as f:
                    database = json.load(f)
                    databases[db_name] = database
                    print(f"âœ… Loaded {db_name}: {len(database)} items")
            else:
                print(f"âš ï¸  File not found: {file_path}")
                databases[db_name] = []
        except Exception as e:
            print(f"âŒ Error loading {db_name}: {e}")
            databases[db_name] = []
    
    return databases


def prepare_document(item: Dict[str, Any], source_db: str) -> Dict[str, Any]:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Elasticsearchç”¨ã«æº–å‚™"""
    doc = {
        "id": item.get("id", 0),
        "search_name": item.get("search_name", ""),
        "description": item.get("description"),
        "data_type": item.get("data_type", "unknown"),
        "nutrition": item.get("nutrition", {}),
        "weight": item.get("weight"),
        "source_db": source_db
    }
    
    # ç©ºã®å€¤ã‚’é™¤å»
    return {k: v for k, v in doc.items() if v is not None}


def bulk_index_documents(es_client: Elasticsearch, index_name: str, documents: List[Dict[str, Any]], batch_size: int = 1000):
    """ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ """
    total_docs = len(documents)
    indexed_count = 0
    
    print(f"ğŸ“¥ Indexing {total_docs} documents in batches of {batch_size}...")
    
    for i in range(0, total_docs, batch_size):
        batch = documents[i:i + batch_size]
        
        # ãƒãƒ«ã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ§‹ç¯‰
        bulk_body = []
        for doc in batch:
            bulk_body.append({
                "index": {
                    "_index": index_name,
                    "_id": f"{doc['source_db']}_{doc['id']}"
                }
            })
            bulk_body.append(doc)
        
        try:
            response = es_client.bulk(body=bulk_body)
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if response.get("errors"):
                error_count = sum(1 for item in response["items"] if "error" in item.get("index", {}))
                print(f"âš ï¸  Batch {i//batch_size + 1}: {error_count} errors in batch")
            
            indexed_count += len(batch)
            print(f"   Progress: {indexed_count}/{total_docs} ({indexed_count/total_docs*100:.1f}%)")
            
        except Exception as e:
            print(f"âŒ Error indexing batch {i//batch_size + 1}: {e}")
    
    print(f"âœ… Indexing completed: {indexed_count} documents")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== Elasticsearch Index Creation ===")
    
    # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    print("\n1. Connecting to Elasticsearch...")
    es_client = Elasticsearch(["http://localhost:9200"])
    
    if not es_client.ping():
        print("âŒ Cannot connect to Elasticsearch. Make sure it's running on localhost:9200")
        return False
    
    print("âœ… Connected to Elasticsearch")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
    index_name = "nutrition_db"
    
    # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‰Šé™¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    print(f"\n2. Checking existing index '{index_name}'...")
    if es_client.indices.exists(index=index_name):
        print(f"   Index '{index_name}' already exists. Deleting...")
        es_client.indices.delete(index=index_name)
        print("   âœ… Deleted existing index")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    print(f"\n3. Creating index '{index_name}'...")
    mapping = create_index_mapping()
    es_client.indices.create(index=index_name, body=mapping)
    print("âœ… Index created with mapping")
    
    # JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
    print("\n4. Loading JSON databases...")
    databases = load_json_databases()
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æº–å‚™
    print("\n5. Preparing documents for indexing...")
    all_documents = []
    
    for db_name, items in databases.items():
        print(f"   Processing {db_name}: {len(items)} items")
        for item in items:
            if "search_name" in item:  # æœ‰åŠ¹ãªã‚¢ã‚¤ãƒ†ãƒ ã®ã¿
                doc = prepare_document(item, db_name)
                all_documents.append(doc)
    
    print(f"âœ… Prepared {len(all_documents)} documents for indexing")
    
    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    print("\n6. Bulk indexing documents...")
    start_time = time.time()
    bulk_index_documents(es_client, index_name, all_documents)
    end_time = time.time()
    
    print(f"âœ… Indexing completed in {end_time - start_time:.2f} seconds")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆã®è¡¨ç¤º
    print("\n7. Index statistics...")
    stats = es_client.indices.stats(index=index_name)
    doc_count = stats["indices"][index_name]["total"]["docs"]["count"]
    index_size = stats["indices"][index_name]["total"]["store"]["size_in_bytes"]
    
    print(f"   Total documents: {doc_count}")
    print(f"   Index size: {index_size / 1024 / 1024:.2f} MB")
    
    # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\n8. Testing sample search...")
    test_query = {
        "query": {
            "multi_match": {
                "query": "chicken",
                "fields": ["search_name", "search_name.exact"]
            }
        },
        "size": 3
    }
    
    response = es_client.search(index=index_name, body=test_query)
    hits = response["hits"]["hits"]
    
    print(f"   Sample search for 'chicken': {len(hits)} results")
    for hit in hits:
        source = hit["_source"]
        print(f"   - {source['search_name']} ({source['source_db']}) score: {hit['_score']:.2f}")
    
    print(f"\nğŸ‰ Elasticsearch index '{index_name}' successfully created!")
    print(f"   Ready for high-speed nutrition search")
    
    return True


if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… Index creation completed successfully!")
    else:
        print("\nâŒ Index creation failed!") 
```

============================================================

ğŸ“ æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
============================================================

ğŸ“„ FILE: db/yazio_db.json
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 504,128 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 11:55:48
å­˜åœ¨: âœ…

CONTENT (æœ€åˆã®50è¡Œ):
```json
[
  {
    "data_type": "unified",
    "id": 1000000000,
    "search_name": "Cheese Coffeecake",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 339.0,
      "protein": 7.0,
      "fat": 15.2,
      "carbs": 44.3
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000001,
    "search_name": "Fruit Fried Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 316.0,
      "protein": 3.0,
      "fat": 16.1,
      "carbs": 42.6
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000002,
    "search_name": "Blueberry Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 245.0,
      "protein": 2.7,
      "fat": 11.9,
      "carbs": 33.5
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000003,
    "search_name": "Apple Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 265.0,
      "protein": 2.4,
      "fat": 12.5,
      "carbs": 37.1

... (23677 more lines)
```

============================================================

ğŸ“„ FILE: db/mynetdiary_db.json
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 349,780 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 11:55:05
å­˜åœ¨: âœ…

CONTENT (æœ€åˆã®50è¡Œ):
```json
[
  {
    "data_type": "unified",
    "id": 10000000000,
    "search_name": "Beans baked canned plain or vegetarian",
    "description": null,
    "nutrition": {
      "calories": 94.09448818897638,
      "protein": 4.724409448818898,
      "fat": 0.35433070866141736,
      "carbs": 21.25984251968504
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000001,
    "search_name": "Black beans boiled without salt",
    "description": null,
    "nutrition": {
      "calories": 131.97674418604652,
      "protein": 8.72093023255814,
      "fat": 0.5232558139534884,
      "carbs": 23.837209302325583
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000002,
    "search_name": "Black beans canned low sodium",
    "description": null,
    "nutrition": {
      "calories": 90.83333333333334,
      "protein": 5.833333333333334,
      "fat": 0.2916666666666667,
      "carbs": 16.666666666666668
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000003,
    "search_name": "Black beans canned no salt added",
    "description": null,
    "nutrition": {
      "calories": 84.61538461538461,
      "protein": 5.384615384615385,
      "fat": 0.0,
      "carbs": 16.153846153846153

... (14798 more lines)
```

============================================================

ğŸ“„ FILE: db/eatthismuch_db.json
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,809,842 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 11:50:01
å­˜åœ¨: âœ…

CONTENT (æœ€åˆã®50è¡Œ):
```json
[
  {
    "data_type": "dish",
    "id": 907072,
    "search_name": "Garlic and Cream Cheese Cauliflower \"Mashed Potatoes\"",
    "description": null,
    "nutrition": {
      "calories": 43.63999999999999,
      "protein": 2.5545171339563866,
      "fat": 1.6926272066458983,
      "carbs": 5.7632398753894085
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 905725,
    "search_name": "Poached Eggs on Toast",
    "description": null,
    "nutrition": {
      "calories": 211.81,
      "protein": 12.653061224489797,
      "fat": 6.530612244897958,
      "carbs": 24.89795918367347
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 3267515,
    "search_name": "Caprese Salad",
    "description": null,
    "nutrition": {
      "calories": 174.96289228159455,
      "protein": 6.530958439355386,
      "fat": 15.436810856658182,
      "carbs": 3.307888040712468
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 906392,
    "search_name": "Spinach and Pear Omelet",
    "description": null,
    "nutrition": {
      "calories": 105.17,
      "protein": 5.143651529193697,
      "fat": 6.580166821130676,
      "carbs": 7.298424467099165

... (115366 more lines)
```

============================================================

ğŸ“ Elasticsearchè¨­å®š
============================================================

ğŸ“„ FILE: elasticsearch-8.10.4/config/elasticsearch.yml
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 3,205 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 16:30:26
å­˜åœ¨: âœ…

CONTENT (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«):
```yaml
# ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
cluster.name: meal-analysis-dev
#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
node.name: node-1
#
# Add custom attributes to the node:
#
#node.attr.rack: r1
#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
#path.data: /path/to/data
#
# Path to log files:
#
#path.logs: /path/to/logs
#
# ----------------------------------- Memory -----------------------------------
#
# Lock the memory on startup:
#
#bootstrap.memory_lock: true
#
# Make sure that the heap size is set to about half the memory available
# on the system and that the owner of the process is allowed to use this
# limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
#
# ---------------------------------- Network -----------------------------------
#
# By default Elasticsearch is only accessible on localhost. Set a different
# address here to expose this node on the network:
#
network.host: 127.0.0.1
#
# By default Elasticsearch listens for HTTP traffic on the first free port it
# finds starting at 9200. Set a specific HTTP port here:
#
http.port: 9200
#
# For more information, consult the network module documentation.
#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when this node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
discovery.type: single-node
#
# Bootstrap the cluster using an initial set of master-eligible nodes:
# single-nodeãƒ¢ãƒ¼ãƒ‰ã§ã¯ cluster.initial_master_nodes ã¯ä¸è¦ï¼ˆç«¶åˆã™ã‚‹ï¼‰
#
# cluster.initial_master_nodes: ["node-1"]
#
# For more information, consult the discovery and cluster formation module documentation.
#
# ---------------------------------- Various -----------------------------------
#
# Allow wildcard deletion of indices:
#
action.destructive_requires_name: false

# ================================ Security ===================================
#
# é–‹ç™ºç’°å¢ƒç”¨: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’ç„¡åŠ¹åŒ–
#
xpack.security.enabled: false
xpack.security.http.ssl.enabled: false
xpack.security.transport.ssl.enabled: false

# ================================ Machine Learning ============================
#
# é–‹ç™ºç’°å¢ƒç”¨: MLã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒªã‚½ãƒ¼ã‚¹ç¯€ç´„ï¼‰
#
xpack.ml.enabled: false
```

============================================================

ğŸ“ ä¾å­˜é–¢ä¿‚ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
============================================================

ğŸ“„ FILE: requirements.txt
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 274 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 13:01:13
å­˜åœ¨: âœ…

CONTENT:
```python
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
google-cloud-aiplatform==1.94.0
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
httpx
pytest==7.4.3
pytest-asyncio==0.21.1
python-dotenv==1.0.0
Pillow==11.2.1
elasticsearch==8.15.1 
```

============================================================

ğŸ“„ FILE: README.md
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 15,369 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 14:55:33
å­˜åœ¨: âœ…

CONTENT:
```python
# é£Ÿäº‹åˆ†æ API (Meal Analysis API) v2.0

## æ¦‚è¦

ã“ã® API ã¯ã€**Google Gemini AI** ã¨ **Elasticsearch ãƒ™ãƒ¼ã‚¹ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ **ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªé£Ÿäº‹ç”»åƒåˆ†æã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚**å‹•çš„æ „é¤Šè¨ˆç®—æ©Ÿèƒ½**ã«ã‚ˆã‚Šã€æ–™ç†ã®ç‰¹æ€§ã«å¿œã˜ã¦æœ€é©ãªæ „é¤Šè¨ˆç®—æˆ¦ç•¥ã‚’è‡ªå‹•é¸æŠã—ã€æ­£ç¢ºãªæ „é¤Šä¾¡æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸŒŸ ä¸»ãªæ©Ÿèƒ½

### **ğŸ”¥ æ–°æ©Ÿèƒ½: Elasticsearch ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ v2.0**

- **âš¡ Elasticsearch é«˜é€Ÿæ¤œç´¢**: é«˜æ€§èƒ½ãª Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ˆã‚‹å¤§è¦æ¨¡æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
- **ğŸ“Š 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆæ¤œç´¢**: 1 ã¤ã®ã‚¯ã‚¨ãƒªã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰åŒ…æ‹¬çš„ãªæ „é¤Šæƒ…å ±ã‚’å–å¾—
  - **YAZIO**: 1,825 é …ç›® - ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé£Ÿå“ã‚«ãƒ†ã‚´ãƒª
  - **MyNetDiary**: 1,142 é …ç›® - ç§‘å­¦çš„/æ „é¤Šå­¦çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
  - **EatThisMuch**: 8,878 é …ç›® - æœ€å¤§ã‹ã¤æœ€ã‚‚åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- **ğŸ” ãƒãƒ«ãƒ DB æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰**: å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€å¤§ 5 ä»¶ãšã¤ã€ç·åˆçš„ãªæ¤œç´¢çµæœã‚’æä¾›
- **ğŸ¯ é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°**: 90.9%ã®æˆåŠŸç‡ã€å„ DB ã‹ã‚‰å‡ç­‰ãªçµæœåˆ†æ•£
- **ğŸ’¾ è©³ç´°çµæœä¿å­˜**: JSONãƒ»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®æ¤œç´¢çµæœè‡ªå‹•ä¿å­˜

### **å¾“æ¥æ©Ÿèƒ½: å‹•çš„æ „é¤Šè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ **

- **ğŸ§  AI é§†å‹•ã®è¨ˆç®—æˆ¦ç•¥æ±ºå®š**: Gemini AI ãŒå„æ–™ç†ã«å¯¾ã—ã¦æœ€é©ãªæ „é¤Šè¨ˆç®—æ–¹æ³•ã‚’è‡ªå‹•é¸æŠ
- **ğŸ¯ é«˜ç²¾åº¦æ „é¤Šè¨ˆç®—**: é£Ÿæé‡é‡ Ã— 100g ã‚ãŸã‚Šæ „é¤Šä¾¡ã§æ­£ç¢ºãªå®Ÿæ „é¤Šä¾¡ã‚’ç®—å‡º
- **ğŸ“Š 3 å±¤é›†è¨ˆã‚·ã‚¹ãƒ†ãƒ **: é£Ÿæ â†’ æ–™ç† â†’ é£Ÿäº‹å…¨ä½“ã®è‡ªå‹•æ „é¤Šé›†è¨ˆ

### **ã‚³ã‚¢æ©Ÿèƒ½**

- **ãƒ•ã‚§ãƒ¼ã‚º 1**: Gemini AI ã«ã‚ˆã‚‹é£Ÿäº‹ç”»åƒã®åˆ†æï¼ˆæ–™ç†è­˜åˆ¥ã€é£ŸææŠ½å‡ºã€é‡é‡æ¨å®šï¼‰
- **ãƒãƒ«ãƒ DB æ¤œç´¢**: 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®åŒ…æ‹¬çš„æ „é¤Šæƒ…å ±å–å¾—
- **è¤‡æ•°æ–™ç†å¯¾å¿œ**: 1 æšã®ç”»åƒã§è¤‡æ•°ã®æ–™ç†ã‚’åŒæ™‚åˆ†æ
- **è‹±èªãƒ»æ—¥æœ¬èªå¯¾å¿œ**: å¤šè¨€èªã§ã®é£Ÿæãƒ»æ–™ç†èªè­˜
- **OpenAPI 3.0 æº–æ‹ **: å®Œå…¨ãª API æ–‡æ›¸åŒ–ã¨ã‚¿ã‚¤ãƒ—å®‰å…¨æ€§

## ğŸ— ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
meal_analysis_api_2/
â”œâ”€â”€ db/                                   # ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
â”‚   â”œâ”€â”€ yazio_db.json                     # YAZIOæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ1,825é …ç›®ï¼‰
â”‚   â”œâ”€â”€ mynetdiary_db.json                # MyNetDiaryæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ1,142é …ç›®ï¼‰
â”‚   â””â”€â”€ eatthismuch_db.json               # EatThisMuchæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ8,878é …ç›®ï¼‰
â”œâ”€â”€ app_v2/                               # æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆ
â”‚   â”œâ”€â”€ components/                       # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ™ãƒ¼ã‚¹è¨­è¨ˆ
â”‚   â”‚   â”œâ”€â”€ local_nutrition_search_component.py  # ãƒãƒ«ãƒDBæ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â”œâ”€â”€ phase1_component.py           # ç”»åƒåˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â””â”€â”€ base.py                       # ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”œâ”€â”€ pipeline/                         # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ orchestrator.py               # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
â”‚   â”‚   â””â”€â”€ result_manager.py             # çµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
â”‚   â”œâ”€â”€ models/                           # ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â”œâ”€â”€ nutrition_search_models.py    # æ „é¤Šæ¤œç´¢ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â””â”€â”€ phase1_models.py              # Phase1ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ app.py                        # FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â””â”€â”€ config/                           # è¨­å®šç®¡ç†
â”œâ”€â”€ test_multi_db_nutrition_search.py     # ãƒãƒ«ãƒDBæ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰
â”œâ”€â”€ test_local_nutrition_search_v2.py     # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ test_images/                          # ãƒ†ã‚¹ãƒˆç”¨ç”»åƒ
â””â”€â”€ requirements.txt                      # Pythonä¾å­˜é–¢ä¿‚
```

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python -m venv venv

# ä»®æƒ³ç’°å¢ƒã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source venv/bin/activate  # macOS/Linux
# ã¾ãŸã¯
venv\Scripts\activate     # Windows

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

### 2. Google Cloud è¨­å®š

#### Google Cloud SDK ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ã¾ã ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ãªã„å ´åˆã¯ã€ä»¥ä¸‹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š
https://cloud.google.com/sdk/docs/install

#### Google Cloud èªè¨¼ã®è¨­å®š

é–‹ç™ºç’°å¢ƒã§ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§èªè¨¼ã‚’è¨­å®šï¼š

```bash
# Google Cloudã«ãƒ­ã‚°ã‚¤ãƒ³
gcloud auth login

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼æƒ…å ±ã‚’è¨­å®š
gcloud auth application-default login

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’è¨­å®š
gcloud config set project YOUR_PROJECT_ID
```

æœ¬ç•ªç’°å¢ƒã§ã¯ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼š

```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your-service-account-key.json"
```

#### Vertex AI API ã®æœ‰åŠ¹åŒ–

```bash
# Vertex AI APIã‚’æœ‰åŠ¹åŒ–
gcloud services enable aiplatform.googleapis.com
```

### 3. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š

```bash
# USDA APIè¨­å®š
export USDA_API_KEY="your-usda-api-key"

# Vertex AIè¨­å®š
export GOOGLE_APPLICATION_CREDENTIALS="path/to/service-account-key.json"
export GEMINI_PROJECT_ID="your-gcp-project-id"
export GEMINI_LOCATION="us-central1"
export GEMINI_MODEL_NAME="gemini-2.5-flash-preview-05-20"
```

## ğŸ–¥ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•

### app_v2 ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ï¼ˆãƒãƒ«ãƒ DB å¯¾å¿œï¼‰

```bash
# app_v2ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
python -m app_v2.main.app
```

**âš ï¸ æ³¨æ„**: ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚ã€å¿…ãšãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å½¢å¼ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã™ã‚‹ã¨ã€ä»¥ä¸‹ã® URL ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ãªã‚Šã¾ã™ï¼š

- **API**: http://localhost:8000
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: http://localhost:8000/docs
- **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯**: http://localhost:8000/health

## ğŸ§ª ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

### ğŸ”¥ ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆæœ€æ–°æ©Ÿèƒ½ï¼‰

**é‡è¦**: ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹çŠ¶æ…‹ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```bash
# åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œ
python test_multi_db_nutrition_search.py
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

- **æ¤œç´¢é€Ÿåº¦**: 11 ã‚¯ã‚¨ãƒªã‚’ 0.10 ç§’ã§å‡¦ç†
- **ãƒãƒƒãƒç‡**: å„ DB90.9%ã®ã‚¯ã‚¨ãƒªã§çµæœç™ºè¦‹
- **ç·ãƒãƒƒãƒæ•°**: 87 ä»¶ï¼ˆå¹³å‡ 7.9 ä»¶/ã‚¯ã‚¨ãƒªï¼‰
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ**:
  - YAZIO: 1,825 é …ç›®
  - MyNetDiary: 1,142 é …ç›®
  - EatThisMuch: 8,878 é …ç›®

**ãƒ†ã‚¹ãƒˆçµæœä¾‹**:

```
ğŸ“ˆ Multi-Database Search Results Summary:
- Total queries: 11
- Total matches found: 87
- Average matches per query: 7.9
- Search time: 0.10s

ğŸ” Detailed Query Results:
1. 'Roasted Potatoes' (dish)
   EatThisMuch: 3 matches
     Best: 'Roasted Potatoes' (score: 1.000)
     Nutrition: 91.0 kcal, 1.9g protein
```

### ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆ

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµ±åˆãƒ†ã‚¹ãƒˆ
python test_local_nutrition_search_v2.py
```

### åŸºæœ¬ãƒ†ã‚¹ãƒˆï¼ˆãƒ•ã‚§ãƒ¼ã‚º 1 ã®ã¿ï¼‰

```bash
python test_phase1_only.py
```

## ğŸš€ ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  v2.0

### **æ–°æ©Ÿèƒ½: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ**

ã‚·ã‚¹ãƒ†ãƒ ãŒ USDA API ä¾å­˜ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«å¯¾å¿œã—ã¾ã—ãŸï¼š

- **ğŸ” BM25F + ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ¤œç´¢**: é«˜ç²¾åº¦ãªé£Ÿæãƒãƒƒãƒãƒ³ã‚°
- **ğŸ“Š 8,878 é …ç›®ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ „é¤Šè¨ˆç®—å¯¾å¿œ
- **âš¡ 90.9%ãƒãƒƒãƒç‡**: å®Ÿæ¸¬å€¤ã«ã‚ˆã‚‹é«˜ã„æˆåŠŸç‡
- **ğŸ”„ USDA äº’æ›æ€§**: æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®å®Œå…¨äº’æ›æ€§ç¶­æŒ

### ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆv2.0 å¯¾å¿œï¼‰

```bash
# app_v2ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
python -m app_v2.main.app
```

### ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆ

**é‡è¦**: ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹çŠ¶æ…‹ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµ±åˆãƒ†ã‚¹ãƒˆ
python test_local_nutrition_search_v2.py
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

- **ãƒãƒƒãƒç‡**: 90.9% (10/11 æ¤œç´¢æˆåŠŸ)
- **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: ~11 ç§’
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ (8,878 é …ç›®)
- **æ¤œç´¢æ–¹æ³•**: BM25F + ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°

**ãƒ†ã‚¹ãƒˆçµæœä¾‹**:

```
ğŸ” Local Nutrition Search Results:
- Matches found: 10
- Match rate: 90.9%
- Search method: local_nutrition_database
- Total searches: 11
- Successful matches: 10

ğŸ½ Final Meal Nutrition:
- Calories: 400.00 kcal
- Protein: 60.00 g
- Carbohydrates: 220.00 g
- Fat: 120.00 g
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©³ç´°

**ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹æˆ**:

- `dish_db.json`: 4,583 æ–™ç†ãƒ‡ãƒ¼ã‚¿
- `ingredient_db.json`: 1,473 é£Ÿæãƒ‡ãƒ¼ã‚¿
- `branded_db.json`: 2,822 ãƒ–ãƒ©ãƒ³ãƒ‰é£Ÿå“
- `unified_nutrition_db.json`: 8,878 çµ±åˆãƒ‡ãƒ¼ã‚¿

## ğŸ“¡ API ä½¿ç”¨æ–¹æ³•

### ğŸ”¥ å®Œå…¨åˆ†æ (æ¨å¥¨): å…¨ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆ

**1 ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å…¨ã¦ã®åˆ†æã‚’å®Ÿè¡Œ**

```bash
curl -X POST "http://localhost:8000/api/v1/meal-analyses/complete" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg"
```

ã“ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã‚’è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ï¼š

- ãƒ•ã‚§ãƒ¼ã‚º 1: ç”»åƒåˆ†æ
- USDA ç…§åˆ: é£Ÿæãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
- ãƒ•ã‚§ãƒ¼ã‚º 2: è¨ˆç®—æˆ¦ç•¥æ±ºå®š
- æ „é¤Šè¨ˆç®—: æœ€çµ‚æ „é¤Šä¾¡ç®—å‡º
- çµæœä¿å­˜: è‡ªå‹•çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

**ä¿å­˜ã•ã‚ŒãŸçµæœã®å–å¾—**

```bash
# å…¨çµæœä¸€è¦§
curl "http://localhost:8000/api/v1/meal-analyses/results"

# ç‰¹å®šã®çµæœå–å¾—
curl "http://localhost:8000/api/v1/meal-analyses/results/{analysis_id}"
```

### ãƒ•ã‚§ãƒ¼ã‚º 1: åŸºæœ¬åˆ†æ

```bash
curl -X POST "http://localhost:8000/api/v1/meal-analyses" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg"
```

### ãƒ•ã‚§ãƒ¼ã‚º 2: å‹•çš„æ „é¤Šè¨ˆç®—

```bash
# æœ€åˆã«ãƒ•ã‚§ãƒ¼ã‚º1ã®çµæœã‚’å–å¾—
initial_result=$(curl -X POST "http://localhost:8000/api/v1/meal-analyses" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg")

# ãƒ•ã‚§ãƒ¼ã‚º2ã§å‹•çš„æ „é¤Šè¨ˆç®—
curl -X POST "http://localhost:8000/api/v1/meal-analyses/refine" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg" \
  -F "initial_analysis_data=$initial_result"
```

## ğŸ“‹ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

### ãƒ•ã‚§ãƒ¼ã‚º 1 ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "dishes": [
    {
      "dish_name": "Fried Fish with Spaghetti and Tomato Sauce",
      "type": "Main Dish",
      "quantity_on_plate": "2 pieces of fish, 1 small serving of spaghetti",
      "ingredients": [
        {
          "ingredient_name": "White Fish Fillet",
          "weight_g": 150.0
        },
        {
          "ingredient_name": "Spaghetti (cooked)",
          "weight_g": 80.0
        }
      ]
    }
  ]
}
```

### ãƒ•ã‚§ãƒ¼ã‚º 2 ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆå‹•çš„æ „é¤Šè¨ˆç®—ï¼‰

```json
{
  "dishes": [
    {
      "dish_name": "Spinach and Daikon Radish Aemono",
      "type": "Side Dish",
      "calculation_strategy": "ingredient_level",
      "fdc_id": null,
      "ingredients": [
        {
          "ingredient_name": "Spinach",
          "weight_g": 80.0,
          "fdc_id": 1905313,
          "usda_source_description": "SPINACH",
          "key_nutrients_per_100g": {
            "calories_kcal": 24.0,
            "protein_g": 3.53,
            "carbohydrates_g": 3.53,
            "fat_g": 0.0
          },
          "actual_nutrients": {
            "calories_kcal": 19.2,
            "protein_g": 2.82,
            "carbohydrates_g": 2.82,
            "fat_g": 0.0
          }
        }
      ],
      "dish_total_actual_nutrients": {
        "calories_kcal": 57.45,
        "protein_g": 3.85,
        "carbohydrates_g": 4.57,
        "fat_g": 3.31
      }
    },
    {
      "dish_name": "Green Tea",
      "type": "Drink",
      "calculation_strategy": "dish_level",
      "fdc_id": 1810668,
      "usda_source_description": "GREEN TEA",
      "key_nutrients_per_100g": {
        "calories_kcal": 0.0,
        "protein_g": 0.0,
        "carbohydrates_g": 0.0,
        "fat_g": 0.0
      },
      "dish_total_actual_nutrients": {
        "calories_kcal": 0.0,
        "protein_g": 0.0,
        "carbohydrates_g": 0.0,
        "fat_g": 0.0
      }
    }
  ],
  "total_meal_nutrients": {
    "calories_kcal": 337.95,
    "protein_g": 13.32,
    "carbohydrates_g": 56.19,
    "fat_g": 6.67
  },
  "warnings": null,
  "errors": null
}
```

## ğŸ”§ æŠ€è¡“ä»•æ§˜

### å‹•çš„è¨ˆç®—æˆ¦ç•¥ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯

**Dish Level (`dish_level`)**:

- ã‚·ãƒ³ãƒ—ãƒ«ãªå˜å“é£Ÿå“ï¼ˆæœç‰©ã€é£²ã¿ç‰©ã€åŸºæœ¬é£Ÿæï¼‰
- æ¨™æº–åŒ–ã•ã‚ŒãŸæ—¢è£½å“ã§é©åˆ‡ãª USDA ID ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
- ä¾‹: ç·‘èŒ¶ã€ã‚Šã‚“ã”ã€ç™½ç±³

**Ingredient Level (`ingredient_level`)**:

- è¤‡é›‘ãªèª¿ç†æ¸ˆã¿æ–™ç†ï¼ˆç‚’ã‚ç‰©ã€ã‚µãƒ©ãƒ€ã€ã‚¹ãƒ¼ãƒ—ï¼‰
- è¤‡æ•°é£Ÿæã®çµ„ã¿åˆã‚ã›ã§æ–™ç†å…¨ä½“ã® USDA ID ãŒä¸é©åˆ‡ãªå ´åˆ
- ä¾‹: é‡èœç‚’ã‚ã€æ‰‹ä½œã‚Šã‚µãƒ©ãƒ€ã€å‘³å™Œæ±

### æ „é¤Šè¨ˆç®—å¼

```
å®Ÿæ „é¤Šä¾¡ = (100gã‚ãŸã‚Šæ „é¤Šä¾¡ Ã· 100) Ã— æ¨å®šé‡é‡(g)
```

### é›†è¨ˆéšå±¤

1. **é£Ÿæãƒ¬ãƒ™ãƒ«**: å€‹åˆ¥é£Ÿæã®é‡é‡ Ã— 100g æ „é¤Šä¾¡
2. **æ–™ç†ãƒ¬ãƒ™ãƒ«**: é£Ÿæãƒ¬ãƒ™ãƒ«ã®åˆè¨ˆ ã¾ãŸã¯ æ–™ç†å…¨ä½“è¨ˆç®—
3. **é£Ÿäº‹ãƒ¬ãƒ™ãƒ«**: å…¨æ–™ç†ã®æ „é¤Šä¾¡åˆè¨ˆ

## âš ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

API ã¯ä»¥ä¸‹ã® HTTP ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã—ã¾ã™ï¼š

- `200 OK`: æ­£å¸¸ãªåˆ†æå®Œäº†
- `400 Bad Request`: ä¸æ­£ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆç”»åƒå½¢å¼ã‚¨ãƒ©ãƒ¼ãªã©ï¼‰
- `422 Unprocessable Entity`: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼
- `503 Service Unavailable`: å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆUSDA/Geminiï¼‰ã‚¨ãƒ©ãƒ¼
- `500 Internal Server Error`: ã‚µãƒ¼ãƒãƒ¼å†…éƒ¨ã‚¨ãƒ©ãƒ¼

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### èªè¨¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆ

```bash
# ç¾åœ¨ã®èªè¨¼çŠ¶æ…‹ã‚’ç¢ºèª
gcloud auth list

# ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã‚’ç¢ºèª
gcloud config list

# å¿…è¦ã«å¿œã˜ã¦å†åº¦èªè¨¼
gcloud auth application-default login
```

### Vertex AI API ãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ãªã„å ´åˆ

```bash
# APIã®æœ‰åŠ¹çŠ¶æ³ã‚’ç¢ºèª
gcloud services list --enabled | grep aiplatform

# æœ‰åŠ¹ã§ãªã„å ´åˆã¯æœ‰åŠ¹åŒ–
gcloud services enable aiplatform.googleapis.com
```

### USDA API ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆ

- API ã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
- ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆï¼ˆ3,600 ä»¶/æ™‚ï¼‰ã«é”ã—ã¦ã„ãªã„ã‹ç¢ºèª
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª

## ğŸ’» é–‹ç™ºæƒ…å ±

- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: FastAPI 0.104+
- **AI ã‚µãƒ¼ãƒ“ã‚¹**: Google Vertex AI (Gemini 2.5 Flash)
- **æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: USDA FoodData Central API
- **èªè¨¼**: Google Cloud ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
- **Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 3.9+
- **ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
  - `google-cloud-aiplatform` (Vertex AI)
  - `httpx` (éåŒæœŸ HTTP)
  - `pydantic` (ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³)
  - `pillow` (ç”»åƒå‡¦ç†)

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ MIT ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## æ³¨æ„äº‹é …

**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: API ã‚­ãƒ¼ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã¯çµ¶å¯¾ã«ãƒªãƒã‚¸ãƒˆãƒªã«ã‚³ãƒŸãƒƒãƒˆã—ãªã„ã§ãã ã•ã„ã€‚ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦å®‰å…¨ã«ç®¡ç†ã—ã¦ãã ã•ã„ã€‚

```

============================================================

ğŸ¯ STRATEGIC MULTI-DB SEARCH ANALYSIS SUMMARY
--------------------------------------------------
ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 22
å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 22
åˆ†æå®Œäº†æ™‚åˆ»: 2025-06-10 17:11:57

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€test_multi_db_nutrition_search.pyå®Ÿè¡Œæ™‚ã«
é–¢ã‚ã‚‹æˆ¦ç•¥çš„ãƒãƒ«ãƒDB Elasticsearchæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
ãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œå…¨ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

ğŸ”¥ STRATEGIC SEARCH SYSTEM HIGHLIGHTS:
- ğŸ½ï¸  Dishæˆ¦ç•¥: EatThisMuch dish â†’ branded fallback
- ğŸ¥• Ingredientæˆ¦ç•¥: EatThisMuch ingredient â†’ Multi-DB fallback
- âš¡ 44%é«˜é€ŸåŒ–: 677ms â†’ 381ms
- ğŸ“Š çµæœæœ€é©åŒ–: 144ä»¶ â†’ 50ä»¶ (é–¢é€£æ€§é‡è¦–)
- ğŸ¯ æˆ¦ç•¥çš„åˆ†æ•£: EatThisMuchä¸­å¿ƒå‹ (72%)
- ğŸ” ã‚¹ã‚³ã‚¢é–¾å€¤: å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (20.0)
- ğŸ’¾ è©³ç´°è¿½è·¡: æˆ¦ç•¥ãƒ•ã‚§ãƒ¼ã‚ºãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
- ğŸš€ ComponentåŒ–: æ‹¡å¼µå¯èƒ½ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
