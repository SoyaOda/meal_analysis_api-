#!/usr/bin/env python3
"""
Raw Nutrition Data Analysis Script

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯raw_nutrition_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å…¨ã¦ã®å†…å®¹ã‚’åˆ†æã—ã€
å„ã‚«ãƒ†ã‚´ãƒªï¼ˆfood, branded, restaurant, recipeï¼‰ã®çµ±è¨ˆæƒ…å ±ã€
processed.jsonã‚µãƒ³ãƒ—ãƒ«ã€JSONã‚­ãƒ¼ã®çµ±ä¸€æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦
è©³ç´°ãªMarkdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
"""

import os
import json
import glob
from pathlib import Path
from datetime import datetime
from collections import defaultdict, Counter
import traceback

def count_directories(base_path):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã®ç›´ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    try:
        path = Path(base_path)
        if not path.exists():
            return 0
        return len([item for item in path.iterdir() if item.is_dir() and not item.name.startswith('.')])
    except Exception as e:
        print(f"Error counting directories in {base_path}: {e}")
        return 0

def find_processed_json_files(category_path):
    """æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªãƒ‘ã‚¹å†…ã®å…¨ã¦ã®processed/*.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
    processed_files = []
    try:
        pattern = os.path.join(category_path, "*", "processed", "*.json")
        files = glob.glob(pattern)
        processed_files = [Path(f) for f in files]
        print(f"Found {len(processed_files)} processed JSON files in {category_path}")
    except Exception as e:
        print(f"Error finding processed files in {category_path}: {e}")
    return processed_files

def load_json_sample(file_path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¿”ã™"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"Error loading JSON from {file_path}: {e}")
        return None

def extract_json_keys(data, prefix="", max_depth=3, current_depth=0):
    """JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨ã¦ã®ã‚­ãƒ¼ã‚’å†å¸°çš„ã«æŠ½å‡ºï¼ˆæ·±ã•åˆ¶é™ä»˜ãï¼‰"""
    keys = set()
    if current_depth >= max_depth:
        return keys
    
    if isinstance(data, dict):
        for key, value in data.items():
            full_key = f"{prefix}.{key}" if prefix else key
            keys.add(full_key)
            if isinstance(value, (dict, list)) and current_depth < max_depth - 1:
                keys.update(extract_json_keys(value, full_key, max_depth, current_depth + 1))
    elif isinstance(data, list) and len(data) > 0:
        # ãƒªã‚¹ãƒˆã®æœ€åˆã®è¦ç´ ã‚’åˆ†æ
        keys.update(extract_json_keys(data[0], prefix, max_depth, current_depth + 1))
    
    return keys

def analyze_category(category_name, category_path):
    """ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†æ"""
    print(f"\nğŸ” Analyzing {category_name} category...")
    
    analysis = {
        'name': category_name,
        'path': category_path,
        'total_directories': 0,
        'processed_files': [],
        'samples': [],
        'all_keys': set(),
        'key_frequency': Counter(),
        'errors': []
    }
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    analysis['total_directories'] = count_directories(category_path)
    
    # processed.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    analysis['processed_files'] = find_processed_json_files(category_path)
    
    # ã‚µãƒ³ãƒ—ãƒ«3å€‹ã‚’å–å¾—
    sample_count = min(3, len(analysis['processed_files']))
    for i in range(sample_count):
        file_path = analysis['processed_files'][i]
        data = load_json_sample(file_path)
        if data is not None:
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            analysis['samples'].append({
                'file_path': str(file_path),
                'data': data
            })
            
            # ã‚­ãƒ¼ã‚’æŠ½å‡º
            keys = extract_json_keys(data)
            analysis['all_keys'].update(keys)
            for key in keys:
                analysis['key_frequency'][key] += 1
        else:
            analysis['errors'].append(f"Failed to load {file_path}")
    
    return analysis

def check_key_consistency(all_analyses):
    """å…¨ã‚«ãƒ†ã‚´ãƒªé–“ã§ã®ã‚­ãƒ¼ã®ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("\nğŸ” Checking key consistency across categories...")
    
    consistency_report = {
        'common_keys': set(),
        'category_specific_keys': {},
        'key_overlap_matrix': {}
    }
    
    # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚­ãƒ¼ã‚»ãƒƒãƒˆã‚’å–å¾—
    category_keys = {}
    for analysis in all_analyses:
        category_keys[analysis['name']] = analysis['all_keys']
    
    # å…±é€šã‚­ãƒ¼ã‚’è¦‹ã¤ã‘ã‚‹
    if category_keys:
        consistency_report['common_keys'] = set.intersection(*category_keys.values())
    
    # ã‚«ãƒ†ã‚´ãƒªå›ºæœ‰ã®ã‚­ãƒ¼ã‚’è¦‹ã¤ã‘ã‚‹
    for category, keys in category_keys.items():
        other_keys = set()
        for other_category, other_category_keys in category_keys.items():
            if other_category != category:
                other_keys.update(other_category_keys)
        
        consistency_report['category_specific_keys'][category] = keys - other_keys
    
    # ã‚­ãƒ¼é‡è¤‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    for cat1, keys1 in category_keys.items():
        consistency_report['key_overlap_matrix'][cat1] = {}
        for cat2, keys2 in category_keys.items():
            overlap = len(keys1 & keys2)
            total_unique = len(keys1 | keys2)
            similarity = overlap / total_unique if total_unique > 0 else 0
            consistency_report['key_overlap_matrix'][cat1][cat2] = {
                'overlap_count': overlap,
                'similarity_ratio': similarity
            }
    
    return consistency_report

def format_json_for_markdown(data, max_lines=50):
    """JSONãƒ‡ãƒ¼ã‚¿ã‚’Markdownç”¨ã«æ•´å½¢ï¼ˆè¡Œæ•°åˆ¶é™ä»˜ãï¼‰"""
    try:
        json_str = json.dumps(data, indent=2, ensure_ascii=False)
        lines = json_str.split('\n')
        if len(lines) > max_lines:
            lines = lines[:max_lines] + ['  ...', '  // (truncated for readability)', '}']
        return '\n'.join(lines)
    except Exception as e:
        return f"Error formatting JSON: {e}"

def generate_markdown_report(all_analyses, consistency_report, output_file):
    """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    print(f"\nğŸ“ Generating Markdown report: {output_file}")
    
    timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
    
    markdown_content = f"""# Raw Nutrition Data åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚:** {timestamp}

## ğŸ“Š æ¦‚è¦çµ±è¨ˆ

ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€`raw_nutrition_data`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®4ã¤ã®ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªï¼ˆfood, branded, restaurant, recipeï¼‰ã®æ§‹é€ ã¨å†…å®¹ã‚’åˆ†æã—ãŸçµæœã§ã™ã€‚

### ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ

| ã‚«ãƒ†ã‚´ãƒª | ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•° | å‡¦ç†æ¸ˆã¿JSONãƒ•ã‚¡ã‚¤ãƒ«æ•° | 
|----------|----------------|------------------------|
"""
    
    # çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
    total_dirs = 0
    total_files = 0
    for analysis in all_analyses:
        total_dirs += analysis['total_directories']
        total_files += len(analysis['processed_files'])
        markdown_content += f"| {analysis['name']} | {analysis['total_directories']:,} | {len(analysis['processed_files']):,} |\n"
    
    markdown_content += f"| **åˆè¨ˆ** | **{total_dirs:,}** | **{total_files:,}** |\n\n"
    
    # å„ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°åˆ†æ
    for analysis in all_analyses:
        markdown_content += f"""## ğŸ“ {analysis['name'].title()} ã‚«ãƒ†ã‚´ãƒª

### åŸºæœ¬æƒ…å ±
- **ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°**: {analysis['total_directories']:,}
- **å‡¦ç†æ¸ˆã¿JSONãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {len(analysis['processed_files']):,}
- **ã‚µãƒ³ãƒ—ãƒ«æ•°**: {len(analysis['samples'])}

### JSONã‚µãƒ³ãƒ—ãƒ«

"""
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ç¤º
        for i, sample in enumerate(analysis['samples'], 1):
            relative_path = sample['file_path'].replace(str(Path.cwd()), '.')
            markdown_content += f"""#### ã‚µãƒ³ãƒ—ãƒ« {i}: `{relative_path}`

```json
{format_json_for_markdown(sample['data'])}
```

"""
        
        # ã‚­ãƒ¼åˆ†æ
        markdown_content += f"""### JSONã‚­ãƒ¼åˆ†æ

**æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼æ•°**: {len(analysis['all_keys'])}

#### ä¸»è¦ã‚­ãƒ¼ï¼ˆå‡ºç¾é »åº¦é †ï¼‰
"""
        
        # é »åº¦ã®é«˜ã„ã‚­ãƒ¼ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
        top_keys = analysis['key_frequency'].most_common(20)
        for key, frequency in top_keys:
            markdown_content += f"- `{key}` (å‡ºç¾: {frequency}å›)\n"
        
        if len(analysis['all_keys']) > 20:
            markdown_content += f"\n*(ä»– {len(analysis['all_keys']) - 20} ã‚­ãƒ¼)*\n"
        
        markdown_content += "\n"
        
        # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°è¡¨ç¤º
        if analysis['errors']:
            markdown_content += f"""### âš ï¸ å‡¦ç†ã‚¨ãƒ©ãƒ¼

ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼š
"""
            for error in analysis['errors']:
                markdown_content += f"- {error}\n"
            markdown_content += "\n"
    
    # ã‚­ãƒ¼ã®ä¸€è²«æ€§åˆ†æ
    markdown_content += f"""## ğŸ” ã‚­ãƒ¼ä¸€è²«æ€§åˆ†æ

### å…±é€šã‚­ãƒ¼

å…¨ã‚«ãƒ†ã‚´ãƒªã§å…±é€šã—ã¦ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼æ•°: **{len(consistency_report['common_keys'])}**

"""
    
    if consistency_report['common_keys']:
        markdown_content += "#### å…±é€šã‚­ãƒ¼ä¸€è¦§\n"
        for key in sorted(consistency_report['common_keys']):
            markdown_content += f"- `{key}`\n"
        markdown_content += "\n"
    else:
        markdown_content += "*å…¨ã‚«ãƒ†ã‚´ãƒªã§å…±é€šã™ã‚‹ã‚­ãƒ¼ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚*\n\n"
    
    # ã‚«ãƒ†ã‚´ãƒªå›ºæœ‰ã‚­ãƒ¼
    markdown_content += "### ã‚«ãƒ†ã‚´ãƒªå›ºæœ‰ã‚­ãƒ¼\n\n"
    for category, specific_keys in consistency_report['category_specific_keys'].items():
        markdown_content += f"#### {category.title()} å›ºæœ‰ã‚­ãƒ¼ ({len(specific_keys)}å€‹)\n"
        if specific_keys:
            for key in sorted(list(specific_keys)[:10]):  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
                markdown_content += f"- `{key}`\n"
            if len(specific_keys) > 10:
                markdown_content += f"\n*(ä»– {len(specific_keys) - 10} ã‚­ãƒ¼)*\n"
        else:
            markdown_content += "*ã‚«ãƒ†ã‚´ãƒªå›ºæœ‰ã®ã‚­ãƒ¼ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚*\n"
        markdown_content += "\n"
    
    # ã‚­ãƒ¼é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
    markdown_content += "### ã‚«ãƒ†ã‚´ãƒªé–“ã‚­ãƒ¼é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹\n\n"
    categories = list(consistency_report['key_overlap_matrix'].keys())
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
    markdown_content += "| | " + " | ".join(categories) + " |\n"
    markdown_content += "|" + "---|" * (len(categories) + 1) + "\n"
    
    # ãƒ‡ãƒ¼ã‚¿è¡Œ
    for cat1 in categories:
        row = f"| **{cat1}** |"
        for cat2 in categories:
            similarity = consistency_report['key_overlap_matrix'][cat1][cat2]['similarity_ratio']
            row += f" {similarity:.2%} |"
        markdown_content += row + "\n"
    
    markdown_content += "\n"
    
    # çµè«–
    markdown_content += f"""## ğŸ“‹ åˆ†æçµæœã¾ã¨ã‚

### ä¸»ãªç™ºè¦‹äº‹é …

1. **ãƒ‡ãƒ¼ã‚¿è¦æ¨¡**: åˆè¨ˆ {total_dirs:,} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€{total_files:,} å‡¦ç†æ¸ˆã¿JSONãƒ•ã‚¡ã‚¤ãƒ«
2. **ã‚­ãƒ¼ä¸€è²«æ€§**: {len(consistency_report['common_keys'])} å€‹ã®å…±é€šã‚­ãƒ¼ãŒå…¨ã‚«ãƒ†ã‚´ãƒªã§ä½¿ç”¨
3. **ã‚«ãƒ†ã‚´ãƒªç‰¹æ€§**: å„ã‚«ãƒ†ã‚´ãƒªã«ã¯å›ºæœ‰ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒå­˜åœ¨

### ã‚«ãƒ†ã‚´ãƒªåˆ¥ç‰¹å¾´

"""
    
    for analysis in all_analyses:
        unique_keys = len(consistency_report['category_specific_keys'][analysis['name']])
        markdown_content += f"- **{analysis['name'].title()}**: {analysis['total_directories']:,} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€{unique_keys} å›ºæœ‰ã‚­ãƒ¼\n"
    
    markdown_content += f"""

### æ¨å¥¨äº‹é …

1. **ãƒ‡ãƒ¼ã‚¿çµ±åˆæ™‚ã®æ³¨æ„**: ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ç•°ãªã‚‹ã‚¹ã‚­ãƒ¼ãƒã‚’æŒã¤ãŸã‚ã€çµ±åˆå‡¦ç†æ™‚ã¯é©åˆ‡ãªãƒãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦
2. **ã‚­ãƒ¼æ­£è¦åŒ–**: å…±é€šã‚­ãƒ¼ã‚’åŸºæº–ã¨ã—ãŸæ¨™æº–åŒ–ã‚’æ¤œè¨
3. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ä¸€éƒ¨ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã§èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ãƒ­ãƒã‚¹ãƒˆãªå‡¦ç†ãŒå¿…è¦

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ `analyze_raw_nutrition_data.py` ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚*
"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã—ãŸ: {output_file}")
    except Exception as e:
        print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Raw Nutrition Data Analysis Script")
    print("=" * 50)
    
    # ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    base_dir = Path("raw_nutrition_data")
    if not base_dir.exists():
        print(f"âŒ {base_dir} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # åˆ†æå¯¾è±¡ã‚«ãƒ†ã‚´ãƒª
    categories = ["food", "branded", "restaurant", "recipe"]
    all_analyses = []
    
    # å„ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†æ
    for category in categories:
        category_path = base_dir / category
        if category_path.exists():
            analysis = analyze_category(category, str(category_path))
            all_analyses.append(analysis)
        else:
            print(f"âš ï¸ ã‚«ãƒ†ã‚´ãƒª {category} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {category_path}")
    
    if not all_analyses:
        print("âŒ åˆ†æã™ã‚‹ã‚«ãƒ†ã‚´ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    # ã‚­ãƒ¼ä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    consistency_report = check_key_consistency(all_analyses)
    
    # Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"raw_nutrition_data_analysis_report_{timestamp}.md"
    generate_markdown_report(all_analyses, consistency_report, output_file)
    
    print("\nğŸ‰ åˆ†æå®Œäº†!")
    print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {output_file}")
    
    # ç°¡å˜ãªã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print("\nğŸ“Š ç°¡æ˜“ã‚µãƒãƒªãƒ¼:")
    for analysis in all_analyses:
        print(f"  {analysis['name']}: {analysis['total_directories']:,} dirs, {len(analysis['processed_files']):,} files")

if __name__ == "__main__":
    main() 