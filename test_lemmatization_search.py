#!/usr/bin/env python3
"""
è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸæ¤œç´¢ç²¾åº¦å‘ä¸Šã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„ï¼š
- ã€Œtomatoã€vsã€Œtomatoesã€å•é¡Œã®è§£æ±ºæ¤œè¨¼
- è¦‹å‡ºã—èªåŒ–å¯¾å¿œæ¤œç´¢ã®åŠ¹æœæ¸¬å®š
- ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°èª¿æ•´ã®ç¢ºèª
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any
import requests

from app_v2.components.elasticsearch_nutrition_search_component import ElasticsearchNutritionSearchComponent
from app_v2.models.nutrition_search_models import NutritionQueryInput
from app_v2.utils.lemmatization import lemmatize_term, test_lemmatization

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼šå•é¡Œã®ã‚ã‚‹ã‚¯ã‚¨ãƒªã¨æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„
TEST_CASES = [
    {
        "query": "tomato",
        "description": "å˜æ•°å½¢ã‚¯ã‚¨ãƒªã§è¤‡æ•°å½¢é …ç›®(tomatoes)ãŒä¸Šä½ã«æ¥ã‚‹ã¹ã",
        "expected_improvements": ["tomatoes", "tomato"],
        "problematic_results": ["tomato soup", "tomato sauce", "tomato juice"]
    },
    {
        "query": "tomatoes", 
        "description": "è¤‡æ•°å½¢ã‚¯ã‚¨ãƒªã¯ãã®ã¾ã¾æ­£å¸¸å‹•ä½œã™ã‚‹ã¯ãš",
        "expected_improvements": ["tomatoes"],
        "problematic_results": []
    },
    {
        "query": "potato",
        "description": "å˜æ•°å½¢ã‚¯ã‚¨ãƒªã§è¤‡æ•°å½¢é …ç›®(potatoes)ãŒä¸Šä½ã«æ¥ã‚‹ã¹ã",
        "expected_improvements": ["potatoes", "potato"],
        "problematic_results": ["potato salad", "potato soup"]
    },
    {
        "query": "apples",
        "description": "è¤‡æ•°å½¢ã‚¯ã‚¨ãƒªã§å˜æ•°å½¢é …ç›®(apple)ã‚‚è©•ä¾¡ã•ã‚Œã‚‹ã¹ã",
        "expected_improvements": ["apples", "apple"],
        "problematic_results": ["apple juice", "apple pie"]
    },
    {
        "query": "onion",
        "description": "å˜æ•°å½¢ã‚¯ã‚¨ãƒªã§è¤‡æ•°å½¢é …ç›®(onions)ãŒä¸Šä½ã«æ¥ã‚‹ã¹ã",
        "expected_improvements": ["onions", "onion"],
        "problematic_results": ["onion soup", "onion rings"]
    }
]

async def test_direct_elasticsearch_queries():
    """ç›´æ¥Elasticsearchã‚¯ã‚¨ãƒªã§ç¾çŠ¶ç¢ºèª"""
    print("=== ç›´æ¥Elasticsearchæ¤œç´¢ã§ã®ç¾çŠ¶ç¢ºèª ===")
    
    base_url = "http://localhost:9200/nutrition_db/_search"
    
    for test_case in TEST_CASES:
        query = test_case["query"]
        print(f"\nğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: '{query}'")
        print(f"ğŸ“‹ èª¬æ˜: {test_case['description']}")
        
        # æ¨™æº–çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒª
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["name", "search_name", "description"]
                }
            },
            "size": 10
        }
        
        try:
            response = requests.post(
                base_url,
                headers={"Content-Type": "application/json"},
                json=search_body
            )
            
            if response.status_code == 200:
                result = response.json()
                hits = result.get("hits", {}).get("hits", [])
                
                print(f"ğŸ“Š æ¤œç´¢çµæœæ•°: {len(hits)}")
                print("ğŸ† ä¸Šä½5ä»¶:")
                
                for i, hit in enumerate(hits[:5]):
                    source = hit["_source"]
                    score = hit["_score"]
                    name = source.get("search_name", "N/A")
                    data_type = source.get("data_type", "N/A")
                    print(f"  {i+1}. {name} (ã‚¹ã‚³ã‚¢: {score:.2f}, ã‚¿ã‚¤ãƒ—: {data_type})")
                
            else:
                print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ æ¤œç´¢å¤±æ•—: {e}")

async def test_lemmatized_component_search():
    """è¦‹å‡ºã—èªåŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã®æ¤œç´¢ãƒ†ã‚¹ãƒˆ"""
    print("\n\n=== è¦‹å‡ºã—èªåŒ–å¯¾å¿œã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œç´¢ãƒ†ã‚¹ãƒˆ ===")
    
    # ElasticsearchNutritionSearchComponentã‚’è¦‹å‡ºã—èªåŒ–å¯¾å¿œã§åˆæœŸåŒ–
    es_component = ElasticsearchNutritionSearchComponent(
        multi_db_search_mode=False,  # è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ã‚’å„ªå…ˆ
        enable_advanced_features=False,  # æ§‹é€ åŒ–æ¤œç´¢ã¯ç„¡åŠ¹
        results_per_db=5
    )
    
    # è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ãŒæœ‰åŠ¹ã‹ã‚’ç¢ºèª
    print(f"ğŸ”§ è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½: {'æœ‰åŠ¹' if es_component.enable_lemmatization else 'ç„¡åŠ¹'}")
    print(f"ğŸ¯ è¦‹å‡ºã—èªåŒ–å®Œå…¨ä¸€è‡´ãƒ–ãƒ¼ã‚¹ãƒˆ: {es_component.lemmatized_exact_match_boost}")
    print(f"âš ï¸ è¤‡åˆèªãƒšãƒŠãƒ«ãƒ†ã‚£: {es_component.compound_word_penalty}")
    
    results_comparison = []
    
    for test_case in TEST_CASES:
        query = test_case["query"]
        print(f"\nğŸ” è¦‹å‡ºã—èªåŒ–æ¤œç´¢ãƒ†ã‚¹ãƒˆ: '{query}'")
        
        # è¦‹å‡ºã—èªåŒ–ã‚¯ã‚¨ãƒªå…¥åŠ›ã‚’ä½œæˆ
        query_input = NutritionQueryInput(
            ingredient_names=[query],
            dish_names=[],
            preferred_source="elasticsearch"
        )
        
        try:
            # è¦‹å‡ºã—èªåŒ–å¯¾å¿œæ¤œç´¢ã‚’å®Ÿè¡Œ
            search_result = await es_component.process(query_input)
            
            matches = search_result.matches.get(query, [])
            search_summary = search_result.search_summary
            
            print(f"ğŸ“Š æ¤œç´¢çµæœ: {len(matches)}ä»¶")
            print(f"ğŸ¯ æ¤œç´¢æ–¹æ³•: {search_summary.get('search_method', 'N/A')}")
            print(f"â±ï¸ æ¤œç´¢æ™‚é–“: {search_summary.get('search_time_ms', 0)}ms")
            
            if matches:
                print("ğŸ† è¦‹å‡ºã—èªåŒ–å¯¾å¿œä¸Šä½5ä»¶:")
                for i, match in enumerate(matches[:5]):
                    score = match.score
                    name = match.search_name
                    data_type = match.data_type
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
                    metadata = getattr(match, 'search_metadata', {})
                    lemmatized_query = metadata.get('lemmatized_query', 'N/A')
                    lemmatized_db_name = metadata.get('lemmatized_db_name', 'N/A')
                    score_adjustment = metadata.get('score_adjustment_factor', 1.0)
                    
                    print(f"  {i+1}. {name}")
                    print(f"     ã‚¹ã‚³ã‚¢: {score:.3f} (èª¿æ•´ä¿‚æ•°: {score_adjustment:.2f})")
                    print(f"     ã‚¿ã‚¤ãƒ—: {data_type}")
                    print(f"     è¦‹å‡ºã—èªåŒ–: '{lemmatized_query}' -> '{lemmatized_db_name}'")
                
                # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„ã®ç¢ºèª
                top_names = [match.search_name.lower() for match in matches[:3]]
                expected = [exp.lower() for exp in test_case["expected_improvements"]]
                improvements_found = [exp for exp in expected if any(exp in name for name in top_names)]
                
                print(f"âœ… æœŸå¾…ã•ã‚ŒãŸæ”¹å–„: {improvements_found}")
                
                # çµæœã‚’ä¿å­˜
                results_comparison.append({
                    "query": query,
                    "description": test_case["description"],
                    "top_results": [(match.search_name, match.score) for match in matches[:5]],
                    "improvements_found": improvements_found,
                    "search_time_ms": search_summary.get('search_time_ms', 0)
                })
            else:
                print("âŒ æ¤œç´¢çµæœãªã—")
                
        except Exception as e:
            print(f"âŒ è¦‹å‡ºã—èªåŒ–æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    return results_comparison

async def compare_search_methods():
    """æ¤œç´¢æ–¹æ³•ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    print("\n\n=== æ¤œç´¢æ–¹æ³•æ¯”è¼ƒãƒ†ã‚¹ãƒˆ ===")
    
    # 1. å¾“æ¥ã®æˆ¦ç•¥çš„æ¤œç´¢
    strategic_component = ElasticsearchNutritionSearchComponent(
        multi_db_search_mode=True,
        enable_advanced_features=False,
        results_per_db=5
    )
    strategic_component.enable_lemmatization = False  # è¦‹å‡ºã—èªåŒ–ã‚’ç„¡åŠ¹
    
    # 2. è¦‹å‡ºã—èªåŒ–å¯¾å¿œæ¤œç´¢
    lemmatized_component = ElasticsearchNutritionSearchComponent(
        multi_db_search_mode=False,
        enable_advanced_features=False,
        results_per_db=5
    )
    
    # æ¯”è¼ƒå¯¾è±¡ã‚¯ã‚¨ãƒª
    test_query = "tomato"
    
    print(f"ğŸ” æ¯”è¼ƒã‚¯ã‚¨ãƒª: '{test_query}'")
    
    query_input = NutritionQueryInput(
        ingredient_names=[test_query],
        dish_names=[],
        preferred_source="elasticsearch"
    )
    
    # å¾“æ¥æ¤œç´¢
    print("\nğŸ“Š å¾“æ¥ã®æˆ¦ç•¥çš„æ¤œç´¢:")
    try:
        strategic_result = await strategic_component.process(query_input)
        strategic_matches = strategic_result.matches.get(test_query, [])
        print(f"  çµæœæ•°: {len(strategic_matches)}")
        for i, match in enumerate(strategic_matches[:3]):
            print(f"  {i+1}. {match.search_name} (ã‚¹ã‚³ã‚¢: {match.score:.3f})")
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    # è¦‹å‡ºã—èªåŒ–æ¤œç´¢
    print("\nğŸ“Š è¦‹å‡ºã—èªåŒ–å¯¾å¿œæ¤œç´¢:")
    try:
        lemmatized_result = await lemmatized_component.process(query_input)
        lemmatized_matches = lemmatized_result.matches.get(test_query, [])
        print(f"  çµæœæ•°: {len(lemmatized_matches)}")
        for i, match in enumerate(lemmatized_matches[:3]):
            metadata = getattr(match, 'search_metadata', {})
            adjustment = metadata.get('score_adjustment_factor', 1.0)
            print(f"  {i+1}. {match.search_name} (ã‚¹ã‚³ã‚¢: {match.score:.3f}, èª¿æ•´: {adjustment:.2f})")
    except Exception as e:
        print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")

def save_test_results(results: List[Dict], filename: str = None):
    """ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"lemmatization_test_results_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump({
            "test_timestamp": datetime.now().isoformat(),
            "test_description": "è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸæ¤œç´¢ç²¾åº¦å‘ä¸Šãƒ†ã‚¹ãƒˆ",
            "test_cases": TEST_CASES,
            "results": results
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")

async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    # 1. è¦‹å‡ºã—èªåŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒ†ã‚¹ãƒˆ
    print("ğŸ“š è¦‹å‡ºã—èªåŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ†ã‚¹ãƒˆ:")
    test_lemmatization()
    
    # 2. ç›´æ¥Elasticsearchæ¤œç´¢ã§ã®ç¾çŠ¶ç¢ºèª
    await test_direct_elasticsearch_queries()
    
    # 3. è¦‹å‡ºã—èªåŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œç´¢ãƒ†ã‚¹ãƒˆ
    results = await test_lemmatized_component_search()
    
    # 4. æ¤œç´¢æ–¹æ³•ã®æ¯”è¼ƒ
    await compare_search_methods()
    
    # 5. çµæœä¿å­˜
    save_test_results(results)
    
    print("\nâœ… è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Œäº†")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main()) 