#!/usr/bin/env python3
"""
MyNetDiary ãƒªã‚¹ãƒˆå½¢å¼å¯¾å¿œ Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

search_nameãŒãƒªã‚¹ãƒˆå½¢å¼ã¨æ–‡å­—åˆ—å½¢å¼ã®ä¸¡æ–¹ã«å¯¾å¿œã—ã€
ãƒªã‚¹ãƒˆå†…ã®å„é …ç›®ã‚’ç‹¬ç«‹ã—ã¦æ¤œç´¢å¯¾è±¡ã«ã§ãã‚‹Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã€‚
"""

import json
import sqlite3
import time
from datetime import datetime
from elasticsearch import Elasticsearch
from typing import Dict, Any, List, Union

# è¨­å®š
INDEX_NAME = "mynetdiary_list_support_db"
ELASTICSEARCH_HOST = "http://localhost:9200"

def create_elasticsearch_index():
    """ãƒªã‚¹ãƒˆå½¢å¼å¯¾å¿œã®Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
    
    # Elasticsearchæ¥ç¶š
    es = Elasticsearch([ELASTICSEARCH_HOST])
    
    # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤
    if es.indices.exists(index=INDEX_NAME):
        print(f"ğŸ—‘ï¸ Deleting existing index: {INDEX_NAME}")
        es.indices.delete(index=INDEX_NAME)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šã¨ãƒãƒƒãƒ”ãƒ³ã‚°
    index_settings = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "tokenizer": {
                    "food_name_tokenizer": {
                        "type": "standard",
                        "max_token_length": 50
                    }
                },
                "filter": {
                    "food_name_stemmer": {
                        "type": "stemmer",
                        "language": "english"
                    },
                    "food_name_stop": {
                        "type": "stop",
                        "stopwords": ["and", "or", "with", "without", "in", "on", "at", "by"]
                    }
                },
                "analyzer": {
                    "food_name_analyzer": {
                        "type": "custom",
                        "tokenizer": "food_name_tokenizer",
                        "filter": ["lowercase", "food_name_stop", "food_name_stemmer"]
                    },
                    "food_exact_analyzer": {
                        "type": "custom",
                        "tokenizer": "keyword",
                        "filter": ["lowercase"]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "id": {"type": "long"},
                "original_name": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "exact": {
                            "type": "text",
                            "analyzer": "food_exact_analyzer"
                        }
                    }
                },
                "search_name": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "exact": {
                            "type": "text", 
                            "analyzer": "food_exact_analyzer"
                        },
                        "lemmatized": {
                            "type": "text",
                            "analyzer": "food_name_analyzer"
                        }
                    }
                },
                "search_name_list": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "exact": {
                            "type": "text", 
                            "analyzer": "food_exact_analyzer"
                        },
                        "lemmatized": {
                            "type": "text",
                            "analyzer": "food_name_analyzer"
                        }
                    }
                },
                "search_name_type": {"type": "keyword"},
                "alternative_names_count": {"type": "integer"},
                "original_search_name": {"type": "text"},
                "description": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "exact": {
                            "type": "text",
                            "analyzer": "food_exact_analyzer"
                        }
                    }
                },
                "nutrition": {
                    "properties": {
                        "calories": {"type": "float"},
                        "protein": {"type": "float"},
                        "fat": {"type": "float"},
                        "carbs": {"type": "float"}
                    }
                },
                "data_type": {"type": "keyword"},
                "source": {"type": "keyword"},
                "processing_method": {"type": "keyword"},
                "conversion_timestamp": {"type": "date"},
                "list_conversion_timestamp": {"type": "date"}
            }
        }
    }
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    print(f"ğŸ”§ Creating index: {INDEX_NAME}")
    es.indices.create(index=INDEX_NAME, body=index_settings)
    print("âœ… Index created successfully")
    
    return es

def process_search_name_field(search_name: Union[str, List[str]]) -> Dict[str, Any]:
    """search_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†ã—ã¦Elasticsearchç”¨ã®å½¢å¼ã«å¤‰æ›"""
    
    if isinstance(search_name, list):
        # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆ
        return {
            "search_name": " ".join(search_name),  # æ¤œç´¢ç”¨ã«çµåˆ
            "search_name_list": search_name,       # å€‹åˆ¥é …ç›®ç”¨
            "search_name_type": "list",
            "alternative_names_count": len(search_name)
        }
    else:
        # æ–‡å­—åˆ—å½¢å¼ã®å ´åˆ
        return {
            "search_name": search_name,
            "search_name_list": [search_name],     # çµ±ä¸€çš„ã«æ‰±ã†ãŸã‚ãƒªã‚¹ãƒˆåŒ–
            "search_name_type": "string", 
            "alternative_names_count": 1
        }

def load_and_index_data(es):
    """ãƒªã‚¹ãƒˆå½¢å¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    print("ğŸ“‚ Loading list format data...")
    with open('db/mynetdiary_converted_tool_calls_list.json', 'r', encoding='utf-8') as f:
        converted_data = json.load(f)
    
    print(f"âœ… Loaded {len(converted_data)} items")
    
    # çµ±è¨ˆæƒ…å ±
    list_entries = sum(1 for item in converted_data if isinstance(item.get('search_name'), list))
    string_entries = len(converted_data) - list_entries
    
    print(f"ğŸ“Š Data distribution:")
    print(f"   ğŸ“ List format entries: {list_entries}")
    print(f"   ğŸ“„ String format entries: {string_entries}")
    
    # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰
    print("ğŸ’¾ Creating SQLite database...")
    conn = sqlite3.connect('db/mynetdiary_list_support.db')
    cursor = conn.cursor()
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS nutrition_data (
            id TEXT PRIMARY KEY,
            original_name TEXT NOT NULL,
            search_name TEXT NOT NULL,
            search_name_list TEXT,
            search_name_type TEXT,
            alternative_names_count INTEGER,
            original_search_name TEXT,
            description TEXT,
            calories REAL,
            protein REAL,
            fat REAL,
            carbs REAL,
            data_type TEXT,
            source_db TEXT,
            processing_method TEXT,
            conversion_timestamp REAL,
            list_conversion_timestamp REAL
        )
    ''')
    
    # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ã¨Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    print("ğŸ“¤ Indexing data to Elasticsearch and SQLite...")
    indexed_count = 0
    batch_size = 100
    batch = []
    
    for item in converted_data:
        # search_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†
        search_name_data = process_search_name_field(item.get('search_name', ''))
        
        # Elasticsearchç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™
        doc = {
            "id": item["id"],
            "original_name": item["original_name"],
            "description": item.get("description", ""),
            "nutrition": item["nutrition"],
            "data_type": item["data_type"],
            "source": item["source"],
            "processing_method": item["processing_method"],
            "conversion_timestamp": datetime.fromtimestamp(item["conversion_timestamp"])
        }
        
        # search_nameé–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
        doc.update(search_name_data)
        
        # ãƒªã‚¹ãƒˆå¤‰æ›ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ 
        if "list_conversion_timestamp" in item:
            doc["list_conversion_timestamp"] = datetime.fromtimestamp(item["list_conversion_timestamp"])
        
        # å…ƒã®search_nameã‚’ä¿å­˜ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆï¼‰
        if "original_search_name" in item:
            doc["original_search_name"] = item["original_search_name"]
        
        batch.append({
            "_index": INDEX_NAME,
            "_id": item["id"],
            "_source": doc
        })
        
        # SQLiteã«æŒ¿å…¥
        nutrition = item["nutrition"]
        cursor.execute('''
            INSERT OR REPLACE INTO nutrition_data 
            (id, original_name, search_name, search_name_list, search_name_type, 
             alternative_names_count, original_search_name, description, 
             calories, protein, fat, carbs, data_type, source_db, processing_method, 
             conversion_timestamp, list_conversion_timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            str(item["id"]),
            item["original_name"],
            doc["search_name"],
            json.dumps(doc["search_name_list"]),
            doc["search_name_type"],
            doc["alternative_names_count"],
            doc.get("original_search_name"),
            item.get("description", ""),
            nutrition.get("calories", 0),
            nutrition.get("protein", 0),
            nutrition.get("fat", 0),
            nutrition.get("carbs", 0),
            item["data_type"],
            item["source"],
            item["processing_method"],
            item["conversion_timestamp"],
            item.get("list_conversion_timestamp")
        ))
        
        # ãƒãƒƒãƒã§Elasticsearchã«é€ä¿¡
        if len(batch) >= batch_size:
            from elasticsearch.helpers import bulk
            try:
                success, failed = bulk(es, batch, stats_only=True)
                indexed_count += success
                if failed > 0:
                    print(f"   âš ï¸ {failed} documents failed in this batch")
                print(f"   ğŸ“Š Progress: {indexed_count}/{len(converted_data)} ({indexed_count/len(converted_data)*100:.1f}%)")
            except Exception as e:
                print(f"   âŒ Batch error: {e}")
                # å€‹åˆ¥ã«å‡¦ç†ã—ã¦ã¿ã‚‹
                for doc in batch:
                    try:
                        es.index(index=INDEX_NAME, id=doc["_id"], body=doc["_source"])
                        indexed_count += 1
                    except Exception as doc_error:
                        print(f"   ğŸ”´ Failed to index document {doc['_id']}: {doc_error}")
            batch = []
    
    # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†
    if batch:
        from elasticsearch.helpers import bulk
        try:
            success, failed = bulk(es, batch, stats_only=True)
            indexed_count += success
            if failed > 0:
                print(f"   âš ï¸ {failed} documents failed in final batch")
        except Exception as e:
            print(f"   âŒ Final batch error: {e}")
            # å€‹åˆ¥ã«å‡¦ç†ã—ã¦ã¿ã‚‹
            for doc in batch:
                try:
                    es.index(index=INDEX_NAME, id=doc["_id"], body=doc["_source"])
                    indexed_count += 1
                except Exception as doc_error:
                    print(f"   ğŸ”´ Failed to index document {doc['_id']}: {doc_error}")
    
    conn.commit()
    conn.close()
    
    print(f"âœ… Successfully indexed {indexed_count} documents")
    print(f"ğŸ’¾ SQLite database saved: db/mynetdiary_list_support.db")
    
    return indexed_count

def test_list_search_functionality(es):
    """ãƒªã‚¹ãƒˆå¯¾å¿œæ¤œç´¢æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("\nğŸ” Testing list-aware search functionality...")
    
    test_queries = [
        "chickpeas",        # ãƒªã‚¹ãƒˆé …ç›®ã®1ã¤ç›®
        "garbanzo beans",   # ãƒªã‚¹ãƒˆé …ç›®ã®2ã¤ç›®  
        "cannellini",       # ãƒªã‚¹ãƒˆé …ç›®ã®1ã¤ç›®
        "white kidney beans", # ãƒªã‚¹ãƒˆé …ç›®ã®2ã¤ç›®
        "tomato",           # ä¸€èˆ¬æ¤œç´¢
        "coffee powder",    # ç‰¹å®šè£½å“æ¤œç´¢
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Testing query: '{query}'")
        
        # ä¸¡æ–¹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ¤œç´¢å¯¾è±¡ã«
        search_body = {
            "query": {
                "bool": {
                    "should": [
                        # çµ±åˆã•ã‚ŒãŸsearch_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œç´¢
                        {
                            "multi_match": {
                                "query": query,
                                "fields": ["search_name^3", "search_name.exact^4"],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        },
                        # å€‹åˆ¥ã®ãƒªã‚¹ãƒˆé …ç›®æ¤œç´¢
                        {
                            "multi_match": {
                                "query": query,
                                "fields": ["search_name_list^5", "search_name_list.exact^6"],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ]
                }
            },
            "size": 5
        }
        
        result = es.search(index=INDEX_NAME, body=search_body)
        
        for i, hit in enumerate(result['hits']['hits'], 1):
            source = hit['_source']
            score = hit['_score']
            search_name_type = source.get('search_name_type', 'unknown')
            
            if search_name_type == 'list':
                # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆ
                alt_count = source.get('alternative_names_count', 0)
                original_search = source.get('original_search_name', 'N/A')
                search_names = source.get('search_name_list', [])
                print(f"   {i}. [{score:.2f}] {search_names} (List: {alt_count} names)")
                print(f"      Original: {source['original_name']}")
                print(f"      From: {original_search}")
            else:
                # æ–‡å­—åˆ—å½¢å¼ã®å ´åˆ
                print(f"   {i}. [{score:.2f}] {source['search_name']} | {source.get('description', '')}")
                print(f"      Original: {source['original_name']}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    print("ğŸš€ MyNetDiary ãƒªã‚¹ãƒˆå½¢å¼å¯¾å¿œ Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ")
    print("=" * 70)
    
    try:
        # Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        es = create_elasticsearch_index()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        indexed_count = load_and_index_data(es)
        
        # æ¤œç´¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
        test_list_search_functionality(es)
        
        print(f"\nğŸ‰ å®Œäº†! {indexed_count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã•ã‚Œã¾ã—ãŸ")
        print(f"ğŸ“Š ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å: {INDEX_NAME}")
        print(f"ğŸ” ãƒªã‚¹ãƒˆå½¢å¼å¯¾å¿œ: search_nameãƒªã‚¹ãƒˆå†…ã®å„é …ç›®ãŒç‹¬ç«‹ã—ã¦æ¤œç´¢å¯èƒ½")
        print(f"ğŸ’¡ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : ãƒªã‚¹ãƒˆå†…ã§æœ€é«˜ã‚¹ã‚³ã‚¢ã®é …ç›®ãŒãã®ã‚¨ãƒ³ãƒˆãƒªã®ã‚¹ã‚³ã‚¢")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 