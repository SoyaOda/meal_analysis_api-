"""
Ê†ÑÈ§äÊ§úÁ¥¢„Ç®„É≥„Ç∏„É≥ - 7ÊÆµÈöéÊ§úÁ¥¢Êà¶Áï•„Å®Ë¶ãÂá∫„ÅóË™ûÂåñÊ©üËÉΩ
"""

import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional

from .models import SearchQuery, SearchResponse, SearchResult, NutritionInfo, BatchSearchQuery, BatchSearchResponse
from utils.lemmatization import lemmatize_term, create_lemmatized_query_variations
from utils.elasticsearch_client import get_elasticsearch_client

logger = logging.getLogger(__name__)


class NutritionSearchEngine:
    """Ê†ÑÈ§ä„Éá„Éº„Çø„Éô„Éº„ÇπÊ§úÁ¥¢„Ç®„É≥„Ç∏„É≥"""
    
    def __init__(self):
        self.es_client = get_elasticsearch_client()
        
        # Ê§úÁ¥¢„Éë„É©„É°„Éº„Çø
        self.lemmatized_exact_match_boost = 2.0
        self.compound_word_penalty = 0.8
        self.enable_lemmatization = True
        
        # Áµ±Ë®à
        self.total_searches = 0
        self.total_response_time = 0
        
        logger.info("üîç NutritionSearchEngine initialized")
    
    async def search(self, query: SearchQuery) -> SearchResponse:
        """Âçò‰∏ÄÊ§úÁ¥¢ÂÆüË°å"""
        start_time = datetime.now()
        
        if not self.es_client.is_connected():
            return SearchResponse(
                query=query.query,
                results=[],
                total_found=0,
                search_time_ms=0,
                lemmatized_query=None
            )
        
        # Ë¶ãÂá∫„ÅóË™ûÂåñÂá¶ÁêÜ
        lemmatized_query = lemmatize_term(query.query) if self.enable_lemmatization else query.query
        
        # 7ÊÆµÈöéÊ§úÁ¥¢„ÇØ„Ç®„É™ÊßãÁØâ
        es_query = self._build_advanced_search_query(query.query, lemmatized_query, query.max_results)
        
        # ElasticsearchÊ§úÁ¥¢ÂÆüË°å
        response = await self.es_client.search(es_query)
        
        # ÁµêÊûúÂ§âÊèõ
        results = []
        if response and response.get('hits', {}).get('hits'):
            results = await self._convert_es_results(response['hits']['hits'], query.query, lemmatized_query)
            
            # „Çπ„Ç≥„Ç¢„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
            results = [r for r in results if r.score >= query.min_score]
            
            # „ÇΩ„Éº„ÇπDB„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
            if query.source_db_filter:
                results = [r for r in results if r.source_db in query.source_db_filter]
        
        # Áµ±Ë®àÊõ¥Êñ∞
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        self.total_searches += 1
        self.total_response_time += search_time_ms
        
        return SearchResponse(
            query=query.query,
            results=results,
            total_found=len(results),
            search_time_ms=search_time_ms,
            lemmatized_query=lemmatized_query if lemmatized_query != query.query else None
        )
    
    async def batch_search(self, batch_query: BatchSearchQuery) -> BatchSearchResponse:
        """„Éê„ÉÉ„ÉÅÊ§úÁ¥¢ÂÆüË°å"""
        start_time = datetime.now()
        
        # ‰∏¶ÂàóÊ§úÁ¥¢ÂÆüË°å
        search_tasks = []
        for query_text in batch_query.queries:
            search_query = SearchQuery(
                query=query_text,
                max_results=batch_query.max_results
            )
            search_tasks.append(self.search(search_query))
        
        responses = await asyncio.gather(*search_tasks)
        
        # Áµ±Ë®àÊÉÖÂ†±‰ΩúÊàê
        end_time = datetime.now()
        total_search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        total_results = sum(len(r.results) for r in responses)
        successful_searches = sum(1 for r in responses if len(r.results) > 0)
        
        summary = {
            "total_queries": len(batch_query.queries),
            "successful_searches": successful_searches,
            "total_results": total_results,
            "average_results_per_query": total_results / len(batch_query.queries) if batch_query.queries else 0,
            "match_rate_percent": (successful_searches / len(batch_query.queries) * 100) if batch_query.queries else 0
        }
        
        return BatchSearchResponse(
            queries=batch_query.queries,
            responses=responses,
            total_search_time_ms=total_search_time_ms,
            summary=summary
        )
    
    def _build_advanced_search_query(self, original_term: str, lemmatized_term: str, max_results: int) -> Dict[str, Any]:
        """7ÊÆµÈöéÊ§úÁ¥¢Êà¶Áï•„ÅÆElasticsearch„ÇØ„Ç®„É™ÊßãÁØâ"""
        
        bool_query = {
            "bool": {
                "should": [
                    # 1. Ë¶ãÂá∫„ÅóË™ûÂåñÂÆåÂÖ®‰∏ÄËá¥ÔºàÊúÄÈ´ò„Éñ„Éº„Çπ„ÉàÔºâ
                    {
                        "match": {
                            "search_name_lemmatized.exact": {
                                "query": lemmatized_term,
                                "boost": self.lemmatized_exact_match_boost * 3.0
                            }
                        }
                    },
                    # 2. Ë¶ãÂá∫„ÅóË™ûÂåñ‰∏ÄËá¥ÔºàÈ´ò„Éñ„Éº„Çπ„ÉàÔºâ
                    {
                        "match": {
                            "search_name_lemmatized": {
                                "query": lemmatized_term,
                                "boost": self.lemmatized_exact_match_boost * 2.0
                            }
                        }
                    },
                    # 3. ÂÖÉ„ÅÆË™û„Åß„ÅÆÂÆåÂÖ®‰∏ÄËá¥
                    {
                        "match": {
                            "search_name.exact": {
                                "query": original_term,
                                "boost": 1.8
                            }
                        }
                    },
                    # 4. ÂÖÉ„ÅÆË™û„Åß„ÅÆ‰∏ÄËá¥
                    {
                        "match": {
                            "search_name": {
                                "query": original_term,
                                "boost": 1.5
                            }
                        }
                    },
                    # 5. Ë¶ãÂá∫„ÅóË™ûÂåñÈÉ®ÂàÜ‰∏ÄËá¥
                    {
                        "match_phrase": {
                            "search_name_lemmatized": {
                                "query": lemmatized_term,
                                "boost": self.lemmatized_exact_match_boost
                            }
                        }
                    },
                    # 6. ÂÖÉ„ÅÆË™û„Åß„ÅÆÈÉ®ÂàÜ‰∏ÄËá¥
                    {
                        "match_phrase": {
                            "search_name": {
                                "query": original_term,
                                "boost": 1.0
                            }
                        }
                    },
                    # 7. „ÅÇ„ÅÑ„Åæ„ÅÑÊ§úÁ¥¢ÔºàtypoÂØæÂøúÔºâ
                    {
                        "fuzzy": {
                            "search_name_lemmatized": {
                                "value": lemmatized_term,
                                "fuzziness": "AUTO",
                                "boost": 0.5
                            }
                        }
                    }
                ],
                "minimum_should_match": 1
            }
        }
        
        return {
            "query": bool_query,
            "size": max_results * 2,  # „Çπ„Ç≥„Ç¢Ë™øÊï¥Âæå„Å´Áµû„ÇäËæº„ÇÄ„Åü„ÇÅÂ§ö„ÇÅ„Å´ÂèñÂæó
            "_source": ["id", "search_name", "description", "original_name", "data_type", "nutrition", "source_db"]
        }
    
    async def _convert_es_results(self, es_hits: List[Dict[str, Any]], original_term: str, lemmatized_term: str) -> List[SearchResult]:
        """ElasticsearchÁµêÊûú„ÇíSearchResult„Å´Â§âÊèõ"""
        results = []
        
        for hit in es_hits:
            source = hit['_source']
            
            # Ê†ÑÈ§äÊÉÖÂ†±Â§âÊèõ
            nutrition_data = source.get('nutrition', {})
            nutrition = NutritionInfo(**nutrition_data)
            
            # „Éû„ÉÉ„ÉÅ„Çø„Ç§„ÉóÂà§ÂÆö
            match_type = self._determine_match_type(
                source.get('search_name', ''),
                source.get('search_name_lemmatized', ''),
                original_term,
                lemmatized_term
            )
            
            # „Çπ„Ç≥„Ç¢Ë™øÊï¥
            adjusted_score = self._calculate_adjusted_score(
                hit['_score'],
                original_term,
                lemmatized_term,
                source.get('search_name', ''),
                source.get('search_name_lemmatized', ''),
                match_type
            )
            
            result = SearchResult(
                id=str(source.get('id', '')),
                name=source.get('search_name', ''),
                description=source.get('description', ''),
                original_name=source.get('original_name', ''),
                nutrition=nutrition,
                source_db=source.get('source_db', ''),
                score=adjusted_score,
                match_type=match_type
            )
            
            results.append(result)
        
        # „Çπ„Ç≥„Ç¢È†Ü„Åß„ÇΩ„Éº„Éà
        results.sort(key=lambda x: x.score, reverse=True)
        
        # Âêå„Çπ„Ç≥„Ç¢ÁµêÊûú„ÅÆÂÜç„É©„É≥„Ç≠„É≥„Ç∞Ôºàsearch_name + description „Åß„ÅÆÂÜçÊ§úÁ¥¢Ôºâ
        results = await self._rerank_tied_scores(results, original_term, lemmatized_term)
        
        return results
    
    def _determine_match_type(self, db_name: str, db_name_lemmatized: str, original_term: str, lemmatized_term: str) -> str:
        """„Éû„ÉÉ„ÉÅ„Çø„Ç§„Éó„ÇíÂà§ÂÆö"""
        db_name_lower = db_name.lower()
        db_lemmatized_lower = db_name_lemmatized.lower()
        original_lower = original_term.lower()
        lemmatized_lower = lemmatized_term.lower()
        
        if db_name_lower == original_lower or db_lemmatized_lower == lemmatized_lower:
            return "exact"
        elif db_lemmatized_lower == lemmatized_lower:
            return "lemmatized"
        elif original_lower in db_name_lower or lemmatized_lower in db_lemmatized_lower:
            return "partial"
        else:
            return "fuzzy"
    
    def _calculate_adjusted_score(
        self, 
        base_score: float, 
        original_term: str, 
        lemmatized_term: str, 
        db_name: str, 
        db_name_lemmatized: str, 
        match_type: str
    ) -> float:
        """„Çπ„Ç≥„Ç¢Ë™øÊï¥Ë®àÁÆó"""
        
        # „Éô„Éº„Çπ„Çπ„Ç≥„Ç¢Ë™øÊï¥
        if match_type == "exact":
            adjustment = self.lemmatized_exact_match_boost
        elif match_type == "lemmatized":
            adjustment = self.lemmatized_exact_match_boost * 0.9
        elif match_type == "partial":
            adjustment = 0.8
        else:  # fuzzy
            adjustment = 0.5
        
        # Ë§áÂêàË™û„Éö„Éä„É´„ÉÜ„Ç£
        if len(original_term.split()) > 1:
            adjustment *= self.compound_word_penalty
        
        return base_score * adjustment
    
    async def _rerank_tied_scores(self, results: List[SearchResult], original_term: str, lemmatized_term: str) -> List[SearchResult]:
        """Âêå„Åò„Çπ„Ç≥„Ç¢„ÅÆÁµêÊûú„Çísearch_name + description„ÅßÂÜç„É©„É≥„Ç≠„É≥„Ç∞"""
        if len(results) <= 1:
            return results
        
        # „Çπ„Ç≥„Ç¢„Ç∞„É´„Éº„Éó„Çí‰ΩúÊàê
        score_groups = {}
        for result in results:
            rounded_score = round(result.score, 2)  # Â∞èÊï∞ÁÇπ2Ê°Å„Åß‰∏∏„ÇÅ„Çã
            if rounded_score not in score_groups:
                score_groups[rounded_score] = []
            score_groups[rounded_score].append(result)
        
        # ÂêÑ„Çπ„Ç≥„Ç¢„Ç∞„É´„Éº„Éó„ÅßÂÜç„É©„É≥„Ç≠„É≥„Ç∞
        reranked_results = []
        for score in sorted(score_groups.keys(), reverse=True):
            group = score_groups[score]
            
            if len(group) == 1:
                # Âçò‰∏ÄÁµêÊûú„ÅØ„Åù„ÅÆ„Åæ„Åæ
                reranked_results.extend(group)
            else:
                # Ë§áÊï∞ÁµêÊûú„ÇíÂÜç„É©„É≥„Ç≠„É≥„Ç∞
                reranked_group = await self._rerank_group_with_description(group, original_term, lemmatized_term)
                reranked_results.extend(reranked_group)
        
        return reranked_results
    
    async def _rerank_group_with_description(self, group: List[SearchResult], original_term: str, lemmatized_term: str) -> List[SearchResult]:
        """Âêå„Çπ„Ç≥„Ç¢„Ç∞„É´„Éº„Éó„Çísearch_name + description„ÅßÂÜç„É©„É≥„Ç≠„É≥„Ç∞"""
        try:
            # „Ç∞„É´„Éº„Éó„ÅÆID„ÇíÂèñÂæó
            group_ids = [result.id for result in group]
            
            # search_name + description „Åß„ÅÆÂÜçÊ§úÁ¥¢„ÇØ„Ç®„É™„ÇíÊßãÁØâ
            rerank_query = self._build_description_search_query(original_term, lemmatized_term, group_ids)
            
            # ÂÜçÊ§úÁ¥¢ÂÆüË°å
            response = await self.es_client.search(rerank_query)
            
            if not response or not response.get('hits', {}).get('hits'):
                return group  # ÂÜçÊ§úÁ¥¢Â§±ÊïóÊôÇ„ÅØÂÖÉ„ÅÆÈ†ÜÂ∫è„ÇíÁ∂≠ÊåÅ
            
            # ÂÜçÊ§úÁ¥¢ÁµêÊûú„Åß„Çπ„Ç≥„Ç¢„Éû„ÉÉ„Éó„Çí‰ΩúÊàê
            rerank_scores = {}
            for hit in response['hits']['hits']:
                item_id = str(hit['_source'].get('id', ''))
                rerank_scores[item_id] = hit['_score']
            
            # ÂÜç„É©„É≥„Ç≠„É≥„Ç∞ÂÆüË°å„Å®„Çπ„Ç≥„Ç¢Êõ¥Êñ∞
            original_base_score = group[0].score  # Âêå„Çπ„Ç≥„Ç¢„Ç∞„É´„Éº„Éó„Å™„ÅÆ„ÅßÊúÄÂàù„ÅÆ„Çπ„Ç≥„Ç¢„ÇíÂü∫Ê∫ñ„Å´
            reranked_group = []
            
            # description„ÅÆÈñ¢ÈÄ£ÊÄß„ÇíË©ï‰æ°„Åó„Å¶Â∑ÆÂà•Âåñ
            for i, result in enumerate(sorted(group, key=lambda r: rerank_scores.get(r.id, 0), reverse=True)):
                # description„Å´„Çà„ÇãËøΩÂä†„Éú„Éº„Éä„ÇπË®àÁÆó
                desc_bonus = self._calculate_description_relevance_bonus(result.description, original_term, lemmatized_term)
                rerank_bonus = rerank_scores.get(result.id, 0) * 0.1
                position_penalty = i * 0.001  # Âêå„Åòrerank„Çπ„Ç≥„Ç¢„Åß„ÇÇÈ†ÜÂ∫è„Çí‰øù„Å§„Åü„ÇÅ„ÅÆÂæÆË™øÊï¥
                
                new_score = original_base_score + rerank_bonus + desc_bonus - position_penalty
                
                # ÁµêÊûú„ÅÆ„Çπ„Ç≥„Ç¢„ÇíÊõ¥Êñ∞
                updated_result = SearchResult(
                    id=result.id,
                    name=result.name,
                    description=result.description,
                    original_name=result.original_name,
                    nutrition=result.nutrition,
                    source_db=result.source_db,
                    score=new_score,
                    match_type=result.match_type
                )
                reranked_group.append(updated_result)
            
            return reranked_group
            
        except Exception as e:
            logger.warning(f"Reranking failed: {e}")
            return group  # „Ç®„É©„ÉºÊôÇ„ÅØÂÖÉ„ÅÆÈ†ÜÂ∫è„ÇíÁ∂≠ÊåÅ
    
    def _calculate_description_relevance_bonus(self, description: str, original_term: str, lemmatized_term: str) -> float:
        """description„ÅÆÈñ¢ÈÄ£ÊÄß„Å´Âü∫„Å•„Åè„Çπ„Ç≥„Ç¢„Éú„Éº„Éä„ÇπË®àÁÆó"""
        if not description or description == "None":
            return 0.0
        
        desc_lower = description.lower()
        original_lower = original_term.lower()
        lemmatized_lower = lemmatized_term.lower()
        
        bonus = 0.0
        
        # Èñ¢ÈÄ£ÊÄß„ÅÆÈ´ò„ÅÑË™øÁêÜÊ≥ï„ÉªÂΩ¢ÊÖã„Ç≠„Éº„ÉØ„Éº„Éâ„Å´„Éú„Éº„Éä„Çπ
        cooking_methods = ["raw", "cooked", "boiled", "baked", "grilled", "fried", "steamed", "roasted"]
        forms = ["boneless", "skinless", "whole", "ground", "tenderloins", "meat only"]
        containers = ["canned", "fresh", "frozen", "dried"]
        
        # Ë™øÁêÜÊ≥ï„ÅÆÈñ¢ÈÄ£ÊÄß
        for method in cooking_methods:
            if method in desc_lower:
                if "raw" in desc_lower and ("fresh" in original_lower or "raw" in original_lower):
                    bonus += 0.02  # Áîü„Å´Èñ¢ÈÄ£„Åô„ÇãÊ§úÁ¥¢„Å´„ÅØÁîüÈ£üÂìÅ„ÇíÂÑ™ÂÖà
                elif method in original_lower or method in lemmatized_lower:
                    bonus += 0.05  # Ê§úÁ¥¢Ë™û„Å´Âê´„Åæ„Çå„ÇãË™øÁêÜÊ≥ï
                else:
                    bonus += 0.01  # ‰∏ÄËà¨ÁöÑ„Å™Ë™øÁêÜÊ≥ï
        
        # ÂΩ¢ÊÖã„ÅÆÈñ¢ÈÄ£ÊÄß
        for form in forms:
            if form in desc_lower:
                if form in original_lower or form in lemmatized_lower:
                    bonus += 0.03
                else:
                    bonus += 0.005
        
        # ÂÆπÂô®„Éª‰øùÂ≠òÂΩ¢ÊÖã
        for container in containers:
            if container in desc_lower:
                bonus += 0.002
        
        # ÂçòË™ûÊï∞„Å´„Çà„ÇãË§áÈõë„Åï„Éö„Éä„É´„ÉÜ„Ç£ÔºàÁ∞°ÊΩî„Å™Ë®òËø∞„ÇíÂÑ™ÂÖàÔºâ
        word_count = len(description.split(", "))
        if word_count > 3:
            bonus -= (word_count - 3) * 0.001
        
        return bonus
    
    def _build_description_search_query(self, original_term: str, lemmatized_term: str, target_ids: List[str]) -> Dict[str, Any]:
        """search_name + description „Åß„ÅÆÂÜçÊ§úÁ¥¢„ÇØ„Ç®„É™ÊßãÁØâ"""
        
        bool_query = {
            "bool": {
                "must": [
                    # ÂØæË±°ID„Åß„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
                    {
                        "terms": {
                            "id": target_ids
                        }
                    }
                ],
                "should": [
                    # search_name + description „ÅÆÁµÑ„ÅøÂêà„Çè„Åõ„Éï„Ç£„Éº„É´„Éâ„Åß„ÅÆÊ§úÁ¥¢
                    {
                        "multi_match": {
                            "query": lemmatized_term,
                            "fields": ["search_name_lemmatized^2.0", "description^1.0"],
                            "type": "best_fields"
                        }
                    },
                    {
                        "multi_match": {
                            "query": original_term,
                            "fields": ["search_name^2.0", "description^1.0"],
                            "type": "best_fields"
                        }
                    },
                    # „Éï„É¨„Éº„Ç∫„Éû„ÉÉ„ÉÅ
                    {
                        "multi_match": {
                            "query": lemmatized_term,
                            "fields": ["search_name_lemmatized", "description"],
                            "type": "phrase",
                            "boost": 1.5
                        }
                    },
                    # „Éï„Ç°„Ç∏„ÉºÊ§úÁ¥¢
                    {
                        "multi_match": {
                            "query": lemmatized_term,
                            "fields": ["search_name_lemmatized", "description"],
                            "fuzziness": "AUTO",
                            "boost": 0.5
                        }
                    }
                ],
                "minimum_should_match": 1
            }
        }
        
        return {
            "query": bool_query,
            "size": len(target_ids),
            "_source": ["id", "search_name", "description", "original_name"]
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """Ê§úÁ¥¢Áµ±Ë®àÂèñÂæó"""
        avg_response_time = (self.total_response_time / self.total_searches) if self.total_searches > 0 else 0
        
        return {
            "total_searches": self.total_searches,
            "average_response_time_ms": avg_response_time,
            "total_documents": self.es_client.get_total_documents(),
            "elasticsearch_health": self.es_client.get_index_stats()
        } 