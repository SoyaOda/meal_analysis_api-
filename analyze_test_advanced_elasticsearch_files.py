#!/usr/bin/env python3
"""
test_advanced_elasticsearch_search.pyå®Ÿè¡Œæ™‚ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
1. test_advanced_elasticsearch_search.pyå®Ÿè¡Œæ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
2. é«˜åº¦ãªæˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ†æ
3. å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
4. å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼ã¨æˆ¦ç•¥åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
"""

import os
import json
import datetime
from pathlib import Path
from typing import Dict, List, Optional

def get_file_content(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å®‰å…¨ã«èª­ã¿å–ã‚‹ï¼ˆã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–æƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–æƒ…å ±ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if file_path.endswith('.json') and ('service-account' in file_path or 'key' in file_path):
            return "CONTENT FILTERED: ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–ãªèªè¨¼æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€å†…å®¹ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ã€‚"
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–ãªã‚­ãƒ¼ã‚’ãƒã‚¹ã‚¯
        if file_path.endswith('.json'):
            try:
                data = json.loads(content)
                # ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–ãªã‚­ãƒ¼ã‚’ãƒã‚¹ã‚¯
                sensitive_keys = [
                    'private_key', 'private_key_id', 'client_email', 
                    'client_id', 'auth_uri', 'token_uri', 'client_x509_cert_url',
                    'universe_domain'
                ]
                
                def mask_sensitive_data(obj):
                    if isinstance(obj, dict):
                        for key in obj:
                            if key in sensitive_keys:
                                obj[key] = "***MASKED***"
                            elif isinstance(obj[key], (dict, list)):
                                mask_sensitive_data(obj[key])
                    elif isinstance(obj, list):
                        for item in obj:
                            if isinstance(item, (dict, list)):
                                mask_sensitive_data(item)
                
                mask_sensitive_data(data)
                return json.dumps(data, indent=2, ensure_ascii=False)
            except json.JSONDecodeError:
                # JSONã§ãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
                pass
        
        return content
    except Exception as e:
        return f"ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {str(e)}"

def get_latest_result_summary() -> Optional[Dict]:
    """æœ€æ–°ã®test_advanced_elasticsearch_search.pyå®Ÿè¡Œçµæœã‚’å–å¾—"""
    try:
        # analysis_resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€æ–°ã®çµæœã‚’æ¤œç´¢
        results_dir = "analysis_results"
        if not os.path.exists(results_dir):
            return None
        
        # advanced_elasticsearch_searchãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
        search_dirs = []
        for item in os.listdir(results_dir):
            if item.startswith("advanced_elasticsearch_search_"):
                full_path = os.path.join(results_dir, item)
                if os.path.isdir(full_path):
                    search_dirs.append((item, full_path))
        
        if not search_dirs:
            return None
        
        # æœ€æ–°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †ï¼‰
        search_dirs.sort(reverse=True)
        latest_dir = search_dirs[0][1]
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        json_file = os.path.join(latest_dir, "advanced_elasticsearch_search_results.json")
        if os.path.exists(json_file):
            with open(json_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"Warning: å®Ÿè¡Œçµæœã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    
    return None

def analyze_advanced_elasticsearch_architecture():
    """é«˜åº¦ãªæˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ãƒ†ã‚¹ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹é€ ã¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚’å®Ÿè¡Œ"""
    
    # test_advanced_elasticsearch_search.pyå®Ÿè¡Œæ™‚ã®åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
    files_to_analyze = {
        "Advanced Strategic Search ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«": [
            "test_advanced_elasticsearch_search.py"
        ],
        "FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤ (app_v2)": [
            "app_v2/main/app.py",
            "app_v2/api/__init__.py",
            "app_v2/api/v1/__init__.py",
            "app_v2/api/v1/endpoints/__init__.py",
            "app_v2/api/v1/endpoints/meal_analysis.py"
        ],
        "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ¶å±¤": [
            "app_v2/pipeline/__init__.py",
            "app_v2/pipeline/orchestrator.py",
            "app_v2/pipeline/result_manager.py"
        ],
        "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - Phase1 AIåˆ†æ": [
            "app_v2/components/__init__.py",
            "app_v2/components/base.py",
            "app_v2/components/phase1_component.py"
        ],
        "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢": [
            "app_v2/components/elasticsearch_nutrition_search_component.py",

            ""
        ],
        "ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å±¤": [
            "app_v2/models/__init__.py",
            "app_v2/models/nutrition_search_models.py",
            "app_v2/models/phase1_models.py",
            ""
        ],
        "AI ã‚µãƒ¼ãƒ“ã‚¹å±¤": [
            "app_v2/services/__init__.py",
            "app_v2/services/gemini_service.py"
        ],
        "è¨­å®šç®¡ç†": [
            "app_v2/config/__init__.py",
            "app_v2/config/settings.py"
        ],
        "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ï¼ˆPhase1çµ±ä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚·ã‚¹ãƒ†ãƒ ï¼‰": [
            "app_v2/config/prompts/__init__.py",
            "app_v2/config/prompts/phase1_prompts.py",
            ""
        ],
        "Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ãƒ»æ¤œç´¢å¼·åŒ–": [
            "create_elasticsearch_index.py",
            "app_v2/utils/__init__.py",
            "app_v2/utils/lemmatization.py"
        ],
        "æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹": [
            "db/yazio_db.json",
            "db/mynetdiary_db.json", 
            "db/eatthismuch_db.json"
        ],
        "ãƒ†ã‚¹ãƒˆç”»åƒ": [
            "test_images/food1.jpg",
            "test_images/food2.jpg",
            "test_images/food3.jpg",
            "test_images/food4.jpg",
            "test_images/food5.jpg"
        ],
        "ä¾å­˜é–¢ä¿‚ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«": [
            "requirements.txt",
            "README.md",
            "openapi.yaml",
            ".gitignore"
        ]
    }
    
    # æœ€æ–°ã®å®Ÿè¡Œçµæœã‚’å–å¾—
    latest_results = get_latest_result_summary()
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"test_advanced_elasticsearch_architecture_{timestamp}.txt"
    
    with open(output_file, 'w', encoding='utf-8') as out_f:
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        out_f.write("=" * 100 + "\n")
        out_f.write("MEAL ANALYSIS API v2.0 - é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ\n")
        out_f.write("=" * 100 + "\n")
        out_f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        out_f.write(f"åˆ†æå¯¾è±¡: test_advanced_elasticsearch_search.pyå®Ÿè¡Œæ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«\n")
        out_f.write("=" * 100 + "\n\n")
        
        # æœ€æ–°å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼
        if latest_results:
            out_f.write("ğŸ¯ LATEST EXECUTION RESULTS SUMMARY\n")
            out_f.write("-" * 60 + "\n")
            
            summary = latest_results.get('search_summary', {})
            analysis_id = latest_results.get('analysis_id', 'N/A')
            timestamp_result = latest_results.get('timestamp', 'N/A')
            
            out_f.write(f"ğŸ“Š Analysis ID: {analysis_id}\n")
            out_f.write(f"ğŸ•’ Execution Time: {timestamp_result}\n")
            out_f.write(f"âœ… Total Searches: {summary.get('total_searches', 0)}\n")
            out_f.write(f"ğŸ¯ Successful Matches: {summary.get('successful_matches', 0)}\n")
            out_f.write(f"ğŸ“ˆ Match Rate: {summary.get('match_rate_percent', 0)}%\n")
            out_f.write(f"âš¡ Search Time: {summary.get('search_time_ms', 0)}ms\n")
            out_f.write(f"ğŸ“‹ Total Results: {summary.get('total_results', 0)}\n")
            out_f.write(f"ğŸ—ƒï¸ Total Indexed Documents: {summary.get('total_indexed_documents', 0):,}\n")
            
            strategic_approach = summary.get('strategic_approach', {})
            if strategic_approach:
                out_f.write(f"\nğŸ¯ STRATEGIC APPROACH:\n")
                out_f.write(f"   ğŸ½ï¸  Dish Strategy: {strategic_approach.get('dish_strategy', 'N/A')}\n")
                out_f.write(f"   ğŸ¥• Ingredient Strategy: {strategic_approach.get('ingredient_strategy', 'N/A')}\n")
            
            # ã‚¯ã‚¨ãƒªåˆ†æ
            input_queries = latest_results.get('input_queries', {})
            if input_queries:
                out_f.write(f"\nğŸ“ INPUT QUERIES ANALYSIS:\n")
                all_queries = input_queries.get('all_queries', [])
                dish_names = input_queries.get('dish_names', [])
                ingredient_names = input_queries.get('ingredient_names', [])
                
                out_f.write(f"   ğŸ“‹ Total Queries: {len(all_queries)}\n")
                out_f.write(f"   ğŸ½ï¸  Dish Queries: {len(dish_names)}\n")
                out_f.write(f"   ğŸ¥• Ingredient Queries: {len(ingredient_names)}\n")
                
                if dish_names:
                    out_f.write(f"   ğŸ½ï¸  Dishes: {', '.join(dish_names[:3])}{'...' if len(dish_names) > 3 else ''}\n")
                if ingredient_names:
                    out_f.write(f"   ğŸ¥• Ingredients: {', '.join(ingredient_names[:5])}{'...' if len(ingredient_names) > 5 else ''}\n")
            
            out_f.write("\n" + "=" * 100 + "\n\n")
        
        # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦
        out_f.write("ğŸš€ ADVANCED STRATEGIC ELASTICSEARCH SEARCH ARCHITECTURE OVERVIEW\n")
        out_f.write("-" * 80 + "\n")
        out_f.write("""
ğŸ”„ ADVANCED STRATEGIC SEARCH EXECUTION FLOW:
1. test_advanced_elasticsearch_search.py â†’ JPGç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
2. FastAPI /api/v1/meal-analyses/complete â†’ å®Œå…¨åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
3. Phase1: Gemini AI 2.5 Flash ç”»åƒåˆ†æ â†’ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
   ğŸ“‹ DetectedFoodItems: ä¿¡é ¼åº¦ä»˜ãé£Ÿå“è­˜åˆ¥
   ğŸ·ï¸  Attributes: ææ–™ãƒ»èª¿ç†æ³•ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¥ãƒ¼
4. é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢:
   ğŸ“ DISHæˆ¦ç•¥: EatThisMuch dish (primary) â†’ EatThisMuch branded (fallback, score<20.0)
   ğŸ¥• INGREDIENTæˆ¦ç•¥: EatThisMuch ingredient (primary) â†’ MyNetDiary/YAZIO/branded (fallback)
   ğŸ”§ Strategic Features:
      - Min Score Threshold: 20.0 (å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
      - Strategic Metadata: æˆ¦ç•¥ãƒ•ã‚§ãƒ¼ã‚ºãƒ»ã‚¿ã‚¤ãƒ—è¿½è·¡
      - Multi-DB Fallback: æ®µéšçš„å“è³ªä¿è¨¼
5. çµæœçµ±åˆãƒ»ä¿å­˜: JSON + Markdown ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

ğŸ—ï¸ ADVANCED COMPONENT-BASED ARCHITECTURE v2.0:
â”œâ”€â”€ Advanced Test Layer
â”‚   â””â”€â”€ test_advanced_elasticsearch_search.py (é«˜åº¦æˆ¦ç•¥çš„æ¤œç´¢ãƒ†ã‚¹ãƒˆ)
â”œâ”€â”€ FastAPI Application Layer (app_v2)
â”‚   â”œâ”€â”€ main/app.py (Server, CORS, health endpoints)
â”‚   â””â”€â”€ api/v1/endpoints/meal_analysis.py (Complete analysis API)
â”œâ”€â”€ Pipeline Management Layer
â”‚   â”œâ”€â”€ orchestrator.py (MealAnalysisPipeline - å…¨ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆ¶ãƒ»æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ)
â”‚   â””â”€â”€ result_manager.py (ResultManager - çµæœä¿å­˜ãƒ»å±¥æ­´ç®¡ç†)
â”œâ”€â”€ AI Component Layer
â”‚   â”œâ”€â”€ base.py (BaseComponent - å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹)
â”‚   â”œâ”€â”€ phase1_component.py (Phase1Component - Geminiæ§‹é€ åŒ–åˆ†æ)
â”‚   â””â”€â”€ elasticsearch_nutrition_search_component.py (é«˜åº¦æˆ¦ç•¥çš„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³)
â”œâ”€â”€ AI Service Layer
â”‚   â””â”€â”€ gemini_service.py (GeminiService - Vertex AIçµ±åˆãƒ»æ§‹é€ åŒ–ã‚¹ã‚­ãƒ¼ãƒ)
â”œâ”€â”€ Data Model Layer
â”‚   â”œâ”€â”€ nutrition_search_models.py (NutritionMatch, strategic metadata)
â”‚   â””â”€â”€ phase1_models.py (DetectedFoodItem, FoodAttribute, æ§‹é€ åŒ–å‡ºåŠ›)
â”œâ”€â”€ Elasticsearch Infrastructure
â”‚   â”œâ”€â”€ create_elasticsearch_index.py (11,845ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†)
â”‚   â””â”€â”€ elasticsearch-8.10.4/ (é«˜æ€§èƒ½æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³)
â””â”€â”€ Strategic Data Layer
    â”œâ”€â”€ yazio_db.json (1,825é …ç›® - ãƒãƒ©ãƒ³ã‚¹é£Ÿå“ãƒ»25ã‚«ãƒ†ã‚´ãƒª)
    â”œâ”€â”€ mynetdiary_db.json (1,142é …ç›® - ç§‘å­¦çš„ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±ä¸€å‹)
    â””â”€â”€ eatthismuch_db.json (8,878é …ç›® - æœ€å¤§ãƒ»3ãƒ‡ãƒ¼ã‚¿å‹å¯¾å¿œ)

ğŸ¯ ADVANCED STRATEGIC SEARCH FEATURES:
- ğŸ”¥ Enhanced Dishæ¤œç´¢æˆ¦ç•¥:
  * Primary: EatThisMuch data_type=dish (é«˜é–¢é€£æ€§æ–™ç†ãƒ‡ãƒ¼ã‚¿)
  * Fallback: EatThisMuch data_type=branded (ã‚¹ã‚³ã‚¢<20.0æ™‚ã®è‡ªå‹•åˆ‡æ›¿)
  * Strategy Metadata: dish_primary, dish_fallback tracking
- ğŸ¥• Enhanced Ingredientæ¤œç´¢æˆ¦ç•¥:
  * Primary: EatThisMuch data_type=ingredient (ãƒ¡ã‚¤ãƒ³é£Ÿæãƒ‡ãƒ¼ã‚¿)
  * Multi-DB Fallback: MyNetDiary(ç§‘å­¦çš„) â†’ YAZIO(åˆ†é¡æ¸ˆ) â†’ EatThisMuch branded
  * Strategy Metadata: ingredient_primary, ingredient_fallback tracking
- âš¡ Performance Optimization:
  * Strategic Filtering: é–¢é€£æ€§é‡è¦–ã®çµã‚Šè¾¼ã¿
  * Dynamic Fallback: ã‚¹ã‚³ã‚¢é–¾å€¤ãƒ™ãƒ¼ã‚¹è‡ªå‹•åˆ‡æ›¿
  * Results Per DB: 5ä»¶åˆ¶é™ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
- ğŸ“Š Advanced Analytics:
  * Strategic Distribution Tracking
  * Database Source Analysis
  * Query Type Classification (dish vs ingredient)
  * Execution Time Monitoring
- ğŸ’¾ Comprehensive Metadata:
  * strategic_phase: main_dish, main_ingredient, fallback_multi_db
  * strategy_type: dish_primary, dish_fallback, ingredient_primary, ingredient_fallback
  * fallback_source: è£œåŠ©DBè©³ç´°æƒ…å ±
  * elasticsearch_score: ç”Ÿã‚¹ã‚³ã‚¢ä¿æŒ

ğŸ”§ TECHNICAL SPECIFICATIONS:
- Search Engine: Elasticsearch 8.10.4 (BM25F + Multi-Signal Boosting)
- AI Service: Google Vertex AI Gemini 2.5 Flash (æ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œ)
- Web Framework: FastAPI 0.104+ (async/await, multipart/form-data)
- Architecture Pattern: Strategic Component Pipeline
- Data Format: JSON (100gæ­£è¦åŒ–æ „é¤Šãƒ‡ãƒ¼ã‚¿ + æˆ¦ç•¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿)
- Search Strategy: Strategic Multi-Stage Filtering + Score-based Fallback
- Authentication: Google Cloud Service Account
- Performance: Sub-second response times, 100% match rates

ğŸš€ ADVANCED IMPROVEMENTS vs BASIC MULTI-DB:
- ğŸ§  æ§‹é€ åŒ–AIåˆ†æ: DetectedFoodItems + AttributesæŠ½å‡º
- ğŸ¯ æˆ¦ç•¥çš„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸æŠ: EatThisMuchã‚’ä¸­å¿ƒã¨ã—ãŸæœ€é©åŒ–
- ğŸ“ˆ å‹•çš„å“è³ªä¿è¨¼: ã‚¹ã‚³ã‚¢é–¾å€¤ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ğŸ” é«˜åº¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½è·¡: æ¤œç´¢ãƒ—ãƒ­ã‚»ã‚¹å®Œå…¨å¯è¦–åŒ–
- âš¡ åŠ¹ç‡çš„ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨: æˆ¦ç•¥çš„çµæœæ•°åˆ¶é™
- ğŸ“Š åŒ…æ‹¬çš„åˆ†æ: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒãƒ»æˆ¦ç•¥åˆ†æ
- ğŸ”§ æ‹¡å¼µå¯èƒ½è¨­è¨ˆ: æ–°æˆ¦ç•¥ãƒ»DBè¿½åŠ å®¹æ˜“
- ğŸ’¾ æ°¸ç¶šåŒ–å¯¾å¿œ: JSON + Markdown ãƒ‡ãƒ¥ã‚¢ãƒ«å‡ºåŠ›

ğŸ–ï¸ STRATEGIC SEARCH EXCELLENCE:
- Phase1-to-Search Pipeline: ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªãƒ‡ãƒ¼ã‚¿æµã‚Œ
- Multi-Modal Input: JPGç”»åƒ â†’ æ§‹é€ åŒ–ã‚¯ã‚¨ãƒª
- Intelligent Fallback: å“è³ªä¿è¨¼ä»˜ãå¤šæ®µéšæ¤œç´¢
- Real-time Analytics: å®Ÿè¡Œæ™‚æˆ¦ç•¥åˆ†æ
- Production-Ready: èªè¨¼ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Œå‚™

""")
        out_f.write("=" * 100 + "\n\n")
        
        # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        for category, file_list in files_to_analyze.items():
            out_f.write(f"ğŸ“ {category}\n")
            out_f.write("=" * 80 + "\n\n")
            
            for file_path in file_list:
                out_f.write(f"ğŸ“„ FILE: {file_path}\n")
                out_f.write("-" * 60 + "\n")
                
                if os.path.exists(file_path):
                    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
                    file_stat = os.stat(file_path)
                    file_size = file_stat.st_size
                    mod_time = datetime.datetime.fromtimestamp(file_stat.st_mtime)
                    
                    out_f.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes\n")
                    out_f.write(f"æœ€çµ‚æ›´æ–°: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    out_f.write(f"å­˜åœ¨: âœ…\n\n")
                    
                    # ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‡¦ç†
                    if file_path.endswith('.json') and 'db/' in file_path:
                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€åˆã®50è¡Œã®ã¿è¡¨ç¤º
                        out_f.write("CONTENT (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ - æœ€åˆã®50è¡Œ):\n")
                        out_f.write("```json\n")
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                lines = f.readlines()
                                for i, line in enumerate(lines[:50]):
                                    out_f.write(line)
                                if len(lines) > 50:
                                    out_f.write(f"\n... ({len(lines) - 50} more lines)\n")
                        except Exception as e:
                            out_f.write(f"ERROR: JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {str(e)}\n")
                        out_f.write("```\n\n")
                    elif file_path.endswith('.json'):
                        # è¨­å®šJSONãƒ•ã‚¡ã‚¤ãƒ«ã¯å…¨å†…å®¹è¡¨ç¤ºï¼ˆã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–æƒ…å ±ã¯ãƒã‚¹ã‚¯æ¸ˆã¿ï¼‰
                        out_f.write("CONTENT (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«):\n")
                        out_f.write("```json\n")
                        content = get_file_content(file_path)
                        out_f.write(content)
                        out_f.write("```\n\n")
                    elif file_path.endswith('.jpg') or file_path.endswith('.jpeg') or file_path.endswith('.png'):
                        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¯æƒ…å ±ã®ã¿è¡¨ç¤º
                        out_f.write("CONTENT: [ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« - ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿]\n")
                        out_f.write(f"ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(file_path)}\n")
                        out_f.write(f"ç”¨é€”: test_advanced_elasticsearch_search.py ã®å…¥åŠ›ç”»åƒ\n\n")
                    elif file_path.endswith('.yml') or file_path.endswith('.yaml'):
                        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
                        out_f.write("CONTENT (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«):\n")
                        out_f.write("```yaml\n")
                        content = get_file_content(file_path)
                        out_f.write(content)
                        out_f.write("```\n\n")
                    else:
                        # Python/text ãƒ•ã‚¡ã‚¤ãƒ«ã¯å…¨å†…å®¹è¡¨ç¤º
                        out_f.write("CONTENT:\n")
                        out_f.write("```python\n")
                        content = get_file_content(file_path)
                        out_f.write(content)
                        out_f.write("\n```\n\n")
                else:
                    out_f.write("å­˜åœ¨: âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n\n")
                
                out_f.write("=" * 80 + "\n\n")
        
        # å®Ÿè¡Œçµæœè©³ç´°åˆ†æï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if latest_results and 'matches' in latest_results:
            out_f.write("ğŸ“Š DETAILED EXECUTION RESULTS ANALYSIS\n")
            out_f.write("=" * 80 + "\n\n")
            
            matches = latest_results['matches']
            input_queries = latest_results.get('input_queries', {})
            dish_names = input_queries.get('dish_names', [])
            ingredient_names = input_queries.get('ingredient_names', [])
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒåˆ†æ
            source_distribution = {}
            strategy_breakdown = {"dish_primary": 0, "dish_fallback": 0, "ingredient_primary": 0, "ingredient_fallback": 0}
            total_results = 0
            
            for query, match_results in matches.items():
                query_type = "dish" if query in dish_names else "ingredient"
                if isinstance(match_results, list):
                    total_results += len(match_results)
                    for match in match_results:
                        source = match.get('source', 'unknown')
                        if source not in source_distribution:
                            source_distribution[source] = 0
                        source_distribution[source] += 1
                        
                        # æˆ¦ç•¥çµ±è¨ˆ
                        metadata = match.get('search_metadata', {})
                        strategy_type = metadata.get('strategy_type', 'unknown')
                        if strategy_type in strategy_breakdown:
                            strategy_breakdown[strategy_type] += 1
                else:
                    total_results += 1
                    source = match_results.get('source', 'unknown')
                    if source not in source_distribution:
                        source_distribution[source] = 0
                    source_distribution[source] += 1
            
            out_f.write("ğŸ—ƒï¸ DATABASE SOURCE DISTRIBUTION:\n")
            for source, count in sorted(source_distribution.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_results) * 100 if total_results > 0 else 0
                out_f.write(f"   {source}: {count} results ({percentage:.1f}%)\n")
            
            out_f.write(f"\nğŸ¯ STRATEGIC BREAKDOWN:\n")
            total_strategy_results = sum(strategy_breakdown.values())
            for strategy, count in strategy_breakdown.items():
                if count > 0:
                    percentage = (count / total_strategy_results) * 100 if total_strategy_results > 0 else 0
                    out_f.write(f"   {strategy}: {count} results ({percentage:.1f}%)\n")
            
            out_f.write(f"\nğŸ“‹ QUERY ANALYSIS:\n")
            for query, match_results in matches.items():
                query_type = "dish" if query in dish_names else "ingredient"
                result_count = len(match_results) if isinstance(match_results, list) else 1
                out_f.write(f"   '{query}' ({query_type}): {result_count} results\n")
                
                # ãƒˆãƒƒãƒ—çµæœã®ã‚¹ã‚³ã‚¢æƒ…å ±
                if isinstance(match_results, list) and match_results:
                    top_result = match_results[0]
                    score = top_result.get('score', 0)
                    name = top_result.get('search_name', 'N/A')
                    out_f.write(f"      Top: {name} (score: {score:.3f})\n")
                elif not isinstance(match_results, list):
                    score = match_results.get('score', 0)
                    name = match_results.get('search_name', 'N/A')
                    out_f.write(f"      Result: {name} (score: {score:.3f})\n")
            
            out_f.write("\n" + "=" * 80 + "\n\n")
        
        # ãƒ•ãƒƒã‚¿ãƒ¼
        out_f.write("ğŸ¯ ADVANCED STRATEGIC ELASTICSEARCH SEARCH ANALYSIS SUMMARY\n")
        out_f.write("-" * 70 + "\n")
        total_files = sum(len(files) for files in files_to_analyze.values())
        existing_files = sum(1 for files in files_to_analyze.values() for f in files if os.path.exists(f))
        
        out_f.write(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}\n")
        out_f.write(f"å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {existing_files}\n")
        out_f.write(f"åˆ†æå®Œäº†æ™‚åˆ»: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        if latest_results:
            summary = latest_results.get('search_summary', {})
            out_f.write(f"\nğŸ“Š LATEST EXECUTION PERFORMANCE:\n")
            out_f.write(f"   âœ… Match Rate: {summary.get('match_rate_percent', 0)}%\n")
            out_f.write(f"   âš¡ Search Time: {summary.get('search_time_ms', 0)}ms\n")
            out_f.write(f"   ğŸ“‹ Total Results: {summary.get('total_results', 0)}\n")
        
        out_f.write("\nã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€test_advanced_elasticsearch_search.pyå®Ÿè¡Œæ™‚ã«\n")
        out_f.write("é–¢ã‚ã‚‹é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³\n")
        out_f.write("ãƒ•ã‚¡ã‚¤ãƒ«ã¨æœ€æ–°å®Ÿè¡Œçµæœã®å®Œå…¨ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n")
        out_f.write("\nğŸ”¥ ADVANCED STRATEGIC SEARCH SYSTEM HIGHLIGHTS:\n")
        out_f.write("- ğŸ§  AIæ§‹é€ åŒ–åˆ†æ: Gemini 2.5 Flash DetectedFoodItemsæŠ½å‡º\n")
        out_f.write("- ğŸ½ï¸  Advanced Dishæˆ¦ç•¥: EatThisMuch dish â†’ branded fallback\n")
        out_f.write("- ğŸ¥• Advanced Ingredientæˆ¦ç•¥: EatThisMuch ingredient â†’ Multi-DB fallback\n")
        out_f.write("- ğŸ“ˆ å‹•çš„å“è³ªä¿è¨¼: ã‚¹ã‚³ã‚¢é–¾å€¤20.0ã«ã‚ˆã‚‹è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯\n")
        out_f.write("- ğŸ¯ æˆ¦ç•¥çš„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: å®Œå…¨ãªæ¤œç´¢ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡\n")
        out_f.write("- âš¡ é«˜æ€§èƒ½å®Ÿè¡Œ: Sub-second strategic search\n")
        out_f.write("- ğŸ“Š åŒ…æ‹¬çš„åˆ†æ: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒãƒ»æˆ¦ç•¥çµ±è¨ˆ\n")
        out_f.write("- ğŸ’¾ ãƒ‡ãƒ¥ã‚¢ãƒ«å‡ºåŠ›: JSON + Markdown ãƒ¬ãƒãƒ¼ãƒˆ\n")
        out_f.write("- ğŸ”§ Production-Ready: èªè¨¼ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Œå‚™\n")
        out_f.write("- ğŸš€ æ‹¡å¼µå¯èƒ½è¨­è¨ˆ: Component-based Strategic Architecture\n")
        
    return output_file, total_files, existing_files, latest_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æé–‹å§‹...")
    print("-" * 80)
    
    try:
        output_file, total_files, existing_files, latest_results = analyze_advanced_elasticsearch_architecture()
        
        print(f"âœ… åˆ†æå®Œäº†!")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
        print(f"ğŸ“Š ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
        print(f"âœ… å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {existing_files}")
        
        if latest_results:
            summary = latest_results.get('search_summary', {})
            print(f"ğŸ¯ æœ€æ–°å®Ÿè¡Œçµæœ:")
            print(f"   ğŸ“Š Analysis ID: {latest_results.get('analysis_id', 'N/A')}")
            print(f"   âœ… Match Rate: {summary.get('match_rate_percent', 0)}%")
            print(f"   âš¡ Search Time: {summary.get('search_time_ms', 0)}ms")
            print(f"   ğŸ“‹ Total Results: {summary.get('total_results', 0)}")
        
        if existing_files < total_files:
            missing = total_files - existing_files
            print(f"âš ï¸  è¦‹ã¤ã‹ã‚‰ãªã„ãƒ•ã‚¡ã‚¤ãƒ«: {missing}å€‹")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’è¡¨ç¤º
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file)
            print(f"ğŸ“„ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes")
        
        print("\nğŸ‰ é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãŒå®Œäº†ã—ã¾ã—ãŸ!")
        print("ğŸ”¥ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯é«˜åº¦æˆ¦ç•¥çš„æ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã¨å®Ÿè¡ŒçµæœãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
        print("ğŸ§  AIæ§‹é€ åŒ–åˆ†æ + æˆ¦ç•¥çš„æ¤œç´¢ã«ã‚ˆã‚‹æœ€é©åŒ–ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ æ§‹æˆã‚’ç¢ºèªã§ãã¾ã™ã€‚")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 