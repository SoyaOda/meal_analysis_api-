================================================================================
MEAL ANALYSIS API v2.0 - ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹é€ ã¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
================================================================================
ç”Ÿæˆæ—¥æ™‚: 2025-06-06 13:59:43
åˆ†æå¯¾è±¡: test_local_nutrition_search_v2.pyå®Ÿè¡Œæ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«
================================================================================

ğŸ“Š LOCAL NUTRITION SEARCH ARCHITECTURE OVERVIEW
----------------------------------------

ğŸ”„ LOCAL NUTRITION SEARCH EXECUTION FLOW:
Phase 1: ç”»åƒ â†’ Gemini AI â†’ æ–™ç†ãƒ»é£Ÿæè­˜åˆ¥ (è‹±èªå)
Local Nutrition Search: é£Ÿæå â†’ BM25F + ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚° â†’ ãƒ­ãƒ¼ã‚«ãƒ«DBæ¤œç´¢
Pipeline Result: æ¤œç´¢çµæœ â†’ æ „é¤Šä¾¡ãƒãƒƒãƒ”ãƒ³ã‚° â†’ å®Œå…¨åˆ†æçµæœ

ğŸ—ï¸ COMPONENT-BASED ARCHITECTURE STRUCTURE:
â”œâ”€â”€ FastAPI Application Layer v2.0
â”‚   â””â”€â”€ app.py (Server, routing, CORS, environment setup)
â”œâ”€â”€ Local Nutrition Search API Layer  
â”‚   â””â”€â”€ meal_analysis.py (Unified endpoint with local search integration)
â”œâ”€â”€ Pipeline Integration Layer
â”‚   â”œâ”€â”€ orchestrator.py (Local/USDA search switching, component coordination)
â”‚   â””â”€â”€ result_manager.py (Phase-based result saving, metadata management)
â”œâ”€â”€ Component Layer
â”‚   â”œâ”€â”€ base.py (Abstract component interface, logging, error handling)
â”‚   â”œâ”€â”€ phase1_component.py (Gemini AI image analysis)
â”‚   â””â”€â”€ local_nutrition_search_component.py (Local database search integration)
â”œâ”€â”€ Model Layer
â”‚   â”œâ”€â”€ nutrition_search_models.py (Generic nutrition search models)
â”‚   â”œâ”€â”€ usda_models.py (USDA compatibility models)
â”‚   â””â”€â”€ phase1_models.py (Image analysis input/output models)
â”œâ”€â”€ Service Layer
â”‚   â”œâ”€â”€ gemini_service.py (Vertex AI Gemini integration)
â”‚   â””â”€â”€ usda_service.py (USDA API compatibility service)
â”œâ”€â”€ Configuration Layer
â”‚   â””â”€â”€ settings.py (Local search flags, environment variables)
â””â”€â”€ Local Nutrition Database Search System
    â”œâ”€â”€ search_handler.py (Main search API, result formatting)
    â”œâ”€â”€ query_builder.py (Query preprocessing, search optimization)
    â”œâ”€â”€ query_preprocessor.py (NLP processing, synonym handling)
    â”œâ”€â”€ data_loader.py (Database loading, caching)
    â”œâ”€â”€ scoring.py (BM25F, multi-signal boosting algorithms)
    â””â”€â”€ search_config.py (Search parameters, algorithm settings)

ğŸ”§ LOCAL NUTRITION SEARCH TECHNICAL FEATURES:
- ğŸ” BM25F + Multi-Signal Boosting: Advanced relevance scoring algorithm
- ğŸ“Š 8,878-Item Local Database: Offline nutrition calculation capability
- âš¡ 90.9% Match Rate: Real-world tested search accuracy
- ğŸ”„ USDA Compatibility: Seamless integration with existing pipeline
- ğŸŒ Elastic Search Fallback: Direct database search when ES unavailable
- ğŸ“± Generic Model Interface: Nutrition search abstraction layer
- ğŸ’¾ Phase-Based Result Saving: Organized file structure by component
- ğŸ›¡ï¸ Component Error Isolation: Independent component failure handling
- ğŸ“ˆ Advanced Search Features: Stemming, synonym matching, word boundary handling
- ğŸ¯ Food-Specific Optimization: Specialized for ingredient/dish search

ğŸ¯ KEY ADVANTAGES OVER USDA API APPROACH:
- Offline capability: No external API dependency
- Higher accuracy: 90.9% vs typical 70-80% match rates
- Faster response: Local database vs network requests
- Food-optimized search: Specialized algorithms for nutrition data
- Comprehensive database: 8,878 curated nutrition items
- Advanced NLP: Word boundary, stemming, synonym processing
- Multi-category support: Dish, ingredient, and branded food data

ğŸ—„ï¸ DATABASE STRUCTURE:
- æ–™ç†ãƒ»ãƒ¬ã‚·ãƒ”ãƒ‡ãƒ¼ã‚¿: 4,583é …ç›®
- é£Ÿæãƒ»åŸºæœ¬é£Ÿå“ãƒ‡ãƒ¼ã‚¿: 1,473é …ç›®  
- ãƒ–ãƒ©ãƒ³ãƒ‰é£Ÿå“ãƒ‡ãƒ¼ã‚¿: 2,822é …ç›®
- çµ±åˆæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: 8,878é …ç›®
- è©³ç´°ä»•æ§˜æ›¸: nutrition_database_specification.md

ğŸ”¬ SEARCH ALGORITHM DETAILS:
- Search Target: search_name field (string, 5-10 words typically)
- Word Boundary Handling: "cook" â†’ "cooking"/"cooked" (high) vs "cookie" (low)
- Scoring Method: BM25F + semantic relevance + exact match boosting
- Performance Target: 90%+ accuracy, <1 second response time
- Fallback Strategy: Direct JSON search when ElasticSearch unavailable

================================================================================

ğŸ“ FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤ v2.0
============================================================

ğŸ“„ FILE: app_v2/main/app.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,030 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:55:53
å­˜åœ¨: âœ…

CONTENT:
```
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ..api.v1.endpoints import meal_analysis
from ..config import get_settings

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆæ—¢å­˜ã®appã¨åŒã˜ï¼‰
os.environ.setdefault("USDA_API_KEY", "vSWtKJ3jYD0Cn9LRyVJUFkuyCt9p8rEtVXz74PZg")
os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", "/Users/odasoya/meal_analysis_api /service-account-key.json")
os.environ.setdefault("GEMINI_PROJECT_ID", "recording-diet-ai-3e7cf")
os.environ.setdefault("GEMINI_LOCATION", "us-central1")
os.environ.setdefault("GEMINI_MODEL_NAME", "gemini-2.5-flash-preview-05-20")

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# FastAPIã‚¢ãƒ—ãƒªã®ä½œæˆ
app = FastAPI(
    title="é£Ÿäº‹åˆ†æ API v2.0",
    description="ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ã•ã‚ŒãŸé£Ÿäº‹åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ç™»éŒ²
app.include_router(
    meal_analysis.router,
    prefix="/api/v1/meal-analyses",
    tags=["Complete Meal Analysis v2.0"]
)

# ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "é£Ÿäº‹åˆ†æ API v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ",
        "version": "2.0.0",
        "architecture": "Component-based Pipeline",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "version": "v2.0",
        "components": ["Phase1Component", "USDAQueryComponent"]
    }

if __name__ == "__main__":
    import uvicorn
    settings = get_settings()
    uvicorn.run(
        "app_v2.main.app:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=True
    ) 
```

============================================================

ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå±¤
============================================================

ğŸ“„ FILE: app_v2/api/v1/endpoints/meal_analysis.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,696 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:11:02
å­˜åœ¨: âœ…

CONTENT:
```
from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from typing import Optional
import logging

from ....pipeline import MealAnalysisPipeline

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/complete")
async def complete_meal_analysis(
    image: UploadFile = File(...),
    save_results: bool = Form(True),
    save_detailed_logs: bool = Form(True)
):
    """
    å®Œå…¨ãªé£Ÿäº‹åˆ†æã‚’å®Ÿè¡Œï¼ˆv2.0 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆï¼‰
    
    - Phase 1: Gemini AIã«ã‚ˆã‚‹ç”»åƒåˆ†æ
    - USDA Query: é£Ÿæã®USDAãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ
    - Phase 2: è¨ˆç®—æˆ¦ç•¥æ±ºå®šã¨æ „é¤Šä¾¡ç²¾ç·»åŒ– (TODO)
    - Nutrition Calculation: æœ€çµ‚æ „é¤Šä¾¡è¨ˆç®— (TODO)
    
    Args:
        image: åˆ†æå¯¾è±¡ã®é£Ÿäº‹ç”»åƒ
        save_results: çµæœã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
        save_detailed_logs: è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
    
    Returns:
        å®Œå…¨ãªåˆ†æçµæœã¨æ „é¤Šä¾¡è¨ˆç®—ã€è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    
    try:
        # ç”»åƒã®æ¤œè¨¼
        if not image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”»åƒã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        image_data = await image.read()
        logger.info(f"Starting complete meal analysis pipeline v2.0 (detailed_logs: {save_detailed_logs})")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
        pipeline = MealAnalysisPipeline()
        result = await pipeline.execute_complete_analysis(
            image_bytes=image_data,
            image_mime_type=image.content_type,
            save_results=save_results,
            save_detailed_logs=save_detailed_logs
        )
        
        logger.info(f"Complete analysis pipeline v2.0 finished successfully")
        
        return JSONResponse(
            status_code=200,
            content=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Complete analysis failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Complete analysis failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "healthy", "version": "v2.0", "message": "é£Ÿäº‹åˆ†æAPI v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ"}


@router.get("/pipeline-info")
async def get_pipeline_info():
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã®å–å¾—"""
    pipeline = MealAnalysisPipeline()
    return pipeline.get_pipeline_info() 
```

============================================================

ğŸ“ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆå±¤
============================================================

ğŸ“„ FILE: app_v2/pipeline/orchestrator.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 11,800 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:43:18
å­˜åœ¨: âœ…

CONTENT:
```
import uuid
import json
from datetime import datetime
from typing import Optional, Dict, Any
import logging

from ..components import Phase1Component, USDAQueryComponent, LocalNutritionSearchComponent
from ..models import (
    Phase1Input, Phase1Output,
    USDAQueryInput, USDAQueryOutput,
    NutritionQueryInput
)
from ..config import get_settings
from .result_manager import ResultManager

logger = logging.getLogger(__name__)


class MealAnalysisPipeline:
    """
    é£Ÿäº‹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
    
    4ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã‚’çµ±åˆã—ã¦å®Œå…¨ãªåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    
    def __init__(self, use_local_nutrition_search: Optional[bool] = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            use_local_nutrition_search: ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
                                      None: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾—
                                      True: LocalNutritionSearchComponentä½¿ç”¨
                                      False: å¾“æ¥ã®USDAQueryComponentä½¿ç”¨
        """
        self.pipeline_id = str(uuid.uuid4())[:8]
        self.settings = get_settings()
        
        # è¨­å®šã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’æ±ºå®š
        if use_local_nutrition_search is None:
            self.use_local_nutrition_search = self.settings.USE_LOCAL_NUTRITION_SEARCH
        else:
            self.use_local_nutrition_search = use_local_nutrition_search
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.phase1_component = Phase1Component()
        
        # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é¸æŠ
        if self.use_local_nutrition_search:
            self.nutrition_search_component = LocalNutritionSearchComponent()
            self.search_component_name = "LocalNutritionSearchComponent"
            logger.info("Using local nutrition database search (nutrition_db_experiment)")
        else:
            self.nutrition_search_component = USDAQueryComponent()
            self.search_component_name = "USDAQueryComponent"
            logger.info("Using traditional USDA API search")
            
        # TODO: Phase2Componentã¨NutritionCalculationComponentã‚’è¿½åŠ 
        
        self.logger = logging.getLogger(f"{__name__}.{self.pipeline_id}")
        
    async def execute_complete_analysis(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        optional_text: Optional[str] = None,
        save_results: bool = True,
        save_detailed_logs: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œå…¨ãªé£Ÿäº‹åˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            image_bytes: ç”»åƒãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆ
            save_results: çµæœã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            save_detailed_logs: è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            å®Œå…¨ãªåˆ†æçµæœ
        """
        analysis_id = str(uuid.uuid4())[:8]
        start_time = datetime.now()
        
        # ResultManagerã®åˆæœŸåŒ–
        result_manager = ResultManager(analysis_id) if save_detailed_logs else None
        
        self.logger.info(f"[{analysis_id}] Starting complete meal analysis pipeline")
        self.logger.info(f"[{analysis_id}] Nutrition search method: {'Local Database' if self.use_local_nutrition_search else 'USDA API'}")
        
        try:
            # === Phase 1: ç”»åƒåˆ†æ ===
            self.logger.info(f"[{analysis_id}] Phase 1: Image analysis")
            
            phase1_input = Phase1Input(
                image_bytes=image_bytes,
                image_mime_type=image_mime_type,
                optional_text=optional_text
            )
            
            # Phase1ã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            phase1_log = result_manager.create_execution_log("Phase1Component", f"{analysis_id}_phase1") if result_manager else None
            
            phase1_result = await self.phase1_component.execute(phase1_input, phase1_log)
            
            self.logger.info(f"[{analysis_id}] Phase 1 completed - Detected {len(phase1_result.dishes)} dishes")
            
            # === Nutrition Search Phase: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ ===
            search_phase_name = "Local Nutrition Search" if self.use_local_nutrition_search else "USDA Query"
            self.logger.info(f"[{analysis_id}] {search_phase_name} Phase: Database matching")
            
            # çµ±ä¸€ã•ã‚ŒãŸæ „é¤Šæ¤œç´¢å…¥åŠ›ã‚’ä½œæˆï¼ˆUSDAäº’æ›æ€§ã‚’ä¿æŒï¼‰
            nutrition_search_input = USDAQueryInput(
                ingredient_names=phase1_result.get_all_ingredient_names(),
                dish_names=phase1_result.get_all_dish_names()
            )
            
            # Nutrition Searchã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            search_log = result_manager.create_execution_log(self.search_component_name, f"{analysis_id}_nutrition_search") if result_manager else None
            
            nutrition_search_result = await self.nutrition_search_component.execute(nutrition_search_input, search_log)
            
            self.logger.info(f"[{analysis_id}] {search_phase_name} completed - {nutrition_search_result.get_match_rate():.1%} match rate")
            
            # === æš«å®šçš„ãªçµæœã®æ§‹ç¯‰ (Phase2ã¨Nutritionã¯å¾Œã§è¿½åŠ ) ===
            
            # Phase1ã®çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆæ¤œç´¢ç‰¹åŒ–ï¼‰
            phase1_dict = {
                "dishes": [
                    {
                        "dish_name": dish.dish_name,
                        "ingredients": [
                            {
                                "ingredient_name": ing.ingredient_name
                            }
                            for ing in dish.ingredients
                        ]
                    }
                    for dish in phase1_result.dishes
                ]
            }
            
            # ç°¡å˜ãªæ „é¤Šè¨ˆç®—ï¼ˆæš«å®šï¼‰
            total_calories = sum(
                len(dish.ingredients) * 50  # ä»®ã®è¨ˆç®—
                for dish in phase1_result.dishes
            )
            
            # å®Œå…¨åˆ†æçµæœã®æ§‹ç¯‰
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            complete_result = {
                "analysis_id": analysis_id,
                "phase1_result": phase1_dict,
                "nutrition_search_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary,
                    "search_method": "local_nutrition_database" if self.use_local_nutrition_search else "usda_api"
                },
                # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®ãŸã‚ã€usdaã‚­ãƒ¼ã‚‚æ®‹ã™
                "usda_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary
                },
                "processing_summary": {
                    "total_dishes": len(phase1_result.dishes),
                    "total_ingredients": len(phase1_result.get_all_ingredient_names()),
                    "nutrition_search_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",
                    "usda_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",  # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§
                    "total_calories": total_calories,
                    "pipeline_status": "completed",
                    "processing_time_seconds": processing_time,
                    "search_method": "local_nutrition_database" if self.use_local_nutrition_search else "usda_api"
                },
                # æš«å®šçš„ãªæœ€çµ‚çµæœ
                "final_nutrition_result": {
                    "dishes": phase1_dict["dishes"],
                    "total_meal_nutrients": {
                        "calories_kcal": total_calories,
                        "protein_g": total_calories * 0.15,  # ä»®ã®å€¤
                        "carbohydrates_g": total_calories * 0.55,  # ä»®ã®å€¤
                        "fat_g": total_calories * 0.30,  # ä»®ã®å€¤
                    }
                },
                "metadata": {
                    "pipeline_version": "v2.0",
                    "timestamp": datetime.now().isoformat(),
                    "components_used": ["Phase1Component", self.search_component_name],
                    "nutrition_search_method": "local_database" if self.use_local_nutrition_search else "usda_api"
                }
            }
            
            # ResultManagerã«æœ€çµ‚çµæœã‚’è¨­å®š
            if result_manager:
                result_manager.set_final_result(complete_result)
                result_manager.finalize_pipeline()
            
            # çµæœã®ä¿å­˜
            saved_files = {}
            if save_detailed_logs and result_manager:
                # æ–°ã—ã„ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ä¿å­˜æ–¹å¼
                saved_files = result_manager.save_phase_results()
                complete_result["analysis_folder"] = result_manager.get_analysis_folder_path()
                complete_result["saved_files"] = saved_files
                
                logger.info(f"[{analysis_id}] Detailed logs saved to folder: {result_manager.get_analysis_folder_path()}")
                logger.info(f"[{analysis_id}] Saved {len(saved_files)} files across all phases")
            
            if save_results:
                # é€šå¸¸ã®çµæœä¿å­˜ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
                saved_file = f"analysis_results/meal_analysis_{analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                complete_result["legacy_saved_to"] = saved_file
            
            self.logger.info(f"[{analysis_id}] Complete analysis pipeline finished successfully in {processing_time:.2f}s")
            
            return complete_result
            
        except Exception as e:
            self.logger.error(f"[{analysis_id}] Complete analysis failed: {str(e)}", exc_info=True)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ResultManagerã‚’ä¿å­˜
            if result_manager:
                result_manager.set_final_result({"error": str(e), "timestamp": datetime.now().isoformat()})
                result_manager.finalize_pipeline()
                error_saved_files = result_manager.save_phase_results()
                self.logger.info(f"[{analysis_id}] Error analysis logs saved to folder: {result_manager.get_analysis_folder_path()}")
            
            raise
    
    def get_pipeline_info(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’å–å¾—"""
        return {
            "pipeline_id": self.pipeline_id,
            "version": "v2.0",
            "nutrition_search_method": "local_database" if self.use_local_nutrition_search else "usda_api",
            "components": [
                {
                    "component_name": "Phase1Component",
                    "component_type": "analysis",
                    "execution_count": 0
                },
                {
                    "component_name": self.search_component_name,
                    "component_type": "nutrition_search",
                    "execution_count": 0
                }
            ]
        } 
```

============================================================

ğŸ“„ FILE: app_v2/pipeline/result_manager.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 30,369 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:55:37
å­˜åœ¨: âœ…

CONTENT:
```
import json
import os
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class DetailedExecutionLog:
    """å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è©³ç´°å®Ÿè¡Œãƒ­ã‚°"""
    
    def __init__(self, component_name: str, execution_id: str):
        self.component_name = component_name
        self.execution_id = execution_id
        self.execution_start_time = datetime.now()
        self.execution_end_time = None
        self.input_data = {}
        self.output_data = {}
        self.processing_details = {}
        self.prompts_used = {}
        self.reasoning = {}
        self.confidence_scores = {}
        self.warnings = []
        self.errors = []
        
    def set_input(self, input_data: Dict[str, Any]):
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ï¼ˆæ©Ÿå¯†æƒ…å ±ã¯é™¤å¤–ï¼‰"""
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯å¤§ãã™ãã‚‹ã®ã§ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜
        safe_input = {}
        for key, value in input_data.items():
            if key == 'image_bytes':
                safe_input[key] = {
                    "size_bytes": len(value) if value else 0,
                    "type": "binary_image_data"
                }
            else:
                safe_input[key] = value
        self.input_data = safe_input
    
    def set_output(self, output_data: Dict[str, Any]):
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        self.output_data = output_data
        
    def add_prompt(self, prompt_name: str, prompt_content: str, variables: Dict[str, Any] = None):
        """ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨˜éŒ²"""
        self.prompts_used[prompt_name] = {
            "content": prompt_content,
            "variables": variables or {},
            "timestamp": datetime.now().isoformat()
        }
    
    def add_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æ¨è«–ç†ç”±ã‚’è¨˜éŒ²"""
        self.reasoning[decision_point] = {
            "reason": reason,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat()
        }
    
    def add_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’è¨˜éŒ²"""
        self.processing_details[detail_key] = detail_value
    
    def add_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²"""
        self.confidence_scores[metric_name] = score
    
    def add_warning(self, warning: str):
        """è­¦å‘Šã‚’è¨˜éŒ²"""
        self.warnings.append({
            "message": warning,
            "timestamp": datetime.now().isoformat()
        })
    
    def add_error(self, error: str):
        """ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²"""
        self.errors.append({
            "message": error,
            "timestamp": datetime.now().isoformat()
        })
    
    def finalize(self):
        """å®Ÿè¡Œå®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.execution_end_time = datetime.now()
    
    def get_execution_time(self) -> float:
        """å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆç§’ï¼‰"""
        if self.execution_end_time:
            return (self.execution_end_time - self.execution_start_time).total_seconds()
        return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "component_name": self.component_name,
            "execution_id": self.execution_id,
            "execution_start_time": self.execution_start_time.isoformat(),
            "execution_end_time": self.execution_end_time.isoformat() if self.execution_end_time else None,
            "execution_time_seconds": self.get_execution_time(),
            "input_data": self.input_data,
            "output_data": self.output_data,
            "processing_details": self.processing_details,
            "prompts_used": self.prompts_used,
            "reasoning": self.reasoning,
            "confidence_scores": self.confidence_scores,
            "warnings": self.warnings,
            "errors": self.errors
        }


class ResultManager:
    """è§£æçµæœã¨è©³ç´°ãƒ­ã‚°ã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆãƒ•ã‚§ãƒ¼ã‚ºåˆ¥æ•´ç†ç‰ˆï¼‰"""
    
    def __init__(self, analysis_id: str, save_directory: str = "analysis_results"):
        self.analysis_id = analysis_id
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Ÿè¡Œã”ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.analysis_folder_name = f"analysis_{self.timestamp}_{self.analysis_id}"
        self.analysis_dir = Path(save_directory) / self.analysis_folder_name
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.phase1_dir = self.analysis_dir / "phase1"
        self.nutrition_search_dir = self.analysis_dir / "nutrition_search_query"
        self.phase2_dir = self.analysis_dir / "phase2"
        self.nutrition_dir = self.analysis_dir / "nutrition_calculation"
        
        for phase_dir in [self.phase1_dir, self.nutrition_search_dir, self.phase2_dir, self.nutrition_dir]:
            phase_dir.mkdir(exist_ok=True)
        
        self.pipeline_start_time = datetime.now()
        self.pipeline_end_time = None
        self.execution_logs: List[DetailedExecutionLog] = []
        self.final_result = {}
        self.pipeline_metadata = {
            "analysis_id": analysis_id,
            "version": "v2.0",
            "analysis_folder": self.analysis_folder_name,
            "pipeline_start_time": self.pipeline_start_time.isoformat()
        }
        
    def create_execution_log(self, component_name: str, execution_id: str) -> DetailedExecutionLog:
        """æ–°ã—ã„å®Ÿè¡Œãƒ­ã‚°ã‚’ä½œæˆ"""
        log = DetailedExecutionLog(component_name, execution_id)
        self.execution_logs.append(log)
        return log
    
    def set_final_result(self, result: Dict[str, Any]):
        """æœ€çµ‚çµæœã‚’è¨­å®š"""
        self.final_result = result
        
    def finalize_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.pipeline_end_time = datetime.now()
        self.pipeline_metadata["pipeline_end_time"] = self.pipeline_end_time.isoformat()
        self.pipeline_metadata["total_execution_time_seconds"] = (
            self.pipeline_end_time - self.pipeline_start_time
        ).total_seconds()
    
    def save_phase_results(self) -> Dict[str, str]:
        """ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã«çµæœã‚’ä¿å­˜"""
        saved_files = {}
        
        # å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ã‚°ã‚’å‡¦ç†
        executed_components = set()
        for log in self.execution_logs:
            if log.component_name == "Phase1Component":
                files = self._save_phase1_results(log)
                saved_files.update(files)
                executed_components.add("Phase1Component")
            elif log.component_name in ["USDAQueryComponent", "LocalNutritionSearchComponent"]:
                files = self._save_nutrition_search_results(log)
                saved_files.update(files)
                executed_components.add(log.component_name)
            elif log.component_name == "Phase2Component":
                files = self._save_phase2_results(log)
                saved_files.update(files)
                executed_components.add("Phase2Component")
            elif log.component_name == "NutritionCalculationComponent":
                files = self._save_nutrition_results(log)
                saved_files.update(files)
                executed_components.add("NutritionCalculationComponent")
        
        # æœªå®Ÿè£…/æœªå®Ÿè¡Œã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        if "Phase2Component" not in executed_components:
            placeholder_log = DetailedExecutionLog("Phase2Component", f"{self.analysis_id}_phase2_placeholder")
            placeholder_log.input_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.output_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.finalize()
            files = self._save_phase2_results(placeholder_log)
            saved_files.update(files)
        
        if "NutritionCalculationComponent" not in executed_components:
            placeholder_log = DetailedExecutionLog("NutritionCalculationComponent", f"{self.analysis_id}_nutrition_placeholder")
            placeholder_log.input_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.output_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.finalize()
            files = self._save_nutrition_results(placeholder_log)
            saved_files.update(files)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
        summary_files = self._save_pipeline_summary()
        saved_files.update(summary_files)
        
        return saved_files
    
    def _save_phase1_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase1ã®çµæœã‚’ä¿å­˜"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase1_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time()
            }, f, indent=2, ensure_ascii=False)
        files["phase1_input_output"] = str(input_output_file)
        
        # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–ç†ç”±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        prompts_md_file = self.phase1_dir / "prompts_and_reasoning.md"
        prompts_content = self._generate_phase1_prompts_md(log)
        with open(prompts_md_file, 'w', encoding='utf-8') as f:
            f.write(prompts_content)
        files["phase1_prompts_md"] = str(prompts_md_file)
        
        # 3. æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆ
        detected_items_file = self.phase1_dir / "detected_items.txt"
        detected_content = self._generate_phase1_detected_items_txt(log)
        with open(detected_items_file, 'w', encoding='utf-8') as f:
            f.write(detected_content)
        files["phase1_detected_txt"] = str(detected_items_file)
        
        return files
    
    def _save_nutrition_search_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµæœã‚’ä¿å­˜ï¼ˆUSDAQueryComponentã€LocalNutritionSearchComponentä¸¡å¯¾å¿œï¼‰"""
        files = {}
        
        # æ¤œç´¢æ–¹æ³•ã®åˆ¤å®š
        search_method = "unknown"
        db_source = "unknown"
        
        if log.component_name == "USDAQueryComponent":
            search_method = "usda_api"
            db_source = "usda_database"
        elif log.component_name == "LocalNutritionSearchComponent":
            search_method = "local_search"
            db_source = "local_nutrition_database"
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢æ–¹æ³•æƒ…å ±ã‚’å«ã‚€ï¼‰
        input_output_file = self.nutrition_search_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "search_metadata": {
                    "component_name": log.component_name,
                    "search_method": search_method,
                    "database_source": db_source,
                    "timestamp": log.execution_end_time.isoformat() if log.execution_end_time else None
                }
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_search_input_output"] = str(input_output_file)
        
        # 2. æ¤œç´¢çµæœã®è©³ç´°ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        search_results_md_file = self.nutrition_search_dir / "search_results.md"
        search_content = self._generate_nutrition_search_results_md(log, search_method, db_source)
        with open(search_results_md_file, 'w', encoding='utf-8') as f:
            f.write(search_content)
        files["nutrition_search_results_md"] = str(search_results_md_file)
        
        # 3. ãƒãƒƒãƒè©³ç´°ã®ãƒ†ã‚­ã‚¹ãƒˆ
        match_details_file = self.nutrition_search_dir / "match_details.txt"
        match_content = self._generate_nutrition_match_details_txt(log, search_method, db_source)
        with open(match_details_file, 'w', encoding='utf-8') as f:
            f.write(match_content)
        files["nutrition_search_match_details"] = str(match_details_file)
        
        return files
    
    def _save_phase2_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase2ã®çµæœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase2_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "Phase2Component is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["phase2_input_output"] = str(input_output_file)
        
        # 2. æˆ¦ç•¥æ±ºå®šã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        strategy_md_file = self.phase2_dir / "strategy_decisions.md"
        with open(strategy_md_file, 'w', encoding='utf-8') as f:
            f.write("# Phase2 Strategy Decisions\n\n*Phase2Component is not yet implemented*\n")
        files["phase2_strategy_md"] = str(strategy_md_file)
        
        # 3. é¸æŠé …ç›®ã®ãƒ†ã‚­ã‚¹ãƒˆ
        selected_items_file = self.phase2_dir / "selected_items.txt"
        with open(selected_items_file, 'w', encoding='utf-8') as f:
            f.write("Phase2Component is not yet implemented\n")
        files["phase2_items_txt"] = str(selected_items_file)
        
        return files
    
    def _save_nutrition_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šè¨ˆç®—ã®çµæœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.nutrition_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "NutritionCalculationComponent is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_input_output"] = str(input_output_file)
        
        # 2. è¨ˆç®—å¼ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        formulas_md_file = self.nutrition_dir / "calculation_formulas.md"
        with open(formulas_md_file, 'w', encoding='utf-8') as f:
            f.write("# Nutrition Calculation Formulas\n\n*NutritionCalculationComponent is not yet implemented*\n")
        files["nutrition_formulas_md"] = str(formulas_md_file)
        
        # 3. æ „é¤Šã‚µãƒãƒªãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆ
        summary_txt_file = self.nutrition_dir / "nutrition_summary.txt"
        with open(summary_txt_file, 'w', encoding='utf-8') as f:
            f.write("NutritionCalculationComponent is not yet implemented\n")
        files["nutrition_summary_txt"] = str(summary_txt_file)
        
        return files
    
    def _save_pipeline_summary(self) -> Dict[str, str]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜"""
        files = {}
        
        # 1. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚µãƒãƒªãƒ¼JSON
        summary_file = self.analysis_dir / "pipeline_summary.json"
        summary_data = {
            "analysis_id": self.analysis_id,
            "timestamp": self.timestamp,
            "pipeline_metadata": self.pipeline_metadata,
            "execution_summary": {
                log.component_name: {
                    "execution_time": log.get_execution_time(),
                    "success": len(log.errors) == 0,
                    "warnings_count": len(log.warnings),
                    "errors_count": len(log.errors)
                }
                for log in self.execution_logs
            },
            "final_result": self.final_result
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        files["pipeline_summary"] = str(summary_file)
        
        # 2. å®Œå…¨ãªè©³ç´°ãƒ­ã‚°JSON
        complete_log_file = self.analysis_dir / "complete_analysis_log.json"
        complete_data = {
            "pipeline_metadata": self.pipeline_metadata,
            "execution_logs": [log.to_dict() for log in self.execution_logs],
            "final_result": self.final_result,
            "summary": {
                "total_components": len(self.execution_logs),
                "total_warnings": sum(len(log.warnings) for log in self.execution_logs),
                "total_errors": sum(len(log.errors) for log in self.execution_logs)
            }
        }
        
        with open(complete_log_file, 'w', encoding='utf-8') as f:
            json.dump(complete_data, f, indent=2, ensure_ascii=False)
        files["complete_log"] = str(complete_log_file)
        
        return files
    
    def _generate_phase1_prompts_md(self, log: DetailedExecutionLog) -> str:
        """Phase1ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–ç†ç”±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆ"""
        content = f"""# Phase1: ç”»åƒåˆ†æ - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–

## å®Ÿè¡Œæƒ…å ±
- å®Ÿè¡ŒID: {log.execution_id}
- é–‹å§‹æ™‚åˆ»: {log.execution_start_time.isoformat()}
- çµ‚äº†æ™‚åˆ»: {log.execution_end_time.isoformat() if log.execution_end_time else 'N/A'}
- å®Ÿè¡Œæ™‚é–“: {log.get_execution_time():.2f}ç§’

## ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
        for prompt_name, prompt_data in log.prompts_used.items():
            content += f"### {prompt_name.replace('_', ' ').title()}\n\n"
            content += f"**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—**: {prompt_data['timestamp']}\n\n"
            content += f"```\n{prompt_data['content']}\n```\n\n"
            
            if prompt_data.get('variables'):
                content += f"**å¤‰æ•°**:\n"
                for var_name, var_value in prompt_data['variables'].items():
                    content += f"- {var_name}: {var_value}\n"
                content += "\n"
        
        # æ¨è«–ç†ç”±
        content += "## AIæ¨è«–ã®è©³ç´°\n\n"
        
        # æ–™ç†è­˜åˆ¥ã®æ¨è«–
        dish_reasoning = [r for r in log.reasoning.items() if r[0].startswith('dish_identification_')]
        if dish_reasoning:
            content += "### æ–™ç†è­˜åˆ¥ã®æ¨è«–\n\n"
            for decision_point, reasoning_data in dish_reasoning:
                dish_num = decision_point.split('_')[-1]
                content += f"**æ–™ç† {dish_num}**:\n"
                content += f"- æ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # é£Ÿæé¸æŠã®æ¨è«–
        ingredient_reasoning = [r for r in log.reasoning.items() if r[0].startswith('ingredient_selection_')]
        if ingredient_reasoning:
            content += "### é£Ÿæé¸æŠã®æ¨è«–\n\n"
            for decision_point, reasoning_data in ingredient_reasoning:
                content += f"**{decision_point.replace('_', ' ').title()}**:\n"
                content += f"- æ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼
        if log.warnings:
            content += "## è­¦å‘Š\n\n"
            for warning in log.warnings:
                content += f"- {warning['message']} (at {warning['timestamp']})\n"
            content += "\n"
        
        if log.errors:
            content += "## ã‚¨ãƒ©ãƒ¼\n\n"
            for error in log.errors:
                content += f"- {error['message']} (at {error['timestamp']})\n"
            content += "\n"
        
        return content
    
    def _generate_phase1_detected_items_txt(self, log: DetailedExecutionLog) -> str:
        """Phase1ã§æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰"""
        content = f"Phase1 æ¤œå‡ºçµæœ - {log.execution_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += "=" * 60 + "\n\n"
        
        if 'output_data' in log.output_data and 'dishes' in log.output_data['output_data']:
            dishes = log.output_data['output_data']['dishes']
            content += f"æ¤œå‡ºã•ã‚ŒãŸæ–™ç†æ•°: {len(dishes)}\n\n"
            
            for i, dish in enumerate(dishes, 1):
                content += f"æ–™ç† {i}: {dish['dish_name']}\n"
                content += f"  é£Ÿææ•°: {len(dish['ingredients'])}\n"
                content += "  é£Ÿæè©³ç´°:\n"
                
                for j, ingredient in enumerate(dish['ingredients'], 1):
                    content += f"    {j}. {ingredient['ingredient_name']}\n"
                content += "\n"
        
        # USDAæ¤œç´¢æº–å‚™æƒ…å ±
        if 'usda_search_terms' in log.processing_details:
            search_terms = log.processing_details['usda_search_terms']
            content += f"USDAæ¤œç´¢èªå½™ ({len(search_terms)}å€‹):\n"
            for i, term in enumerate(search_terms, 1):
                content += f"  {i}. {term}\n"
            content += "\n"
        
        # å‡¦ç†è©³ç´°
        if log.processing_details:
            content += "å‡¦ç†è©³ç´°:\n"
            for detail_key, detail_value in log.processing_details.items():
                if detail_key == 'usda_search_terms':
                    continue  # æ—¢ã«ä¸Šã§è¡¨ç¤ºæ¸ˆã¿
                if isinstance(detail_value, (dict, list)):
                    content += f"  {detail_key}: {json.dumps(detail_value, ensure_ascii=False)}\n"
                else:
                    content += f"  {detail_key}: {detail_value}\n"
        
        return content
    
    def _generate_nutrition_search_results_md(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆï¼ˆUSDA/ãƒ­ãƒ¼ã‚«ãƒ«å¯¾å¿œï¼‰"""
        content = []
        
        content.append(f"# Nutrition Database Search Results")
        content.append(f"")
        content.append(f"**Search Method:** {search_method}")
        content.append(f"**Database Source:** {db_source}")
        content.append(f"**Component:** {log.component_name}")
        content.append(f"**Execution Time:** {log.get_execution_time():.3f} seconds")
        content.append(f"**Timestamp:** {log.execution_start_time.isoformat()}")
        content.append(f"")
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.input_data:
            content.append(f"## Input Data")
            if 'ingredient_names' in log.input_data:
                ingredients = log.input_data['ingredient_names']
                content.append(f"**Ingredients ({len(ingredients)}):** {', '.join(ingredients)}")
            
            if 'dish_names' in log.input_data:
                dishes = log.input_data['dish_names']
                content.append(f"**Dishes ({len(dishes)}):** {', '.join(dishes)}")
            content.append(f"")
        
        # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            content.append(f"## Search Results")
            content.append(f"**Total Matches:** {len(matches)}")
            content.append(f"")
            
            for i, (search_term, match_data) in enumerate(matches.items(), 1):
                content.append(f"### {i}. {search_term}")
                if isinstance(match_data, dict):
                    content.append(f"**ID:** {match_data.get('id', 'N/A')}")
                    content.append(f"**Description:** {match_data.get('description', 'N/A')}")
                    content.append(f"**Data Type:** {match_data.get('data_type', 'N/A')}")
                    content.append(f"**Source:** {match_data.get('source', 'N/A')}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        content.append(f"**Nutrients ({len(match_data['nutrients'])}):**")
                        for nutrient in match_data['nutrients'][:5]:  # æœ€åˆã®5ã¤ã ã‘è¡¨ç¤º
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                content.append(f"  - {name}: {amount} {unit}")
                        if len(match_data['nutrients']) > 5:
                            content.append(f"  - ... and {len(match_data['nutrients']) - 5} more nutrients")
                content.append(f"")
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            content.append(f"## Search Summary")
            content.append(f"**Total Searches:** {summary.get('total_searches', 0)}")
            content.append(f"**Successful Matches:** {summary.get('successful_matches', 0)}")
            content.append(f"**Failed Searches:** {summary.get('failed_searches', 0)}")
            content.append(f"**Match Rate:** {summary.get('match_rate_percent', 0)}%")
            content.append(f"**Search Method:** {summary.get('search_method', 'unknown')}")
            content.append(f"")
        
        # æ¨è«–ç†ç”±ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.reasoning:
            content.append(f"## Search Reasoning")
            for decision_point, reason_data in log.reasoning.items():
                reason = reason_data.get('reason', '') if isinstance(reason_data, dict) else str(reason_data)
                content.append(f"**{decision_point}:** {reason}")
            content.append(f"")
        
        # è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.warnings:
            content.append(f"## Warnings")
            for warning in log.warnings:
                content.append(f"- {warning}")
            content.append(f"")
        
        if log.errors:
            content.append(f"## Errors")
            for error in log.errors:
                content.append(f"- {error}")
            content.append(f"")
        
        return "\n".join(content)
    
    def _generate_nutrition_match_details_txt(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®ãƒãƒƒãƒè©³ç´°ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆUSDA/ãƒ­ãƒ¼ã‚«ãƒ«å¯¾å¿œï¼‰"""
        lines = []
        
        lines.append(f"Nutrition Database Search Match Details")
        lines.append(f"=" * 50)
        lines.append(f"Search Method: {search_method}")
        lines.append(f"Database Source: {db_source}")
        lines.append(f"Component: {log.component_name}")
        lines.append(f"Execution Time: {log.get_execution_time():.3f} seconds")
        lines.append(f"Timestamp: {log.execution_start_time.isoformat()}")
        lines.append(f"")
        
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            lines.append(f"Total Matches: {len(matches)}")
            lines.append(f"")
            
            for search_term, match_data in matches.items():
                lines.append(f"Search Term: {search_term}")
                lines.append(f"-" * 30)
                
                if isinstance(match_data, dict):
                    lines.append(f"  ID: {match_data.get('id', 'N/A')}")
                    lines.append(f"  Description: {match_data.get('description', 'N/A')}")
                    lines.append(f"  Data Type: {match_data.get('data_type', 'N/A')}")
                    lines.append(f"  Source: {match_data.get('source', 'N/A')}")
                    lines.append(f"  Score: {match_data.get('score', 'N/A')}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        lines.append(f"  Nutrients ({len(match_data['nutrients'])}):")
                        for nutrient in match_data['nutrients']:
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                lines.append(f"    - {name}: {amount} {unit}")
                    
                    if 'original_data' in match_data:
                        original_data = match_data['original_data']
                        if isinstance(original_data, dict):
                            lines.append(f"  Original Data Source: {original_data.get('source', 'Unknown')}")
                            if search_method == "local_search":
                                lines.append(f"  Local DB Source: {original_data.get('db_source', 'Unknown')}")
                
                lines.append(f"")
        
        # æ¤œç´¢çµ±è¨ˆ
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            lines.append(f"Search Statistics:")
            lines.append(f"  Total Searches: {summary.get('total_searches', 0)}")
            lines.append(f"  Successful Matches: {summary.get('successful_matches', 0)}")
            lines.append(f"  Failed Searches: {summary.get('failed_searches', 0)}")
            lines.append(f"  Match Rate: {summary.get('match_rate_percent', 0)}%")
            
            if search_method == "local_search":
                lines.append(f"  Total Database Items: {summary.get('total_database_items', 0)}")
        
        return "\n".join(lines)
    
    def get_analysis_folder_path(self) -> str:
        """è§£æãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—"""
        return str(self.analysis_dir) 
```

============================================================

ğŸ“ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤
============================================================

ğŸ“„ FILE: app_v2/components/base.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 6,824 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:08:38
å­˜åœ¨: âœ…

CONTENT:
```
from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Any, Optional
import logging
from datetime import datetime

# å‹å¤‰æ•°ã®å®šç¾©
InputType = TypeVar('InputType')
OutputType = TypeVar('OutputType')


class BaseComponent(ABC, Generic[InputType, OutputType]):
    """
    é£Ÿäº‹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæŠ½è±¡ã‚¯ãƒ©ã‚¹
    
    å…¨ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã“ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã€process ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    """
    
    def __init__(self, component_name: str, logger: Optional[logging.Logger] = None):
        """
        ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        
        Args:
            component_name: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå
            logger: ãƒ­ã‚¬ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        self.component_name = component_name
        self.logger = logger or logging.getLogger(f"{__name__}.{component_name}")
        self.created_at = datetime.now()
        self.execution_count = 0
        self.current_execution_log = None  # è©³ç´°ãƒ­ã‚°
        
    @abstractmethod
    async def process(self, input_data: InputType) -> OutputType:
        """
        ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            OutputType: å‡¦ç†çµæœ
            
        Raises:
            ComponentError: å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        pass
    
    async def execute(self, input_data: InputType, execution_log: Optional['DetailedExecutionLog'] = None) -> OutputType:
        """
        ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ­ã‚°è¨˜éŒ²ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            execution_log: è©³ç´°å®Ÿè¡Œãƒ­ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            OutputType: å‡¦ç†çµæœ
        """
        self.execution_count += 1
        execution_id = f"{self.component_name}_{self.execution_count}"
        
        # è©³ç´°ãƒ­ã‚°ã®è¨­å®š
        if execution_log:
            self.current_execution_log = execution_log
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            self.current_execution_log.set_input(self._safe_serialize_input(input_data))
        
        self.logger.info(f"[{execution_id}] Starting {self.component_name} processing")
        
        try:
            start_time = datetime.now()
            result = await self.process(input_data)
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            self.logger.info(f"[{execution_id}] {self.component_name} completed in {processing_time:.2f}s")
            
            # è©³ç´°ãƒ­ã‚°ã«å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.set_output(self._safe_serialize_output(result))
                self.current_execution_log.finalize()
            
            return result
            
        except Exception as e:
            self.logger.error(f"[{execution_id}] {self.component_name} failed: {str(e)}", exc_info=True)
            
            # è©³ç´°ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.add_error(str(e))
                self.current_execution_log.finalize()
            
            raise ComponentError(f"{self.component_name} processing failed: {str(e)}") from e
        finally:
            self.current_execution_log = None
    
    def log_prompt(self, prompt_name: str, prompt_content: str, variables: dict = None):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_prompt(prompt_name, prompt_content, variables)
    
    def log_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_reasoning(decision_point, reason, confidence)
    
    def log_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_processing_detail(detail_key, detail_value)
    
    def log_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_confidence_score(metric_name, score)
    
    def log_warning(self, warning: str):
        """è­¦å‘Šã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_warning(warning)
    
    def _safe_serialize_input(self, input_data: InputType) -> dict:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(input_data, 'model_dump'):
                return input_data.model_dump()
            elif hasattr(input_data, '__dict__'):
                return input_data.__dict__
            else:
                return {"data": str(input_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def _safe_serialize_output(self, output_data: OutputType) -> dict:
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(output_data, 'model_dump'):
                return output_data.model_dump()
            elif hasattr(output_data, '__dict__'):
                return output_data.__dict__
            else:
                return {"data": str(output_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def get_component_info(self) -> dict:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
        return {
            "component_name": self.component_name,
            "created_at": self.created_at.isoformat(),
            "execution_count": self.execution_count,
            "component_type": self.__class__.__name__
        }


class ComponentError(Exception):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, component_name: str = None, original_error: Exception = None):
        super().__init__(message)
        self.component_name = component_name
        self.original_error = original_error
        self.timestamp = datetime.now()
    
    def to_dict(self) -> dict:
        """ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "error_message": str(self),
            "component_name": self.component_name,
            "timestamp": self.timestamp.isoformat(),
            "original_error": str(self.original_error) if self.original_error else None
        } 
```

============================================================

ğŸ“„ FILE: app_v2/components/phase1_component.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 5,285 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 15:28:30
å­˜åœ¨: âœ…

CONTENT:
```
import json
from typing import Optional

from .base import BaseComponent
from ..models.phase1_models import Phase1Input, Phase1Output, Dish, Ingredient
from ..services.gemini_service import GeminiService
from ..config import get_settings
from ..config.prompts import Phase1Prompts


class Phase1Component(BaseComponent[Phase1Input, Phase1Output]):
    """
    Phase1: ç”»åƒåˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
    
    Gemini AIã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æã—ã€USDAæ¤œç´¢ã«é©ã—ãŸæ–™ç†ã¨é£Ÿæåã‚’è­˜åˆ¥ã—ã¾ã™ã€‚
    """
    
    def __init__(self, gemini_service: Optional[GeminiService] = None):
        super().__init__("Phase1Component")
        
        # GeminiServiceã®åˆæœŸåŒ–
        if gemini_service is None:
            settings = get_settings()
            self.gemini_service = GeminiService(
                project_id=settings.GEMINI_PROJECT_ID,
                location=settings.GEMINI_LOCATION,
                model_name=settings.GEMINI_MODEL_NAME
            )
        else:
            self.gemini_service = gemini_service
    
    async def process(self, input_data: Phase1Input) -> Phase1Output:
        """
        Phase1ã®ä¸»å‡¦ç†: ç”»åƒåˆ†æï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
        
        Args:
            input_data: Phase1Input (image_bytes, image_mime_type, optional_text)
            
        Returns:
            Phase1Output: åˆ†æçµæœï¼ˆæ–™ç†åãƒ»é£Ÿæåã®ã¿ï¼‰
        """
        self.logger.info(f"Starting Phase1 image analysis for USDA query generation")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã¨è¨˜éŒ²
        system_prompt = Phase1Prompts.get_system_prompt()
        user_prompt = Phase1Prompts.get_user_prompt(input_data.optional_text)
        
        self.log_prompt("system_prompt", system_prompt)
        self.log_prompt("user_prompt", user_prompt, {
            "optional_text": input_data.optional_text,
            "image_mime_type": input_data.image_mime_type
        })
        
        # ç”»åƒæƒ…å ±ã®ãƒ­ã‚°è¨˜éŒ²
        self.log_processing_detail("image_size_bytes", len(input_data.image_bytes))
        self.log_processing_detail("image_mime_type", input_data.image_mime_type)
        
        try:
            # Gemini AIã«ã‚ˆã‚‹ç”»åƒåˆ†æ
            self.log_processing_detail("gemini_api_call_start", "Calling Gemini API for image analysis")
            
            gemini_result = await self.gemini_service.analyze_phase1(
                image_bytes=input_data.image_bytes,
                image_mime_type=input_data.image_mime_type,
                optional_text=input_data.optional_text
            )
            
            self.log_processing_detail("gemini_raw_response", gemini_result)
            
            # çµæœã‚’Pydanticãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›
            dishes = []
            for dish_index, dish_data in enumerate(gemini_result.get("dishes", [])):
                ingredients = []
                for ingredient_index, ingredient_data in enumerate(dish_data.get("ingredients", [])):
                    ingredient = Ingredient(
                        ingredient_name=ingredient_data["ingredient_name"]
                    )
                    ingredients.append(ingredient)
                    
                    # é£Ÿæè­˜åˆ¥ã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                    self.log_reasoning(
                        f"ingredient_identification_dish{dish_index}_ingredient{ingredient_index}",
                        f"Identified ingredient '{ingredient_data['ingredient_name']}' for USDA search based on visual analysis"
                    )
                
                dish = Dish(
                    dish_name=dish_data["dish_name"],
                    ingredients=ingredients
                )
                dishes.append(dish)
                
                # æ–™ç†è­˜åˆ¥ã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"dish_identification_{dish_index}",
                    f"Identified dish as '{dish_data['dish_name']}' for USDA search based on visual characteristics"
                )
            
            # åˆ†æçµ±è¨ˆã®è¨˜éŒ²
            self.log_processing_detail("detected_dishes_count", len(dishes))
            self.log_processing_detail("total_ingredients_count", sum(len(dish.ingredients) for dish in dishes))
            
            # USDAæ¤œç´¢é©åˆæ€§ãƒã‚§ãƒƒã‚¯
            search_terms = []
            for dish in dishes:
                search_terms.append(dish.dish_name)
                for ingredient in dish.ingredients:
                    search_terms.append(ingredient.ingredient_name)
            
            self.log_processing_detail("usda_search_terms", search_terms)
            self.log_reasoning(
                "usda_search_preparation",
                f"Generated {len(search_terms)} search terms for USDA database queries"
            )
            
            result = Phase1Output(
                dishes=dishes,
                warnings=[]
            )
            
            self.logger.info(f"Phase1 completed: identified {len(dishes)} dishes with {len(search_terms)} total search terms")
            return result
            
        except Exception as e:
            self.logger.error(f"Phase1 processing failed: {str(e)}")
            raise 
```

============================================================

ğŸ“„ FILE: app_v2/components/local_nutrition_search_component.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 25,544 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 13:03:15
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Local Nutrition Search Component

USDA database queryã‚’ nutrition_db_experiment ã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã«ç½®ãæ›ãˆã‚‹
"""

import os
import sys
import json
import asyncio
from typing import Optional, List, Dict, Any
from pathlib import Path

from .base import BaseComponent
from ..models.usda_models import USDAQueryInput, USDAQueryOutput
from ..models.nutrition_search_models import (
    NutritionQueryInput, NutritionQueryOutput, NutritionMatch, NutritionNutrient,
    convert_usda_query_input_to_nutrition, convert_nutrition_to_usda_query_output
)
from ..config import get_settings

# nutrition_db_experimentã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
NUTRITION_DB_EXPERIMENT_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "nutrition_db_experiment"
)
sys.path.append(NUTRITION_DB_EXPERIMENT_PATH)

class LocalNutritionSearchComponent(BaseComponent[USDAQueryInput, USDAQueryOutput]):
    """
    ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    
    nutrition_db_experimentã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦é£Ÿæåã‚’æ¤œç´¢ã—ã€
    USDAQueryComponentã¨äº’æ›æ€§ã®ã‚ã‚‹çµæœã‚’è¿”ã—ã¾ã™ã€‚
    
    å†…éƒ¨çš„ã«ã¯æ±ç”¨çš„ãªNutritionQueryãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã€
    å¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã¯USDAQueryãƒ¢ãƒ‡ãƒ«ã¨ã®äº’æ›æ€§ã‚’ä¿æŒã—ã¾ã™ã€‚
    """
    
    def __init__(self):
        super().__init__("LocalNutritionSearchComponent")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self._initialize_local_search_system()
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæ­£ã—ã„ãƒ‘ã‚¹ã«ä¿®æ­£ï¼‰
        self.local_db_paths = {
            "dish_db": os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "dish_db.json"),
            "ingredient_db": os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "ingredient_db.json"),
            "branded_db": os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "branded_db.json"),
            "unified_db": os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "unified_nutrition_db.json")
        }
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
        self.local_databases = self._load_local_databases()
    
    def _initialize_local_search_system(self):
        """ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        try:
            # nutrition_db_experimentã®æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            search_service_path = os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "search_service")
            sys.path.append(search_service_path)
            
            from nlp.query_preprocessor import FoodQueryPreprocessor
            from api.search_handler import NutritionSearchHandler, SearchRequest
            
            self.query_preprocessor = FoodQueryPreprocessor()
            self.search_handler = NutritionSearchHandler()
            
            self.logger.info("Local nutrition search system initialized successfully")
            
        except ImportError as e:
            self.logger.error(f"Failed to import local search components: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°
            self.query_preprocessor = None
            self.search_handler = None
            self.logger.warning("Using fallback simple string matching for local search")
        except Exception as e:
            self.logger.error(f"Error initializing local search system: {e}")
            self.query_preprocessor = None
            self.search_handler = None
    
    def _load_local_databases(self) -> Dict[str, List[Dict[str, Any]]]:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        databases = {}
        
        for db_name, db_path in self.local_db_paths.items():
            try:
                if os.path.exists(db_path):
                    with open(db_path, 'r', encoding='utf-8') as f:
                        databases[db_name] = json.load(f)
                    self.logger.info(f"Loaded {db_name}: {len(databases[db_name])} items")
                else:
                    self.logger.warning(f"Local database file not found: {db_path}")
                    databases[db_name] = []
            except Exception as e:
                self.logger.error(f"Error loading {db_name}: {e}")
                databases[db_name] = []
        
        total_items = sum(len(db) for db in databases.values())
        self.logger.info(f"Total local database items loaded: {total_items}")
        
        return databases
    
    async def process(self, input_data: USDAQueryInput) -> USDAQueryOutput:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®ä¸»å‡¦ç†ï¼ˆUSDAäº’æ›ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰
        
        Args:
            input_data: USDAQueryInput
            
        Returns:
            USDAQueryOutput: USDAäº’æ›ã®ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢çµæœ
        """
        # USDAQueryInputã‚’æ±ç”¨NutritionQueryInputã«å¤‰æ›
        nutrition_input = convert_usda_query_input_to_nutrition(input_data)
        nutrition_input.preferred_source = "local_database"
        
        # å†…éƒ¨çš„ãªæ±ç”¨æ¤œç´¢å‡¦ç†ã‚’å®Ÿè¡Œ
        nutrition_result = await self._process_nutrition_search(nutrition_input)
        
        # çµæœã‚’USDAQueryOutputå½¢å¼ã«å¤‰æ›ã—ã¦è¿”ã™
        return convert_nutrition_to_usda_query_output(nutrition_result)
    
    async def _process_nutrition_search(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        æ±ç”¨æ „é¤Šæ¤œç´¢ã®ä¸»å‡¦ç†
        
        Args:
            input_data: NutritionQueryInput
            
        Returns:
            NutritionQueryOutput: æ±ç”¨æ¤œç´¢çµæœ
        """
        self.logger.info(f"Starting local nutrition search for {len(input_data.get_all_search_terms())} terms")
        
        search_terms = input_data.get_all_search_terms()
        
        # æ¤œç´¢å¯¾è±¡ã®è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("ingredient_names", input_data.ingredient_names)
        self.log_processing_detail("dish_names", input_data.dish_names)
        self.log_processing_detail("total_search_terms", len(search_terms))
        self.log_processing_detail("search_method", "local_nutrition_database")
        self.log_processing_detail("preferred_source", input_data.preferred_source)
        
        matches = {}
        warnings = []
        errors = []
        
        successful_matches = 0
        total_searches = len(search_terms)
        
        # å„æ¤œç´¢èªå½™ã«ã¤ã„ã¦ç…§åˆã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Searching local database for: {search_term}")
            
            # æ¤œç´¢é–‹å§‹ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_term", search_term)
            self.log_processing_detail(f"search_{search_index}_start", f"Starting local search for '{search_term}'")
            
            try:
                # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®å®Ÿè¡Œ
                if self.search_handler and self.query_preprocessor:
                    # é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
                    match_result = await self._advanced_local_search(search_term, search_index, input_data)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°
                    match_result = await self._simple_local_search(search_term, search_index, input_data)
                
                if match_result:
                    matches[search_term] = match_result
                    successful_matches += 1
                    self.logger.debug(f"Found local match for '{search_term}': ID {match_result.id}")
                else:
                    self.log_reasoning(
                        f"no_match_{search_index}",
                        f"No local database match found for '{search_term}' - may not exist in local nutrition database"
                    )
                    self.logger.warning(f"No local match found for: {search_term}")
                    warnings.append(f"No local match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Local search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"search_error_{search_index}",
                    f"Local database search error for '{search_term}': {str(e)}"
                )
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆï¼ˆæ±ç”¨å½¢å¼ï¼‰
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "local_nutrition_database",
            "database_source": "nutrition_db_experiment",
            "preferred_source": input_data.preferred_source,
            "total_database_items": sum(len(db) for db in self.local_databases.values())
        }
        
        # å…¨ä½“çš„ãªæ¤œç´¢æˆåŠŸç‡ã‚’ãƒ­ã‚°
        overall_success_rate = successful_matches / total_searches if total_searches > 0 else 0
        self.log_processing_detail("search_summary", search_summary)
        
        # æ¤œç´¢å“è³ªã®è©•ä¾¡ã‚’ãƒ­ã‚°
        if overall_success_rate >= 0.8:
            self.log_reasoning("search_quality", "Excellent local search results with high match rate")
        elif overall_success_rate >= 0.6:
            self.log_reasoning("search_quality", "Good local search results with acceptable match rate")
        elif overall_success_rate >= 0.4:
            self.log_reasoning("search_quality", "Moderate local search results, some items may need manual review")
        else:
            self.log_reasoning("search_quality", "Poor local search results, many items not found in local database")
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Local nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%})")
        
        return result
    
    async def _advanced_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        nutrition_db_experimentã®é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        try:
            from api.search_handler import SearchRequest
            
            # æ¤œç´¢ã‚¿ã‚¤ãƒ—ã®æ±ºå®šï¼ˆæ–™ç†ã‹é£Ÿæã‹ã®æ¨å®šï¼‰
            db_type_filter = None  # å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢
            
            # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ã¨ã—ã¦å„ªå…ˆæ¤œç´¢
            if search_term in input_data.dish_names:
                db_type_filter = "dish"
                self.log_processing_detail(f"search_{search_index}_type", "dish")
            elif search_term in input_data.ingredient_names:
                db_type_filter = "ingredient"
                self.log_processing_detail(f"search_{search_index}_type", "ingredient")
            
            # æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
            request = SearchRequest(
                query=search_term,
                db_type_filter=db_type_filter,
                size=5  # ä¸Šä½5ä»¶ã‚’å–å¾—
            )
            
            # æ¤œç´¢å®Ÿè¡Œ
            response = self.search_handler.search(request)
            
            # æ¤œç´¢çµæœã®è©³ç´°ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_results_count", response.total_hits)
            self.log_processing_detail(f"search_{search_index}_processing_time_ms", response.took_ms)
            self.log_processing_detail(f"search_{search_index}_processed_query", response.query_info.get('processed_query'))
            
            if response.results:
                # nutrition_db_experimentã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ãŸå ´åˆã¯ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                best_result = response.results[0]
                
                # æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆIDãŒ123456ã®å ´åˆã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼‰
                if best_result.get('id') == 123456:
                    self.logger.warning(f"nutrition_db_experiment returned mock data for '{search_term}', falling back to direct database search")
                    return await self._direct_database_search(search_term, search_index, input_data)
                
                # ãƒãƒƒãƒé¸æŠã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_result['search_name']}' (ID: {best_result['id']}) for search term '{search_term}' based on local search algorithm (score: {best_result.get('_score', 'N/A')})"
                )
                
                # è©³ç´°ãªãƒãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_selected_id", best_result['id'])
                self.log_processing_detail(f"search_{search_index}_selected_name", best_result['search_name'])
                self.log_processing_detail(f"search_{search_index}_db_type", best_result['db_type'])
                self.log_processing_detail(f"search_{search_index}_score", best_result.get('_score'))
                
                # NutritionMatchå½¢å¼ã«å¤‰æ›
                return self._convert_to_nutrition_match(best_result, search_term)
            
            # çµæœãŒãªã„å ´åˆã¯ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._direct_database_search(search_term, search_index, input_data)
            
        except Exception as e:
            self.logger.error(f"Advanced local search failed for '{search_term}': {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._direct_database_search(search_term, search_index, input_data)
    
    async def _direct_database_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        try:
            self.log_processing_detail(f"search_{search_index}_method", "direct_database_search")
            
            search_term_lower = search_term.lower()
            best_match = None
            best_score = 0
            best_db_source = None
            
            # æ¤œç´¢å„ªå…ˆé †ä½ã®æ±ºå®š
            search_order = []
            
            # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å„ªå…ˆ
            if search_term in input_data.dish_names:
                search_order = ["dish_db", "unified_db", "ingredient_db", "branded_db"]
            elif search_term in input_data.ingredient_names:
                search_order = ["ingredient_db", "unified_db", "dish_db", "branded_db"]
            else:
                search_order = ["unified_db", "dish_db", "ingredient_db", "branded_db"]
            
            # å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
            for db_name in search_order:
                if db_name not in self.local_databases:
                    continue
                    
                database = self.local_databases[db_name]
                
                for item in database:
                    # search_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§æ¤œç´¢
                    if 'search_name' not in item:
                        continue
                        
                    item_name = item['search_name'].lower()
                    score = 0
                    
                    # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                    if search_term_lower == item_name:
                        score = 1.0  # å®Œå…¨ä¸€è‡´
                    elif search_term_lower in item_name:
                        # éƒ¨åˆ†ä¸€è‡´ï¼ˆèªé †è€ƒæ…®ï¼‰
                        if item_name.startswith(search_term_lower):
                            score = 0.9  # å‰æ–¹ä¸€è‡´
                        elif item_name.endswith(search_term_lower):
                            score = 0.8  # å¾Œæ–¹ä¸€è‡´
                        else:
                            score = 0.7  # ä¸­é–“ä¸€è‡´
                    elif item_name in search_term_lower:
                        score = 0.6  # é€†éƒ¨åˆ†ä¸€è‡´
                    else:
                        # å˜èªãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                        search_words = search_term_lower.split()
                        item_words = item_name.split()
                        
                        common_words = set(search_words) & set(item_words)
                        if common_words:
                            score = len(common_words) / max(len(search_words), len(item_words)) * 0.5
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å„ªå…ˆåº¦ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
                    db_bonus = 1.0
                    if db_name == search_order[0]:
                        db_bonus = 1.2  # ç¬¬ä¸€å„ªå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                    elif db_name == search_order[1]:
                        db_bonus = 1.1  # ç¬¬äºŒå„ªå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                    
                    final_score = score * db_bonus
                    
                    if final_score > best_score:
                        best_score = final_score
                        best_match = item.copy()
                        best_db_source = db_name
            
            if best_match and best_score > 0.1:  # æœ€ä½é–¾å€¤
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
                best_match['_db_source'] = best_db_source
                best_match['_match_score'] = best_score
                
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_match['search_name']}' (ID: {best_match.get('id', 'N/A')}) for search term '{search_term}' from {best_db_source} using direct database search (score: {best_score:.3f})"
                )
                
                # è©³ç´°ãªãƒãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_selected_id", best_match.get('id', 'N/A'))
                self.log_processing_detail(f"search_{search_index}_selected_name", best_match['search_name'])
                self.log_processing_detail(f"search_{search_index}_db_source", best_db_source)
                self.log_processing_detail(f"search_{search_index}_match_score", best_score)
                
                return self._convert_to_nutrition_match(best_match, search_term)
            
            return None
            
        except Exception as e:
            self.logger.error(f"Direct database search failed for '{search_term}': {e}")
            return None
    
    async def _simple_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½¿ç”¨ï¼‰
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        # é«˜åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã€ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨
        return await self._direct_database_search(search_term, search_index, input_data)
    
    def _convert_to_nutrition_match(self, local_item: Dict[str, Any], search_term: str) -> NutritionMatch:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã‚’NutritionMatchå½¢å¼ã«å¤‰æ›
        
        Args:
            local_item: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¤ãƒ†ãƒ 
            search_term: å…ƒã®æ¤œç´¢èªå½™
            
        Returns:
            NutritionMatch: å¤‰æ›ã•ã‚ŒãŸãƒãƒƒãƒçµæœ
        """
        # æ „é¤Šç´ æƒ…å ±ã®å¤‰æ›
        nutrients = []
        if 'nutrition' in local_item and local_item['nutrition']:
            nutrition_data = local_item['nutrition']
            
            # ä¸»è¦æ „é¤Šç´ ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«DBã®å½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            nutrient_mapping = {
                'calories_kcal': ('Energy', '208', 'kcal'),
                'calories': ('Energy', '208', 'kcal'),  # åˆ¥åå¯¾å¿œ
                'protein_g': ('Protein', '203', 'g'),
                'protein': ('Protein', '203', 'g'),  # åˆ¥åå¯¾å¿œ
                'fat_g': ('Total lipid (fat)', '204', 'g'),
                'fat': ('Total lipid (fat)', '204', 'g'),  # åˆ¥åå¯¾å¿œ
                'carbohydrates_g': ('Carbohydrate, by difference', '205', 'g'),
                'carbs': ('Carbohydrate, by difference', '205', 'g'),  # åˆ¥åå¯¾å¿œ
                'carbohydrates': ('Carbohydrate, by difference', '205', 'g'),  # åˆ¥åå¯¾å¿œ
                'fiber_g': ('Fiber, total dietary', '291', 'g'),
                'fiber': ('Fiber, total dietary', '291', 'g'),  # åˆ¥åå¯¾å¿œ
                'sugars_g': ('Sugars, total', '269', 'g'),
                'sugars': ('Sugars, total', '269', 'g'),  # åˆ¥åå¯¾å¿œ
                'sodium_mg': ('Sodium, Na', '307', 'mg'),
                'sodium': ('Sodium, Na', '307', 'mg')  # åˆ¥åå¯¾å¿œ
            }
            
            for local_key, (usda_name, nutrient_number, unit) in nutrient_mapping.items():
                if local_key in nutrition_data and nutrition_data[local_key] is not None:
                    try:
                        amount = float(nutrition_data[local_key])
                        nutrients.append(NutritionNutrient(
                            name=usda_name,
                            amount=amount,
                            unit_name=unit,
                            nutrient_id=None,  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ã¯IDãŒãªã„
                            nutrient_number=nutrient_number
                        ))
                    except (ValueError, TypeError):
                        # æ•°å€¤ã«å¤‰æ›ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        continue
        
        # IDã®å–å¾—ï¼ˆæ§˜ã€…ãªå½¢å¼ã«å¯¾å¿œï¼‰
        item_id = local_item.get('id') or local_item.get('fdc_id') or local_item.get('_id') or 0
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        data_type = "Local_Unknown"
        if 'db_type' in local_item:
            data_type = f"Local_{local_item['db_type'].title()}"
        elif '_db_source' in local_item:
            db_source = local_item['_db_source'].replace('_db', '')
            data_type = f"Local_{db_source.title()}"
        
        # èª¬æ˜ã®å–å¾—ï¼ˆæ§˜ã€…ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«å¯¾å¿œï¼‰
        description = (
            local_item.get('search_name') or 
            local_item.get('description') or 
            local_item.get('name') or 
            search_term
        )
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ï¼ˆbranded_dbã®å ´åˆï¼‰
        brand_owner = local_item.get('brand_owner') or local_item.get('brand_name')
        
        # é£Ÿæãƒªã‚¹ãƒˆï¼ˆdish_dbã®å ´åˆï¼‰
        ingredients_text = None
        if 'ingredients' in local_item:
            if isinstance(local_item['ingredients'], list):
                ingredients_text = ', '.join(local_item['ingredients'])
            elif isinstance(local_item['ingredients'], str):
                ingredients_text = local_item['ingredients']
        
        # ãƒãƒƒãƒã‚¹ã‚³ã‚¢
        score = local_item.get('_match_score') or local_item.get('_score') or 1.0
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        original_data = {
            "source": "local_nutrition_database",
            "original_data": local_item,
            "search_term": search_term,
            "db_source": local_item.get('_db_source', 'unknown'),
            "match_score": score
        }
        
        # NutritionMatchã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
        return NutritionMatch(
            id=item_id,
            description=description,
            data_type=data_type,
            source="local_database",
            brand_owner=brand_owner,
            ingredients_text=ingredients_text,
            nutrients=nutrients,
            score=score,
            original_data=original_data
        ) 
```

============================================================

ğŸ“ ãƒ¢ãƒ‡ãƒ«å±¤
============================================================

ğŸ“„ FILE: app_v2/models/phase1_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 1,764 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:44:12
å­˜åœ¨: âœ…

CONTENT:
```
from typing import List, Optional
from pydantic import BaseModel, Field


class Ingredient(BaseModel):
    """é£Ÿææƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
    ingredient_name: str = Field(..., description="é£Ÿæã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ï¼‰")


class Dish(BaseModel):
    """æ–™ç†æƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
    dish_name: str = Field(..., description="ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ï¼‰")
    ingredients: List[Ingredient] = Field(..., description="ãã®æ–™ç†ã«å«ã¾ã‚Œã‚‹é£Ÿæã®ãƒªã‚¹ãƒˆ")


class Phase1Input(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    image_bytes: bytes = Field(..., description="ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆå½¢å¼ï¼‰")
    image_mime_type: str = Field(..., description="ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—")
    optional_text: Optional[str] = Field(None, description="ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±")

    class Config:
        arbitrary_types_allowed = True


class Phase1Output(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰"""
    dishes: List[Dish] = Field(..., description="ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆ")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_all_ingredient_names(self) -> List[str]:
        """å…¨ã¦ã®é£Ÿæåã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
        ingredient_names = []
        for dish in self.dishes:
            for ingredient in dish.ingredients:
                ingredient_names.append(ingredient.ingredient_name)
        return ingredient_names

    def get_all_dish_names(self) -> List[str]:
        """å…¨ã¦ã®æ–™ç†åã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
        return [dish.dish_name for dish in self.dishes] 
```

============================================================

ğŸ“„ FILE: app_v2/models/usda_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 3,030 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 15:28:52
å­˜åœ¨: âœ…

CONTENT:
```
from typing import List, Dict, Optional
from pydantic import BaseModel, Field


class USDANutrient(BaseModel):
    """USDAæ „é¤Šç´ æƒ…å ±ãƒ¢ãƒ‡ãƒ«"""
    name: str = Field(..., description="æ „é¤Šç´ å")
    amount: float = Field(..., description="100gã¾ãŸã¯100mlã‚ãŸã‚Šã®é‡")
    unit_name: str = Field(..., description="å˜ä½å (ä¾‹: g, mg, kcal)")
    nutrient_id: Optional[int] = Field(None, description="USDAæ „é¤Šç´ ID")
    nutrient_number: Optional[str] = Field(None, description="USDAæ „é¤Šç´ ç•ªå·")


class USDAMatch(BaseModel):
    """USDAç…§åˆçµæœãƒ¢ãƒ‡ãƒ«"""
    fdc_id: int = Field(..., description="USDA FoodData Central ID")
    description: str = Field(..., description="é£Ÿå“ã®å…¬å¼åç§°")
    data_type: Optional[str] = Field(None, description="USDAãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (ä¾‹: SR Legacy, Branded)")
    brand_owner: Optional[str] = Field(None, description="ãƒ–ãƒ©ãƒ³ãƒ‰æ‰€æœ‰è€… (Branded Foodsã®å ´åˆ)")
    ingredients_text: Optional[str] = Field(None, description="åŸææ–™ãƒªã‚¹ãƒˆæ–‡å­—åˆ— (Branded Foodsã®å ´åˆ)")
    food_nutrients: List[USDANutrient] = Field(default_factory=list, description="ä¸»è¦ãªæ „é¤Šç´ æƒ…å ±ã®ãƒªã‚¹ãƒˆ")
    score: Optional[float] = Field(None, description="æ¤œç´¢çµæœã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢")
    original_usda_data: Optional[Dict] = Field(None, description="USDA APIã‹ã‚‰ã®ã‚ªãƒªã‚¸ãƒŠãƒ«JSONãƒ‡ãƒ¼ã‚¿")


class USDAQueryInput(BaseModel):
    """USDAã‚¯ã‚¨ãƒªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    ingredient_names: List[str] = Field(..., description="æ¤œç´¢ã™ã‚‹é£Ÿæåã®ãƒªã‚¹ãƒˆ")
    dish_names: List[str] = Field(default_factory=list, description="æ¤œç´¢ã™ã‚‹æ–™ç†åã®ãƒªã‚¹ãƒˆ")
    search_options: Optional[Dict] = Field(None, description="æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")

    def get_all_search_terms(self) -> List[str]:
        """å…¨ã¦ã®æ¤œç´¢ç”¨èªã‚’å–å¾—"""
        return list(set(self.ingredient_names + self.dish_names))


class USDAQueryOutput(BaseModel):
    """USDAã‚¯ã‚¨ãƒªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«"""
    matches: Dict[str, USDAMatch] = Field(default_factory=dict, description="æ¤œç´¢èªå½™ã¨ãƒãƒƒãƒã—ãŸçµæœã®ãƒãƒƒãƒ—")
    search_summary: Dict[str, int] = Field(default_factory=dict, description="æ¤œç´¢ã‚µãƒãƒªãƒ¼ï¼ˆæˆåŠŸæ•°ã€å¤±æ•—æ•°ãªã©ï¼‰")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    errors: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_match_rate(self) -> float:
        """ç…§åˆæˆåŠŸç‡ã‚’è¨ˆç®—"""
        total_searches = self.search_summary.get("total_searches", 0)
        successful_matches = self.search_summary.get("successful_matches", 0)
        if total_searches == 0:
            return 0.0
        return successful_matches / total_searches

    def has_match(self, search_term: str) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸæ¤œç´¢èªå½™ã«ãƒãƒƒãƒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return search_term in self.matches and self.matches[search_term] is not None 
```

============================================================

ğŸ“„ FILE: app_v2/models/nutrition_search_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 6,916 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:40:52
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Nutrition Search Models

USDA database queryã¨ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®ä¸¡æ–¹ã§ä½¿ç”¨ã§ãã‚‹æ±ç”¨çš„ãªãƒ¢ãƒ‡ãƒ«
"""

from typing import List, Dict, Optional, Any, Union
from pydantic import BaseModel, Field


class NutritionNutrient(BaseModel):
    """æ „é¤Šç´ æƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆæ±ç”¨ï¼‰"""
    name: str = Field(..., description="æ „é¤Šç´ å")
    amount: float = Field(..., description="100gã¾ãŸã¯100mlã‚ãŸã‚Šã®é‡")
    unit_name: str = Field(..., description="å˜ä½å (ä¾‹: g, mg, kcal)")
    nutrient_id: Optional[Union[int, str]] = Field(None, description="æ „é¤Šç´ IDï¼ˆUSDA IDã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«IDï¼‰")
    nutrient_number: Optional[str] = Field(None, description="æ „é¤Šç´ ç•ªå·")


class NutritionMatch(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆçµæœãƒ¢ãƒ‡ãƒ«ï¼ˆæ±ç”¨ï¼‰"""
    id: Union[int, str] = Field(..., description="é£Ÿå“IDï¼ˆUSDA FDC IDã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«IDï¼‰")
    description: str = Field(..., description="é£Ÿå“ã®åç§°")
    data_type: Optional[str] = Field(None, description="ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (ä¾‹: SR Legacy, Branded, Local_Dish)")
    source: str = Field(..., description="ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ'usda_api' ã¾ãŸã¯ 'local_database'ï¼‰")
    brand_owner: Optional[str] = Field(None, description="ãƒ–ãƒ©ãƒ³ãƒ‰æ‰€æœ‰è€… (Branded Foodsã®å ´åˆ)")
    ingredients_text: Optional[str] = Field(None, description="åŸææ–™ãƒªã‚¹ãƒˆæ–‡å­—åˆ—")
    nutrients: List[NutritionNutrient] = Field(default_factory=list, description="æ „é¤Šç´ æƒ…å ±ã®ãƒªã‚¹ãƒˆ")
    score: Optional[float] = Field(None, description="æ¤œç´¢çµæœã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢")
    original_data: Optional[Dict[str, Any]] = Field(None, description="å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿")
    
    # USDAäº’æ›æ€§ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    @property
    def fdc_id(self) -> Union[int, str]:
        """USDAäº’æ›æ€§ã®ãŸã‚ã®fdc_idãƒ—ãƒ­ãƒ‘ãƒ†ã‚£"""
        return self.id
    
    @property 
    def food_nutrients(self) -> List[NutritionNutrient]:
        """USDAäº’æ›æ€§ã®ãŸã‚ã®food_nutrientsãƒ—ãƒ­ãƒ‘ãƒ†ã‚£"""
        return self.nutrients
    
    @property
    def original_usda_data(self) -> Optional[Dict[str, Any]]:
        """USDAäº’æ›æ€§ã®ãŸã‚ã®original_usda_dataãƒ—ãƒ­ãƒ‘ãƒ†ã‚£"""
        return self.original_data


class NutritionQueryInput(BaseModel):
    """æ „é¤Šæ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆæ±ç”¨ï¼‰"""
    ingredient_names: List[str] = Field(..., description="æ¤œç´¢ã™ã‚‹é£Ÿæåã®ãƒªã‚¹ãƒˆ")
    dish_names: List[str] = Field(default_factory=list, description="æ¤œç´¢ã™ã‚‹æ–™ç†åã®ãƒªã‚¹ãƒˆ")
    search_options: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    preferred_source: Optional[str] = Field(None, description="å„ªå…ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ'usda_api' ã¾ãŸã¯ 'local_database'ï¼‰")

    def get_all_search_terms(self) -> List[str]:
        """å…¨ã¦ã®æ¤œç´¢ç”¨èªã‚’å–å¾—"""
        return list(set(self.ingredient_names + self.dish_names))


class NutritionQueryOutput(BaseModel):
    """æ „é¤Šæ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆæ±ç”¨ï¼‰"""
    matches: Dict[str, NutritionMatch] = Field(default_factory=dict, description="æ¤œç´¢èªå½™ã¨ãƒãƒƒãƒã—ãŸçµæœã®ãƒãƒƒãƒ—")
    search_summary: Dict[str, Any] = Field(default_factory=dict, description="æ¤œç´¢ã‚µãƒãƒªãƒ¼ï¼ˆæˆåŠŸæ•°ã€å¤±æ•—æ•°ã€æ¤œç´¢æ–¹æ³•ãªã©ï¼‰")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    errors: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_match_rate(self) -> float:
        """ç…§åˆæˆåŠŸç‡ã‚’è¨ˆç®—"""
        total_searches = self.search_summary.get("total_searches", 0)
        successful_matches = self.search_summary.get("successful_matches", 0)
        if total_searches == 0:
            return 0.0
        return successful_matches / total_searches

    def has_match(self, search_term: str) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸæ¤œç´¢èªå½™ã«ãƒãƒƒãƒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return search_term in self.matches and self.matches[search_term] is not None
    
    def get_source_summary(self) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã®ãƒãƒƒãƒæ•°ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        source_counts = {}
        for match in self.matches.values():
            source = match.source
            source_counts[source] = source_counts.get(source, 0) + 1
        return source_counts


# USDAäº’æ›æ€§ã®ãŸã‚ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é–¢æ•°
def convert_usda_query_input_to_nutrition(usda_input) -> NutritionQueryInput:
    """USDAQueryInputã‚’NutritionQueryInputã«å¤‰æ›"""
    return NutritionQueryInput(
        ingredient_names=usda_input.ingredient_names,
        dish_names=usda_input.dish_names,
        search_options=usda_input.search_options,
        preferred_source="usda_api"
    )

def convert_nutrition_to_usda_query_output(nutrition_output: NutritionQueryOutput):
    """NutritionQueryOutputã‚’USDAQueryOutputäº’æ›å½¢å¼ã«å¤‰æ›"""
    from .usda_models import USDAQueryOutput, USDAMatch, USDANutrient
    
    # USDAMatchã«å¤‰æ›
    usda_matches = {}
    for term, nutrition_match in nutrition_output.matches.items():
        usda_nutrients = []
        for nutrient in nutrition_match.nutrients:
            usda_nutrients.append(USDANutrient(
                name=nutrient.name,
                amount=nutrient.amount,
                unit_name=nutrient.unit_name,
                nutrient_id=nutrient.nutrient_id if isinstance(nutrient.nutrient_id, int) else None,
                nutrient_number=nutrient.nutrient_number
            ))
        
        usda_matches[term] = USDAMatch(
            fdc_id=nutrition_match.id if isinstance(nutrition_match.id, int) else 0,
            description=nutrition_match.description,
            data_type=nutrition_match.data_type,
            brand_owner=nutrition_match.brand_owner,
            ingredients_text=nutrition_match.ingredients_text,
            food_nutrients=usda_nutrients,
            score=nutrition_match.score,
            original_usda_data=nutrition_match.original_data
        )
    
    # æ•°å€¤ã®ã¿ã®search_summaryã‚’ä½œæˆï¼ˆUSDAäº’æ›æ€§ã®ãŸã‚ï¼‰
    numeric_summary = {}
    for key, value in nutrition_output.search_summary.items():
        if isinstance(value, (int, float)):
            numeric_summary[key] = int(value)
        elif key in ["total_searches", "successful_matches", "failed_searches", "match_rate_percent"]:
            try:
                numeric_summary[key] = int(value) if value is not None else 0
            except (ValueError, TypeError):
                numeric_summary[key] = 0
    
    return USDAQueryOutput(
        matches=usda_matches,
        search_summary=numeric_summary,
        warnings=nutrition_output.warnings,
        errors=nutrition_output.errors
    ) 
```

============================================================

ğŸ“ ã‚µãƒ¼ãƒ“ã‚¹å±¤
============================================================

ğŸ“„ FILE: app_v2/services/gemini_service.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 11,545 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:41:30
å­˜åœ¨: âœ…

CONTENT:
```
import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig, HarmCategory, HarmBlockThreshold
from typing import Dict, Optional
import json
import logging
from PIL import Image
import io

from ..config.prompts import Phase1Prompts, Phase2Prompts

logger = logging.getLogger(__name__)

# Geminiã®æ§‹é€ åŒ–å‡ºåŠ›ã®ãŸã‚ã®JSONã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
            "description": "ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰ã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰ã€‚"},
                    "ingredients": {
                        "type": "array",
                        "description": "ã“ã®æ–™ç†ã«å«ã¾ã‚Œã‚‹ã¨æ¨å®šã•ã‚Œã‚‹ææ–™ã®ãƒªã‚¹ãƒˆï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰ã€‚",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "ææ–™ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰ã€‚"}
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}

REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
            "description": "ç”»åƒã‹ã‚‰ç‰¹å®šãƒ»ç²¾ç·»åŒ–ã•ã‚ŒãŸæ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "ç‰¹å®šã•ã‚ŒãŸæ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®åç§°ã€‚"},
                    "type": {"type": "string", "description": "æ–™ç†ã®ç¨®é¡ï¼ˆä¾‹: ä¸»èœ, å‰¯èœ, å˜å“é£Ÿå“ï¼‰ã€‚"},
                    "quantity_on_plate": {"type": "string", "description": "çš¿ã®ä¸Šã®é‡ã€‚"},
                    "calculation_strategy": {
                        "type": "string",
                        "enum": ["dish_level", "ingredient_level"],
                        "description": "ã“ã®ã‚¢ã‚¤ãƒ†ãƒ ã®æ „é¤Šè¨ˆç®—æ–¹é‡ã€‚"
                    },
                    "fdc_id": {
                        "type": "integer",
                        "description": "calculation_strategyãŒ'dish_level'ã®å ´åˆã€ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“ã®FDC IDã€‚ãã‚Œä»¥å¤–ã¯nullã€‚"
                    },
                    "usda_source_description": {
                        "type": "string",
                        "description": "calculation_strategyãŒ'dish_level'ã®å ´åˆã€ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“ã®USDAå…¬å¼åç§°ã€‚ãã‚Œä»¥å¤–ã¯nullã€‚"
                    },
                    "ingredients": {
                        "type": "array",
                        "description": "ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã«å«ã¾ã‚Œã‚‹ã¨æ¨å®šã•ã‚Œã‚‹ææ–™ã®ãƒªã‚¹ãƒˆã€‚",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "ææ–™ã®åç§°ã€‚"},
                                "fdc_id": {
                                    "type": "integer",
                                    "description": "calculation_strategyãŒ'ingredient_level'ã®å ´åˆã€ã“ã®ææ–™ã®FDC IDã€‚ãã‚Œä»¥å¤–ã¯nullã¾ãŸã¯çœç•¥å¯ã€‚"
                                },
                                "usda_source_description": {
                                    "type": "string",
                                    "description": "calculation_strategyãŒ'ingredient_level'ã®å ´åˆã€ã“ã®ææ–™ã®USDAå…¬å¼åç§°ã€‚ãã‚Œä»¥å¤–ã¯nullã¾ãŸã¯çœç•¥å¯ã€‚"
                                }
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "type", "quantity_on_plate", "calculation_strategy", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}


class GeminiService:
    """Vertex AIçµŒç”±ã§Geminiã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_id: str, location: str, model_name: str = "gemini-2.5-flash-preview-05-20"):
        """
        åˆæœŸåŒ–
        
        Args:
            project_id: GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
            location: Vertex AIã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: us-central1ï¼‰
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        # Vertex AIã®åˆæœŸåŒ–
        vertexai.init(project=project_id, location=location)
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.model = GenerativeModel(model_name=model_name)
        
        # generation_configã‚’ä½œæˆ (Phase1ç”¨ - å‡ºåŠ›å®‰å®šåŒ–)
        self.generation_config = GenerationConfig(
            temperature=0.0,  # å®Œå…¨ã«deterministicã«
            top_p=1.0,       # nucleus samplingã‚’ç„¡åŠ¹åŒ–
            top_k=1,         # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é¸æŠè‚¢ã®ã¿
            max_output_tokens=8192,
            candidate_count=1,  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å€™è£œã‚’1ã¤ã«åˆ¶é™
            response_mime_type="application/json",
            response_schema=MEAL_ANALYSIS_GEMINI_SCHEMA
        )
        
        # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }
    
    async def analyze_phase1(
        self, 
        image_bytes: bytes, 
        image_mime_type: str, 
        optional_text: Optional[str] = None
    ) -> Dict:
        """
        Phase1: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã¦é£Ÿäº‹æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜
            
        Returns:
            åˆ†æçµæœã®è¾æ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt = Phase1Prompts.get_system_prompt()
            user_prompt = Phase1Prompts.get_user_prompt(optional_text)
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase1 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e
    
    async def analyze_phase2(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        usda_candidates_text: str,
        initial_analysis_data: str
    ) -> Dict:
        """
        Phase2: USDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’å†åˆ†æ
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            usda_candidates_text: USDAå€™è£œæƒ…å ±ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
            initial_analysis_data: Phase1ã®AIå‡ºåŠ›ï¼ˆJSONæ–‡å­—åˆ—ï¼‰
            
        Returns:
            ç²¾ç·»åŒ–ã•ã‚ŒãŸåˆ†æçµæœã®è¾æ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt = Phase2Prompts.get_system_prompt()
            user_prompt = Phase2Prompts.get_user_prompt(
                usda_candidates_text=usda_candidates_text,
                initial_analysis_data=initial_analysis_data
            )
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Phase2ç”¨ã®Generation Config (å‡ºåŠ›å®‰å®šåŒ–)
            phase2_generation_config = GenerationConfig(
                temperature=0.0,  # å®Œå…¨ã«deterministicã«
                top_p=1.0,       # nucleus samplingã‚’ç„¡åŠ¹åŒ–
                top_k=1,         # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é¸æŠè‚¢ã®ã¿
                max_output_tokens=8192,
                candidate_count=1,  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å€™è£œã‚’1ã¤ã«åˆ¶é™
                response_mime_type="application/json",
                response_schema=REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA
            )
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=phase2_generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase2 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e 
```

============================================================

ğŸ“„ FILE: app_v2/services/usda_service.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 16,395 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 13:55:24
å­˜åœ¨: âœ…

CONTENT:
```
# app/services/usda_service.py
import httpx
import json
import logging
from typing import List, Optional, Dict, Any
from functools import lru_cache

from ..config import get_settings

logger = logging.getLogger(__name__)


class USDANutrient:
    """USDAæ „é¤Šç´ æƒ…å ±ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""
    def __init__(self, name: str, amount: float, unit_name: str, 
                 nutrient_id: Optional[int] = None, 
                 nutrient_number: Optional[str] = None):
        self.name = name
        self.amount = amount
        self.unit_name = unit_name
        self.nutrient_id = nutrient_id
        self.nutrient_number = nutrient_number


class USDASearchResultItem:
    """USDAæ¤œç´¢çµæœã‚¢ã‚¤ãƒ†ãƒ ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""
    def __init__(self, fdc_id: int, description: str, 
                 data_type: Optional[str] = None,
                 brand_owner: Optional[str] = None,
                 ingredients_text: Optional[str] = None,
                 food_nutrients: List[USDANutrient] = None,
                 score: Optional[float] = None,
                 original_data: Optional[Dict[str, Any]] = None):
        self.fdc_id = fdc_id
        self.description = description
        self.data_type = data_type
        self.brand_owner = brand_owner
        self.ingredients_text = ingredients_text
        self.food_nutrients = food_nutrients or []
        self.score = score
        self.original_data = original_data or {}


class USDAService:
    """USDA FoodData Central APIã¨ã®é€šä¿¡ã‚’ç®¡ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        settings = get_settings()
        self.api_key = settings.USDA_API_KEY
        self.base_url = settings.USDA_API_BASE_URL
        self.timeout = settings.USDA_API_TIMEOUT
        self.key_nutrient_numbers = settings.USDA_KEY_NUTRIENT_NUMBERS
        
        if not self.api_key:
            logger.error("USDA_API_KEY is not configured.")
            raise ValueError("USDA API key not configured.")
        
        # httpx.AsyncClientã®è¨­å®š
        self.client = httpx.AsyncClient(
            timeout=self.timeout,
            headers={"X-Api-Key": self.api_key}
        )
    
    async def search_foods(
        self,
        query: str,
        data_types: Optional[List[str]] = None,
        page_size: int = 5,
        page_number: int = 1,
        sort_by: str = "score",
        sort_order: str = "desc"
    ) -> List[USDASearchResultItem]:
        """
        USDA FoodData Central APIã§é£Ÿå“ã‚’æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
            data_types: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: ["Foundation", "SR Legacy", "Branded"]ï¼‰
            page_size: 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®çµæœæ•°
            page_number: å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸ç•ªå·
            sort_by: ã‚½ãƒ¼ãƒˆã‚­ãƒ¼
            sort_order: ã‚½ãƒ¼ãƒˆé †ï¼ˆ"asc" ã¾ãŸã¯ "desc"ï¼‰
            
        Returns:
            USDASearchResultItemã®ãƒªã‚¹ãƒˆ
        """
        params = {
            "query": query,
            "api_key": self.api_key,
            "pageSize": page_size,
            "pageNumber": page_number,
            "sortBy": sort_by,
            "sortOrder": sort_order
        }
        
        if data_types:
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã¨ã—ã¦æ¸¡ã™
            params["dataType"] = ",".join(data_types)
        
        try:
            logger.info(f"USDA API search: query='{query}', page_size={page_size}")
            response = await self.client.get(f"{self.base_url}/foods/search", params=params)
            
            # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæƒ…å ±ã®ãƒ­ã‚°
            if "X-RateLimit-Remaining" in response.headers:
                logger.info(f"USDA API Rate Limit Remaining: {response.headers.get('X-RateLimit-Remaining')}")
            
            response.raise_for_status()
            data = response.json()
            
            results = []
            for food_data in data.get("foods", [])[:page_size]:
                nutrients_extracted = self._extract_nutrients(food_data.get("foodNutrients", []))
                
                results.append(USDASearchResultItem(
                    fdc_id=food_data.get("fdcId"),
                    description=food_data.get("description"),
                    data_type=food_data.get("dataType"),
                    brand_owner=food_data.get("brandOwner"),
                    ingredients_text=food_data.get("ingredients"),
                    food_nutrients=nutrients_extracted,
                    score=food_data.get("score"),
                    original_data=food_data
                ))
            
            logger.info(f"USDA API search returned {len(results)} results for query '{query}'")
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(f"USDA API HTTP error: {e.response.status_code} - {e.response.text}")
            if e.response.status_code == 429:
                raise RuntimeError(f"USDA API rate limit exceeded. Detail: {e.response.text}") from e
            raise RuntimeError(f"USDA API error: {e.response.status_code} - {e.response.text}") from e
        except httpx.RequestError as e:
            logger.error(f"USDA API request failed: {str(e)}")
            raise RuntimeError(f"USDA API request failed: {str(e)}") from e
        except (json.JSONDecodeError, TypeError, KeyError) as e:
            logger.error(f"USDA API response parsing error: {str(e)}")
            raise RuntimeError(f"USDA API response parsing error: {str(e)}") from e
        except Exception as e:
            logger.error(f"Unexpected error in USDAService.search_foods: {str(e)}")
            raise RuntimeError(f"Unexpected error in USDA service: {str(e)}") from e
    
    def _extract_nutrients(self, food_nutrients: List[Dict[str, Any]]) -> List[USDANutrient]:
        """
        foodNutrientsãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸»è¦æ „é¤Šç´ ã‚’æŠ½å‡º
        
        Args:
            food_nutrients: USDA APIã‹ã‚‰è¿”ã•ã‚Œã‚‹æ „é¤Šç´ ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            USDANutrientã®ãƒªã‚¹ãƒˆ
        """
        nutrients_extracted = []
        
        for nutrient_entry in food_nutrients:
            # æ „é¤Šç´ æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰
            nutrient_detail = nutrient_entry.get("nutrient", {})
            amount = nutrient_entry.get("amount")
            
            # Branded Foodsã®abridgedãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¸ã®å¯¾å¿œ
            if not nutrient_detail and "nutrientId" in nutrient_entry:
                nutrient_id = nutrient_entry.get("nutrientId")
                name = nutrient_entry.get("nutrientName")
                number = nutrient_entry.get("nutrientNumber")
                unit_name = nutrient_entry.get("unitName")
                amount = nutrient_entry.get("value")  # Branded abridgedã§ã¯"value"
            else:
                # SR Legacy, Foundation, ã¾ãŸã¯ full Branded
                nutrient_id = nutrient_detail.get("id")
                name = nutrient_detail.get("name")
                number = nutrient_detail.get("number")
                unit_name = nutrient_detail.get("unitName")
            
            # ä¸»è¦æ „é¤Šç´ ã®ã¿ã‚’æŠ½å‡º
            if number and str(number) in self.key_nutrient_numbers:
                if name and amount is not None and unit_name:
                    nutrients_extracted.append(USDANutrient(
                        name=name,
                        amount=float(amount),
                        unit_name=unit_name,
                        nutrient_id=int(nutrient_id) if nutrient_id else None,
                        nutrient_number=str(number) if number else None
                    ))
        
        return nutrients_extracted
    
    async def get_food_details(
        self, 
        fdc_id: int, 
        format: str = "full",
        target_nutrient_numbers: Optional[List[str]] = None
    ) -> Optional[USDASearchResultItem]:
        """
        ç‰¹å®šã®FDC IDã®é£Ÿå“è©³ç´°æƒ…å ±ã‚’å–å¾—
        
        Args:
            fdc_id: é£Ÿå“ã®FDC ID
            format: ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ï¼ˆ"abridged" ã¾ãŸã¯ "full"ï¼‰
            target_nutrient_numbers: å–å¾—ã™ã‚‹æ „é¤Šç´ ç•ªå·ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            USDASearchResultItem ã¾ãŸã¯ None
        """
        params = {
            "api_key": self.api_key,
            "format": format
        }
        
        if target_nutrient_numbers:
            params["nutrients"] = ",".join(target_nutrient_numbers)
        
        try:
            logger.info(f"USDA API get food details: fdc_id={fdc_id}")
            response = await self.client.get(f"{self.base_url}/food/{fdc_id}", params=params)
            response.raise_for_status()
            
            food_data = response.json()
            nutrients_extracted = self._extract_nutrients(food_data.get("foodNutrients", []))
            
            return USDASearchResultItem(
                fdc_id=food_data.get("fdcId"),
                description=food_data.get("description"),
                data_type=food_data.get("dataType"),
                brand_owner=food_data.get("brandOwner"),
                ingredients_text=food_data.get("ingredients"),
                food_nutrients=nutrients_extracted,
                score=food_data.get("score"),
                original_data=food_data
            )
            
        except Exception as e:
            logger.error(f"Error fetching food details for FDC ID {fdc_id}: {str(e)}")
            return None

    async def get_food_details_for_nutrition(self, fdc_id: int) -> Optional[Dict[str, float]]:
        """
        æ „é¤Šè¨ˆç®—ç”¨ã®é£Ÿå“è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰
        
        å…¥åŠ›: FDC ID
        å‡¦ç†: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªå¾Œã€å¿…è¦ãªã‚‰USDA APIã‹ã‚‰é£Ÿå“è©³ç´°ã‚’å–å¾—ã—ã€ä¸»è¦æ „é¤Šç´ ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã•ã‚ŒãŸIDï¼‰ã‚’100gã‚ãŸã‚Šã§æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹ã€‚çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã€‚
        å‡ºåŠ›: 100gã‚ãŸã‚Šã®ä¸»è¦æ „é¤Šç´ è¾æ›¸ã€ã¾ãŸã¯ Noneã€‚
        
        Args:
            fdc_id: é£Ÿå“ã®FDC ID
            
        Returns:
            Optional[Dict[str, float]]: 100gã‚ãŸã‚Šã®ä¸»è¦æ „é¤Šç´ è¾æ›¸ã€ã¾ãŸã¯ None
        """
        if not fdc_id:
            logger.warning("Invalid FDC ID provided")
            return None
        
        try:
            # TODO: å°†æ¥çš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã‚’å®Ÿè£…ï¼ˆRedisç­‰ï¼‰
            # ç¾çŠ¶ã¯ç›´æ¥APIã‹ã‚‰å–å¾—
            
            logger.info(f"USDA API get food details for nutrition: fdc_id={fdc_id}")
            
            params = {
                "api_key": self.api_key,
                "format": "full",  # è©³ç´°ãªæ „é¤Šæƒ…å ±ãŒå¿…è¦
                "nutrients": ",".join(self.key_nutrient_numbers)  # ä¸»è¦æ „é¤Šç´ ã®ã¿ã‚’å–å¾—
            }
            
            response = await self.client.get(f"{self.base_url}/food/{fdc_id}", params=params)
            
            # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæƒ…å ±ã®ãƒ­ã‚°
            if "X-RateLimit-Remaining" in response.headers:
                logger.info(f"USDA API Rate Limit Remaining: {response.headers.get('X-RateLimit-Remaining')}")
            
            response.raise_for_status()
            food_data_raw = response.json()
            
            # ä¸»è¦æ „é¤Šç´ ã‚’æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹
            key_nutrients = self._parse_nutrients_for_calculation(food_data_raw)
            
            if key_nutrients:
                logger.info(f"Successfully extracted {len(key_nutrients)} key nutrients for FDC ID {fdc_id}")
                # TODO: å°†æ¥çš„ã«ã“ã“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                return key_nutrients
            else:
                logger.warning(f"No key nutrients found for FDC ID {fdc_id}")
                return None
                
        except httpx.HTTPStatusError as e:
            logger.error(f"USDA API HTTP error for FDC ID {fdc_id}: {e.response.status_code} - {e.response.text}")
            if e.response.status_code == 404:
                logger.warning(f"Food with FDC ID {fdc_id} not found")
                return None
            elif e.response.status_code == 429:
                raise RuntimeError(f"USDA API rate limit exceeded for FDC ID {fdc_id}") from e
            raise RuntimeError(f"USDA API error for FDC ID {fdc_id}: {e.response.status_code}") from e
        except httpx.RequestError as e:
            logger.error(f"USDA API request failed for FDC ID {fdc_id}: {str(e)}")
            raise RuntimeError(f"USDA API request failed for FDC ID {fdc_id}: {str(e)}") from e
        except Exception as e:
            logger.error(f"Unexpected error getting food details for nutrition (FDC ID {fdc_id}): {str(e)}")
            return None

    def _parse_nutrients_for_calculation(self, food_data_raw: dict) -> Dict[str, float]:
        """
        USDA APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æ „é¤Šè¨ˆç®—ç”¨ã®ä¸»è¦æ „é¤Šç´ ã‚’æŠ½å‡ºï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        
        Args:
            food_data_raw: USDA APIã‹ã‚‰ã®ç”Ÿã®é£Ÿå“ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Dict[str, float]: ä¸»è¦æ „é¤Šç´ è¾æ›¸ï¼ˆã‚­ãƒ¼ã¯æ¨™æº–åŒ–ã•ã‚ŒãŸåå‰ï¼‰
        """
        key_nutrients = {}
        
        try:
            food_nutrients = food_data_raw.get("foodNutrients", [])
            
            for nutrient_entry in food_nutrients:
                # æ „é¤Šç´ æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰
                nutrient_detail = nutrient_entry.get("nutrient", {})
                amount = nutrient_entry.get("amount")
                
                # Branded Foodsã®abridgedãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¸ã®å¯¾å¿œ
                if not nutrient_detail and "nutrientId" in nutrient_entry:
                    number = nutrient_entry.get("nutrientNumber")
                    amount = nutrient_entry.get("value")  # Branded abridgedã§ã¯"value"
                else:
                    # SR Legacy, Foundation, ã¾ãŸã¯ full Branded
                    number = nutrient_detail.get("number")
                
                # ä¸»è¦æ „é¤Šç´ ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ „é¤Šç´ ç•ªå·ã‹ã‚‰æ¨™æº–åŒ–ã•ã‚ŒãŸã‚­ãƒ¼åã¸ï¼‰
                if number and str(number) in self.key_nutrient_numbers and amount is not None:
                    if str(number) == "208":  # Energy (calories)
                        key_nutrients["calories_kcal"] = float(amount)
                    elif str(number) == "203":  # Protein
                        key_nutrients["protein_g"] = float(amount)
                    elif str(number) == "204":  # Total lipid (fat)
                        key_nutrients["fat_g"] = float(amount)
                    elif str(number) == "205":  # Carbohydrate, by difference
                        key_nutrients["carbohydrates_g"] = float(amount)
                    elif str(number) == "291":  # Fiber, total dietary (optional)
                        key_nutrients["fiber_g"] = float(amount)
                    elif str(number) == "269":  # Sugars, total (optional)
                        key_nutrients["sugars_g"] = float(amount)
                    elif str(number) == "307":  # Sodium (optional)
                        key_nutrients["sodium_mg"] = float(amount)
            
            # å¿…é ˆæ „é¤Šç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0.0ã¨ã—ã¦è¨­å®š
            essential_nutrients = ["calories_kcal", "protein_g", "fat_g", "carbohydrates_g"]
            for nutrient in essential_nutrients:
                if nutrient not in key_nutrients:
                    key_nutrients[nutrient] = 0.0
                    logger.debug(f"Missing essential nutrient {nutrient}, set to 0.0")
            
            logger.debug(f"Parsed key nutrients: {key_nutrients}")
            return key_nutrients
            
        except Exception as e:
            logger.error(f"Error parsing nutrients for calculation: {str(e)}")
            return {}
    
    async def close_client(self):
        """HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ã‚º"""
        await self.client.aclose()


# FastAPIã®ä¾å­˜æ€§æ³¨å…¥ç”¨é–¢æ•°
async def get_usda_service():
    """
    USDAServiceã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æä¾›ã™ã‚‹ä¾å­˜æ€§æ³¨å…¥é–¢æ•°
    """
    service = USDAService()
    try:
        yield service
    finally:
        await service.close_client() 
```

============================================================

ğŸ“ è¨­å®šç®¡ç†
============================================================

ğŸ“„ FILE: app_v2/config/settings.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,417 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:11:08
å­˜åœ¨: âœ…

CONTENT:
```
from typing import Optional, List
from pydantic_settings import BaseSettings
from functools import lru_cache


class Settings(BaseSettings):
    """
    APIè¨­å®šã‚¯ãƒ©ã‚¹
    ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šå€¤ã‚’èª­ã¿è¾¼ã‚€
    """
    # Vertex AIè¨­å®š
    GEMINI_PROJECT_ID: str  # GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDï¼ˆå¿…é ˆï¼‰
    GEMINI_LOCATION: str = "us-central1"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
    GEMINI_MODEL_NAME: str = "gemini-2.5-flash-preview-05-20"
    
    # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢è¨­å®š
    USE_LOCAL_NUTRITION_SEARCH: bool = True  # ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
    NUTRITION_DB_EXPERIMENT_PATH: Optional[str] = None  # nutrition_db_experimentã¸ã®ãƒ‘ã‚¹ï¼ˆè‡ªå‹•æ¤œå‡ºã™ã‚‹å ´åˆã¯Noneï¼‰
    
    # USDA APIè¨­å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
    USDA_API_KEY: str  # USDA FoodData Central APIã‚­ãƒ¼ï¼ˆå¿…é ˆï¼‰
    USDA_API_BASE_URL: str = "https://api.nal.usda.gov/fdc/v1"
    USDA_API_TIMEOUT: float = 10.0  # APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
    USDA_SEARCH_CANDIDATES_LIMIT: int = 5  # 1å›ã®æ¤œç´¢ã§å–å¾—ã™ã‚‹æœ€å¤§å€™è£œæ•°
    # ä¸»è¦æ „é¤Šç´ ç•ªå·ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã¨ã—ã¦ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
    USDA_KEY_NUTRIENT_NUMBERS_STR: str = "208,203,204,205,291,269,307"
    # 208: Energy (kcal), 203: Protein, 204: Total lipid (fat), 
    # 205: Carbohydrate, 291: Fiber, 269: Total sugars, 307: Sodium
    
    @property
    def USDA_KEY_NUTRIENT_NUMBERS(self) -> List[str]:
        """ä¸»è¦æ „é¤Šç´ ç•ªå·ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        return self.USDA_KEY_NUTRIENT_NUMBERS_STR.split(",")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    CACHE_TYPE: str = "simple"  # "simple", "redis", "memcached"
    CACHE_REDIS_URL: Optional[str] = None  # Redisã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®URL
    USDA_CACHE_TTL_SECONDS: int = 3600  # USDAãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé–“ï¼ˆ1æ™‚é–“ï¼‰
    
    # APIè¨­å®š
    API_LOG_LEVEL: str = "INFO"
    FASTAPI_ENV: str = "development"
    
    # ã‚µãƒ¼ãƒãƒ¼è¨­å®š
    HOST: str = "0.0.0.0"
    PORT: int = 8000
    
    # APIãƒãƒ¼ã‚¸ãƒ§ãƒ³
    API_VERSION: str = "v1"
    
    # çµæœä¿å­˜è¨­å®š
    RESULTS_DIR: str = "analysis_results"
    
    class Config:
        env_file = ".env"
        case_sensitive = True


@lru_cache()
def get_settings() -> Settings:
    """
    è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ï¼‰
    """
    return Settings() 
```

============================================================

ğŸ“ ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ 
============================================================

ğŸ“„ FILE: nutrition_db_experiment/search_service/api/search_handler.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 9,344 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 11:36:26
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Search Handler - æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã—ã€ã‚¯ã‚¨ãƒªå‰å‡¦ç†ã€ã‚¯ã‚¨ãƒªæ§‹ç¯‰ã€Elasticsearchæ¤œç´¢ã‚’çµ±åˆ
"""

import os
import sys
import json
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from nlp.query_preprocessor import preprocess_query, analyze_query
from api.query_builder import build_nutrition_search_query

# Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯è¨­å®šã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    print("Warning: Elasticsearch client not available. Install with: pip install elasticsearch")

@dataclass
class SearchRequest:
    """æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    query: str
    db_type_filter: Optional[str] = None
    size: int = 20
    enable_highlight: bool = True
    enable_synonyms: bool = True
    custom_weights: Optional[Dict[str, float]] = None

@dataclass
class SearchResponse:
    """æ¤œç´¢ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    results: List[Dict[str, Any]]
    total_hits: int
    query_info: Dict[str, Any]
    took_ms: int
    max_score: float

class NutritionSearchHandler:
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self, elasticsearch_host: str = "localhost:9200", index_name: str = "nutrition_db"):
        """
        æ¤œç´¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–
        
        Args:
            elasticsearch_host: Elasticsearchãƒ›ã‚¹ãƒˆ
            index_name: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
        """
        self.elasticsearch_host = elasticsearch_host
        self.index_name = index_name
        self.es_client = None
        
        # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        if ELASTICSEARCH_AVAILABLE:
            try:
                self.es_client = Elasticsearch([elasticsearch_host])
                # æ¥ç¶šãƒ†ã‚¹ãƒˆ
                if self.es_client.ping():
                    self.logger.info(f"Elasticsearchæ¥ç¶šæˆåŠŸ: {elasticsearch_host}")
                else:
                    self.logger.warning(f"Elasticsearchæ¥ç¶šå¤±æ•—: {elasticsearch_host}")
            except Exception as e:
                self.logger.error(f"ElasticsearchåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            self.logger.warning("Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    def search(self, request: SearchRequest) -> SearchResponse:
        """
        æ¤œç´¢ã‚’å®Ÿè¡Œ
        
        Args:
            request: æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            
        Returns:
            æ¤œç´¢ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        start_time = datetime.now()
        
        try:
            # 1. ã‚¯ã‚¨ãƒªå‰å‡¦ç†
            processed_query = preprocess_query(
                request.query, 
                expand_synonyms=request.enable_synonyms
            )
            
            # 2. ã‚¯ã‚¨ãƒªåˆ†æï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼‰
            query_analysis = analyze_query(request.query)
            
            # 3. Elasticsearchã‚¯ã‚¨ãƒªæ§‹ç¯‰
            es_query = build_nutrition_search_query(
                processed_query=processed_query,
                original_query=request.query,
                db_type_filter=request.db_type_filter,
                size=request.size,
                custom_weights=request.custom_weights
            )
            
            # 4. Elasticsearchæ¤œç´¢å®Ÿè¡Œ
            if self.es_client:
                response = self.es_client.search(
                    index=self.index_name,
                    body=es_query
                )
                results = self._format_search_results(response)
                total_hits = response['hits']['total']['value']
                max_score = response['hits']['max_score'] or 0.0
            else:
                # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆElasticsearchæœªæ¥ç¶šæ™‚ï¼‰
                results = self._mock_search_results(request.query)
                total_hits = len(results)
                max_score = 1.0
            
            # 5. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
            end_time = datetime.now()
            took_ms = int((end_time - start_time).total_seconds() * 1000)
            
            return SearchResponse(
                results=results,
                total_hits=total_hits,
                query_info={
                    "original_query": request.query,
                    "processed_query": processed_query,
                    "analysis": query_analysis,
                    "elasticsearch_query": es_query,
                    "db_type_filter": request.db_type_filter
                },
                took_ms=took_ms,
                max_score=max_score
            )
            
        except Exception as e:
            self.logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return SearchResponse(
                results=[],
                total_hits=0,
                query_info={
                    "original_query": request.query,
                    "error": str(e)
                },
                took_ms=0,
                max_score=0.0
            )
    
    def _format_search_results(self, es_response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Elasticsearchãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ•´å½¢
        
        Args:
            es_response: Elasticsearchãƒ¬ã‚¹ãƒãƒ³ã‚¹
            
        Returns:
            æ•´å½¢æ¸ˆã¿çµæœãƒªã‚¹ãƒˆ
        """
        results = []
        
        for hit in es_response['hits']['hits']:
            result = {
                **hit['_source'],
                '_score': hit['_score'],
                '_id': hit['_id']
            }
            
            # ãƒã‚¤ãƒ©ã‚¤ãƒˆæƒ…å ±ã‚’è¿½åŠ 
            if 'highlight' in hit:
                result['_highlight'] = hit['highlight']
            
            results.append(result)
        
        return results
    
    def _mock_search_results(self, query: str) -> List[Dict[str, Any]]:
        """
        ãƒ¢ãƒƒã‚¯æ¤œç´¢çµæœã‚’ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            
        Returns:
            ãƒ¢ãƒƒã‚¯çµæœãƒªã‚¹ãƒˆ
        """
        # ç°¡å˜ãªãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
        mock_results = [
            {
                "db_type": "dish",
                "id": 123456,
                "search_name": f"Cooked {query} with vegetables",
                "nutrition": {
                    "calories": 150.0,
                    "protein": 25.0,
                    "fat": 5.0,
                    "carbs": 15.0
                },
                "weight": 100.0,
                "_score": 2.5
            },
            {
                "db_type": "ingredient", 
                "id": 789012,
                "search_name": f"{query.capitalize()}, raw, fresh",
                "nutrition": {
                    "calories": 120.0,
                    "protein": 20.0,
                    "fat": 3.0,
                    "carbs": 10.0
                },
                "weight": 100.0,
                "_score": 2.0
            }
        ]
        
        return mock_results
    
    def health_check(self) -> Dict[str, Any]:
        """
        ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        
        Returns:
            ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹æƒ…å ±
        """
        status = {
            "service": "nutrition_search",
            "status": "healthy",
            "elasticsearch": {
                "available": ELASTICSEARCH_AVAILABLE,
                "connected": False,
                "host": self.elasticsearch_host,
                "index": self.index_name
            },
            "components": {
                "query_preprocessor": True,
                "query_builder": True
            }
        }
        
        if self.es_client:
            try:
                status["elasticsearch"]["connected"] = self.es_client.ping()
                if status["elasticsearch"]["connected"]:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å­˜åœ¨ç¢ºèª
                    index_exists = self.es_client.indices.exists(index=self.index_name)
                    status["elasticsearch"]["index_exists"] = index_exists
            except Exception as e:
                status["elasticsearch"]["error"] = str(e)
                status["status"] = "degraded"
        
        return status

# ä¾¿åˆ©é–¢æ•°
def create_search_handler(
    elasticsearch_host: str = "localhost:9200",
    index_name: str = "nutrition_db"
) -> NutritionSearchHandler:
    """æ¤œç´¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°"""
    return NutritionSearchHandler(elasticsearch_host, index_name)

def search_nutrition_db(
    query: str,
    db_type_filter: Optional[str] = None,
    size: int = 20,
    elasticsearch_host: str = "localhost:9200",
    index_name: str = "nutrition_db"
) -> SearchResponse:
    """ä¾¿åˆ©ãªæ¤œç´¢é–¢æ•°"""
    handler = create_search_handler(elasticsearch_host, index_name)
    request = SearchRequest(
        query=query,
        db_type_filter=db_type_filter,
        size=size
    )
    return handler.search(request) 
```

============================================================

ğŸ“„ FILE: nutrition_db_experiment/search_service/api/query_builder.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 11,619 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 11:40:30
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Query Builder - Elasticsearch JSONã‚¯ã‚¨ãƒªæ§‹ç¯‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

BM25F + function_scoreã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
ä»•æ§˜æ›¸ã«å¾“ã£ãŸãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’å®Ÿè£…
"""

from typing import Dict, List, Optional, Any
import json

class NutritionSearchQueryBuilder:
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ç”¨ã®Elasticsearchã‚¯ã‚¨ãƒªãƒ“ãƒ«ãƒ€ãƒ¼"""
    
    def __init__(self):
        """ã‚¯ã‚¨ãƒªãƒ“ãƒ«ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°é‡ã¿
        self.default_weights = {
            "exact_phrase_bonus": 100.0,
            "exact_word_bonus": 80.0,
            "phrase_proximity_bonus": 50.0,
            "prefix_match_bonus": 10.0,
            "base_field_boost": 1.0,
            "exact_field_boost": 3.0
        }
    
    def build_search_query(
        self,
        processed_query: str,
        original_query: str,
        db_type_filter: Optional[str] = None,
        size: int = 20,
        weights: Optional[Dict[str, float]] = None,
        enable_highlight: bool = True,
        enable_synonyms: bool = True
    ) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            processed_query: å‰å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
            original_query: å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
            db_type_filter: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ ("dish", "ingredient", "branded")
            size: è¿”å´ã™ã‚‹çµæœæ•°
            weights: ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚³ã‚¢é‡ã¿
            enable_highlight: ãƒã‚¤ãƒ©ã‚¤ãƒˆæ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
            enable_synonyms: é¡ç¾©èªå±•é–‹ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ï¼ˆå°†æ¥æ‹¡å¼µç”¨ï¼‰
            
        Returns:
            Elasticsearch JSONã‚¯ã‚¨ãƒª
        """
        # é‡ã¿ã®ãƒãƒ¼ã‚¸
        final_weights = self.default_weights.copy()
        if weights:
            final_weights.update(weights)
        
        # ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªï¼ˆBM25Fã‚¹ã‚³ã‚¢ç”¨ï¼‰
        base_query = self._build_base_query(processed_query, final_weights)
        
        # function_scoreã‚¯ã‚¨ãƒªã§ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿é©ç”¨å‰ï¼‰
        function_score_query = self._build_function_score_query(
            base_query, 
            original_query, 
            processed_query,
            final_weights
        )
        
        # ãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ ï¼ˆfunction_scoreã‚¯ã‚¨ãƒªå…¨ä½“ã«é©ç”¨ï¼‰
        if db_type_filter and db_type_filter != "all":
            function_score_query = {
                "bool": {
                    "must": [function_score_query],
                    "filter": [
                        {"term": {"db_type": db_type_filter}}
                    ]
                }
            }
        
        # å®Œå…¨ãªã‚¯ã‚¨ãƒªæ§‹ç¯‰
        search_query = {
            "query": function_score_query,
            "size": size,
            "_source": ["db_type", "id", "search_name", "nutrition", "weight"]
        }
        
        # ãƒã‚¤ãƒ©ã‚¤ãƒˆè¿½åŠ 
        if enable_highlight:
            search_query["highlight"] = self._build_highlight_config()
        
        return search_query
    
    def _build_base_query(self, processed_query: str, weights: Dict[str, float]) -> Dict[str, Any]:
        """
        BM25Fãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            processed_query: å‰å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒª
            weights: ã‚¹ã‚³ã‚¢é‡ã¿
            
        Returns:
            ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªè¾æ›¸
        """
        return {
            "multi_match": {
                "query": processed_query,
                "fields": [
                    f"search_name^{weights['base_field_boost']}",
                    f"search_name.exact^{weights['exact_field_boost']}"
                ],
                "type": "best_fields",
                "operator": "OR",
                "fuzziness": "AUTO",
                "max_expansions": 50,
                "prefix_length": 2
            }
        }
    
    def _add_filters(self, base_query: Dict[str, Any], db_type: str) -> Dict[str, Any]:
        """
        ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¿½åŠ ã—ã¦boolã‚¯ã‚¨ãƒªã«å¤‰æ›
        
        Args:
            base_query: ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª
            db_type: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãboolã‚¯ã‚¨ãƒª
        """
        return {
            "bool": {
                "must": [base_query],
                "filter": [
                    {"term": {"db_type": db_type}}
                ]
            }
        }
    
    def _build_function_score_query(
        self, 
        base_query: Dict[str, Any], 
        original_query: str,
        processed_query: str,
        weights: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        function_scoreã‚¯ã‚¨ãƒªã§ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚’æ§‹ç¯‰
        
        Args:
            base_query: ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª
            original_query: å…ƒã®ã‚¯ã‚¨ãƒª
            processed_query: å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒª
            weights: ã‚¹ã‚³ã‚¢é‡ã¿
            
        Returns:
            function_scoreã‚¯ã‚¨ãƒª
        """
        functions = []
        
        # 1. å®Œå…¨ä¸€è‡´ãƒ•ãƒ¬ãƒ¼ã‚ºãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€å„ªå…ˆï¼‰
        functions.append({
            "filter": {
                "match_phrase": {
                    "search_name.exact": {
                        "query": original_query,
                        "slop": 0
                    }
                }
            },
            "weight": weights["exact_phrase_bonus"]
        })
        
        # 2. è¿‘æ¥ãƒ•ãƒ¬ãƒ¼ã‚ºãƒœãƒ¼ãƒŠã‚¹ï¼ˆslop=1è¨±å®¹ï¼‰
        functions.append({
            "filter": {
                "match_phrase": {
                    "search_name": {
                        "query": original_query,
                        "slop": 1
                    }
                }
            },
            "weight": weights["phrase_proximity_bonus"]
        })
        
        # 3. å®Œå…¨ä¸€è‡´å˜èªãƒœãƒ¼ãƒŠã‚¹ï¼ˆå€‹åˆ¥å˜èªãƒ¬ãƒ™ãƒ«ï¼‰
        # å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒªã®å„å˜èªã«å¯¾ã—ã¦
        for word in processed_query.split():
            if len(word) > 2:  # çŸ­ã™ãã‚‹å˜èªã¯é™¤å¤–
                functions.append({
                    "filter": {
                        "term": {
                            "search_name.exact": word
                        }
                    },
                    "weight": weights["exact_word_bonus"] * 0.5  # å˜èªãƒ¬ãƒ™ãƒ«ã¯å°‘ã—ä½ã‚ã«
                })
        
        # 4. å‰æ–¹ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆä½å„ªå…ˆåº¦ï¼‰
        functions.append({
            "filter": {
                "match_phrase_prefix": {
                    "search_name": {
                        "query": original_query,
                        "max_expansions": 10
                    }
                }
            },
            "weight": weights["prefix_match_bonus"]
        })
        
        return {
            "function_score": {
                "query": base_query,
                "functions": functions,
                "score_mode": "sum",  # å„ãƒœãƒ¼ãƒŠã‚¹ã‚’ç´¯ç©
                "boost_mode": "sum",  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ã«ãƒœãƒ¼ãƒŠã‚¹ã‚’åŠ ç®—
                "max_boost": 1000.0  # ä¸Šé™è¨­å®š
            }
        }
    
    def _build_highlight_config(self) -> Dict[str, Any]:
        """
        ãƒã‚¤ãƒ©ã‚¤ãƒˆè¨­å®šã‚’æ§‹ç¯‰
        
        Returns:
            ãƒã‚¤ãƒ©ã‚¤ãƒˆè¨­å®šè¾æ›¸
        """
        return {
            "fields": {
                "search_name": {
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"],
                    "fragment_size": 150,
                    "number_of_fragments": 1
                },
                "search_name.exact": {
                    "pre_tags": ["<strong>"],
                    "post_tags": ["</strong>"],
                    "fragment_size": 150,
                    "number_of_fragments": 1
                }
            },
            "require_field_match": False
        }
    
    def build_simple_query(
        self, 
        query: str, 
        db_type_filter: Optional[str] = None,
        size: int = 20
    ) -> Dict[str, Any]:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            db_type_filter: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
            size: è¿”å´ã™ã‚‹çµæœæ•°
            
        Returns:
            ã‚·ãƒ³ãƒ—ãƒ«ãªElasticsearchã‚¯ã‚¨ãƒª
        """
        base_query = {
            "multi_match": {
                "query": query,
                "fields": ["search_name^2", "search_name.exact^3"],
                "type": "best_fields",
                "fuzziness": "AUTO"
            }
        }
        
        if db_type_filter and db_type_filter != "all":
            base_query = {
                "bool": {
                    "must": [base_query],
                    "filter": [{"term": {"db_type": db_type_filter}}]
                }
            }
        
        return {
            "query": base_query,
            "size": size,
            "_source": ["db_type", "id", "search_name", "nutrition", "weight"]
        }
    
    def build_analysis_query(self, text: str, analyzer: str = "custom_food_analyzer") -> Dict[str, Any]:
        """
        ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ã®ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            text: åˆ†æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            analyzer: ä½¿ç”¨ã™ã‚‹ã‚¢ãƒŠãƒ©ã‚¤ã‚¶å
            
        Returns:
            åˆ†æç”¨ã‚¯ã‚¨ãƒª
        """
        return {
            "analyzer": analyzer,
            "text": text
        }
    
    def get_weight_explanation(self) -> Dict[str, Any]:
        """
        ç¾åœ¨ã®é‡ã¿è¨­å®šã®èª¬æ˜ã‚’å–å¾—
        
        Returns:
            é‡ã¿è¨­å®šã®èª¬æ˜è¾æ›¸
        """
        return {
            "weights": self.default_weights,
            "explanations": {
                "exact_phrase_bonus": "å®Œå…¨ãƒ•ãƒ¬ãƒ¼ã‚ºä¸€è‡´æ™‚ã®æœ€é«˜ãƒœãƒ¼ãƒŠã‚¹",
                "exact_word_bonus": "å®Œå…¨å˜èªä¸€è‡´æ™‚ã®ãƒœãƒ¼ãƒŠã‚¹",
                "phrase_proximity_bonus": "è¿‘æ¥ãƒ•ãƒ¬ãƒ¼ã‚ºä¸€è‡´æ™‚ã®ãƒœãƒ¼ãƒŠã‚¹",
                "prefix_match_bonus": "å‰æ–¹ä¸€è‡´æ™‚ã®ä½ãƒœãƒ¼ãƒŠã‚¹",
                "base_field_boost": "search_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®åŸºæœ¬ãƒ–ãƒ¼ã‚¹ãƒˆ",
                "exact_field_boost": "search_name.exactãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ–ãƒ¼ã‚¹ãƒˆ"
            },
            "strategy": {
                "score_mode": "sum",
                "boost_mode": "sum",
                "description": "BM25Fãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ + ç´¯ç©ãƒœãƒ¼ãƒŠã‚¹ã‚¹ã‚³ã‚¢"
            }
        }

# ä¾¿åˆ©é–¢æ•°
def build_nutrition_search_query(
    processed_query: str,
    original_query: str,
    db_type_filter: Optional[str] = None,
    size: int = 20,
    custom_weights: Optional[Dict[str, float]] = None
) -> Dict[str, Any]:
    """
    æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰
    
    Args:
        processed_query: å‰å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒª
        original_query: å…ƒã®ã‚¯ã‚¨ãƒª
        db_type_filter: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
        size: çµæœæ•°
        custom_weights: ã‚«ã‚¹ã‚¿ãƒ é‡ã¿
        
    Returns:
        Elasticsearchã‚¯ã‚¨ãƒªè¾æ›¸
    """
    builder = NutritionSearchQueryBuilder()
    return builder.build_search_query(
        processed_query=processed_query,
        original_query=original_query,
        db_type_filter=db_type_filter,
        size=size,
        weights=custom_weights
    ) 
```

============================================================

ğŸ“„ FILE: nutrition_db_experiment/search_service/nlp/query_preprocessor.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 11,049 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 11:41:37
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Query Preprocessor - spaCyãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

é£Ÿå“æ¤œç´¢ã®ãŸã‚ã®ã‚¯ã‚¨ãƒªå‰å‡¦ç†ã‚’è¡Œã„ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ï¼š
- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
- å°æ–‡å­—åŒ–
- ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
- ä¿è­·ã‚¿ãƒ¼ãƒ å‡¦ç†
- ãƒ¬ãƒ³ãƒåŒ–ï¼ˆä¸Šæ›¸ããƒ«ãƒ¼ãƒ«é©ç”¨ï¼‰
- é¡ç¾©èªå±•é–‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
"""

import os
import spacy
from spacy.tokens import Token
from spacy.language import Language
from typing import List, Dict, Set, Optional
import re

class FoodQueryPreprocessor:
    def __init__(self):
        """ã‚¯ã‚¨ãƒªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–"""
        self.nlp = None
        self.protected_terms: Set[str] = set()
        self.lemma_overrides: Dict[str, str] = {}
        self.custom_stopwords: Set[str] = set()
        self.food_synonyms: Dict[str, List[str]] = {}
        
        # ãƒ¬ã‚­ã‚·ã‚³ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
        self.lexicon_base_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            "lexicon_data"
        )
        
        self._load_lexicon_data()
        self._setup_spacy_pipeline()
    
    def _load_lexicon_data(self):
        """ãƒ¬ã‚­ã‚·ã‚³ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # ä¿è­·ã‚¿ãƒ¼ãƒ ã®èª­ã¿è¾¼ã¿
        protected_file = os.path.join(self.lexicon_base_path, "protected_food_terms.txt")
        try:
            with open(protected_file, "r", encoding="utf-8") as f:
                for line in f:
                    term = line.strip().lower()
                    if term and not term.startswith("#"):
                        self.protected_terms.add(term)
        except FileNotFoundError:
            print(f"Warning: protected_food_terms.txt not found at {protected_file}")
        
        # ãƒ¬ãƒ³ãƒä¸Šæ›¸ããƒ«ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿
        override_file = os.path.join(self.lexicon_base_path, "food_lemma_overrides.txt")
        try:
            with open(override_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        parts = line.split("=>")
                        if len(parts) == 2:
                            original = parts[0].strip().lower()
                            override = parts[1].strip().lower()
                            self.lemma_overrides[original] = override
        except FileNotFoundError:
            print(f"Warning: food_lemma_overrides.txt not found at {override_file}")
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿
        stopwords_file = os.path.join(self.lexicon_base_path, "custom_food_stopwords.txt")
        try:
            with open(stopwords_file, "r", encoding="utf-8") as f:
                for line in f:
                    word = line.strip().lower()
                    if word and not word.startswith("#"):
                        self.custom_stopwords.add(word)
        except FileNotFoundError:
            print(f"Warning: custom_food_stopwords.txt not found at {stopwords_file}")
        
        # é¡ç¾©èªã®èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        synonyms_file = os.path.join(self.lexicon_base_path, "food_synonyms.txt")
        try:
            with open(synonyms_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        # åŒæ–¹å‘é¡ç¾©èª: word1, word2, word3
                        if "=>" not in line and "," in line:
                            words = [w.strip().lower() for w in line.split(",")]
                            for word in words:
                                if word not in self.food_synonyms:
                                    self.food_synonyms[word] = []
                                self.food_synonyms[word].extend([w for w in words if w != word])
                        
                        # ç‰‡æ–¹å‘é¡ç¾©èª: source => target1, target2
                        elif "=>" in line:
                            parts = line.split("=>")
                            if len(parts) == 2:
                                source = parts[0].strip().lower()
                                targets = [t.strip().lower() for t in parts[1].split(",")]
                                self.food_synonyms[source] = targets
        except FileNotFoundError:
            print(f"Warning: food_synonyms.txt not found at {synonyms_file}")
    
    def _setup_spacy_pipeline(self):
        """spaCyãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            # è‹±èªã®å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆåŠ¹ç‡æ€§é‡è¦–ï¼‰
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("Warning: en_core_web_sm model not found. Installing...")
            try:
                import subprocess
                subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"], check=True)
                self.nlp = spacy.load("en_core_web_sm")
            except Exception as e:
                print(f"Error: Could not load spaCy model: {e}")
                return
        
        # ã‚«ã‚¹ã‚¿ãƒ æ‹¡å¼µå±æ€§ã‚’è¿½åŠ 
        if not Token.has_extension("is_protected"):
            Token.set_extension("is_protected", default=False)
        if not Token.has_extension("custom_lemma"):
            Token.set_extension("custom_lemma", default=None)
    
    def food_lexicon_processor_component(self, doc):
        """é£Ÿå“ãƒ¬ã‚­ã‚·ã‚³ãƒ³å‡¦ç†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
        for token in doc:
            token_lower = token.lower_
            
            # ä¿è­·ã‚¿ãƒ¼ãƒ ã®ãƒã‚§ãƒƒã‚¯
            if token_lower in self.protected_terms:
                token._.is_protected = True
                token._.custom_lemma = token_lower
            
            # ãƒ¬ãƒ³ãƒä¸Šæ›¸ãã®ãƒã‚§ãƒƒã‚¯
            elif token_lower in self.lemma_overrides:
                token._.is_protected = True  # ä¸Šæ›¸ãã‚¿ãƒ¼ã‚‚ãƒ¬ãƒ³ãƒåŒ–ã‹ã‚‰ä¿è­·
                token._.custom_lemma = self.lemma_overrides[token_lower]
        
        return doc
    
    def preprocess_query(self, query_text: str, expand_synonyms: bool = False) -> str:
        """
        ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†
        
        Args:
            query_text: ç”Ÿã®ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            expand_synonyms: é¡ç¾©èªå±•é–‹ã‚’è¡Œã†ã‹ã©ã†ã‹
            
        Returns:
            å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
        """
        if not self.nlp:
            return query_text.lower()
        
        # spaCyã§å‡¦ç†
        doc = self.nlp(query_text)
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ‰‹å‹•ã§é©ç”¨
        doc = self.food_lexicon_processor_component(doc)
        
        processed_tokens = []
        
        for token in doc:
            # å¥èª­ç‚¹ã€ç©ºç™½ã€æ•°å­—ã®ã¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if token.is_punct or token.is_space or (token.is_digit and len(token.text) > 2):
                continue
            
            # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            if token.lower_ in self.custom_stopwords:
                continue
            
            # ä¿è­·ã•ã‚ŒãŸå˜èªã®ãƒ¬ãƒ³ãƒå‡¦ç†
            if token._.is_protected and token._.custom_lemma:
                processed_tokens.append(token._.custom_lemma)
            
            # æ¨™æº–ãƒ¬ãƒ³ãƒåŒ–
            else:
                lemma = token.lemma_.lower()
                processed_tokens.append(lemma)
        
        # é¡ç¾©èªå±•é–‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if expand_synonyms:
            expanded_tokens = []
            for token in processed_tokens:
                expanded_tokens.append(token)
                if token in self.food_synonyms:
                    expanded_tokens.extend(self.food_synonyms[token])
            processed_tokens = expanded_tokens
        
        # é‡è¤‡é™¤å»ã¨çµåˆ
        processed_tokens = list(dict.fromkeys(processed_tokens))  # é †åºã‚’ä¿æŒã—ã¦é‡è¤‡é™¤å»
        
        return " ".join(processed_tokens)
    
    def get_processed_tokens(self, query_text: str) -> List[str]:
        """
        å‡¦ç†æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Args:
            query_text: ç”Ÿã®ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            å‡¦ç†æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        processed_query = self.preprocess_query(query_text)
        return processed_query.split()
    
    def analyze_query(self, query_text: str) -> Dict:
        """
        ã‚¯ã‚¨ãƒªã®è©³ç´°åˆ†ææƒ…å ±ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        
        Args:
            query_text: ç”Ÿã®ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            åˆ†æçµæœã®è¾æ›¸
        """
        if not self.nlp:
            return {"error": "spaCy model not loaded"}
        
        doc = self.nlp(query_text)
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ‰‹å‹•ã§é©ç”¨
        doc = self.food_lexicon_processor_component(doc)
        
        analysis = {
            "original": query_text,
            "tokens": [],
            "processed": self.preprocess_query(query_text),
            "statistics": {
                "original_tokens": len(doc),
                "processed_tokens": len(self.get_processed_tokens(query_text)),
                "protected_terms": 0,
                "overridden_terms": 0,
                "removed_stopwords": 0
            }
        }
        
        for token in doc:
            token_info = {
                "text": token.text,
                "lemma": token.lemma_,
                "is_protected": getattr(token._, 'is_protected', False),
                "custom_lemma": getattr(token._, 'custom_lemma', None),
                "is_stopword": token.lower_ in self.custom_stopwords,
                "is_punct": token.is_punct,
                "pos": token.pos_
            }
            
            if token_info["is_protected"]:
                analysis["statistics"]["protected_terms"] += 1
            if token_info["custom_lemma"]:
                analysis["statistics"]["overridden_terms"] += 1
            if token_info["is_stopword"]:
                analysis["statistics"]["removed_stopwords"] += 1
            
            analysis["tokens"].append(token_info)
        
        return analysis

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_preprocessor = None

def get_preprocessor() -> FoodQueryPreprocessor:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚µã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _preprocessor
    if _preprocessor is None:
        _preprocessor = FoodQueryPreprocessor()
    return _preprocessor

def preprocess_query(query_text: str, expand_synonyms: bool = False) -> str:
    """ä¾¿åˆ©é–¢æ•°ï¼šã‚¯ã‚¨ãƒªã‚’å‰å‡¦ç†"""
    return get_preprocessor().preprocess_query(query_text, expand_synonyms)

def analyze_query(query_text: str) -> Dict:
    """ä¾¿åˆ©é–¢æ•°ï¼šã‚¯ã‚¨ãƒªã‚’åˆ†æ"""
    return get_preprocessor().analyze_query(query_text) 
```

============================================================

ğŸ“„ FILE: nutrition_db_experiment/search_service/utils/data_loader.py
--------------------------------------------------
å­˜åœ¨: âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“

============================================================

ğŸ“„ FILE: nutrition_db_experiment/search_service/utils/scoring.py
--------------------------------------------------
å­˜åœ¨: âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“

============================================================

ğŸ“„ FILE: nutrition_db_experiment/search_service/config/search_config.py
--------------------------------------------------
å­˜åœ¨: âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“

============================================================

ğŸ“ æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä»•æ§˜
============================================================

ğŸ“„ FILE: nutrition_db_experiment/nutrition_database_specification.md
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 5,856 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 13:42:19
å­˜åœ¨: âœ…

CONTENT:
```
# æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä»•æ§˜æ›¸ (Nutrition Database Specification)

## æ¦‚è¦ (Overview)

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€USDA Food Data Central API ã‹ã‚‰åé›†ã—ãŸç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«æ§‹ç¯‰ã—ãŸçµ±ä¸€æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä»•æ§˜ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€  (Database Structure)

### çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (Unified Format)

å…¨ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ä»¥ä¸‹ã®çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã„ã¾ã™ï¼š

```json
{
  "db_type": "string",        // "dish", "ingredient", "branded"ã®ã„ãšã‚Œã‹
  "id": number,               // USDA Food Data Centralã®ID
  "search_name": "string",    // æ¤œç´¢ç”¨ã®åå‰
  "nutrition": {
    "calories": number,       // ã‚«ãƒ­ãƒªãƒ¼ (kcal/100g)
    "protein": number,        // ã‚¿ãƒ³ãƒ‘ã‚¯è³ª (g/100g)
    "fat": number,           // è„‚è³ª (g/100g)
    "carbs": number          // ç‚­æ°´åŒ–ç‰© (g/100g)
  },
  "weight": number            // åŸºæº–é‡é‡ (g)
}
```

## ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´° (Category Details)

### 1. Dish (æ–™ç†ãƒ»ãƒ¬ã‚·ãƒ”)

**èª¬æ˜**: å®Œæˆã•ã‚ŒãŸæ–™ç†ã‚„ãƒ¬ã‚·ãƒ”ã®ãƒ‡ãƒ¼ã‚¿
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: USDA Recipe ãƒ‡ãƒ¼ã‚¿
**ç‰¹å¾´**: è¤‡æ•°ã®é£Ÿæã‚’çµ„ã¿åˆã‚ã›ãŸå®Œæˆå“

#### JSON ã‚µãƒ³ãƒ—ãƒ«:

```json
{
  "db_type": "dish",
  "id": 123456,
  "search_name": "Chicken stir-fry with vegetables",
  "nutrition": {
    "calories": 145.5,
    "protein": 18.2,
    "fat": 6.8,
    "carbs": 4.3
  },
  "weight": 150.0
}
```

#### å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹:

- `title` â†’ `search_name`
- `nutrients.servingSize` â†’ `weight` (gram æŠ½å‡º)
- æ „é¤Šç´ ã¯ 100g ã‚ãŸã‚Šã«æ­£è¦åŒ–

### 2. Ingredient (é£Ÿæãƒ»åŸºæœ¬é£Ÿå“)

**èª¬æ˜**: å€‹åˆ¥ã®é£Ÿæã‚„åŸºæœ¬çš„ãªé£Ÿå“ã®ãƒ‡ãƒ¼ã‚¿
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: USDA Food ãƒ‡ãƒ¼ã‚¿
**ç‰¹å¾´**: å˜ä¸€é£Ÿæã€èª¿ç†å‰ã®çŠ¶æ…‹

#### JSON ã‚µãƒ³ãƒ—ãƒ«:

```json
{
  "db_type": "ingredient",
  "id": 789012,
  "search_name": "Chicken, breast, boneless, skinless, raw",
  "nutrition": {
    "calories": 165.0,
    "protein": 31.0,
    "fat": 3.6,
    "carbs": 0.0
  },
  "weight": 100.0
}
```

#### å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹:

- `name` + `description` â†’ `search_name`
- `units`ã‹ã‚‰`description="grams"`ã® amount ã‚’`weight`ã¨ã—ã¦ä½¿ç”¨
- æ „é¤Šç´ ã¯ 100g ã‚ãŸã‚Šã«æ­£è¦åŒ–

### 3. Branded (ãƒ–ãƒ©ãƒ³ãƒ‰é£Ÿå“)

**èª¬æ˜**: ç‰¹å®šãƒ–ãƒ©ãƒ³ãƒ‰ã®å•†å“ãƒ‡ãƒ¼ã‚¿
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: USDA Branded Food ãƒ‡ãƒ¼ã‚¿
**ç‰¹å¾´**: å•†ç”¨è£½å“ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é£Ÿå“

#### JSON ã‚µãƒ³ãƒ—ãƒ«:

```json
{
  "db_type": "branded",
  "id": 345678,
  "search_name": "KRAFT, Macaroni & Cheese Dinner Original",
  "nutrition": {
    "calories": 370.0,
    "protein": 11.0,
    "fat": 3.0,
    "carbs": 71.0
  },
  "weight": 70.0
}
```

#### å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹:

- `food_name` + `description` â†’ `search_name`
- `unit_weights`ã‹ã‚‰`description="grams"`ã® amount ã‚’`weight`ã¨ã—ã¦ä½¿ç”¨
- æ „é¤Šç´ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯`calories`/`serving_calories`, `proteins`/`serving_proteins`ç­‰ã‹ã‚‰å–å¾—
- æ „é¤Šç´ ã¯ 100g ã‚ãŸã‚Šã«æ­£è¦åŒ–

## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ (Database File Structure)

```
nutrition_db/
â”œâ”€â”€ dish_db.json              # æ–™ç†ãƒ‡ãƒ¼ã‚¿ã®ã¿
â”œâ”€â”€ ingredient_db.json        # é£Ÿæãƒ‡ãƒ¼ã‚¿ã®ã¿
â”œâ”€â”€ branded_db.json           # ãƒ–ãƒ©ãƒ³ãƒ‰é£Ÿå“ãƒ‡ãƒ¼ã‚¿ã®ã¿
â”œâ”€â”€ unified_nutrition_db.json # å…¨ã‚«ãƒ†ã‚´ãƒªçµ±åˆ
â””â”€â”€ build_stats.json          # æ§‹ç¯‰çµ±è¨ˆæƒ…å ±
```

## æ¤œç´¢ãƒ»åˆ©ç”¨æ–¹æ³• (Search & Usage)

### 1. ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¤œç´¢

ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æ¤œç´¢ã—ãŸã„å ´åˆã¯ã€å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã€‚

### 2. çµ±åˆæ¤œç´¢

å…¨ã‚«ãƒ†ã‚´ãƒªã‚’æ¨ªæ–­ã—ã¦æ¤œç´¢ã—ãŸã„å ´åˆã¯ã€`unified_nutrition_db.json`ã‚’ä½¿ç”¨ã€‚

### 3. æ¤œç´¢ã‚­ãƒ¼

- `search_name`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
- `db_type`ã§ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- `id`ã§ç‰¹å®šã‚¢ã‚¤ãƒ†ãƒ ã®ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹

## æ „é¤Šå€¤ã®æ­£è¦åŒ– (Nutrition Value Normalization)

- **åŸºæº–**: å…¨ã¦ã®æ „é¤Šå€¤ã¯ 100g ã‚ãŸã‚Šã§æ­£è¦åŒ–
- **è¨ˆç®—æ–¹æ³•**: `(å…ƒã®æ „é¤Šå€¤ / å…ƒã®é‡é‡) Ã— 100`
- **åˆ©ç‚¹**: ç•°ãªã‚‹é£Ÿå“é–“ã§ã®æ „é¤Šä¾¡æ¯”è¼ƒãŒå®¹æ˜“

## æ¤œç´¢ä»•æ§˜ (Search Specification)

### æ¤œç´¢å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è©³ç´°

#### search_name ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

- **ãƒ‡ãƒ¼ã‚¿å‹**: string
- **æ§‹æˆ**: å…ƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã® `name` ã¨ `description` ã‚’çµåˆ
- **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: ãƒ¡ã‚¤ãƒ³åç§° + ä¿®é£¾èªï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰
  - ä¾‹: `"Onions, Cooked, boiled, drained, with salt"`
  - ä¾‹: `"Roasted Cherry Tomatoes with Mint"`
- **é•·ã•**:
  - **é€šå¸¸**: 5 å˜èªä»¥å†…
  - **æœ€å¤§**: 10 å˜èªç¨‹åº¦
- **ç‰¹å¾´**: è‹±èªãƒ™ãƒ¼ã‚¹ã€é£Ÿæãƒ»æ–™ç†ã®è©³ç´°ãªèª¬æ˜ã‚’å«ã‚€

### æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¦ä»¶

#### å˜èªå¢ƒç•Œå•é¡Œã¸ã®å¯¾å‡¦

æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®å˜èªå¢ƒç•Œå•é¡Œã«å¯¾å‡¦ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š

**å•é¡Œä¾‹**: `"cook"` ã§ã‚¯ã‚¨ãƒªã—ãŸå ´åˆã®æœŸå¾…ã•ã‚Œã‚‹çµæœ

```
âœ… é«˜ã‚¹ã‚³ã‚¢ï¼ˆæ„å‘³çš„ã«é–¢é€£ï¼‰:
- "cook" (å®Œå…¨ä¸€è‡´)
- "cooking" (åŒã˜èªå¹¹)
- "cooked" (åŒã˜èªå¹¹)

âŒ ä½ã‚¹ã‚³ã‚¢ï¼ˆæ„å‘³çš„ã«ç„¡é–¢ä¿‚ï¼‰:
- "cookie" (æ–‡å­—åˆ—çš„ã«ã¯ä¼¼ã¦ã„ã‚‹ãŒæ„å‘³ãŒç•°ãªã‚‹)
- "cookies" (æ–‡å­—åˆ—çš„ã«ã¯ä¼¼ã¦ã„ã‚‹ãŒæ„å‘³ãŒç•°ãªã‚‹)
```

#### å®Ÿè£…æ¨å¥¨äº‹é …

1. **èªå¹¹å‡¦ç† (Stemming)**: èªå°¾å¤‰åŒ–ã‚’æ­£è¦åŒ–
2. **BM25F æ¤œç´¢**: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥é‡ã¿ä»˜ãæ¤œç´¢
3. **ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°**: è¤‡æ•°ã®é–¢é€£åº¦æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›
4. **åŒç¾©èªè¾æ›¸**: é£Ÿæãƒ»æ–™ç†å›ºæœ‰ã®åŒç¾©èªå¯¾å¿œ
5. **éƒ¨åˆ†ä¸€è‡´ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°**: æ–‡å­—åˆ—é¡ä¼¼åº¦ vs æ„å‘³çš„é¡ä¼¼åº¦ã®ãƒãƒ©ãƒ³ã‚¹

### æ¤œç´¢æ€§èƒ½æŒ‡æ¨™

- **ç›®æ¨™ç²¾åº¦**: 90%ä»¥ä¸Šã®ãƒãƒƒãƒç‡
- **å¿œç­”æ™‚é–“**: 1 ç§’ä»¥å†…ï¼ˆ8,878 é …ç›®å¯¾è±¡ï¼‰
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: ElasticSearch åˆ©ç”¨ä¸å¯æ™‚ã®ç›´æ¥æ¤œç´¢å¯¾å¿œ

```

============================================================

ğŸ“ ãƒ†ã‚¹ãƒˆãƒ»å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«
============================================================

ğŸ“„ FILE: test_local_nutrition_search_v2.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 13,113 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:49:32
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Local Nutrition Search System Test v2.0

nutrition_db_experimentã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¨çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ†ã‚¹ãƒˆ
"""

import requests
import json
import time
import os
from datetime import datetime

# APIè¨­å®šï¼ˆæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆï¼‰
BASE_URL = "http://localhost:8000/api/v1"

# ãƒ†ã‚¹ãƒˆç”»åƒã®ãƒ‘ã‚¹
image_path = "test_images/food3.jpg"

def test_local_nutrition_search_complete_analysis():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸå®Œå…¨åˆ†æã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("=== Local Nutrition Search Complete Analysis Test v2.0 ===")
    print(f"Using image: {image_path}")
    print("ğŸ” Testing nutrition_db_experiment integration")
    
    try:
        # å®Œå…¨åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
        with open(image_path, "rb") as f:
            files = {"image": ("food3.jpg", f, "image/jpeg")}
            data = {"save_results": True}  # çµæœã‚’ä¿å­˜
            
            print("Starting complete analysis with local nutrition search...")
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/meal-analyses/complete", files=files, data=data)
            end_time = time.time()
        
        print(f"Status Code: {response.status_code}")
        print(f"Response Time: {end_time - start_time:.2f}s")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… Local nutrition search analysis successful!")
            
            # åˆ†æID
            analysis_id = result.get("analysis_id")
            print(f"Analysis ID: {analysis_id}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢æ–¹æ³•ã®ç¢ºèªï¼‰
            metadata = result.get("metadata", {})
            print(f"\nğŸ“Š Pipeline Info:")
            print(f"- Version: {metadata.get('pipeline_version')}")
            print(f"- Components: {', '.join(metadata.get('components_used', []))}")
            print(f"- Nutrition Search Method: {metadata.get('nutrition_search_method')}")
            print(f"- Timestamp: {metadata.get('timestamp')}")
            
            # å‡¦ç†ã‚µãƒãƒªãƒ¼
            summary = result.get("processing_summary", {})
            print(f"\nğŸ“ˆ Processing Summary:")
            print(f"- Total dishes: {summary.get('total_dishes')}")
            print(f"- Total ingredients: {summary.get('total_ingredients')}")
            print(f"- Search method: {summary.get('search_method')}")
            
            # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢çµæœ
            nutrition_search_result = result.get("nutrition_search_result", {})
            print(f"\nğŸ” Local Nutrition Search Results:")
            print(f"- Matches found: {nutrition_search_result.get('matches_count', 0)}")
            print(f"- Match rate: {nutrition_search_result.get('match_rate', 0):.1%}")
            print(f"- Search method: {nutrition_search_result.get('search_method', 'unknown')}")
            
            search_summary = nutrition_search_result.get('search_summary', {})
            if search_summary:
                print(f"- Database source: {search_summary.get('database_source', 'unknown')}")
                print(f"- Total searches: {search_summary.get('total_searches', 0)}")
                print(f"- Successful matches: {search_summary.get('successful_matches', 0)}")
                print(f"- Failed searches: {search_summary.get('failed_searches', 0)}")
            
            # Phase1 çµæœ
            phase1_result = result.get("phase1_result", {})
            phase1_dishes = len(phase1_result.get("dishes", []))
            print(f"\nğŸ” Phase1 Results:")
            print(f"- Detected dishes: {phase1_dishes}")
            
            if phase1_dishes > 0:
                print("- Dish details:")
                for i, dish in enumerate(phase1_result.get("dishes", [])[:3], 1):  # æœ€åˆã®3æ–™ç†ã®ã¿è¡¨ç¤º
                    print(f"  {i}. {dish.get('dish_name', 'Unknown')}")
                    ingredients = dish.get('ingredients', [])
                    print(f"     Ingredients ({len(ingredients)}): {', '.join([ing.get('ingredient_name', 'Unknown') for ing in ingredients[:5]])}")
                    if len(ingredients) > 5:
                        print(f"     ... and {len(ingredients) - 5} more")
            
            # æœ€çµ‚æ „é¤Šä¾¡çµæœï¼ˆæš«å®šï¼‰
            final_nutrition = result.get("final_nutrition_result", {})
            total_nutrients = final_nutrition.get("total_meal_nutrients", {})
            
            print(f"\nğŸ½ Final Meal Nutrition (Preliminary):")
            print(f"- Calories: {total_nutrients.get('calories_kcal', 0):.2f} kcal")
            print(f"- Protein: {total_nutrients.get('protein_g', 0):.2f} g")
            print(f"- Carbohydrates: {total_nutrients.get('carbohydrates_g', 0):.2f} g")
            print(f"- Fat: {total_nutrients.get('fat_g', 0):.2f} g")
            
            # ä¿å­˜ã•ã‚ŒãŸè©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
            analysis_folder = result.get("analysis_folder")
            saved_files = result.get("saved_files", {})
            
            if analysis_folder:
                print(f"\nğŸ“ Analysis Folder:")
                print(f"- Path: {analysis_folder}")
                print(f"- Contains organized phase-by-phase results")
            
            if saved_files:
                print(f"\nğŸ’¾ Saved Files by Phase ({len(saved_files)} total):")
                
                # Phase1 files
                phase1_files = [k for k in saved_files.keys() if k.startswith('phase1_')]
                if phase1_files:
                    print("  ğŸ“Š Phase1 (Image Analysis):")
                    for file_key in phase1_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
                
                # Local search files  
                search_files = [k for k in saved_files.keys() if 'nutrition_search' in k or 'local' in k.lower()]
                if search_files:
                    print("  ğŸ” Local Nutrition Search:")
                    for file_key in search_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
                
                # Pipeline files
                pipeline_files = [k for k in saved_files.keys() if k in ['pipeline_summary', 'complete_log']]
                if pipeline_files:
                    print("  ğŸ“‹ Pipeline Summary:")
                    for file_key in pipeline_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
            
            return True, analysis_id
            
        else:
            print("âŒ Local nutrition search analysis failed!")
            print(f"Error: {response.text}")
            return False, None
            
    except Exception as e:
        print(f"âŒ Error during local nutrition search analysis: {e}")
        return False, None

def test_pipeline_info_local():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Local Nutrition Search Pipeline Info ===")
    
    try:
        response = requests.get(f"{BASE_URL}/meal-analyses/pipeline-info")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… Pipeline info retrieved!")
            print(f"Pipeline ID: {result.get('pipeline_id')}")
            print(f"Version: {result.get('version')}")
            print(f"Nutrition Search Method: {result.get('nutrition_search_method')}")
            
            components = result.get('components', [])
            print(f"\nğŸ”§ Components ({len(components)}):")
            for i, comp in enumerate(components, 1):
                print(f"  {i}. {comp.get('component_name')} ({comp.get('component_type')})")
                print(f"     Executions: {comp.get('execution_count')}")
        else:
            print(f"âŒ Pipeline info failed: {response.status_code}")
            
    except Exception as e:
        print(f"âŒ Error getting pipeline info: {e}")

def test_nutrition_db_experiment_availability():
    """nutrition_db_experimentã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Nutrition DB Experiment Availability Test ===")
    
    try:
        # nutrition_db_experimentãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
        nutrition_db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "nutrition_db_experiment")
        
        print(f"ğŸ” Checking nutrition_db_experiment path: {nutrition_db_path}")
        
        if os.path.exists(nutrition_db_path):
            print("âœ… nutrition_db_experiment directory found")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªï¼ˆæ­£ã—ã„ãƒ‘ã‚¹ã«ä¿®æ­£ï¼‰
            db_files = [
                "nutrition_db/dish_db.json",
                "nutrition_db/ingredient_db.json", 
                "nutrition_db/branded_db.json",
                "nutrition_db/unified_nutrition_db.json"
            ]
            
            print("ğŸ“Š Database Files:")
            for db_file in db_files:
                full_path = os.path.join(nutrition_db_path, db_file)
                if os.path.exists(full_path):
                    try:
                        with open(full_path, 'r', encoding='utf-8') as f:
                            # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ä¸€éƒ¨ã ã‘èª­ã¿è¾¼ã¿
                            if os.path.getsize(full_path) > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                                f.seek(0)
                                first_chunk = f.read(1024)
                                if first_chunk.strip().startswith('['):
                                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‹ã‚‰æ¨å®šã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’è¨ˆç®—
                                    file_size_mb = os.path.getsize(full_path) / (1024 * 1024)
                                    estimated_items = int(file_size_mb * 1000)  # å¤§ã¾ã‹ãªæ¨å®š
                                    print(f"  âœ… {db_file}: ~{estimated_items} items (file size: {file_size_mb:.1f}MB)")
                                else:
                                    print(f"  âœ… {db_file}: Large file ({file_size_mb:.1f}MB)")
                            else:
                                data = json.load(f)
                                print(f"  âœ… {db_file}: {len(data)} items")
                    except Exception as e:
                        print(f"  âŒ {db_file}: Error reading - {e}")
                else:
                    print(f"  âŒ {db_file}: Not found")
            
            # æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
            print("ğŸ”§ Search Components:")
            
            search_service_path = os.path.join(nutrition_db_path, "search_service")
            if os.path.exists(search_service_path):
                print(f"  âœ… search_service directory found: {search_service_path}")
                
                # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                component_files = [
                    "nlp/query_preprocessor.py",
                    "api/search_handler.py", 
                    "api/query_builder.py"
                ]
                
                for comp_file in component_files:
                    full_path = os.path.join(search_service_path, comp_file)
                    if os.path.exists(full_path):
                        print(f"    âœ… {comp_file}")
                    else:
                        print(f"    âŒ {comp_file}: Not found")
            else:
                print(f"  âŒ search_service directory not found")
                
        else:
            print("âŒ nutrition_db_experiment directory not found")
            print("ğŸ’¡ Please ensure nutrition_db_experiment is in the same directory as this script")
            
    except Exception as e:
        print(f"âŒ Error checking nutrition_db_experiment: {e}")

def compare_search_methods():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã¨USDAæ¤œç´¢ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    print("\n=== Search Methods Comparison ===")
    print("ğŸ”¬ This would compare local search vs USDA API search")
    print("ğŸ“ TODO: Implement when both methods are available")

if __name__ == "__main__":
    print("Testing Local Nutrition Search Integration v2.0")
    print("=" * 70)
    
    # nutrition_db_experimentã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
    test_nutrition_db_experiment_availability()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±
    test_pipeline_info_local()
    
    # ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ã‚’ä½¿ã£ãŸå®Œå…¨åˆ†æã®ãƒ†ã‚¹ãƒˆ
    success, analysis_id = test_local_nutrition_search_complete_analysis()
    
    if success:
        print("\nğŸ‰ Local nutrition search integration test completed successfully!")
        print("ğŸš€ nutrition_db_experiment search system is working with the meal analysis pipeline!")
        print(f"ğŸ“‹ Analysis ID: {analysis_id}")
    else:
        print("\nğŸ’¥ Local nutrition search integration test failed!")
        print("ğŸ”§ Check the local search system setup and logs.")
        
    # æ¯”è¼ƒãƒ†ã‚¹ãƒˆï¼ˆå°†æ¥å®Ÿè£…äºˆå®šï¼‰
    compare_search_methods() 
```

============================================================

ğŸ¯ LOCAL NUTRITION SEARCH SYSTEM SUMMARY
----------------------------------------
ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 21
å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 18
åˆ†æå®Œäº†æ™‚åˆ»: 2025-06-06 13:59:43

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€test_local_nutrition_search_v2.pyå®Ÿè¡Œæ™‚ã«é–¢ã‚ã‚‹
ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®
å®Œå…¨ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

ğŸ”¥ LOCAL NUTRITION SEARCH FEATURES:
- Phase 1: Gemini AI image analysis
- Local Nutrition Search: BM25F + multi-signal boosting algorithm
- Database Integration: 8,878-item offline nutrition database
- Advanced NLP: Word boundary handling, stemming, synonym matching
- Result Management: Phase-based organized file saving
- USDA Compatibility: Seamless migration from USDA API
- Performance: 90.9% match rate, <1 second response time
