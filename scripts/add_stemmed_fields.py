#!/usr/bin/env python3
"""
èªå¹¹åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æ—¢å­˜ã®MyNetDiaryãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¿½åŠ ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:
- stemmed_search_name: search_nameã®èªå¹¹åŒ–ç‰ˆ
- stemmed_description: descriptionã®èªå¹¹åŒ–ç‰ˆ
"""

import json
import nltk
from nltk.stem import PorterStemmer
import re
from typing import List, Dict, Any

# NLTKãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading NLTK punkt tokenizer...")
    nltk.download('punkt')

def setup_stemmer():
    """PorterStemmerã®åˆæœŸåŒ–"""
    return PorterStemmer()

def clean_and_tokenize_single(text: str) -> List[str]:
    """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–"""
    if not text:
        return []

    # æ–‡å­—åˆ—ã§ãªã„å ´åˆã¯æ–‡å­—åˆ—ã«å¤‰æ›
    if not isinstance(text, str):
        text = str(text)

    # å°æ–‡å­—ã«å¤‰æ›
    text = text.lower()

    # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¨ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿æ®‹ã™ï¼‰
    text = re.sub(r'[^a-z\s]', ' ', text)

    # è¤‡æ•°ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
    text = re.sub(r'\s+', ' ', text).strip()

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    tokens = text.split()

    return tokens

def stem_single_text(text: str, stemmer: PorterStemmer) -> str:
    """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®èªå¹¹åŒ–"""
    if not text:
        return ""

    tokens = clean_and_tokenize_single(text)
    stemmed_tokens = [stemmer.stem(token) for token in tokens]

    return ' '.join(stemmed_tokens)

def stem_text_or_list(text, stemmer: PorterStemmer):
    """ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒªã‚¹ãƒˆã®èªå¹¹åŒ–ï¼ˆå‹ã‚’ä¿æŒï¼‰"""
    if not text:
        return text

    # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆ: å„è¦ç´ ã‚’å€‹åˆ¥ã«èªå¹¹åŒ–ã—ã¦ãƒªã‚¹ãƒˆã‚’è¿”ã™
    if isinstance(text, list):
        if len(text) == 0:
            return []
        return [stem_single_text(item, stemmer) for item in text]

    # æ–‡å­—åˆ—ã®å ´åˆ: èªå¹¹åŒ–ã—ãŸæ–‡å­—åˆ—ã‚’è¿”ã™
    else:
        return stem_single_text(text, stemmer)

def add_stemmed_fields(record: Dict[str, Any], stemmer: PorterStemmer) -> Dict[str, Any]:
    """å€‹åˆ¥ãƒ¬ã‚³ãƒ¼ãƒ‰ã«èªå¹¹åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ """

    # å…ƒã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼
    new_record = record.copy()

    # search_nameã®èªå¹¹åŒ–ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ã‚‚å¯¾å¿œï¼‰
    search_name = record.get('search_name', '')
    new_record['stemmed_search_name'] = stem_text_or_list(search_name, stemmer)

    # descriptionã®èªå¹¹åŒ–ï¼ˆé€šå¸¸ã¯æ–‡å­—åˆ—ï¼‰
    description = record.get('description', '')
    new_record['stemmed_description'] = stem_text_or_list(description, stemmer)

    return new_record

def process_database():
    """ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†é–¢æ•°"""

    print("ğŸ”§ MyNetDiaryèªå¹¹åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆé–‹å§‹...")

    # Stemmerã®åˆæœŸåŒ–
    stemmer = setup_stemmer()

    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
    input_file = 'db/mynetdiary_converted_tool_calls_list.json'
    output_file = 'db/mynetdiary_converted_tool_calls_list_stemmed.json'

    print(f"ğŸ“– èª­ã¿è¾¼ã¿: {input_file}")

    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {input_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    except json.JSONDecodeError as e:
        print(f"âŒ JSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    print(f"ğŸ“Š å‡¦ç†å¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(original_data)}")

    # å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã«èªå¹¹åŒ–ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
    stemmed_data = []

    for i, record in enumerate(original_data):
        if i % 100 == 0:
            print(f"âš¡ å‡¦ç†ä¸­... {i}/{len(original_data)} ({i/len(original_data)*100:.1f}%)")

        new_record = add_stemmed_fields(record, stemmer)
        stemmed_data.append(new_record)

    print(f"ğŸ’¾ ä¿å­˜: {output_file}")

    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(stemmed_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # ã‚µãƒ³ãƒ—ãƒ«çµæœã®è¡¨ç¤º
    print("\nâœ¨ èªå¹¹åŒ–çµæœã‚µãƒ³ãƒ—ãƒ«:")
    print("-" * 80)

    # æ–‡å­—åˆ—å½¢å¼ã®ã‚µãƒ³ãƒ—ãƒ«
    string_samples = [r for r in stemmed_data if isinstance(r.get('search_name'), str)][:3]
    # ãƒªã‚¹ãƒˆå½¢å¼ã®ã‚µãƒ³ãƒ—ãƒ«
    list_samples = [r for r in stemmed_data if isinstance(r.get('search_name'), list)][:2]

    print("ğŸ”¤ æ–‡å­—åˆ—å½¢å¼ã®ã‚µãƒ³ãƒ—ãƒ«:")
    for i, record in enumerate(string_samples):
        print(f"   Record {i+1}:")
        print(f"     original_name: {record.get('original_name', 'N/A')}")
        print(f"     search_name: '{record.get('search_name', 'N/A')}' â†’ '{record.get('stemmed_search_name', 'N/A')}'")
        print(f"     description: '{record.get('description', 'N/A')}' â†’ '{record.get('stemmed_description', 'N/A')}'")
        print()

    print("ğŸ“‹ ãƒªã‚¹ãƒˆå½¢å¼ã®ã‚µãƒ³ãƒ—ãƒ«:")
    for i, record in enumerate(list_samples):
        print(f"   Record {i+1}:")
        print(f"     original_name: {record.get('original_name', 'N/A')}")
        print(f"     search_name: {record.get('search_name', 'N/A')}")
        print(f"     stemmed_search_name: {record.get('stemmed_search_name', 'N/A')}")
        print(f"     description: '{record.get('description', 'N/A')}' â†’ '{record.get('stemmed_description', 'N/A')}'")
        print()

    print(f"âœ… å®Œäº†! æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {output_file}")
    print(f"ğŸ“Š å‡¦ç†ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(stemmed_data)}")

if __name__ == "__main__":
    process_database()