====================================================================================================
MEAL ANALYSIS API v2.0 - é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ
====================================================================================================
ç”Ÿæˆæ—¥æ™‚: 2025-06-11 12:41:22
åˆ†æå¯¾è±¡: test_advanced_elasticsearch_search.pyå®Ÿè¡Œæ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«
====================================================================================================

ğŸ¯ LATEST EXECUTION RESULTS SUMMARY
------------------------------------------------------------
ğŸ“Š Analysis ID: fc58aeb4
ğŸ•’ Execution Time: 20250611_105809
âœ… Total Searches: 6
ğŸ¯ Successful Matches: 6
ğŸ“ˆ Match Rate: 100.0%
âš¡ Search Time: 383ms
ğŸ“‹ Total Results: 30
ğŸ—ƒï¸ Total Indexed Documents: 11,845

ğŸ¯ STRATEGIC APPROACH:
   ğŸ½ï¸  Dish Strategy: eatthismuch_dish_primary + eatthismuch_branded_fallback
   ğŸ¥• Ingredient Strategy: eatthismuch_ingredient_primary + multi_db_fallback

ğŸ“ INPUT QUERIES ANALYSIS:
   ğŸ“‹ Total Queries: 6
   ğŸ½ï¸  Dish Queries: 1
   ğŸ¥• Ingredient Queries: 5
   ğŸ½ï¸  Dishes: Glazed Chicken Thighs with Roasted Baby Potatoes and Mixed Green Salad
   ğŸ¥• Ingredients: mixed greens, corn, tomato, baby potato, chicken thigh

====================================================================================================

ğŸš€ ADVANCED STRATEGIC ELASTICSEARCH SEARCH ARCHITECTURE OVERVIEW
--------------------------------------------------------------------------------

ğŸ”„ ADVANCED STRATEGIC SEARCH EXECUTION FLOW:
1. test_advanced_elasticsearch_search.py â†’ JPGç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
2. FastAPI /api/v1/meal-analyses/complete â†’ å®Œå…¨åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
3. Phase1: Gemini AI 2.5 Flash ç”»åƒåˆ†æ â†’ æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
   ğŸ“‹ DetectedFoodItems: ä¿¡é ¼åº¦ä»˜ãé£Ÿå“è­˜åˆ¥
   ğŸ·ï¸  Attributes: ææ–™ãƒ»èª¿ç†æ³•ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¥ãƒ¼
4. é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢:
   ğŸ“ DISHæˆ¦ç•¥: EatThisMuch dish (primary) â†’ EatThisMuch branded (fallback, score<20.0)
   ğŸ¥• INGREDIENTæˆ¦ç•¥: EatThisMuch ingredient (primary) â†’ MyNetDiary/YAZIO/branded (fallback)
   ğŸ”§ Strategic Features:
      - Min Score Threshold: 20.0 (å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
      - Strategic Metadata: æˆ¦ç•¥ãƒ•ã‚§ãƒ¼ã‚ºãƒ»ã‚¿ã‚¤ãƒ—è¿½è·¡
      - Multi-DB Fallback: æ®µéšçš„å“è³ªä¿è¨¼
5. çµæœçµ±åˆãƒ»ä¿å­˜: JSON + Markdown ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

ğŸ—ï¸ ADVANCED COMPONENT-BASED ARCHITECTURE v2.0:
â”œâ”€â”€ Advanced Test Layer
â”‚   â””â”€â”€ test_advanced_elasticsearch_search.py (é«˜åº¦æˆ¦ç•¥çš„æ¤œç´¢ãƒ†ã‚¹ãƒˆ)
â”œâ”€â”€ FastAPI Application Layer (app_v2)
â”‚   â”œâ”€â”€ main/app.py (Server, CORS, health endpoints)
â”‚   â””â”€â”€ api/v1/endpoints/meal_analysis.py (Complete analysis API)
â”œâ”€â”€ Pipeline Management Layer
â”‚   â”œâ”€â”€ orchestrator.py (MealAnalysisPipeline - å…¨ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆ¶ãƒ»æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ)
â”‚   â””â”€â”€ result_manager.py (ResultManager - çµæœä¿å­˜ãƒ»å±¥æ­´ç®¡ç†)
â”œâ”€â”€ AI Component Layer
â”‚   â”œâ”€â”€ base.py (BaseComponent - å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹)
â”‚   â”œâ”€â”€ phase1_component.py (Phase1Component - Geminiæ§‹é€ åŒ–åˆ†æ)
â”‚   â””â”€â”€ elasticsearch_nutrition_search_component.py (é«˜åº¦æˆ¦ç•¥çš„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³)
â”œâ”€â”€ AI Service Layer
â”‚   â””â”€â”€ gemini_service.py (GeminiService - Vertex AIçµ±åˆãƒ»æ§‹é€ åŒ–ã‚¹ã‚­ãƒ¼ãƒ)
â”œâ”€â”€ Data Model Layer
â”‚   â”œâ”€â”€ nutrition_search_models.py (NutritionMatch, strategic metadata)
â”‚   â””â”€â”€ phase1_models.py (DetectedFoodItem, FoodAttribute, æ§‹é€ åŒ–å‡ºåŠ›)
â”œâ”€â”€ Elasticsearch Infrastructure
â”‚   â”œâ”€â”€ create_elasticsearch_index.py (11,845ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†)
â”‚   â””â”€â”€ elasticsearch-8.10.4/ (é«˜æ€§èƒ½æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³)
â””â”€â”€ Strategic Data Layer
    â”œâ”€â”€ yazio_db.json (1,825é …ç›® - ãƒãƒ©ãƒ³ã‚¹é£Ÿå“ãƒ»25ã‚«ãƒ†ã‚´ãƒª)
    â”œâ”€â”€ mynetdiary_db.json (1,142é …ç›® - ç§‘å­¦çš„ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±ä¸€å‹)
    â””â”€â”€ eatthismuch_db.json (8,878é …ç›® - æœ€å¤§ãƒ»3ãƒ‡ãƒ¼ã‚¿å‹å¯¾å¿œ)

ğŸ¯ ADVANCED STRATEGIC SEARCH FEATURES:
- ğŸ”¥ Enhanced Dishæ¤œç´¢æˆ¦ç•¥:
  * Primary: EatThisMuch data_type=dish (é«˜é–¢é€£æ€§æ–™ç†ãƒ‡ãƒ¼ã‚¿)
  * Fallback: EatThisMuch data_type=branded (ã‚¹ã‚³ã‚¢<20.0æ™‚ã®è‡ªå‹•åˆ‡æ›¿)
  * Strategy Metadata: dish_primary, dish_fallback tracking
- ğŸ¥• Enhanced Ingredientæ¤œç´¢æˆ¦ç•¥:
  * Primary: EatThisMuch data_type=ingredient (ãƒ¡ã‚¤ãƒ³é£Ÿæãƒ‡ãƒ¼ã‚¿)
  * Multi-DB Fallback: MyNetDiary(ç§‘å­¦çš„) â†’ YAZIO(åˆ†é¡æ¸ˆ) â†’ EatThisMuch branded
  * Strategy Metadata: ingredient_primary, ingredient_fallback tracking
- âš¡ Performance Optimization:
  * Strategic Filtering: é–¢é€£æ€§é‡è¦–ã®çµã‚Šè¾¼ã¿
  * Dynamic Fallback: ã‚¹ã‚³ã‚¢é–¾å€¤ãƒ™ãƒ¼ã‚¹è‡ªå‹•åˆ‡æ›¿
  * Results Per DB: 5ä»¶åˆ¶é™ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
- ğŸ“Š Advanced Analytics:
  * Strategic Distribution Tracking
  * Database Source Analysis
  * Query Type Classification (dish vs ingredient)
  * Execution Time Monitoring
- ğŸ’¾ Comprehensive Metadata:
  * strategic_phase: main_dish, main_ingredient, fallback_multi_db
  * strategy_type: dish_primary, dish_fallback, ingredient_primary, ingredient_fallback
  * fallback_source: è£œåŠ©DBè©³ç´°æƒ…å ±
  * elasticsearch_score: ç”Ÿã‚¹ã‚³ã‚¢ä¿æŒ

ğŸ”§ TECHNICAL SPECIFICATIONS:
- Search Engine: Elasticsearch 8.10.4 (BM25F + Multi-Signal Boosting)
- AI Service: Google Vertex AI Gemini 2.5 Flash (æ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œ)
- Web Framework: FastAPI 0.104+ (async/await, multipart/form-data)
- Architecture Pattern: Strategic Component Pipeline
- Data Format: JSON (100gæ­£è¦åŒ–æ „é¤Šãƒ‡ãƒ¼ã‚¿ + æˆ¦ç•¥ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿)
- Search Strategy: Strategic Multi-Stage Filtering + Score-based Fallback
- Authentication: Google Cloud Service Account
- Performance: Sub-second response times, 100% match rates

ğŸš€ ADVANCED IMPROVEMENTS vs BASIC MULTI-DB:
- ğŸ§  æ§‹é€ åŒ–AIåˆ†æ: DetectedFoodItems + AttributesæŠ½å‡º
- ğŸ¯ æˆ¦ç•¥çš„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸æŠ: EatThisMuchã‚’ä¸­å¿ƒã¨ã—ãŸæœ€é©åŒ–
- ğŸ“ˆ å‹•çš„å“è³ªä¿è¨¼: ã‚¹ã‚³ã‚¢é–¾å€¤ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ğŸ” é«˜åº¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½è·¡: æ¤œç´¢ãƒ—ãƒ­ã‚»ã‚¹å®Œå…¨å¯è¦–åŒ–
- âš¡ åŠ¹ç‡çš„ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨: æˆ¦ç•¥çš„çµæœæ•°åˆ¶é™
- ğŸ“Š åŒ…æ‹¬çš„åˆ†æ: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒãƒ»æˆ¦ç•¥åˆ†æ
- ğŸ”§ æ‹¡å¼µå¯èƒ½è¨­è¨ˆ: æ–°æˆ¦ç•¥ãƒ»DBè¿½åŠ å®¹æ˜“
- ğŸ’¾ æ°¸ç¶šåŒ–å¯¾å¿œ: JSON + Markdown ãƒ‡ãƒ¥ã‚¢ãƒ«å‡ºåŠ›

ğŸ–ï¸ STRATEGIC SEARCH EXCELLENCE:
- Phase1-to-Search Pipeline: ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªãƒ‡ãƒ¼ã‚¿æµã‚Œ
- Multi-Modal Input: JPGç”»åƒ â†’ æ§‹é€ åŒ–ã‚¯ã‚¨ãƒª
- Intelligent Fallback: å“è³ªä¿è¨¼ä»˜ãå¤šæ®µéšæ¤œç´¢
- Real-time Analytics: å®Ÿè¡Œæ™‚æˆ¦ç•¥åˆ†æ
- Production-Ready: èªè¨¼ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Œå‚™

====================================================================================================

ğŸ“ Advanced Strategic Search ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«
================================================================================

ğŸ“„ FILE: test_advanced_elasticsearch_search.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 28,907 bytes
æœ€çµ‚æ›´æ–°: 2025-06-11 12:28:54
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Advanced Elasticsearch Search Test v1.0 - Strategic Search Edition (Multi-Image)

ElasticsearchNutritionSearchComponentã®æˆ¦ç•¥çš„æ¤œç´¢æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
test_imageså†…ã®å…¨JPGç”»åƒã‚’å¯¾è±¡ã¨ã—ã€Phase1è§£æçµæœã‹ã‚‰æŠ½å‡ºã—ãŸã‚¯ã‚¨ãƒªã§
é«˜åº¦ãªElasticsearchæ¤œç´¢æˆ¦ç•¥ï¼ˆdish/ingredientæˆ¦ç•¥çš„æ¤œç´¢ï¼‰ã‚’ãƒ†ã‚¹ãƒˆ
"""

import requests
import json
import time
import os
import glob
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional

# Elasticsearch Nutrition Search Component
from app_v2.components.elasticsearch_nutrition_search_component import ElasticsearchNutritionSearchComponent
from app_v2.models.nutrition_search_models import NutritionQueryInput

# APIè¨­å®š
BASE_URL = "http://localhost:8000/api/v1"

# ãƒ†ã‚¹ãƒˆç”»åƒã®ãƒ‘ã‚¹ï¼ˆå…¨ã¦ã®food*.jpgãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
test_images_dir = "test_images"
image_files = sorted(glob.glob(os.path.join(test_images_dir, "food*.jpg")))

async def test_single_image_advanced_elasticsearch_search(image_path: str, main_results_dir: str) -> Optional[Dict[str, Any]]:
    """å˜ä¸€ç”»åƒã§Advanced Elasticsearchæˆ¦ç•¥çš„æ¤œç´¢ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print(f"\n{'='*60}")
    print(f"ğŸ–¼ï¸  Testing image: {os.path.basename(image_path)}")
    print(f"{'='*60}")
    
    try:
        # å®Œå…¨åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—ã¦Phase1çµæœã‚’å–å¾—
        with open(image_path, "rb") as f:
            files = {"image": (os.path.basename(image_path), f, "image/jpeg")}
            data = {"save_results": True}
            
            print("Starting complete analysis to get Phase1 results...")
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/meal-analyses/complete", files=files, data=data)
            end_time = time.time()
        
        print(f"Status Code: {response.status_code}")
        print(f"Response Time: {end_time - start_time:.2f}s")
        
        if response.status_code != 200:
            print("âŒ Failed to get Phase1 results!")
            print(f"Error: {response.text}")
            return None
        
        result = response.json()
        analysis_id = result.get("analysis_id")
        print(f"Analysis ID: {analysis_id}")
        
        # Phase1çµæœã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
        phase1_result = result.get("phase1_result", {})
        dishes = phase1_result.get("dishes", [])
        
        all_queries = []
        dish_names = []
        ingredient_names = []
        
        for dish in dishes:
            dish_name = dish.get("dish_name")
            if dish_name:
                dish_names.append(dish_name)
                all_queries.append(dish_name)
            
            ingredients = dish.get("ingredients", [])
            for ingredient in ingredients:
                ingredient_name = ingredient.get("ingredient_name")
                if ingredient_name:
                    ingredient_names.append(ingredient_name)
                    all_queries.append(ingredient_name)
        
        # é‡è¤‡ã‚’é™¤å»
        all_queries = list(set(all_queries))
        dish_names = list(set(dish_names))
        ingredient_names = list(set(ingredient_names))
        
        print(f"\nğŸ“Š Extracted Search Queries from Phase1:")
        print(f"- Total dishes: {len(dish_names)}")
        print(f"- Total ingredients: {len(ingredient_names)}")
        print(f"- Total unique queries: {len(all_queries)}")
        
        if len(all_queries) == 0:
            print("âŒ No search queries extracted from Phase1 results!")
            return None
        
        # ElasticsearchNutritionSearchComponentã‚’æˆ¦ç•¥çš„æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã§åˆæœŸåŒ–
        print(f"\nğŸ”§ Initializing ElasticsearchNutritionSearchComponent (Strategic Search Mode)...")
        es_component = ElasticsearchNutritionSearchComponent(
            multi_db_search_mode=True,    # æˆ¦ç•¥çš„æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–
            results_per_db=5,             # å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰5ã¤ãšã¤çµæœã‚’å–å¾—
            enable_advanced_features=False # ä»•æ§˜æ›¸é€šã‚Šã®æˆ¦ç•¥çš„æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ç„¡åŠ¹åŒ–
        )
        
        # æ¤œç´¢å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        nutrition_query_input = NutritionQueryInput(
            ingredient_names=ingredient_names,
            dish_names=dish_names,
            preferred_source="elasticsearch"
        )
        
        print(f"ğŸ“ Strategic Query Input:")
        print(f"- Ingredient names: {len(ingredient_names)} items")
        print(f"- Dish names: {len(dish_names)} items")
        print(f"- Total search terms: {len(nutrition_query_input.get_all_search_terms())}")
        
        # Advanced Elasticsearchæˆ¦ç•¥æ¤œç´¢ã‚’å®Ÿè¡Œ
        print(f"\nğŸ” Starting Advanced Elasticsearch strategic search...")
        search_start_time = time.time()
        
        search_results = await es_component.execute(nutrition_query_input)
        
        search_end_time = time.time()
        search_time = search_end_time - search_start_time
        
        print(f"âœ… Advanced Elasticsearch strategic search completed in {search_time:.3f}s")
        
        # çµæœã®åˆ†æ
        matches = search_results.matches
        search_summary = search_results.search_summary
        
        print(f"\nğŸ“ˆ Advanced Elasticsearch Strategic Search Results Summary:")
        print(f"- Total queries: {search_summary.get('total_searches', 0)}")
        print(f"- Successful matches: {search_summary.get('successful_matches', 0)}")
        print(f"- Failed searches: {search_summary.get('failed_searches', 0)}")
        print(f"- Match rate: {search_summary.get('match_rate_percent', 0):.1f}%")
        print(f"- Search time: {search_summary.get('search_time_ms', 0)}ms")
        print(f"- Total results: {search_summary.get('total_results', 0)}")
        
        # çµæœã‚’ä¿å­˜
        await save_advanced_elasticsearch_results(
            analysis_id, search_results, all_queries, dish_names, ingredient_names, 
            image_filename=os.path.basename(image_path), main_results_dir=main_results_dir
        )
        
        # ã“ã®ç”»åƒã®çµæœã‚’ã‚µãƒãƒªãƒ¼ç”¨ã«è¿”ã™
        summary_result = {
            "image_name": os.path.basename(image_path),
            "analysis_id": analysis_id,
            "total_queries": search_summary.get('total_searches', 0),
            "successful_matches": search_summary.get('successful_matches', 0),
            "failed_searches": search_summary.get('failed_searches', 0),
            "match_rate_percent": search_summary.get('match_rate_percent', 0),
            "search_time_ms": search_summary.get('search_time_ms', 0),
            "total_results": search_summary.get('total_results', 0),
            "dish_names": dish_names,
            "ingredient_names": ingredient_names,
            "all_queries": all_queries
        }
        
        # è©³ç´°çµæœã‚‚å«ã‚ã¦è¿”ã™
        detailed_result = {
            "analysis_id": analysis_id,
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "image_filename": os.path.basename(image_path),
            "input_queries": {
                "all_queries": all_queries,
                "dish_names": dish_names,
                "ingredient_names": ingredient_names
            },
            "search_summary": search_results.search_summary,
            "matches": {},
            "warnings": search_results.warnings,
            "errors": search_results.errors
        }
        
        # æ¤œç´¢çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        for query, match_results in search_results.matches.items():
            if isinstance(match_results, list):
                detailed_result["matches"][query] = [
                    {
                        "id": match.id,
                        "search_name": match.search_name,
                        "description": match.description,
                        "data_type": match.data_type,
                        "source": match.source,
                        "nutrition": match.nutrition,
                        "weight": match.weight,
                        "score": match.score,
                        "search_metadata": match.search_metadata
                    } for match in match_results
                ]
            else:
                detailed_result["matches"][query] = {
                    "id": match_results.id,
                    "search_name": match_results.search_name,
                    "description": match_results.description,
                    "data_type": match_results.data_type,
                    "source": match_results.source,
                    "nutrition": match_results.nutrition,
                    "weight": match_results.weight,
                    "score": match_results.score,
                    "search_metadata": match_results.search_metadata
                }
        
        return summary_result, detailed_result
        
    except Exception as e:
        print(f"âŒ Error testing {os.path.basename(image_path)}: {str(e)}")
        return None

async def test_advanced_elasticsearch_search():
    """å…¨ç”»åƒã§Advanced Elasticsearchæˆ¦ç•¥çš„æ¤œç´¢ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("ğŸš€ Starting Advanced Elasticsearch Strategic Search Test (Multi-Image)")
    print("=== Advanced Elasticsearch Search Test v1.0 - Strategic Search Edition ===")
    print(f"ğŸ“ Testing {len(image_files)} images: {[os.path.basename(f) for f in image_files]}")
    print("ğŸ” Testing Advanced Elasticsearch strategic search (dish/ingredient optimization)")
    print("ğŸ“Š Strategic database targeting: EatThisMuch dishes/ingredients + fallback optimization")
    
    if not image_files:
        print("âŒ No food*.jpg images found in test_images directory!")
        return False
    
    # å®Ÿè¡Œç”¨ã®ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    main_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    main_results_dir = f"analysis_results/multi_image_test_{main_timestamp}"
    os.makedirs(main_results_dir, exist_ok=True)
    print(f"ğŸ“ Created main results directory: {main_results_dir}")
    
    # å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ç”¨å¤‰æ•°
    all_results = []
    all_detailed_results = []  # è©³ç´°æ¤œç´¢çµæœã‚’ä¿å­˜
    total_queries = 0
    total_successful = 0
    total_failed = 0
    total_search_time = 0
    total_results_count = 0
    
    # å„ç”»åƒã§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    for image_path in image_files:
        result = await test_single_image_advanced_elasticsearch_search(image_path, main_results_dir)
        if result:
            summary_result, detailed_result = result
            all_results.append(summary_result)
            all_detailed_results.append(detailed_result)
            
            total_queries += summary_result["total_queries"]
            total_successful += summary_result["successful_matches"]
            total_failed += summary_result["failed_searches"]
            total_search_time += summary_result["search_time_ms"]
            total_results_count += summary_result["total_results"]
    
    # å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print(f"\n{'='*80}")
    print(f"ğŸ¯ OVERALL MULTI-IMAGE TEST SUMMARY")
    print(f"{'='*80}")
    print(f"ğŸ“Š Images tested: {len(all_results)}/{len(image_files)}")
    print(f"ğŸ“ˆ Overall Statistics:")
    print(f"   - Total queries across all images: {total_queries}")
    print(f"   - Total successful matches: {total_successful}")
    print(f"   - Total failed searches: {total_failed}")
    print(f"   - Overall match rate: {(total_successful/total_queries*100) if total_queries > 0 else 0:.1f}%")
    print(f"   - Total search time: {total_search_time}ms")
    print(f"   - Average search time per image: {total_search_time/len(all_results) if all_results else 0:.1f}ms")
    print(f"   - Total results found: {total_results_count}")
    print(f"   - Average results per image: {total_results_count/len(all_results) if all_results else 0:.1f}")
    
    print(f"\nğŸ“‹ Per-Image Results Breakdown:")
    for i, result in enumerate(all_results, 1):
        print(f"   {i}. {result['image_name']}:")
        print(f"      - Queries: {result['total_queries']} | Matches: {result['successful_matches']} | Success: {result['match_rate_percent']:.1f}%")
        print(f"      - Time: {result['search_time_ms']}ms | Results: {result['total_results']}")
        print(f"      - Dishes: {len(result['dish_names'])} | Ingredients: {len(result['ingredient_names'])}")
    
    # é›†ç´„çµæœã‚’ä¿å­˜ï¼ˆè©³ç´°çµæœã‚‚å«ã‚ã‚‹ï¼‰
    await save_multi_image_summary(all_results, total_queries, total_successful, total_failed, total_search_time, total_results_count, all_detailed_results, main_results_dir)
    
    print(f"\nâœ… Multi-image Advanced Elasticsearch strategic search test completed!")
    print(f"ğŸ¯ Overall success rate: {(total_successful/total_queries*100) if total_queries > 0 else 0:.1f}%")
    
    return len(all_results) > 0

async def save_advanced_elasticsearch_results(analysis_id: str, search_results, all_queries: List[str], dish_names: List[str], ingredient_names: List[str], image_filename: str, main_results_dir: str):
    """Advanced Elasticsearchæˆ¦ç•¥çš„æ¤œç´¢çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    
    # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    image_base = os.path.splitext(image_filename)[0]  # food1, food2, etc.
    results_dir = f"{main_results_dir}/{image_base}_{analysis_id}"
    os.makedirs(results_dir, exist_ok=True)
    
    # æ¤œç´¢çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
    matches_dict = {}
    for query, match_results in search_results.matches.items():
        if isinstance(match_results, list):
            matches_dict[query] = [
                {
                    "id": match.id,
                    "search_name": match.search_name,
                    "description": match.description,
                    "data_type": match.data_type,
                    "source": match.source,
                    "nutrition": match.nutrition,
                    "weight": match.weight,
                    "score": match.score,
                    "search_metadata": match.search_metadata
                } for match in match_results
            ]
        else:
            matches_dict[query] = {
                "id": match_results.id,
                "search_name": match_results.search_name,
                "description": match_results.description,
                "data_type": match_results.data_type,
                "source": match_results.source,
                "nutrition": match_results.nutrition,
                "weight": match_results.weight,
                "score": match_results.score,
                "search_metadata": match_results.search_metadata
            }
    
    # 1. å…¨æ¤œç´¢çµæœã‚’JSONã§ä¿å­˜
    results_file = os.path.join(results_dir, "advanced_elasticsearch_search_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "analysis_id": analysis_id,
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "image_filename": image_filename,
            "search_method": "elasticsearch_strategic",
            "input_queries": {
                "all_queries": all_queries,
                "dish_names": dish_names,
                "ingredient_names": ingredient_names
            },
            "search_summary": search_results.search_summary,
            "matches": matches_dict,
            "warnings": search_results.warnings,
            "errors": search_results.errors
        }, f, indent=2, ensure_ascii=False)
    
    # 2. æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§ä¿å­˜
    summary_file = os.path.join(results_dir, "advanced_elasticsearch_summary.md")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"# Advanced Elasticsearch Strategic Search Results\n\n")
        f.write(f"**Analysis ID:** {analysis_id}\n")
        f.write(f"**Image:** {image_filename}\n")
        f.write(f"**Timestamp:** {datetime.now().strftime('%Y%m%d_%H%M%S')}\n")
        f.write(f"**Search Method:** Advanced Elasticsearch Strategic Search\n")
        f.write(f"**Total Queries:** {len(all_queries)}\n\n")
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼
        summary = search_results.search_summary
        f.write(f"## Search Summary\n\n")
        f.write(f"- **Total searches:** {summary.get('total_searches', 0)}\n")
        f.write(f"- **Successful matches:** {summary.get('successful_matches', 0)}\n")
        f.write(f"- **Failed searches:** {summary.get('failed_searches', 0)}\n")
        f.write(f"- **Match rate:** {summary.get('match_rate_percent', 0):.1f}%\n")
        f.write(f"- **Search time:** {summary.get('search_time_ms', 0)}ms\n")
        f.write(f"- **Total results:** {summary.get('total_results', 0)}\n\n")
    
    print(f"   ğŸ’¾ Results saved: {results_dir}/")

async def save_multi_image_summary(all_results: List[Dict[str, Any]], total_queries: int, total_successful: int, total_failed: int, total_search_time: int, total_results_count: int, detailed_results: List[Dict[str, Any]], main_results_dir: str):
    """ãƒãƒ«ãƒç”»åƒãƒ†ã‚¹ãƒˆã®å…¨ä½“ã‚µãƒãƒªãƒ¼ã¨è©³ç´°çµæœã‚’1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    
    # ãƒ¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç›´æ¥ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. å…¨ä½“ã‚µãƒãƒªãƒ¼ã¨è©³ç´°çµæœã‚’JSONã§ä¿å­˜
    comprehensive_file = os.path.join(main_results_dir, "comprehensive_multi_image_results.json")
    with open(comprehensive_file, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": timestamp,
            "test_type": "comprehensive_multi_image_advanced_elasticsearch_strategic",
            "overall_summary": {
                "images_tested": len(all_results),
                "total_queries": total_queries,
                "total_successful": total_successful,
                "total_failed": total_failed,
                "overall_match_rate_percent": (total_successful/total_queries*100) if total_queries > 0 else 0,
                "total_search_time_ms": total_search_time,
                "average_search_time_per_image_ms": total_search_time/len(all_results) if all_results else 0,
                "total_results_found": total_results_count,
                "average_results_per_image": total_results_count/len(all_results) if all_results else 0
            },
            "per_image_summary": all_results,
            "detailed_search_results": detailed_results or []
        }, f, indent=2, ensure_ascii=False)
    
    # 2. åŒ…æ‹¬çš„ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    comprehensive_md_file = os.path.join(main_results_dir, "comprehensive_multi_image_results.md")
    with open(comprehensive_md_file, 'w', encoding='utf-8') as f:
        f.write(f"# Comprehensive Multi-Image Advanced Elasticsearch Strategic Search Results\n\n")
        f.write(f"**Test Date:** {timestamp}\n")
        f.write(f"**Test Type:** Comprehensive Multi-Image Advanced Elasticsearch Strategic Search\n")
        f.write(f"**Images Tested:** {len(all_results)}\n\n")
        
        # å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        f.write(f"## Overall Performance Summary\n\n")
        f.write(f"- **Total Queries:** {total_queries}\n")
        f.write(f"- **Successful Matches:** {total_successful}\n")
        f.write(f"- **Failed Searches:** {total_failed}\n")
        f.write(f"- **Overall Success Rate:** {(total_successful/total_queries*100) if total_queries > 0 else 0:.1f}%\n")
        f.write(f"- **Total Search Time:** {total_search_time}ms\n")
        f.write(f"- **Average Time per Image:** {total_search_time/len(all_results) if all_results else 0:.1f}ms\n")
        f.write(f"- **Total Results Found:** {total_results_count}\n")
        f.write(f"- **Average Results per Image:** {total_results_count/len(all_results) if all_results else 0:.1f}\n\n")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒè¡¨
        f.write(f"## Performance Comparison Table\n\n")
        f.write(f"| Image | Queries | Matches | Success % | Time (ms) | Results | Dishes | Ingredients |\n")
        f.write(f"|-------|---------|---------|-----------|-----------|---------|--------|-------------|\n")
        for result in all_results:
            f.write(f"| {result['image_name']} | {result['total_queries']} | {result['successful_matches']} | {result['match_rate_percent']:.1f}% | {result['search_time_ms']} | {result['total_results']} | {len(result['dish_names'])} | {len(result['ingredient_names'])} |\n")
        
        f.write(f"\n**Average Performance:** {(total_successful/total_queries*100) if total_queries > 0 else 0:.1f}% success rate, {total_search_time/len(all_results) if all_results else 0:.1f}ms per image\n\n")
        
        # å„ç”»åƒã®è©³ç´°çµæœ
        if detailed_results:
            f.write(f"## Detailed Search Results by Image\n\n")
            
            for i, detail in enumerate(detailed_results):
                image_info = all_results[i]
                f.write(f"### {i+1}. {image_info['image_name']}\n\n")
                f.write(f"- **Analysis ID:** {image_info['analysis_id']}\n")
                f.write(f"- **Success Rate:** {image_info['match_rate_percent']:.1f}% ({image_info['successful_matches']}/{image_info['total_queries']})\n")
                f.write(f"- **Search Time:** {image_info['search_time_ms']}ms\n")
                f.write(f"- **Total Results:** {image_info['total_results']}\n\n")
                
                # æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ã¨ææ–™
                f.write(f"#### Detected Items\n\n")
                if image_info['dish_names']:
                    f.write(f"**Dishes ({len(image_info['dish_names'])}):** {', '.join(image_info['dish_names'])}\n\n")
                if image_info['ingredient_names']:
                    f.write(f"**Ingredients ({len(image_info['ingredient_names'])}):** {', '.join(image_info['ingredient_names'])}\n\n")
                
                # æ¤œç´¢çµæœè©³ç´°
                f.write(f"#### Search Results Detail\n\n")
                matches = detail.get('matches', {})
                dish_names = image_info['dish_names']
                
                # æ–™ç†çµæœã‚’å…ˆã«è¡¨ç¤º
                dish_results = {k: v for k, v in matches.items() if k in dish_names}
                ingredient_results = {k: v for k, v in matches.items() if k not in dish_names}
                
                if dish_results:
                    f.write(f"##### Dish Search Results\n\n")
                    for j, (query, match_results) in enumerate(dish_results.items(), 1):
                        f.write(f"**{j}. {query} (dish)**\n\n")
                        if isinstance(match_results, list):
                            f.write(f"Found {len(match_results)} results:\n\n")
                            for k, match in enumerate(match_results[:3], 1):  # ä¸Šä½3ä»¶ã®ã¿è¡¨ç¤º
                                f.write(f"   {k}. **{match.get('search_name', 'Unknown')}** (score: {match.get('score', 0):.2f})\n")
                                f.write(f"      - Source: {match.get('source', 'Unknown')}\n")
                                f.write(f"      - Data Type: {match.get('data_type', 'Unknown')}\n")
                                if match.get('nutrition'):
                                    nutrition = match['nutrition']
                                    calories = nutrition.get('calories', 0)
                                    protein = nutrition.get('protein', 0)
                                    fat = nutrition.get('fat', 0)
                                    carbs = nutrition.get('carbs', nutrition.get('carbohydrates', 0))
                                    f.write(f"      - Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g\n")
                                f.write(f"\n")
                            if len(match_results) > 3:
                                f.write(f"   ... and {len(match_results) - 3} more results\n\n")
                        f.write(f"\n")
                
                if ingredient_results:
                    f.write(f"##### Ingredient Search Results\n\n")
                    for j, (query, match_results) in enumerate(ingredient_results.items(), 1):
                        f.write(f"**{j}. {query} (ingredient)**\n\n")
                        if isinstance(match_results, list):
                            f.write(f"Found {len(match_results)} results:\n\n")
                            for k, match in enumerate(match_results[:2], 1):  # ä¸Šä½2ä»¶ã®ã¿è¡¨ç¤º
                                f.write(f"   {k}. **{match.get('search_name', 'Unknown')}** (score: {match.get('score', 0):.2f})\n")
                                f.write(f"      - Source: {match.get('source', 'Unknown')}\n")
                                f.write(f"      - Data Type: {match.get('data_type', 'Unknown')}\n")
                                if match.get('nutrition'):
                                    nutrition = match['nutrition']
                                    calories = nutrition.get('calories', 0)
                                    protein = nutrition.get('protein', 0)
                                    fat = nutrition.get('fat', 0)
                                    carbs = nutrition.get('carbs', nutrition.get('carbohydrates', 0))
                                    f.write(f"      - Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g\n")
                                f.write(f"\n")
                            if len(match_results) > 2:
                                f.write(f"   ... and {len(match_results) - 2} more results\n\n")
                        f.write(f"\n")
                
                f.write(f"---\n\n")
        
        # æˆ¦ç•¥çš„æ¤œç´¢çµ±è¨ˆï¼ˆå…¨ç”»åƒçµ±åˆï¼‰
        if detailed_results:
            f.write(f"## Strategic Search Statistics (All Images)\n\n")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒçµ±è¨ˆ
            total_db_stats = {"elasticsearch_eatthismuch": 0, "elasticsearch_yazio": 0, "elasticsearch_mynetdiary": 0}
            strategy_stats = {"dish_primary": 0, "dish_fallback": 0, "ingredient_primary": 0, "ingredient_fallback": 0}
            total_individual_results = 0
            
            for detail in detailed_results:
                matches = detail.get('matches', {})
                dish_names = set(detail.get('input_queries', {}).get('dish_names', []))
                
                for query, match_results in matches.items():
                    if isinstance(match_results, list):
                        total_individual_results += len(match_results)
                        for match in match_results:
                            source = match.get('source', '')
                            if source in total_db_stats:
                                total_db_stats[source] += 1
                            
                            # æˆ¦ç•¥çµ±è¨ˆ
                            metadata = match.get('search_metadata', {})
                            strategy_type = metadata.get('strategy_type', '')
                            if strategy_type in strategy_stats:
                                strategy_stats[strategy_type] += 1
            
            f.write(f"### Database Distribution\n\n")
            for db, count in total_db_stats.items():
                if count > 0:
                    percentage = (count / total_individual_results) * 100 if total_individual_results > 0 else 0
                    db_name = db.replace('elasticsearch_', '').title()
                    f.write(f"- **{db_name}:** {count} results ({percentage:.1f}%)\n")
            
            f.write(f"\n### Strategy Distribution\n\n")
            total_strategy_results = sum(strategy_stats.values())
            for strategy, count in strategy_stats.items():
                if count > 0:
                    percentage = (count / total_strategy_results) * 100 if total_strategy_results > 0 else 0
                    strategy_name = strategy.replace('_', ' ').title()
                    f.write(f"- **{strategy_name}:** {count} results ({percentage:.1f}%)\n")
    
    print(f"\nğŸ“Š Comprehensive multi-image results saved to:")
    print(f"   ğŸ“ {main_results_dir}/")
    print(f"   ğŸ“„ comprehensive_multi_image_results.json")
    print(f"   ğŸ“„ comprehensive_multi_image_results.md")

if __name__ == "__main__":
    print("ğŸš€ Starting Advanced Elasticsearch Strategic Search Test")
    success = asyncio.run(test_advanced_elasticsearch_search())
    
    if success:
        print("\nâœ… Advanced Elasticsearch strategic search test completed successfully!")
        print("ğŸ¯ Strategic search optimization: dish/ingredient targeting with fallback strategies")
    else:
        print("\nâŒ Advanced Elasticsearch strategic search test failed!") 
```

================================================================================

ğŸ“ FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤ (app_v2)
================================================================================

ğŸ“„ FILE: app_v2/main/app.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,030 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:55:53
å­˜åœ¨: âœ…

CONTENT:
```python
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ..api.v1.endpoints import meal_analysis
from ..config import get_settings

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆæ—¢å­˜ã®appã¨åŒã˜ï¼‰
os.environ.setdefault("USDA_API_KEY", "vSWtKJ3jYD0Cn9LRyVJUFkuyCt9p8rEtVXz74PZg")
os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", "/Users/odasoya/meal_analysis_api /service-account-key.json")
os.environ.setdefault("GEMINI_PROJECT_ID", "recording-diet-ai-3e7cf")
os.environ.setdefault("GEMINI_LOCATION", "us-central1")
os.environ.setdefault("GEMINI_MODEL_NAME", "gemini-2.5-flash-preview-05-20")

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# FastAPIã‚¢ãƒ—ãƒªã®ä½œæˆ
app = FastAPI(
    title="é£Ÿäº‹åˆ†æ API v2.0",
    description="ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ã•ã‚ŒãŸé£Ÿäº‹åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ç™»éŒ²
app.include_router(
    meal_analysis.router,
    prefix="/api/v1/meal-analyses",
    tags=["Complete Meal Analysis v2.0"]
)

# ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "é£Ÿäº‹åˆ†æ API v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ",
        "version": "2.0.0",
        "architecture": "Component-based Pipeline",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "version": "v2.0",
        "components": ["Phase1Component", "USDAQueryComponent"]
    }

if __name__ == "__main__":
    import uvicorn
    settings = get_settings()
    uvicorn.run(
        "app_v2.main.app:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=True
    ) 
```

================================================================================

ğŸ“„ FILE: app_v2/api/v1/endpoints/meal_analysis.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,696 bytes
æœ€çµ‚æ›´æ–°: 2025-06-09 11:27:28
å­˜åœ¨: âœ…

CONTENT:
```python
from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from typing import Optional
import logging

from ....pipeline import MealAnalysisPipeline

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/complete")
async def complete_meal_analysis(
    image: UploadFile = File(...),
    save_results: bool = Form(True),
    save_detailed_logs: bool = Form(True)
):
    """
    å®Œå…¨ãªé£Ÿäº‹åˆ†æã‚’å®Ÿè¡Œï¼ˆv2.0 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆï¼‰
    
    - Phase 1: Gemini AIã«ã‚ˆã‚‹ç”»åƒåˆ†æ
    - USDA Query: é£Ÿæã®USDAãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ
    - Phase 2: è¨ˆç®—æˆ¦ç•¥æ±ºå®šã¨æ „é¤Šä¾¡ç²¾ç·»åŒ– (TODO)
    - Nutrition Calculation: æœ€çµ‚æ „é¤Šä¾¡è¨ˆç®— (TODO)
    
    Args:
        image: åˆ†æå¯¾è±¡ã®é£Ÿäº‹ç”»åƒ
        save_results: çµæœã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
        save_detailed_logs: è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
    
    Returns:
        å®Œå…¨ãªåˆ†æçµæœã¨æ „é¤Šä¾¡è¨ˆç®—ã€è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    
    try:
        # ç”»åƒã®æ¤œè¨¼
        if not image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”»åƒã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        image_data = await image.read()
        logger.info(f"Starting complete meal analysis pipeline v2.0 (detailed_logs: {save_detailed_logs})")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
        pipeline = MealAnalysisPipeline()
        result = await pipeline.execute_complete_analysis(
            image_bytes=image_data,
            image_mime_type=image.content_type,
            save_results=save_results,
            save_detailed_logs=save_detailed_logs
        )
        
        logger.info(f"Complete analysis pipeline v2.0 finished successfully")
        
        return JSONResponse(
            status_code=200,
            content=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Complete analysis failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Complete analysis failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "healthy", "version": "v2.0", "message": "é£Ÿäº‹åˆ†æAPI v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ"}


@router.get("/pipeline-info")
async def get_pipeline_info():
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã®å–å¾—"""
    pipeline = MealAnalysisPipeline()
    return pipeline.get_pipeline_info() 
```

================================================================================

ğŸ“ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ¶å±¤
================================================================================

ğŸ“„ FILE: app_v2/pipeline/__init__.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 142 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:08:07
å­˜åœ¨: âœ…

CONTENT:
```python
from .orchestrator import MealAnalysisPipeline
from .result_manager import ResultManager

__all__ = ["MealAnalysisPipeline", "ResultManager"] 
```

================================================================================

ğŸ“„ FILE: app_v2/pipeline/orchestrator.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 16,426 bytes
æœ€çµ‚æ›´æ–°: 2025-06-11 10:51:22
å­˜åœ¨: âœ…

CONTENT:
```python
import uuid
import json
from datetime import datetime
from typing import Optional, Dict, Any
import logging

from ..components import Phase1Component, USDAQueryComponent, LocalNutritionSearchComponent, ElasticsearchNutritionSearchComponent
from ..models import (
    Phase1Input, Phase1Output,
    USDAQueryInput, USDAQueryOutput,
    NutritionQueryInput
)
from ..config import get_settings
from .result_manager import ResultManager

logger = logging.getLogger(__name__)


class MealAnalysisPipeline:
    """
    é£Ÿäº‹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
    
    4ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã‚’çµ±åˆã—ã¦å®Œå…¨ãªåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    
    def __init__(self, use_local_nutrition_search: Optional[bool] = None, use_elasticsearch_search: Optional[bool] = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            use_local_nutrition_search: ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ï¼‰
            use_elasticsearch_search: Elasticsearchæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
                                    None: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾—
                                    True: ElasticsearchNutritionSearchComponentä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
                                    False: å¾“æ¥ã®USDAQueryComponentä½¿ç”¨
        """
        self.pipeline_id = str(uuid.uuid4())[:8]
        self.settings = get_settings()
        
        # Elasticsearchæ¤œç´¢å„ªå…ˆåº¦ã®æ±ºå®š
        if use_elasticsearch_search is not None:
            self.use_elasticsearch_search = use_elasticsearch_search
        elif hasattr(self.settings, 'USE_ELASTICSEARCH_SEARCH'):
            self.use_elasticsearch_search = self.settings.USE_ELASTICSEARCH_SEARCH
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Elasticsearchä½¿ç”¨
            self.use_elasticsearch_search = True
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®å‡¦ç†
        if use_local_nutrition_search is not None and use_elasticsearch_search is None:
            # æ—§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆ
            if use_local_nutrition_search:
                self.use_elasticsearch_search = False
                self.use_local_nutrition_search = True
            else:
                self.use_elasticsearch_search = False
                self.use_local_nutrition_search = False
        else:
            self.use_local_nutrition_search = not self.use_elasticsearch_search and (
                use_local_nutrition_search or getattr(self.settings, 'USE_LOCAL_NUTRITION_SEARCH', False)
            )
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.phase1_component = Phase1Component()
        
        # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é¸æŠ
        if self.use_elasticsearch_search:
            self.nutrition_search_component = ElasticsearchNutritionSearchComponent(
                multi_db_search_mode=True,
                results_per_db=5
            )
            self.search_component_name = "ElasticsearchNutritionSearchComponent"
            logger.info("Using Elasticsearch nutrition database search (high-performance, multi-DB mode)")
        elif self.use_local_nutrition_search:
            self.nutrition_search_component = LocalNutritionSearchComponent()
            self.search_component_name = "LocalNutritionSearchComponent"
            logger.info("Using local nutrition database search (nutrition_db_experiment)")
        else:
            self.nutrition_search_component = USDAQueryComponent()
            self.search_component_name = "USDAQueryComponent"
            logger.info("Using traditional USDA API search")
            
        # TODO: Phase2Componentã¨NutritionCalculationComponentã‚’è¿½åŠ 
        
        self.logger = logging.getLogger(f"{__name__}.{self.pipeline_id}")
        
    async def execute_complete_analysis(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        optional_text: Optional[str] = None,
        save_results: bool = True,
        save_detailed_logs: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œå…¨ãªé£Ÿäº‹åˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            image_bytes: ç”»åƒãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆ
            save_results: çµæœã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            save_detailed_logs: è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            å®Œå…¨ãªåˆ†æçµæœ
        """
        analysis_id = str(uuid.uuid4())[:8]
        start_time = datetime.now()
        
        # ResultManagerã®åˆæœŸåŒ–
        result_manager = ResultManager(analysis_id) if save_detailed_logs else None
        
        self.logger.info(f"[{analysis_id}] Starting complete meal analysis pipeline")
        if self.use_elasticsearch_search:
            self.logger.info(f"[{analysis_id}] Nutrition search method: Elasticsearch (high-performance)")
        elif self.use_local_nutrition_search:
            self.logger.info(f"[{analysis_id}] Nutrition search method: Local Database")
        else:
            self.logger.info(f"[{analysis_id}] Nutrition search method: USDA API")
        
        try:
            # === Phase 1: ç”»åƒåˆ†æ ===
            self.logger.info(f"[{analysis_id}] Phase 1: Image analysis")
            
            phase1_input = Phase1Input(
                image_bytes=image_bytes,
                image_mime_type=image_mime_type,
                optional_text=optional_text
            )
            
            # Phase1ã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            phase1_log = result_manager.create_execution_log("Phase1Component", f"{analysis_id}_phase1") if result_manager else None
            
            phase1_result = await self.phase1_component.execute(phase1_input, phase1_log)
            
            self.logger.info(f"[{analysis_id}] Phase 1 completed - Detected {len(phase1_result.dishes)} dishes")
            
            # === Nutrition Search Phase: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ ===
            if self.use_elasticsearch_search:
                search_phase_name = "Elasticsearch Search"
            elif self.use_local_nutrition_search:
                search_phase_name = "Local Nutrition Search"
            else:
                search_phase_name = "USDA Query"
                
            self.logger.info(f"[{analysis_id}] {search_phase_name} Phase: Database matching")
            
            # === çµ±ä¸€ã•ã‚ŒãŸæ „é¤Šæ¤œç´¢å…¥åŠ›ã‚’ä½œæˆ ===
            if self.use_elasticsearch_search or self.use_local_nutrition_search:
                # Elasticsearchæ¤œç´¢ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®å ´åˆã¯NutritionQueryInputã‚’ä½¿ç”¨
                preferred_source = "elasticsearch" if self.use_elasticsearch_search else "local_database"
                nutrition_search_input = NutritionQueryInput(
                    ingredient_names=phase1_result.get_all_ingredient_names(),
                    dish_names=phase1_result.get_all_dish_names(),
                    preferred_source=preferred_source
                )
            else:
                # USDAæ¤œç´¢ã®å ´åˆã¯USDAQueryInputã‚’ä½¿ç”¨ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ï¼‰
                nutrition_search_input = USDAQueryInput(
                    ingredient_names=phase1_result.get_all_ingredient_names(),
                    dish_names=phase1_result.get_all_dish_names()
                )
            
            # Nutrition Searchã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            search_log = result_manager.create_execution_log(self.search_component_name, f"{analysis_id}_nutrition_search") if result_manager else None
            
            nutrition_search_result = await self.nutrition_search_component.execute(nutrition_search_input, search_log)
            
            self.logger.info(f"[{analysis_id}] {search_phase_name} completed - {nutrition_search_result.get_match_rate():.1%} match rate")
            
            # === æš«å®šçš„ãªçµæœã®æ§‹ç¯‰ (Phase2ã¨Nutritionã¯å¾Œã§è¿½åŠ ) ===
            
            # Phase1ã®çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰
            phase1_dict = {
                "detected_food_items": [
                    {
                        "item_name": item.item_name,
                        "confidence": item.confidence,
                        "attributes": [
                            {
                                "type": attr.type.value if hasattr(attr.type, 'value') else str(attr.type),
                                "value": attr.value,
                                "confidence": attr.confidence
                            }
                            for attr in item.attributes
                        ],
                        "brand": item.brand or "",
                        "category_hints": item.category_hints,
                        "negative_cues": item.negative_cues
                    }
                    for item in phase1_result.detected_food_items
                ],
                "dishes": [
                    {
                        "dish_name": dish.dish_name,
                        "confidence": dish.confidence,
                        "ingredients": [
                            {
                                "ingredient_name": ing.ingredient_name,
                                "confidence": ing.confidence
                            }
                            for ing in dish.ingredients
                        ],
                        "attributes": [
                            {
                                "type": attr.type.value if hasattr(attr.type, 'value') else str(attr.type),
                                "value": attr.value,
                                "confidence": attr.confidence
                            }
                            for attr in dish.detected_attributes
                        ]
                    }
                    for dish in phase1_result.dishes
                ],
                "analysis_confidence": phase1_result.analysis_confidence,
                "processing_notes": phase1_result.processing_notes
            }
            
            # ç°¡å˜ãªæ „é¤Šè¨ˆç®—ï¼ˆæš«å®šï¼‰
            total_calories = sum(
                len(dish.ingredients) * 50  # ä»®ã®è¨ˆç®—
                for dish in phase1_result.dishes
            )
            
            # æ¤œç´¢æ–¹æ³•ã®ç‰¹å®š
            if self.use_elasticsearch_search:
                search_method = "elasticsearch"
                search_api_method = "elasticsearch"
            elif self.use_local_nutrition_search:
                search_method = "local_nutrition_database"
                search_api_method = "local_database"
            else:
                search_method = "usda_api"
                search_api_method = "usda_api"
            
            # å®Œå…¨åˆ†æçµæœã®æ§‹ç¯‰
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            complete_result = {
                "analysis_id": analysis_id,
                "phase1_result": phase1_dict,
                "nutrition_search_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary,
                    "search_method": search_method
                },
                # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®ãŸã‚ã€usdaã‚­ãƒ¼ã‚‚æ®‹ã™
                "usda_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary
                },
                "processing_summary": {
                    "total_dishes": len(phase1_result.dishes),
                    "total_ingredients": len(phase1_result.get_all_ingredient_names()),
                    "nutrition_search_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",
                    "usda_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",  # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§
                    "total_calories": total_calories,
                    "pipeline_status": "completed",
                    "processing_time_seconds": processing_time,
                    "search_method": search_method
                },
                # æš«å®šçš„ãªæœ€çµ‚çµæœ
                "final_nutrition_result": {
                    "dishes": phase1_dict["dishes"],
                    "total_meal_nutrients": {
                        "calories_kcal": total_calories,
                        "protein_g": total_calories * 0.15,  # ä»®ã®å€¤
                        "carbohydrates_g": total_calories * 0.55,  # ä»®ã®å€¤
                        "fat_g": total_calories * 0.30,  # ä»®ã®å€¤
                    }
                },
                "metadata": {
                    "pipeline_version": "v2.0",
                    "timestamp": datetime.now().isoformat(),
                    "components_used": ["Phase1Component", self.search_component_name],
                    "nutrition_search_method": search_api_method
                }
            }
            
            # ResultManagerã«æœ€çµ‚çµæœã‚’è¨­å®š
            if result_manager:
                result_manager.set_final_result(complete_result)
                result_manager.finalize_pipeline()
            
            # çµæœã®ä¿å­˜
            saved_files = {}
            if save_detailed_logs and result_manager:
                # æ–°ã—ã„ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ä¿å­˜æ–¹å¼
                saved_files = result_manager.save_phase_results()
                complete_result["analysis_folder"] = result_manager.get_analysis_folder_path()
                complete_result["saved_files"] = saved_files
                
                logger.info(f"[{analysis_id}] Detailed logs saved to folder: {result_manager.get_analysis_folder_path()}")
                logger.info(f"[{analysis_id}] Saved {len(saved_files)} files across all phases")
            
            if save_results:
                # é€šå¸¸ã®çµæœä¿å­˜ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
                saved_file = f"analysis_results/meal_analysis_{analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                complete_result["legacy_saved_to"] = saved_file
            
            self.logger.info(f"[{analysis_id}] Complete analysis pipeline finished successfully in {processing_time:.2f}s")
            
            return complete_result
            
        except Exception as e:
            self.logger.error(f"[{analysis_id}] Complete analysis failed: {str(e)}", exc_info=True)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ResultManagerã‚’ä¿å­˜
            if result_manager:
                result_manager.set_final_result({"error": str(e), "timestamp": datetime.now().isoformat()})
                result_manager.finalize_pipeline()
                error_saved_files = result_manager.save_phase_results()
                self.logger.info(f"[{analysis_id}] Error analysis logs saved to folder: {result_manager.get_analysis_folder_path()}")
            
            raise
    
    def get_pipeline_info(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’å–å¾—"""
        if self.use_elasticsearch_search:
            search_method = "elasticsearch"
        elif self.use_local_nutrition_search:
            search_method = "local_database"
        else:
            search_method = "usda_api"
            
        return {
            "pipeline_id": self.pipeline_id,
            "version": "v2.0",
            "nutrition_search_method": search_method,
            "components": [
                {
                    "component_name": "Phase1Component",
                    "component_type": "analysis",
                    "execution_count": 0
                },
                {
                    "component_name": self.search_component_name,
                    "component_type": "nutrition_search",
                    "execution_count": 0
                }
            ]
        } 
```

================================================================================

ğŸ“„ FILE: app_v2/pipeline/result_manager.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 36,336 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 14:06:44
å­˜åœ¨: âœ…

CONTENT:
```python
import json
import os
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class DetailedExecutionLog:
    """å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è©³ç´°å®Ÿè¡Œãƒ­ã‚°"""
    
    def __init__(self, component_name: str, execution_id: str):
        self.component_name = component_name
        self.execution_id = execution_id
        self.execution_start_time = datetime.now()
        self.execution_end_time = None
        self.input_data = {}
        self.output_data = {}
        self.processing_details = {}
        self.prompts_used = {}
        self.reasoning = {}
        self.confidence_scores = {}
        self.warnings = []
        self.errors = []
        
    def set_input(self, input_data: Dict[str, Any]):
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ï¼ˆæ©Ÿå¯†æƒ…å ±ã¯é™¤å¤–ï¼‰"""
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯å¤§ãã™ãã‚‹ã®ã§ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜
        safe_input = {}
        for key, value in input_data.items():
            if key == 'image_bytes':
                safe_input[key] = {
                    "size_bytes": len(value) if value else 0,
                    "type": "binary_image_data"
                }
            else:
                safe_input[key] = value
        self.input_data = safe_input
    
    def set_output(self, output_data: Dict[str, Any]):
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        self.output_data = output_data
        
    def add_prompt(self, prompt_name: str, prompt_content: str, variables: Dict[str, Any] = None):
        """ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨˜éŒ²"""
        self.prompts_used[prompt_name] = {
            "content": prompt_content,
            "variables": variables or {},
            "timestamp": datetime.now().isoformat()
        }
    
    def add_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æ¨è«–ç†ç”±ã‚’è¨˜éŒ²"""
        self.reasoning[decision_point] = {
            "reason": reason,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat()
        }
    
    def add_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’è¨˜éŒ²"""
        self.processing_details[detail_key] = detail_value
    
    def add_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²"""
        self.confidence_scores[metric_name] = score
    
    def add_warning(self, warning: str):
        """è­¦å‘Šã‚’è¨˜éŒ²"""
        self.warnings.append({
            "message": warning,
            "timestamp": datetime.now().isoformat()
        })
    
    def add_error(self, error: str):
        """ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²"""
        self.errors.append({
            "message": error,
            "timestamp": datetime.now().isoformat()
        })
    
    def finalize(self):
        """å®Ÿè¡Œå®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.execution_end_time = datetime.now()
    
    def get_execution_time(self) -> float:
        """å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆç§’ï¼‰"""
        if self.execution_end_time:
            return (self.execution_end_time - self.execution_start_time).total_seconds()
        return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "component_name": self.component_name,
            "execution_id": self.execution_id,
            "execution_start_time": self.execution_start_time.isoformat(),
            "execution_end_time": self.execution_end_time.isoformat() if self.execution_end_time else None,
            "execution_time_seconds": self.get_execution_time(),
            "input_data": self.input_data,
            "output_data": self.output_data,
            "processing_details": self.processing_details,
            "prompts_used": self.prompts_used,
            "reasoning": self.reasoning,
            "confidence_scores": self.confidence_scores,
            "warnings": self.warnings,
            "errors": self.errors
        }


class ResultManager:
    """è§£æçµæœã¨è©³ç´°ãƒ­ã‚°ã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆãƒ•ã‚§ãƒ¼ã‚ºåˆ¥æ•´ç†ç‰ˆï¼‰"""
    
    def __init__(self, analysis_id: str, save_directory: str = "analysis_results"):
        self.analysis_id = analysis_id
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Ÿè¡Œã”ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.analysis_folder_name = f"analysis_{self.timestamp}_{self.analysis_id}"
        self.analysis_dir = Path(save_directory) / self.analysis_folder_name
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.phase1_dir = self.analysis_dir / "phase1"
        self.nutrition_search_dir = self.analysis_dir / "nutrition_search_query"
        self.phase2_dir = self.analysis_dir / "phase2"
        self.nutrition_dir = self.analysis_dir / "nutrition_calculation"
        
        for phase_dir in [self.phase1_dir, self.nutrition_search_dir, self.phase2_dir, self.nutrition_dir]:
            phase_dir.mkdir(exist_ok=True)
        
        self.pipeline_start_time = datetime.now()
        self.pipeline_end_time = None
        self.execution_logs: List[DetailedExecutionLog] = []
        self.final_result = {}
        self.pipeline_metadata = {
            "analysis_id": analysis_id,
            "version": "v2.0",
            "analysis_folder": self.analysis_folder_name,
            "pipeline_start_time": self.pipeline_start_time.isoformat()
        }
        
    def create_execution_log(self, component_name: str, execution_id: str) -> DetailedExecutionLog:
        """æ–°ã—ã„å®Ÿè¡Œãƒ­ã‚°ã‚’ä½œæˆ"""
        log = DetailedExecutionLog(component_name, execution_id)
        self.execution_logs.append(log)
        return log
    
    def set_final_result(self, result: Dict[str, Any]):
        """æœ€çµ‚çµæœã‚’è¨­å®š"""
        self.final_result = result
        
    def finalize_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.pipeline_end_time = datetime.now()
        self.pipeline_metadata["pipeline_end_time"] = self.pipeline_end_time.isoformat()
        self.pipeline_metadata["total_execution_time_seconds"] = (
            self.pipeline_end_time - self.pipeline_start_time
        ).total_seconds()
    
    def save_phase_results(self) -> Dict[str, str]:
        """ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã«çµæœã‚’ä¿å­˜"""
        saved_files = {}
        
        # å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ã‚°ã‚’å‡¦ç†
        executed_components = set()
        for log in self.execution_logs:
            if log.component_name == "Phase1Component":
                files = self._save_phase1_results(log)
                saved_files.update(files)
                executed_components.add("Phase1Component")
            elif log.component_name in ["USDAQueryComponent", "LocalNutritionSearchComponent", "ElasticsearchNutritionSearchComponent"]:
                files = self._save_nutrition_search_results(log)
                saved_files.update(files)
                executed_components.add(log.component_name)
            elif log.component_name == "Phase2Component":
                files = self._save_phase2_results(log)
                saved_files.update(files)
                executed_components.add("Phase2Component")
            elif log.component_name == "NutritionCalculationComponent":
                files = self._save_nutrition_results(log)
                saved_files.update(files)
                executed_components.add("NutritionCalculationComponent")
        
        # æœªå®Ÿè£…/æœªå®Ÿè¡Œã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        if "Phase2Component" not in executed_components:
            placeholder_log = DetailedExecutionLog("Phase2Component", f"{self.analysis_id}_phase2_placeholder")
            placeholder_log.input_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.output_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.finalize()
            files = self._save_phase2_results(placeholder_log)
            saved_files.update(files)
        
        if "NutritionCalculationComponent" not in executed_components:
            placeholder_log = DetailedExecutionLog("NutritionCalculationComponent", f"{self.analysis_id}_nutrition_placeholder")
            placeholder_log.input_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.output_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.finalize()
            files = self._save_nutrition_results(placeholder_log)
            saved_files.update(files)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
        summary_files = self._save_pipeline_summary()
        saved_files.update(summary_files)
        
        return saved_files
    
    def _save_phase1_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase1ã®çµæœã‚’ä¿å­˜"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase1_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time()
            }, f, indent=2, ensure_ascii=False)
        files["phase1_input_output"] = str(input_output_file)
        
        # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–ç†ç”±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        prompts_md_file = self.phase1_dir / "prompts_and_reasoning.md"
        prompts_content = self._generate_phase1_prompts_md(log)
        with open(prompts_md_file, 'w', encoding='utf-8') as f:
            f.write(prompts_content)
        files["phase1_prompts_md"] = str(prompts_md_file)
        
        # 3. æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆ
        detected_items_file = self.phase1_dir / "detected_items.txt"
        detected_content = self._generate_phase1_detected_items_txt(log)
        with open(detected_items_file, 'w', encoding='utf-8') as f:
            f.write(detected_content)
        files["phase1_detected_txt"] = str(detected_items_file)
        
        return files
    
    def _save_nutrition_search_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµæœã‚’ä¿å­˜ï¼ˆUSDAQueryComponentã€LocalNutritionSearchComponentã€ElasticsearchNutritionSearchComponentå¯¾å¿œï¼‰"""
        files = {}
        
        # æ¤œç´¢æ–¹æ³•ã®åˆ¤å®š
        search_method = "unknown"
        db_source = "unknown"
        
        if log.component_name == "USDAQueryComponent":
            search_method = "usda_api"
            db_source = "usda_database"
        elif log.component_name == "LocalNutritionSearchComponent":
            search_method = "local_search"
            db_source = "local_nutrition_database"
        elif log.component_name == "ElasticsearchNutritionSearchComponent":
            search_method = "elasticsearch"
            db_source = "elasticsearch_nutrition_db"
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢æ–¹æ³•æƒ…å ±ã‚’å«ã‚€ï¼‰
        input_output_file = self.nutrition_search_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "search_metadata": {
                    "component_name": log.component_name,
                    "search_method": search_method,
                    "database_source": db_source,
                    "timestamp": log.execution_end_time.isoformat() if log.execution_end_time else None
                }
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_search_input_output"] = str(input_output_file)
        
        # 2. æ¤œç´¢çµæœã®è©³ç´°ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        search_results_md_file = self.nutrition_search_dir / "search_results.md"
        search_content = self._generate_nutrition_search_results_md(log, search_method, db_source)
        with open(search_results_md_file, 'w', encoding='utf-8') as f:
            f.write(search_content)
        files["nutrition_search_results_md"] = str(search_results_md_file)
        
        # 3. ãƒãƒƒãƒè©³ç´°ã®ãƒ†ã‚­ã‚¹ãƒˆ
        match_details_file = self.nutrition_search_dir / "match_details.txt"
        match_content = self._generate_nutrition_match_details_txt(log, search_method, db_source)
        with open(match_details_file, 'w', encoding='utf-8') as f:
            f.write(match_content)
        files["nutrition_search_match_details"] = str(match_details_file)
        
        return files
    
    def _save_phase2_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase2ã®çµæœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase2_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "Phase2Component is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["phase2_input_output"] = str(input_output_file)
        
        # 2. æˆ¦ç•¥æ±ºå®šã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        strategy_md_file = self.phase2_dir / "strategy_decisions.md"
        with open(strategy_md_file, 'w', encoding='utf-8') as f:
            f.write("# Phase2 Strategy Decisions\n\n*Phase2Component is not yet implemented*\n")
        files["phase2_strategy_md"] = str(strategy_md_file)
        
        # 3. é¸æŠé …ç›®ã®ãƒ†ã‚­ã‚¹ãƒˆ
        selected_items_file = self.phase2_dir / "selected_items.txt"
        with open(selected_items_file, 'w', encoding='utf-8') as f:
            f.write("Phase2Component is not yet implemented\n")
        files["phase2_items_txt"] = str(selected_items_file)
        
        return files
    
    def _save_nutrition_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šè¨ˆç®—ã®çµæœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.nutrition_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "NutritionCalculationComponent is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_input_output"] = str(input_output_file)
        
        # 2. è¨ˆç®—å¼ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        formulas_md_file = self.nutrition_dir / "calculation_formulas.md"
        with open(formulas_md_file, 'w', encoding='utf-8') as f:
            f.write("# Nutrition Calculation Formulas\n\n*NutritionCalculationComponent is not yet implemented*\n")
        files["nutrition_formulas_md"] = str(formulas_md_file)
        
        # 3. æ „é¤Šã‚µãƒãƒªãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆ
        summary_txt_file = self.nutrition_dir / "nutrition_summary.txt"
        with open(summary_txt_file, 'w', encoding='utf-8') as f:
            f.write("NutritionCalculationComponent is not yet implemented\n")
        files["nutrition_summary_txt"] = str(summary_txt_file)
        
        return files
    
    def _save_pipeline_summary(self) -> Dict[str, str]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜"""
        files = {}
        
        # 1. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚µãƒãƒªãƒ¼JSON
        summary_file = self.analysis_dir / "pipeline_summary.json"
        summary_data = {
            "analysis_id": self.analysis_id,
            "timestamp": self.timestamp,
            "pipeline_metadata": self.pipeline_metadata,
            "execution_summary": {
                log.component_name: {
                    "execution_time": log.get_execution_time(),
                    "success": len(log.errors) == 0,
                    "warnings_count": len(log.warnings),
                    "errors_count": len(log.errors)
                }
                for log in self.execution_logs
            },
            "final_result": self.final_result
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        files["pipeline_summary"] = str(summary_file)
        
        # 2. å®Œå…¨ãªè©³ç´°ãƒ­ã‚°JSON
        complete_log_file = self.analysis_dir / "complete_analysis_log.json"
        complete_data = {
            "pipeline_metadata": self.pipeline_metadata,
            "execution_logs": [log.to_dict() for log in self.execution_logs],
            "final_result": self.final_result,
            "summary": {
                "total_components": len(self.execution_logs),
                "total_warnings": sum(len(log.warnings) for log in self.execution_logs),
                "total_errors": sum(len(log.errors) for log in self.execution_logs)
            }
        }
        
        with open(complete_log_file, 'w', encoding='utf-8') as f:
            json.dump(complete_data, f, indent=2, ensure_ascii=False)
        files["complete_log"] = str(complete_log_file)
        
        return files
    
    def _generate_phase1_prompts_md(self, log: DetailedExecutionLog) -> str:
        """Phase1ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–ç†ç”±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆ"""
        content = f"""# Phase1: ç”»åƒåˆ†æ - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–

## å®Ÿè¡Œæƒ…å ±
- å®Ÿè¡ŒID: {log.execution_id}
- é–‹å§‹æ™‚åˆ»: {log.execution_start_time.isoformat()}
- çµ‚äº†æ™‚åˆ»: {log.execution_end_time.isoformat() if log.execution_end_time else 'N/A'}
- å®Ÿè¡Œæ™‚é–“: {log.get_execution_time():.2f}ç§’

## ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
        for prompt_name, prompt_data in log.prompts_used.items():
            content += f"### {prompt_name.replace('_', ' ').title()}\n\n"
            content += f"**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—**: {prompt_data['timestamp']}\n\n"
            content += f"```\n{prompt_data['content']}\n```\n\n"
            
            if prompt_data.get('variables'):
                content += f"**å¤‰æ•°**:\n"
                for var_name, var_value in prompt_data['variables'].items():
                    content += f"- {var_name}: {var_value}\n"
                content += "\n"
        
        # æ¨è«–ç†ç”±
        content += "## AIæ¨è«–ã®è©³ç´°\n\n"
        
        # æ–™ç†è­˜åˆ¥ã®æ¨è«–
        dish_reasoning = [r for r in log.reasoning.items() if r[0].startswith('dish_identification_')]
        if dish_reasoning:
            content += "### æ–™ç†è­˜åˆ¥ã®æ¨è«–\n\n"
            for decision_point, reasoning_data in dish_reasoning:
                dish_num = decision_point.split('_')[-1]
                content += f"**æ–™ç† {dish_num}**:\n"
                content += f"- æ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # é£Ÿæé¸æŠã®æ¨è«–
        ingredient_reasoning = [r for r in log.reasoning.items() if r[0].startswith('ingredient_selection_')]
        if ingredient_reasoning:
            content += "### é£Ÿæé¸æŠã®æ¨è«–\n\n"
            for decision_point, reasoning_data in ingredient_reasoning:
                content += f"**{decision_point.replace('_', ' ').title()}**:\n"
                content += f"- æ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼
        if log.warnings:
            content += "## è­¦å‘Š\n\n"
            for warning in log.warnings:
                content += f"- {warning['message']} (at {warning['timestamp']})\n"
            content += "\n"
        
        if log.errors:
            content += "## ã‚¨ãƒ©ãƒ¼\n\n"
            for error in log.errors:
                content += f"- {error['message']} (at {error['timestamp']})\n"
            content += "\n"
        
        return content
    
    def _generate_phase1_detected_items_txt(self, log: DetailedExecutionLog) -> str:
        """Phase1ã§æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰"""
        content = f"Phase1 æ¤œå‡ºçµæœ - {log.execution_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += "=" * 60 + "\n\n"
        
        if 'output_data' in log.output_data and 'dishes' in log.output_data['output_data']:
            dishes = log.output_data['output_data']['dishes']
            content += f"æ¤œå‡ºã•ã‚ŒãŸæ–™ç†æ•°: {len(dishes)}\n\n"
            
            for i, dish in enumerate(dishes, 1):
                content += f"æ–™ç† {i}: {dish['dish_name']}\n"
                content += f"  é£Ÿææ•°: {len(dish['ingredients'])}\n"
                content += "  é£Ÿæè©³ç´°:\n"
                
                for j, ingredient in enumerate(dish['ingredients'], 1):
                    content += f"    {j}. {ingredient['ingredient_name']}\n"
                content += "\n"
        
        # USDAæ¤œç´¢æº–å‚™æƒ…å ±
        if 'usda_search_terms' in log.processing_details:
            search_terms = log.processing_details['usda_search_terms']
            content += f"USDAæ¤œç´¢èªå½™ ({len(search_terms)}å€‹):\n"
            for i, term in enumerate(search_terms, 1):
                content += f"  {i}. {term}\n"
            content += "\n"
        
        # å‡¦ç†è©³ç´°
        if log.processing_details:
            content += "å‡¦ç†è©³ç´°:\n"
            for detail_key, detail_value in log.processing_details.items():
                if detail_key == 'usda_search_terms':
                    continue  # æ—¢ã«ä¸Šã§è¡¨ç¤ºæ¸ˆã¿
                if isinstance(detail_value, (dict, list)):
                    content += f"  {detail_key}: {json.dumps(detail_value, ensure_ascii=False)}\n"
                else:
                    content += f"  {detail_key}: {detail_value}\n"
        
        return content
    
    def _generate_nutrition_search_results_md(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆï¼ˆUSDA/ãƒ­ãƒ¼ã‚«ãƒ«å¯¾å¿œï¼‰"""
        content = []
        
        content.append(f"# Nutrition Database Search Results")
        content.append(f"")
        content.append(f"**Search Method:** {search_method}")
        content.append(f"**Database Source:** {db_source}")
        content.append(f"**Component:** {log.component_name}")
        content.append(f"**Execution Time:** {log.get_execution_time():.3f} seconds")
        content.append(f"**Timestamp:** {log.execution_start_time.isoformat()}")
        content.append(f"")
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.input_data:
            content.append(f"## Input Data")
            if 'ingredient_names' in log.input_data:
                ingredients = log.input_data['ingredient_names']
                content.append(f"**Ingredients ({len(ingredients)}):** {', '.join(ingredients)}")
            
            if 'dish_names' in log.input_data:
                dishes = log.input_data['dish_names']
                content.append(f"**Dishes ({len(dishes)}):** {', '.join(dishes)}")
            content.append(f"")
        
        # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            content.append(f"## Search Results")
            content.append(f"**Total Matches:** {len(matches)}")
            content.append(f"")
            
            for i, (search_term, match_data) in enumerate(matches.items(), 1):
                content.append(f"### {i}. {search_term}")
                if isinstance(match_data, dict):
                    content.append(f"**ID:** {match_data.get('id', 'N/A')}")
                    
                    # search_name ã¨ description ã‚’é©åˆ‡ã«è¡¨ç¤º
                    search_name = match_data.get('search_name', 'N/A')
                    description = match_data.get('description', None)
                    content.append(f"**Search Name:** {search_name}")
                    if description:
                        content.append(f"**Description:** {description}")
                    else:
                        content.append(f"**Description:** None")
                    
                    content.append(f"**Data Type:** {match_data.get('data_type', 'N/A')}")
                    content.append(f"**Source:** {match_data.get('source', 'N/A')}")
                    
                    # ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’æ”¹å–„
                    score = match_data.get('score', 'N/A')
                    if score != 'N/A' and 'search_metadata' in match_data:
                        metadata = match_data['search_metadata']
                        score_breakdown = metadata.get('score_breakdown', {})
                        calculation = metadata.get('calculation', '')
                        match_type = score_breakdown.get('match_type', 'unknown')
                        
                        if calculation:
                            content.append(f"**Score:** {score} *({match_type}: {calculation})*")
                        else:
                            content.append(f"**Score:** {score} *(text similarity + data type priority)*")
                    else:
                        content.append(f"**Score:** {score}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        content.append(f"**Nutrients ({len(match_data['nutrients'])}):**")
                        for nutrient in match_data['nutrients'][:5]:  # æœ€åˆã®5ã¤ã ã‘è¡¨ç¤º
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                content.append(f"  - {name}: {amount} {unit}")
                        if len(match_data['nutrients']) > 5:
                            content.append(f"  - ... and {len(match_data['nutrients']) - 5} more nutrients")
                content.append(f"")
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            content.append(f"## Search Summary")
            content.append(f"**Total Searches:** {summary.get('total_searches', 0)}")
            content.append(f"**Successful Matches:** {summary.get('successful_matches', 0)}")
            content.append(f"**Failed Searches:** {summary.get('failed_searches', 0)}")
            content.append(f"**Match Rate:** {summary.get('match_rate_percent', 0)}%")
            content.append(f"**Search Method:** {summary.get('search_method', 'unknown')}")
            content.append(f"")
        
        # æ¨è«–ç†ç”±ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.reasoning:
            content.append(f"## Search Reasoning")
            for decision_point, reason_data in log.reasoning.items():
                reason = reason_data.get('reason', '') if isinstance(reason_data, dict) else str(reason_data)
                content.append(f"**{decision_point}:** {reason}")
            content.append(f"")
        
        # è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.warnings:
            content.append(f"## Warnings")
            for warning in log.warnings:
                content.append(f"- {warning}")
            content.append(f"")
        
        if log.errors:
            content.append(f"## Errors")
            for error in log.errors:
                content.append(f"- {error}")
            content.append(f"")
        
        return "\n".join(content)
    
    def _generate_nutrition_match_details_txt(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®ãƒãƒƒãƒè©³ç´°ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆUSDA/ãƒ­ãƒ¼ã‚«ãƒ«/ãƒãƒ«ãƒDBå¯¾å¿œï¼‰"""
        lines = []
        
        lines.append(f"Nutrition Database Search Match Details")
        lines.append(f"=" * 50)
        lines.append(f"Search Method: {search_method}")
        lines.append(f"Database Source: {db_source}")
        lines.append(f"Component: {log.component_name}")
        lines.append(f"Execution Time: {log.get_execution_time():.3f} seconds")
        lines.append(f"Timestamp: {log.execution_start_time.isoformat()}")
        lines.append(f"")
        
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            
            # ç·ãƒãƒƒãƒæ•°ã‚’è¨ˆç®—ï¼ˆå˜ä¸€çµæœã¨ãƒªã‚¹ãƒˆçµæœä¸¡æ–¹ã«å¯¾å¿œï¼‰
            total_matches = 0
            for match_data in matches.values():
                if isinstance(match_data, list):
                    total_matches += len(match_data)
                elif isinstance(match_data, dict):
                    total_matches += 1
            
            lines.append(f"Total Matches: {total_matches}")
            lines.append(f"")
            
            for search_term, match_data in matches.items():
                lines.append(f"Query: {search_term}")
                lines.append(f"-" * 30)
                
                # ãƒãƒ«ãƒDBæ¤œç´¢çµæœï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼‰ã¸ã®å¯¾å¿œ
                if isinstance(match_data, list):
                    lines.append(f"  Found {len(match_data)} results from multiple databases:")
                    lines.append(f"")
                    
                    for i, match_item in enumerate(match_data, 1):
                        lines.append(f"  Result {i}:")
                        lines.append(f"    ID: {match_item.get('id', 'N/A')}")
                        
                        search_name = match_item.get('search_name', 'N/A')
                        description = match_item.get('description', None)
                        lines.append(f"    Search Name: {search_name}")
                        if description:
                            lines.append(f"    Description: {description}")
                        else:
                            lines.append(f"    Description: None")
                        
                        lines.append(f"    Data Type: {match_item.get('data_type', 'N/A')}")
                        lines.append(f"    Source: {match_item.get('source', 'N/A')}")
                        
                        # ã‚¹ã‚³ã‚¢æƒ…å ±
                        score = match_item.get('score', 'N/A')
                        if score != 'N/A' and 'search_metadata' in match_item:
                            metadata = match_item['search_metadata']
                            source_db = metadata.get('source_database', 'unknown')
                            lines.append(f"    Score: {score:.3f} (from {source_db})")
                        else:
                            lines.append(f"    Score: {score}")
                        
                        # æ „é¤Šæƒ…å ±ï¼ˆç°¡ç•¥ç‰ˆï¼‰
                        if 'nutrition' in match_item and match_item['nutrition']:
                            nutrition = match_item['nutrition']
                            calories = nutrition.get('calories', 0)
                            protein = nutrition.get('protein', 0)
                            fat = nutrition.get('fat', 0)
                            carbs = nutrition.get('carbs', 0)
                            lines.append(f"    Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g")
                        
                        lines.append(f"")
                
                # å˜ä¸€çµæœï¼ˆè¾æ›¸å½¢å¼ï¼‰ã¸ã®å¯¾å¿œï¼ˆå¾“æ¥ã®æ–¹å¼ï¼‰
                elif isinstance(match_data, dict):
                    lines.append(f"  ID: {match_data.get('id', 'N/A')}")
                    
                    search_name = match_data.get('search_name', 'N/A')
                    description = match_data.get('description', None)
                    lines.append(f"  Search Name: {search_name}")
                    if description:
                        lines.append(f"  Description: {description}")
                    else:
                        lines.append(f"  Description: None")
                    
                    lines.append(f"  Data Type: {match_data.get('data_type', 'N/A')}")
                    lines.append(f"  Source: {match_data.get('source', 'N/A')}")
                    
                    # ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’æ”¹å–„
                    score = match_data.get('score', 'N/A')
                    if score != 'N/A' and 'search_metadata' in match_data:
                        metadata = match_data['search_metadata']
                        score_breakdown = metadata.get('score_breakdown', {})
                        calculation = metadata.get('calculation', '')
                        match_type = score_breakdown.get('match_type', 'unknown')
                        
                        if calculation:
                            lines.append(f"  Score: {score} ({match_type}: {calculation})")
                        else:
                            lines.append(f"  Score: {score} (text similarity + data type priority)")
                    else:
                        lines.append(f"  Score: {score}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        lines.append(f"  Nutrients ({len(match_data['nutrients'])}):")
                        for nutrient in match_data['nutrients']:
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                lines.append(f"    - {name}: {amount} {unit}")
                    
                    if 'original_data' in match_data:
                        original_data = match_data['original_data']
                        if isinstance(original_data, dict):
                            lines.append(f"  Original Data Source: {original_data.get('source', 'Unknown')}")
                            if search_method == "local_search":
                                lines.append(f"  Local DB Source: {original_data.get('db_source', 'Unknown')}")
                    
                    lines.append(f"")
        
        # æ¤œç´¢çµ±è¨ˆ
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            lines.append(f"Search Statistics:")
            lines.append(f"  Total Searches: {summary.get('total_searches', 0)}")
            lines.append(f"  Successful Matches: {summary.get('successful_matches', 0)}")
            lines.append(f"  Failed Searches: {summary.get('failed_searches', 0)}")
            lines.append(f"  Match Rate: {summary.get('match_rate_percent', 0)}%")
            
            # ãƒãƒ«ãƒDBæ¤œç´¢ã®å ´åˆã®è¿½åŠ æƒ…å ±
            if 'target_databases' in summary:
                lines.append(f"  Target Databases: {', '.join(summary['target_databases'])}")
                lines.append(f"  Results per Database: {summary.get('results_per_db', 'N/A')}")
                lines.append(f"  Total Results: {summary.get('total_results', 'N/A')}")
            
            if search_method == "local_search":
                lines.append(f"  Total Database Items: {summary.get('total_database_items', 0)}")
        
        return "\n".join(lines)
    
    def get_analysis_folder_path(self) -> str:
        """è§£æãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—"""
        return str(self.analysis_dir) 
```

================================================================================

ğŸ“ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - Phase1 AIåˆ†æ
================================================================================

ğŸ“„ FILE: app_v2/components/__init__.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 713 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 13:00:58
å­˜åœ¨: âœ…

CONTENT:
```python
from .base import BaseComponent
from .phase1_component import Phase1Component
from .usda_query_component import USDAQueryComponent
from .local_nutrition_search_component import LocalNutritionSearchComponent
from .elasticsearch_nutrition_search_component import ElasticsearchNutritionSearchComponent
# TODO: Phase2Componentã¨NutritionCalculationComponentã‚’å®Ÿè£…
# from .phase2_component import Phase2Component
# from .nutrition_calc_component import NutritionCalculationComponent

__all__ = [
    "BaseComponent",
    "Phase1Component", 
    "USDAQueryComponent",
    "LocalNutritionSearchComponent",
    "ElasticsearchNutritionSearchComponent",
    # "Phase2Component",
    # "NutritionCalculationComponent"
] 
```

================================================================================

ğŸ“„ FILE: app_v2/components/base.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 6,824 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:08:38
å­˜åœ¨: âœ…

CONTENT:
```python
from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Any, Optional
import logging
from datetime import datetime

# å‹å¤‰æ•°ã®å®šç¾©
InputType = TypeVar('InputType')
OutputType = TypeVar('OutputType')


class BaseComponent(ABC, Generic[InputType, OutputType]):
    """
    é£Ÿäº‹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæŠ½è±¡ã‚¯ãƒ©ã‚¹
    
    å…¨ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã“ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã€process ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    """
    
    def __init__(self, component_name: str, logger: Optional[logging.Logger] = None):
        """
        ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        
        Args:
            component_name: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå
            logger: ãƒ­ã‚¬ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        self.component_name = component_name
        self.logger = logger or logging.getLogger(f"{__name__}.{component_name}")
        self.created_at = datetime.now()
        self.execution_count = 0
        self.current_execution_log = None  # è©³ç´°ãƒ­ã‚°
        
    @abstractmethod
    async def process(self, input_data: InputType) -> OutputType:
        """
        ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            OutputType: å‡¦ç†çµæœ
            
        Raises:
            ComponentError: å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        pass
    
    async def execute(self, input_data: InputType, execution_log: Optional['DetailedExecutionLog'] = None) -> OutputType:
        """
        ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ­ã‚°è¨˜éŒ²ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            execution_log: è©³ç´°å®Ÿè¡Œãƒ­ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            OutputType: å‡¦ç†çµæœ
        """
        self.execution_count += 1
        execution_id = f"{self.component_name}_{self.execution_count}"
        
        # è©³ç´°ãƒ­ã‚°ã®è¨­å®š
        if execution_log:
            self.current_execution_log = execution_log
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            self.current_execution_log.set_input(self._safe_serialize_input(input_data))
        
        self.logger.info(f"[{execution_id}] Starting {self.component_name} processing")
        
        try:
            start_time = datetime.now()
            result = await self.process(input_data)
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            self.logger.info(f"[{execution_id}] {self.component_name} completed in {processing_time:.2f}s")
            
            # è©³ç´°ãƒ­ã‚°ã«å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.set_output(self._safe_serialize_output(result))
                self.current_execution_log.finalize()
            
            return result
            
        except Exception as e:
            self.logger.error(f"[{execution_id}] {self.component_name} failed: {str(e)}", exc_info=True)
            
            # è©³ç´°ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.add_error(str(e))
                self.current_execution_log.finalize()
            
            raise ComponentError(f"{self.component_name} processing failed: {str(e)}") from e
        finally:
            self.current_execution_log = None
    
    def log_prompt(self, prompt_name: str, prompt_content: str, variables: dict = None):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_prompt(prompt_name, prompt_content, variables)
    
    def log_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_reasoning(decision_point, reason, confidence)
    
    def log_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_processing_detail(detail_key, detail_value)
    
    def log_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_confidence_score(metric_name, score)
    
    def log_warning(self, warning: str):
        """è­¦å‘Šã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_warning(warning)
    
    def _safe_serialize_input(self, input_data: InputType) -> dict:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(input_data, 'model_dump'):
                return input_data.model_dump()
            elif hasattr(input_data, '__dict__'):
                return input_data.__dict__
            else:
                return {"data": str(input_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def _safe_serialize_output(self, output_data: OutputType) -> dict:
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(output_data, 'model_dump'):
                return output_data.model_dump()
            elif hasattr(output_data, '__dict__'):
                return output_data.__dict__
            else:
                return {"data": str(output_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def get_component_info(self) -> dict:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
        return {
            "component_name": self.component_name,
            "created_at": self.created_at.isoformat(),
            "execution_count": self.execution_count,
            "component_type": self.__class__.__name__
        }


class ComponentError(Exception):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, component_name: str = None, original_error: Exception = None):
        super().__init__(message)
        self.component_name = component_name
        self.original_error = original_error
        self.timestamp = datetime.now()
    
    def to_dict(self) -> dict:
        """ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "error_message": str(self),
            "component_name": self.component_name,
            "timestamp": self.timestamp.isoformat(),
            "original_error": str(self.original_error) if self.original_error else None
        } 
```

================================================================================

ğŸ“„ FILE: app_v2/components/phase1_component.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 16,155 bytes
æœ€çµ‚æ›´æ–°: 2025-06-11 10:52:34
å­˜åœ¨: âœ…

CONTENT:
```python
import json
from typing import Optional

from .base import BaseComponent
from ..models.phase1_models import (
    Phase1Input, Phase1Output, Dish, Ingredient, 
    DetectedFoodItem, FoodAttribute, AttributeType
)
from ..services.gemini_service import GeminiService
from ..config import get_settings
from ..config.prompts import Phase1Prompts


class Phase1Component(BaseComponent[Phase1Input, Phase1Output]):
    """
    Phase1: ç”»åƒåˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œãƒ»USDAæ¤œç´¢ç‰¹åŒ–ï¼‰
    
    Gemini AIã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸè©³ç´°æƒ…å ±
    ï¼ˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã€å±æ€§ã€ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ç­‰ï¼‰ã‚’å«ã‚€USDAæ¤œç´¢ã«é©ã—ãŸå‡ºåŠ›ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    
    def __init__(self, gemini_service: Optional[GeminiService] = None):
        super().__init__("Phase1Component")
        
        # GeminiServiceã®åˆæœŸåŒ–
        if gemini_service is None:
            settings = get_settings()
            self.gemini_service = GeminiService(
                project_id=settings.GEMINI_PROJECT_ID,
                location=settings.GEMINI_LOCATION,
                model_name=settings.GEMINI_MODEL_NAME
            )
        else:
            self.gemini_service = gemini_service
    
    async def process(self, input_data: Phase1Input) -> Phase1Output:
        """
        Phase1ã®ä¸»å‡¦ç†: æ§‹é€ åŒ–ç”»åƒåˆ†æï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
        
        Args:
            input_data: Phase1Input (image_bytes, image_mime_type, optional_text)
            
        Returns:
            Phase1Output: æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æçµæœï¼ˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã€å±æ€§ã€ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ç­‰ã‚’å«ã‚€ï¼‰
        """
        self.logger.info(f"Starting Phase1 structured image analysis for enhanced USDA query generation")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã¨è¨˜éŒ²ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›ç”¨ã«æ‹¡å¼µï¼‰
        system_prompt = self._get_structured_system_prompt()
        user_prompt = Phase1Prompts.get_user_prompt(input_data.optional_text)
        
        self.log_prompt("structured_system_prompt", system_prompt)
        self.log_prompt("user_prompt", user_prompt, {
            "optional_text": input_data.optional_text,
            "image_mime_type": input_data.image_mime_type
        })
        
        # ç”»åƒæƒ…å ±ã®ãƒ­ã‚°è¨˜éŒ²
        self.log_processing_detail("image_size_bytes", len(input_data.image_bytes))
        self.log_processing_detail("image_mime_type", input_data.image_mime_type)
        
        try:
            # Gemini AIã«ã‚ˆã‚‹æ§‹é€ åŒ–ç”»åƒåˆ†æ
            self.log_processing_detail("gemini_structured_api_call_start", "Calling Gemini API for structured image analysis")
            
            gemini_result = await self.gemini_service.analyze_phase1_structured(
                image_bytes=input_data.image_bytes,
                image_mime_type=input_data.image_mime_type,
                optional_text=input_data.optional_text,
                system_prompt=system_prompt
            )
            
            self.log_processing_detail("gemini_structured_response", gemini_result)
            
            # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
            detected_food_items = []
            if "detected_food_items" in gemini_result:
                for item_index, item_data in enumerate(gemini_result["detected_food_items"]):
                    # å±æ€§ã‚’å‡¦ç†
                    attributes = []
                    for attr_data in item_data.get("attributes", []):
                        # AttributeTypeã«å­˜åœ¨ã—ãªã„å ´åˆã¯PREPARATIONã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        attr_type_str = attr_data.get("type", "ingredient")
                        try:
                            attr_type = AttributeType(attr_type_str)
                        except ValueError:
                            # æœªçŸ¥ã®å±æ€§ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯æœ€ã‚‚è¿‘ã„æ—¢å­˜ã‚¿ã‚¤ãƒ—ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                            if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                attr_type = AttributeType.PREPARATION
                            elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                attr_type = AttributeType.COOKING_METHOD
                            elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                attr_type = AttributeType.TEXTURE
                            else:
                                attr_type = AttributeType.PREPARATION  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                        
                        attribute = FoodAttribute(
                            type=attr_type,
                            value=attr_data["value"],
                            confidence=attr_data.get("confidence", 0.5)
                        )
                        attributes.append(attribute)
                    
                    # DetectedFoodItemã‚’ä½œæˆ
                    detected_item = DetectedFoodItem(
                        item_name=item_data["item_name"],
                        confidence=item_data.get("confidence", 0.5),
                        attributes=attributes,
                        brand=item_data.get("brand"),
                        category_hints=item_data.get("category_hints", []),
                        negative_cues=item_data.get("negative_cues", [])
                    )
                    detected_food_items.append(detected_item)
                    
                    # æ§‹é€ åŒ–ã‚¢ã‚¤ãƒ†ãƒ è­˜åˆ¥ã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                    self.log_reasoning(
                        f"structured_item_identification_{item_index}",
                        f"Structured identification: '{item_data['item_name']}' (confidence: {item_data.get('confidence', 0.5):.2f}, "
                        f"attributes: {len(attributes)}, brand: {item_data.get('brand', 'N/A')})"
                    )
            
            # å¾“æ¥äº’æ›æ€§ã®ãŸã‚ã®dishesã‚‚ç”Ÿæˆ
            dishes = []
            if "dishes" in gemini_result:
                for dish_index, dish_data in enumerate(gemini_result.get("dishes", [])):
                    ingredients = []
                    for ingredient_index, ingredient_data in enumerate(dish_data.get("ingredients", [])):
                        # æ§‹é€ åŒ–å±æ€§ã‚’å¾“æ¥å½¢å¼ã«å¤‰æ›
                        ingredient_attributes = []
                        if "attributes" in ingredient_data:
                            for attr_data in ingredient_data["attributes"]:
                                # AttributeTypeã«å­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
                                attr_type_str = attr_data.get("type", "ingredient")
                                try:
                                    attr_type = AttributeType(attr_type_str)
                                except ValueError:
                                    # æœªçŸ¥ã®å±æ€§ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯ãƒãƒƒãƒ”ãƒ³ã‚°
                                    if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                        attr_type = AttributeType.PREPARATION
                                    elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                        attr_type = AttributeType.COOKING_METHOD
                                    elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                        attr_type = AttributeType.TEXTURE
                                    else:
                                        attr_type = AttributeType.PREPARATION
                                
                                attr = FoodAttribute(
                                    type=attr_type,
                                    value=attr_data["value"],
                                    confidence=attr_data.get("confidence", 0.5)
                                )
                                ingredient_attributes.append(attr)
                        
                        ingredient = Ingredient(
                            ingredient_name=ingredient_data["ingredient_name"],
                            confidence=ingredient_data.get("confidence"),
                            detected_attributes=ingredient_attributes
                        )
                        ingredients.append(ingredient)
                    
                    # æ–™ç†ãƒ¬ãƒ™ãƒ«ã®å±æ€§
                    dish_attributes = []
                    if "attributes" in dish_data:
                        for attr_data in dish_data["attributes"]:
                            # AttributeTypeã«å­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
                            attr_type_str = attr_data.get("type", "preparation")
                            try:
                                attr_type = AttributeType(attr_type_str)
                            except ValueError:
                                # æœªçŸ¥ã®å±æ€§ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯ãƒãƒƒãƒ”ãƒ³ã‚°
                                if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                    attr_type = AttributeType.PREPARATION
                                elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                    attr_type = AttributeType.COOKING_METHOD
                                elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                    attr_type = AttributeType.TEXTURE
                                else:
                                    attr_type = AttributeType.PREPARATION
                            
                            attr = FoodAttribute(
                                type=attr_type,
                                value=attr_data["value"],
                                confidence=attr_data.get("confidence", 0.5)
                            )
                            dish_attributes.append(attr)
                    
                    dish = Dish(
                        dish_name=dish_data["dish_name"],
                        confidence=dish_data.get("confidence"),
                        ingredients=ingredients,
                        detected_attributes=dish_attributes
                    )
                    dishes.append(dish)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾“æ¥å½¢å¼ã‚’ç”Ÿæˆ
            if not dishes and detected_food_items:
                dishes = self._convert_structured_to_legacy(detected_food_items)
            
            # åˆ†æçµ±è¨ˆã®è¨˜éŒ²
            self.log_processing_detail("detected_structured_items_count", len(detected_food_items))
            self.log_processing_detail("detected_dishes_count", len(dishes))
            self.log_processing_detail("total_ingredients_count", sum(len(dish.ingredients) for dish in dishes))
            
            # å…¨ä½“çš„ãªåˆ†æä¿¡é ¼åº¦ã‚’è¨ˆç®—
            overall_confidence = self._calculate_overall_confidence(detected_food_items, dishes)
            
            # å‡¦ç†ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
            processing_notes = [
                f"Structured analysis generated {len(detected_food_items)} food items",
                f"Overall confidence: {overall_confidence:.2f}",
                f"Legacy compatibility: {len(dishes)} dishes generated"
            ]
            
            result = Phase1Output(
                detected_food_items=detected_food_items,
                dishes=dishes,
                analysis_confidence=overall_confidence,
                processing_notes=processing_notes,
                warnings=[]
            )
            
            self.log_processing_detail("structured_search_terms", result.get_structured_search_terms())
            self.log_reasoning(
                "structured_analysis_completion",
                f"Phase1 structured analysis completed: {len(detected_food_items)} structured items, "
                f"overall confidence {overall_confidence:.2f}"
            )
            
            self.logger.info(f"Phase1 structured analysis completed: {len(detected_food_items)} items, "
                           f"confidence {overall_confidence:.2f}")
            return result
            
        except Exception as e:
            self.logger.error(f"Phase1 structured processing failed: {str(e)}")
            raise
    
    def _get_structured_system_prompt(self) -> str:
        """æ§‹é€ åŒ–å‡ºåŠ›ç”¨ã®è©³ç´°ãªã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        return """You are an advanced food recognition AI that analyzes food images and provides detailed structured output.

Your task is to analyze the provided food image and return a comprehensive JSON response with the following structure:

{
  "detected_food_items": [
    {
      "item_name": "Primary food item name (e.g., 'Spaghetti Carbonara')",
      "confidence": 0.85,
      "attributes": [
        {"type": "ingredient", "value": "pasta", "confidence": 0.9},
        {"type": "ingredient", "value": "egg", "confidence": 0.7},
        {"type": "preparation", "value": "creamy", "confidence": 0.8},
        {"type": "cooking_method", "value": "boiled", "confidence": 0.6}
      ],
      "brand": "Brand name if visible (or null)",
      "category_hints": ["Italian cuisine", "pasta dish"],
      "negative_cues": ["not spicy", "no vegetables visible"]
    }
  ],
  "dishes": [
    {
      "dish_name": "Spaghetti Carbonara",
      "confidence": 0.85,
      "ingredients": [
        {
          "ingredient_name": "pasta",
          "confidence": 0.9,
          "attributes": [
            {"type": "ingredient", "value": "spaghetti", "confidence": 0.95}
          ]
        }
      ],
      "attributes": [
        {"type": "preparation", "value": "creamy", "confidence": 0.8}
      ]
    }
  ],
  "analysis_confidence": 0.85
}

Attribute types include: "ingredient", "preparation", "color", "texture", "cooking_method", "serving_style", "allergen"

Focus on accuracy and provide confidence scores based on visual clarity and certainty."""
    
    def _convert_structured_to_legacy(self, detected_items: list) -> list:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å¾“æ¥å½¢å¼ã«å¤‰æ›ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        dishes = []
        
        for item in detected_items:
            # é£Ÿæå±æ€§ã‚’æŠ½å‡º
            ingredients = []
            ingredient_attrs = [attr for attr in item.attributes if attr.type == AttributeType.INGREDIENT]
            
            if ingredient_attrs:
                for attr in ingredient_attrs:
                    ingredient = Ingredient(
                        ingredient_name=attr.value,
                        confidence=attr.confidence,
                        detected_attributes=[attr]
                    )
                    ingredients.append(ingredient)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¢ã‚¤ãƒ†ãƒ åã‚’é£Ÿæã¨ã—ã¦ä½¿ç”¨
                ingredient = Ingredient(
                    ingredient_name=item.item_name,
                    confidence=item.confidence,
                    detected_attributes=item.attributes
                )
                ingredients.append(ingredient)
            
            dish = Dish(
                dish_name=item.item_name,
                confidence=item.confidence,
                ingredients=ingredients,
                detected_attributes=item.attributes
            )
            dishes.append(dish)
        
        return dishes
    
    def _calculate_overall_confidence(self, structured_items: list, dishes: list) -> float:
        """å…¨ä½“çš„ãªåˆ†æä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        if not structured_items and not dishes:
            return 0.0
        
        total_confidence = 0.0
        count = 0
        
        # æ§‹é€ åŒ–ã‚¢ã‚¤ãƒ†ãƒ ã®ä¿¡é ¼åº¦
        for item in structured_items:
            total_confidence += item.confidence
            count += 1
        
        # æ–™ç†ã®ä¿¡é ¼åº¦
        for dish in dishes:
            if dish.confidence is not None:
                total_confidence += dish.confidence
                count += 1
        
        return total_confidence / count if count > 0 else 0.5 
```

================================================================================

ğŸ“ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢
================================================================================

ğŸ“„ FILE: app_v2/components/elasticsearch_nutrition_search_component.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 44,187 bytes
æœ€çµ‚æ›´æ–°: 2025-06-11 10:16:41
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Elasticsearch Nutrition Search Component

é«˜åº¦ãªã‚¯ã‚¨ãƒªæˆ¦ç•¥å¯¾å¿œç‰ˆï¼šæ§‹é€ åŒ–å…¥åŠ›ã€bool queryã€function_scoreã€æ–‡å­—åˆ—é¡ä¼¼åº¦ã€äºŒæ®µéšæ¤œç´¢
"""

import os
import json
import asyncio
from typing import Optional, List, Dict, Any, Tuple
from datetime import datetime

from .base import BaseComponent
from ..models.nutrition_search_models import (
    NutritionQueryInput, NutritionQueryOutput, NutritionMatch
)
from ..models.phase1_models import Phase1Output, DetectedFoodItem, FoodAttribute, AttributeType
from ..config import get_settings

# æ–‡å­—åˆ—é¡ä¼¼åº¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
try:
    from rapidfuzz.distance import JaroWinkler, Levenshtein
    from rapidfuzz.fuzz import ratio, partial_ratio
    RAPIDFUZZ_AVAILABLE = True
except ImportError:
    RAPIDFUZZ_AVAILABLE = False

# Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False


class ElasticsearchNutritionSearchComponent(BaseComponent[NutritionQueryInput, NutritionQueryOutput]):
    """
    Elasticsearchæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆé«˜åº¦ãªã‚¯ã‚¨ãƒªæˆ¦ç•¥å¯¾å¿œï¼‰
    
    æ§‹é€ åŒ–å…¥åŠ›å¯¾å¿œã€bool queryã€function_scoreã€æ–‡å­—åˆ—é¡ä¼¼åº¦ã€äºŒæ®µéšæ¤œç´¢ã‚’å®Ÿè£…
    """
    
    def __init__(
        self, 
        elasticsearch_url: str = "http://localhost:9200", 
        multi_db_search_mode: bool = False, 
        results_per_db: int = 3,
        enable_advanced_features: bool = True
    ):
        super().__init__("ElasticsearchNutritionSearchComponent")
        
        self.elasticsearch_url = elasticsearch_url
        self.es_client = None
        self.index_name = "nutrition_db"
        self.multi_db_search_mode = multi_db_search_mode
        self.results_per_db = results_per_db
        self.target_databases = ["yazio", "mynetdiary", "eatthismuch"]
        
        # é«˜åº¦ãªæ©Ÿèƒ½ãƒ•ãƒ©ã‚°
        self.enable_advanced_features = enable_advanced_features
        self.enable_fuzzy_matching = RAPIDFUZZ_AVAILABLE and enable_advanced_features
        self.enable_two_stage_search = enable_advanced_features
        
        # äºŒæ®µéšæ¤œç´¢ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.first_stage_size = 50  # ç¬¬ä¸€æ®µéšã§å–å¾—ã™ã‚‹å€™è£œæ•°
        self.final_result_size = 10  # æœ€çµ‚çš„ãªçµæœæ•°
        
        # ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã¨ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.primary_term_boost = 3.0
        self.brand_boost = 2.5
        self.ingredient_boost = 1.5
        self.preparation_boost = 1.2
        self.jaro_winkler_threshold = 0.8
        self.levenshtein_threshold = 0.7
        
        # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self._initialize_elasticsearch()
        
        self.logger.info(f"ElasticsearchNutritionSearchComponent initialized")
        self.logger.info(f"Advanced features enabled: {self.enable_advanced_features}")
        self.logger.info(f"Fuzzy matching available: {self.enable_fuzzy_matching}")
        self.logger.info(f"Two-stage search enabled: {self.enable_two_stage_search}")
        
    def _initialize_elasticsearch(self):
        """Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        if not ELASTICSEARCH_AVAILABLE:
            self.logger.error("Elasticsearch library not available. Please install: pip install elasticsearch")
            return
        
        try:
            self.es_client = Elasticsearch([self.elasticsearch_url])
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            if self.es_client.ping():
                self.logger.info(f"Successfully connected to Elasticsearch at {self.elasticsearch_url}")
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å­˜åœ¨ç¢ºèª
                if self.es_client.indices.exists(index=self.index_name):
                    self.logger.info(f"Index '{self.index_name}' exists and ready")
                else:
                    self.logger.error(f"Index '{self.index_name}' does not exist. Please run create_elasticsearch_index.py first.")
                    self.es_client = None
            else:
                self.logger.error("Elasticsearch ping failed. Please ensure Elasticsearch is running.")
                self.es_client = None
                
        except Exception as e:
            self.logger.error(f"Failed to connect to Elasticsearch: {e}")
            self.es_client = None
    
    async def process(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        é«˜åº¦ãªElasticsearchæ¤œç´¢ã®ä¸»å‡¦ç†
        
        Args:
            input_data: NutritionQueryInput (æ§‹é€ åŒ–å…¥åŠ›å¯¾å¿œ)
            
        Returns:
            NutritionQueryOutput: é«˜åº¦ãªElasticsearchæ¤œç´¢çµæœ
            
        Raises:
            RuntimeError: ElasticsearchãŒåˆ©ç”¨ã§ããªã„å ´åˆ
        """
        # Elasticsearchåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        if not ELASTICSEARCH_AVAILABLE or not self.es_client:
            return self._create_error_response("Elasticsearch not available")
        
        start_time = datetime.now()
        
        # æ§‹é€ åŒ–å…¥åŠ›ã®æ¤œå‡ºã¨å‡¦ç†
        structured_data = self._extract_structured_data(input_data)
        search_terms = input_data.get_all_search_terms()
        
        self.logger.info(f"Starting advanced Elasticsearch search for {len(search_terms)} terms")
        self.log_processing_detail("structured_data_available", structured_data is not None)
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("advanced_features_enabled", self.enable_advanced_features)
        
        if self.enable_advanced_features and structured_data:
            # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªæ¤œç´¢
            return await self._advanced_structured_search(input_data, structured_data, search_terms, start_time)
        elif self.multi_db_search_mode:
            # å¾“æ¥ã®æˆ¦ç•¥çš„æ¤œç´¢
            return await self._elasticsearch_strategic_search(input_data, search_terms)
        else:
            # åŸºæœ¬æ¤œç´¢
            return await self._elasticsearch_search(input_data, search_terms)
    
    def _extract_structured_data(self, input_data: NutritionQueryInput) -> Optional[Dict[str, Any]]:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        # NutritionQueryInputãŒæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if hasattr(input_data, 'structured_analysis') and input_data.structured_analysis:
            return input_data.structured_analysis
        
        # ä»£æ›¿: Phase1Outputã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        if hasattr(input_data, 'phase1_output') and input_data.phase1_output:
            phase1_output = input_data.phase1_output
            if hasattr(phase1_output, 'get_structured_search_terms'):
                return phase1_output.get_structured_search_terms()
        
        return None
    
    async def _advanced_structured_search(
        self, 
        input_data: NutritionQueryInput, 
        structured_data: Dict[str, Any], 
        search_terms: List[str],
        start_time: datetime
    ) -> NutritionQueryOutput:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªæ¤œç´¢"""
        self.log_processing_detail("search_method", "advanced_structured")
        
        matches = {}
        successful_matches = 0
        total_searches = len(search_terms)
        
        # é«˜ä¿¡é ¼åº¦ã‚¢ã‚¤ãƒ†ãƒ ã®å‡¦ç†
        high_confidence_items = structured_data.get('high_confidence_items', [])
        medium_confidence_items = structured_data.get('medium_confidence_items', [])
        brands = structured_data.get('brands', [])
        ingredients = structured_data.get('ingredients', [])
        cooking_methods = structured_data.get('cooking_methods', [])
        negative_cues = structured_data.get('negative_cues', [])
        
        self.log_processing_detail("high_confidence_items_count", len(high_confidence_items))
        self.log_processing_detail("brands_detected", brands)
        self.log_processing_detail("negative_cues", negative_cues)
        
        # å„æ¤œç´¢èªå½™ã«å¯¾ã—ã¦é«˜åº¦ãªæ¤œç´¢ã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            try:
                # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸã‚¯ã‚¨ãƒªæ§‹ç¯‰
                advanced_query = self._build_advanced_structured_query(
                    search_term, 
                    structured_data, 
                    input_data
                )
                
                if self.enable_two_stage_search:
                    # äºŒæ®µéšæ¤œç´¢
                    nutrition_matches = await self._two_stage_search(
                        advanced_query, 
                        search_term, 
                        structured_data
                    )
                else:
                    # å˜æ®µéšé«˜åº¦æ¤œç´¢
                    nutrition_matches = await self._single_stage_advanced_search(
                        advanced_query, 
                        search_term
                    )
                
                if nutrition_matches:
                    matches[search_term] = nutrition_matches[0]  # ãƒ™ã‚¹ãƒˆãƒãƒƒãƒã‚’é¸æŠ
                    successful_matches += 1
                    
                    self.log_reasoning(
                        f"advanced_match_{search_index}",
                        f"Advanced structured match for '{search_term}': {nutrition_matches[0].name} "
                        f"(score: {nutrition_matches[0].score:.3f}, db: {nutrition_matches[0].source_db})"
                    )
                
            except Exception as e:
                self.logger.error(f"Advanced search failed for '{search_term}': {e}")
                continue
        
        # çµæœã®çµ±è¨ˆå‡¦ç†
        search_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        match_rate = (successful_matches / total_searches * 100) if total_searches > 0 else 0
        
        return NutritionQueryOutput(
            matches=matches,
            search_summary={
                "total_searches": total_searches,
                "successful_matches": successful_matches,
                "failed_searches": total_searches - successful_matches,
                "match_rate_percent": round(match_rate, 1),
                "search_method": "advanced_structured_elasticsearch",
                "search_time_ms": search_time_ms,
                "high_confidence_items": len(high_confidence_items),
                "brands_used": len(brands),
                "negative_cues_applied": len(negative_cues),
                "fuzzy_matching_enabled": self.enable_fuzzy_matching,
                "two_stage_search_enabled": self.enable_two_stage_search
            },
            errors=[]
        )
    
    def _build_advanced_structured_query(
        self, 
        search_term: str, 
        structured_data: Dict[str, Any], 
        input_data: NutritionQueryInput
    ) -> Dict[str, Any]:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªElasticsearchã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰"""
        
        # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æŠ½å‡º
        high_confidence_items = structured_data.get('high_confidence_items', [])
        brands = structured_data.get('brands', [])
        ingredients = structured_data.get('ingredients', [])
        cooking_methods = structured_data.get('cooking_methods', [])
        negative_cues = structured_data.get('negative_cues', [])
        
        # ç¾åœ¨ã®æ¤œç´¢èªå¥ãŒé«˜ä¿¡é ¼åº¦ã‚¢ã‚¤ãƒ†ãƒ ã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        is_high_confidence = any(
            item.item_name.lower() == search_term.lower() 
            for item in high_confidence_items 
            if hasattr(item, 'item_name')
        )
        
        # Bool ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰
        bool_query = {
            "bool": {
                "must": [],
                "should": [],
                "must_not": []
            }
        }
        
        # Mustå¥: é«˜ä¿¡é ¼åº¦ã®å ´åˆã¯å¿…é ˆæ¡ä»¶ã¨ã—ã¦ä½¿ç”¨
        if is_high_confidence:
            bool_query["bool"]["must"].append({
                "match_phrase": {
                    "search_name": {
                        "query": search_term,
                        "boost": self.primary_term_boost
                    }
                }
            })
        else:
            # ä¸­ãƒ»ä½ä¿¡é ¼åº¦ã®å ´åˆã¯shouldã§æŸ”è»Ÿã«
            bool_query["bool"]["should"].append({
                "match_phrase": {
                    "search_name": {
                        "query": search_term,
                        "boost": self.primary_term_boost,
                        "slop": 1  # å˜èªé–“ã®è·é›¢ã‚’1ã¤ã¾ã§è¨±å¯
                    }
                }
            })
            
            # ä»£æ›¿ãƒãƒƒãƒãƒ³ã‚°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
            bool_query["bool"]["should"].append({
                "match": {
                    "search_name": {
                        "query": search_term,
                        "boost": self.primary_term_boost * 0.8
                    }
                }
            })
        
        # Shouldå¥: ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ã«ã‚ˆã‚‹ãƒ–ãƒ¼ã‚¹ãƒˆ
        for brand in brands:
            if brand:
                bool_query["bool"]["should"].append({
                    "match": {
                        "search_name": {
                            "query": brand,
                            "boost": self.brand_boost
                        }
                    }
                })
        
        # Shouldå¥: ææ–™æƒ…å ±ã«ã‚ˆã‚‹ãƒ–ãƒ¼ã‚¹ãƒˆ
        for ingredient in ingredients:
            if ingredient:
                bool_query["bool"]["should"].append({
                    "match": {
                        "search_name": {
                            "query": ingredient,
                            "boost": self.ingredient_boost
                        }
                    }
                })
        
        # Shouldå¥: èª¿ç†æ³•æƒ…å ±ã«ã‚ˆã‚‹ãƒ–ãƒ¼ã‚¹ãƒˆ
        for cooking_method in cooking_methods:
            if cooking_method:
                bool_query["bool"]["should"].append({
                    "multi_match": {
                        "query": cooking_method,
                        "fields": ["search_name", "source_description"],
                        "boost": self.preparation_boost
                    }
                })
        
        # Must_notå¥: ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¥ãƒ¼ï¼ˆé™¤å¤–æ¡ä»¶ï¼‰
        for negative_cue in negative_cues:
            if negative_cue:
                bool_query["bool"]["must_not"].append({
                    "match": {
                        "search_name": negative_cue
                    }
                })
        
        # Function_score ã‚¯ã‚¨ãƒªã§ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—åˆ—é¡ä¼¼åº¦é©ç”¨ï¼‰
        # Painlessã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å•é¡Œã®ãŸã‚ã€RapidFuzzãƒ™ãƒ¼ã‚¹ã®äºŒæ®µéšæ¤œç´¢ã‚’ä½¿ç”¨
        if False:  # self.enable_fuzzy_matching:
            try:
                query = {
                    "function_score": {
                        "query": bool_query,
                        "functions": [
                            {
                                "script_score": {
                                    "script": {
                                        "source": self._get_similarity_script(),
                                        "params": {
                                            "search_term": search_term
                                        }
                                    }
                                }
                            }
                        ],
                        "score_mode": "max",
                        "boost_mode": "multiply"
                    }
                }
            except Exception as script_error:
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯bool queryã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                self.logger.warning(f"Script score error, falling back to bool query: {script_error}")
                query = bool_query
        else:
            query = bool_query
        
        return {
            "query": query,
            "size": self.first_stage_size if self.enable_two_stage_search else self.final_result_size,
            "_source": True,
            "sort": [
                {"_score": {"order": "desc"}}
            ]
        }
    
    def _get_similarity_script(self) -> str:
        """æ–‡å­—åˆ—é¡ä¼¼åº¦è¨ˆç®—ç”¨ã®Painlessã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
        return """
        if (doc['search_name.keyword'].size() == 0) {
            return _score;
        }
        String dbName = doc['search_name.keyword'].value;
        String searchTerm = params.search_term;
        
        if (dbName == null || searchTerm == null) {
            return _score;
        }
        
        String dbLower = dbName.toLowerCase();
        String searchLower = searchTerm.toLowerCase();
        
        // å®Œå…¨ä¸€è‡´
        if (dbLower.equals(searchLower)) {
            return _score * 2.0;
        }
        
        // éƒ¨åˆ†ä¸€è‡´
        if (dbLower.contains(searchLower) || searchLower.contains(dbLower)) {
            return _score * 1.5;
        }
        
        return _score;
        """
    
    async def _two_stage_search(
        self, 
        first_stage_query: Dict[str, Any], 
        search_term: str, 
        structured_data: Dict[str, Any]
    ) -> List[NutritionMatch]:
        """äºŒæ®µéšæ¤œç´¢ã®å®Ÿè¡Œ"""
        
        # ç¬¬ä¸€æ®µéš: åºƒã‚ã«å€™è£œã‚’å–å¾—
        response = self.es_client.search(
            index=self.index_name,
            body=first_stage_query
        )
        
        first_stage_hits = response.get('hits', {}).get('hits', [])
        self.log_processing_detail("first_stage_hits_count", len(first_stage_hits))
        
        if not first_stage_hits:
            return []
        
        # ç¬¬äºŒæ®µéš: è©³ç´°ãªå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        candidates = []
        for hit in first_stage_hits:
            nutrition_match = self._convert_es_hit_to_nutrition_match(hit, search_term)
            
            # è©³ç´°ãªé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆRapidFuzzã‚’ä½¿ç”¨ï¼‰
            if self.enable_fuzzy_matching:
                enhanced_score = self._calculate_enhanced_similarity_score(
                    nutrition_match, 
                    search_term, 
                    structured_data
                )
                nutrition_match.score = enhanced_score
            
            candidates.append(nutrition_match)
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        candidates.sort(key=lambda x: x.score, reverse=True)
        
        # ä¸Šä½çµæœã‚’è¿”ã™
        final_results = candidates[:self.final_result_size]
        
        self.log_processing_detail("second_stage_results_count", len(final_results))
        if final_results:
            self.log_processing_detail("top_result_score", final_results[0].score)
        
        return final_results
    
    def _calculate_enhanced_similarity_score(
        self, 
        nutrition_match: NutritionMatch, 
        search_term: str, 
        structured_data: Dict[str, Any]
    ) -> float:
        """RapidFuzzã‚’ä½¿ç”¨ã—ãŸè©³ç´°ãªé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if not RAPIDFUZZ_AVAILABLE:
            return nutrition_match.score
        
        base_score = nutrition_match.score
        db_name = nutrition_match.name.lower()
        search_lower = search_term.lower()
        
        # Jaro-Winkler é¡ä¼¼åº¦
        jaro_winkler_score = JaroWinkler.similarity(db_name, search_lower)
        
        # Levenshtein é¡ä¼¼åº¦ï¼ˆæ­£è¦åŒ–ï¼‰
        levenshtein_score = 1.0 - (Levenshtein.distance(db_name, search_lower) / max(len(db_name), len(search_lower)))
        
        # FuzzyWuzzy Ratio
        fuzzy_ratio = ratio(db_name, search_lower) / 100.0
        
        # FuzzyWuzzy Partial Ratio
        partial_ratio_score = partial_ratio(db_name, search_lower) / 100.0
        
        # é‡ã¿ä»˜ãçµ„ã¿åˆã‚ã›
        similarity_boost = (
            jaro_winkler_score * 0.4 +
            levenshtein_score * 0.3 +
            fuzzy_ratio * 0.2 +
            partial_ratio_score * 0.1
        )
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹
        brands = structured_data.get('brands', [])
        brand_bonus = 0.0
        for brand in brands:
            if brand and brand.lower() in db_name:
                brand_bonus = 0.3
                break
        
        # æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
        enhanced_score = base_score * (1.0 + similarity_boost + brand_bonus)
        
        return enhanced_score
    
    async def _single_stage_advanced_search(
        self, 
        advanced_query: Dict[str, Any], 
        search_term: str
    ) -> List[NutritionMatch]:
        """å˜æ®µéšé«˜åº¦æ¤œç´¢ã®å®Ÿè¡Œ"""
        
        response = self.es_client.search(
            index=self.index_name,
            body=advanced_query
        )
        
        hits = response.get('hits', {}).get('hits', [])
        
        results = []
        for hit in hits:
            nutrition_match = self._convert_es_hit_to_nutrition_match(hit, search_term)
            results.append(nutrition_match)
        
        return results
    
    def _create_error_response(self, error_message: str) -> NutritionQueryOutput:
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ"""
        return NutritionQueryOutput(
            matches={},
            search_summary={
                "total_searches": 0,
                "successful_matches": 0,
                "failed_searches": 0,
                "match_rate_percent": 0,
                "search_method": "error",
                "search_time_ms": 0
            },
            errors=[error_message]
        )

    async def _elasticsearch_search(self, input_data: NutritionQueryInput, search_terms: List[str]) -> NutritionQueryOutput:
        """
        Elasticsearchã‚’ä½¿ç”¨ã—ãŸå˜ä¸€çµæœæ¤œç´¢ï¼ˆå¾“æ¥ã®æ–¹å¼ï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            search_terms: æ¤œç´¢èªå½™ãƒªã‚¹ãƒˆ
            
        Returns:
            NutritionQueryOutput: Elasticsearchæ¤œç´¢çµæœ
        """
        matches = {}
        warnings = []
        errors = []
        successful_matches = 0
        total_searches = len(search_terms)
        
        start_time = datetime.now()
        
        # å„æ¤œç´¢èªå½™ã«ã¤ã„ã¦Elasticsearchæ¤œç´¢ã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Elasticsearch search for: {search_term}")
            
            self.log_processing_detail(f"es_search_{search_index}_term", search_term)
            
            try:
                # Elasticsearchã‚¯ã‚¨ãƒªã®æ§‹ç¯‰
                es_query = self._build_elasticsearch_query(search_term, input_data)
                
                # æ¤œç´¢å®Ÿè¡Œ
                response = self.es_client.search(
                    index=self.index_name,
                    body=es_query
                )
                
                # çµæœå‡¦ç†
                hits = response.get('hits', {}).get('hits', [])
                
                if hits:
                    # æœ€è‰¯ã®ãƒãƒƒãƒã‚’é¸æŠ
                    best_hit = hits[0]
                    source = best_hit['_source']
                    score = best_hit['_score']
                    
                    match_result = self._convert_es_hit_to_nutrition_match(best_hit, search_term)
                    matches[search_term] = match_result
                    successful_matches += 1
                    
                    self.log_reasoning(
                        f"es_match_{search_index}",
                        f"Found Elasticsearch match for '{search_term}': {source.get('search_name', 'N/A')} (score: {score:.3f}, db: {source.get('source_db', 'N/A')})"
                    )
                    
                    self.logger.debug(f"ES match for '{search_term}': {source.get('search_name', 'N/A')} from {source.get('source_db', 'N/A')}")
                else:
                    self.log_reasoning(
                        f"es_no_match_{search_index}",
                        f"No Elasticsearch match found for '{search_term}'"
                    )
                    warnings.append(f"No Elasticsearch match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Elasticsearch search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                self.log_reasoning(
                    f"es_search_error_{search_index}",
                    f"Elasticsearch search error for '{search_term}': {str(e)}"
                )
        
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "elasticsearch",
            "database_source": "elasticsearch_nutrition_db",
            "preferred_source": input_data.preferred_source,
            "search_time_ms": search_time_ms,
            "index_name": self.index_name,
            "total_indexed_documents": await self._get_total_document_count()
        }
        
        self.log_processing_detail("elasticsearch_search_summary", search_summary)
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Elasticsearch nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%}) in {search_time_ms}ms")
        
        return result
    
    async def _elasticsearch_strategic_search(self, input_data: NutritionQueryInput, search_terms: List[str]) -> NutritionQueryOutput:
        """
        æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ï¼ˆdish/ingredientåˆ¥ã®æœ€é©åŒ–ã•ã‚ŒãŸæ¤œç´¢ï¼‰
        
        Dishæˆ¦ç•¥:
        - ãƒ¡ã‚¤ãƒ³: EatThisMuch data_type=dish
        - è£œåŠ©: EatThisMuch data_type=branded (ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆ)
        
        Ingredientæˆ¦ç•¥:
        - ãƒ¡ã‚¤ãƒ³: EatThisMuch data_type=ingredient  
        - è£œåŠ©: MyNetDiary, YAZIO, EatThisMuch branded
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            search_terms: æ¤œç´¢èªå½™ãƒªã‚¹ãƒˆ
            
        Returns:
            NutritionQueryOutput: æˆ¦ç•¥çš„æ¤œç´¢çµæœ
        """
        matches = {}
        warnings = []
        errors = []
        successful_matches = 0
        total_searches = len(search_terms)
        
        start_time = datetime.now()
        
        # å„æ¤œç´¢èªå½™ã«ã¤ã„ã¦æˆ¦ç•¥çš„æ¤œç´¢ã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Strategic Elasticsearch search for: {search_term}")
            
            self.log_processing_detail(f"strategic_search_{search_index}_term", search_term)
            
            try:
                # ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
                query_type = "dish" if search_term in input_data.dish_names else "ingredient"
                self.log_processing_detail(f"search_{search_index}_type", query_type)
                
                # æˆ¦ç•¥çš„æ¤œç´¢ã‚’å®Ÿè¡Œ
                if query_type == "dish":
                    strategic_results = await self._strategic_dish_search(search_term, input_data)
                else:
                    strategic_results = await self._strategic_ingredient_search(search_term, input_data)
                
                if strategic_results:
                    matches[search_term] = strategic_results
                    successful_matches += 1
                    
                    self.log_reasoning(
                        f"strategic_match_{search_index}",
                        f"Found strategic matches for '{search_term}' ({query_type}): {len(strategic_results)} results"
                    )
                else:
                    self.log_reasoning(
                        f"strategic_no_results_{search_index}",
                        f"No strategic results for '{search_term}' ({query_type})"
                    )
                    warnings.append(f"No strategic results found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Strategic Elasticsearch search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                self.log_reasoning(
                    f"strategic_search_error_{search_index}",
                    f"Strategic Elasticsearch search error for '{search_term}': {str(e)}"
                )
        
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        # æˆ¦ç•¥çš„æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        total_results = sum(len(result_list) if isinstance(result_list, list) else 1 for result_list in matches.values())
        
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "elasticsearch_strategic",
            "database_source": "elasticsearch_nutrition_db",
            "preferred_source": input_data.preferred_source,
            "search_time_ms": search_time_ms,
            "index_name": self.index_name,
            "total_indexed_documents": await self._get_total_document_count(),
            "strategic_approach": {
                "dish_strategy": "eatthismuch_dish_primary + eatthismuch_branded_fallback",
                "ingredient_strategy": "eatthismuch_ingredient_primary + multi_db_fallback"
            },
            "total_results": total_results
        }
        
        self.log_processing_detail("elasticsearch_strategic_search_summary", search_summary)
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Strategic Elasticsearch nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%}) with {total_results} total results in {search_time_ms}ms")
        
        return result

    async def _strategic_dish_search(self, search_term: str, input_data: NutritionQueryInput) -> List[NutritionMatch]:
        """
        Dishæ¤œç´¢æˆ¦ç•¥ã‚’å®Ÿè¡Œ
        
        æˆ¦ç•¥:
        1. EatThisMuch data_type=dish ã‚’ãƒ¡ã‚¤ãƒ³æ¤œç´¢
        2. ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆã¯ EatThisMuch data_type=branded ã‚’è£œåŠ©æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            List[NutritionMatch]: æˆ¦ç•¥çš„æ¤œç´¢çµæœ
        """
        results = []
        MIN_SCORE_THRESHOLD = 20.0  # ã‚¹ã‚³ã‚¢é–¾å€¤
        
        self.logger.info(f"Strategic dish search for '{search_term}': EatThisMuch dish -> branded fallback")
        
        # Step 1: EatThisMuch dish ã‚’ãƒ¡ã‚¤ãƒ³æ¤œç´¢
        main_query = self._build_strategic_query(search_term, "eatthismuch", "dish")
        
        try:
            response = self.es_client.search(index=self.index_name, body=main_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                best_score = hits[0].get('_score', 0)
                self.logger.info(f"Dish main search: found {len(hits)} results, best score: {best_score}")
                
                if best_score >= MIN_SCORE_THRESHOLD:
                    # é«˜ã‚¹ã‚³ã‚¢: ãƒ¡ã‚¤ãƒ³çµæœã®ã¿ä½¿ç”¨
                    for hit in hits[:self.results_per_db]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "main_dish"
                        match.search_metadata["strategy_type"] = "dish_primary"
                        results.append(match)
                    
                    self.logger.info(f"High score dish results: using {len(results)} main results")
                    return results
                else:
                    # ä½ã‚¹ã‚³ã‚¢: ãƒ¡ã‚¤ãƒ³çµæœã‚’ä¿æŒã—ã¦è£œåŠ©æ¤œç´¢ã‚‚å®Ÿè¡Œ
                    for hit in hits[:max(1, self.results_per_db // 2)]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "main_dish_low_score"
                        match.search_metadata["strategy_type"] = "dish_primary"
                        results.append(match)
        
        except Exception as e:
            self.logger.error(f"Error in dish main search: {e}")
        
        # Step 2: EatThisMuch branded ã‚’è£œåŠ©æ¤œç´¢
        fallback_query = self._build_strategic_query(search_term, "eatthismuch", "branded")
        
        try:
            response = self.es_client.search(index=self.index_name, body=fallback_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                remaining_slots = self.results_per_db - len(results)
                for hit in hits[:remaining_slots]:
                    match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                    match.search_metadata["strategic_phase"] = "fallback_branded"
                    match.search_metadata["strategy_type"] = "dish_fallback"
                    results.append(match)
                
                self.logger.info(f"Dish fallback search: added {min(len(hits), remaining_slots)} branded results")
        
        except Exception as e:
            self.logger.error(f"Error in dish fallback search: {e}")
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.score, reverse=True)
        
        self.logger.info(f"Strategic dish search completed: {len(results)} total results")
        return results

    async def _strategic_ingredient_search(self, search_term: str, input_data: NutritionQueryInput) -> List[NutritionMatch]:
        """
        Ingredientæ¤œç´¢æˆ¦ç•¥ã‚’å®Ÿè¡Œ
        
        æˆ¦ç•¥:
        1. EatThisMuch data_type=ingredient ã‚’ãƒ¡ã‚¤ãƒ³æ¤œç´¢
        2. MyNetDiary, YAZIO, EatThisMuch branded ã‚’è£œåŠ©æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            List[NutritionMatch]: æˆ¦ç•¥çš„æ¤œç´¢çµæœ
        """
        results = []
        
        self.logger.info(f"Strategic ingredient search for '{search_term}': EatThisMuch ingredient -> multi-DB fallback")
        
        # Step 1: EatThisMuch ingredient ã‚’ãƒ¡ã‚¤ãƒ³æ¤œç´¢
        main_query = self._build_strategic_query(search_term, "eatthismuch", "ingredient")
        
        try:
            response = self.es_client.search(index=self.index_name, body=main_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                # ãƒ¡ã‚¤ãƒ³çµæœã‚’è¿½åŠ ï¼ˆæœ€å¤§åŠåˆ†ã®ã‚¹ãƒ­ãƒƒãƒˆä½¿ç”¨ï¼‰
                main_slots = max(1, self.results_per_db // 2)
                for hit in hits[:main_slots]:
                    match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                    match.search_metadata["strategic_phase"] = "main_ingredient"
                    match.search_metadata["strategy_type"] = "ingredient_primary"
                    results.append(match)
                
                self.logger.info(f"Ingredient main search: added {len(results)} primary results")
        
        except Exception as e:
            self.logger.error(f"Error in ingredient main search: {e}")
        
        # Step 2: è£œåŠ©ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
        fallback_sources = [
            ("mynetdiary", "unified"),
            ("yazio", "unified"), 
            ("eatthismuch", "branded")
        ]
        
        remaining_slots = self.results_per_db - len(results)
        slots_per_source = max(1, remaining_slots // len(fallback_sources))
        
        for db_name, data_type in fallback_sources:
            if remaining_slots <= 0:
                break
                
            try:
                fallback_query = self._build_strategic_query(search_term, db_name, data_type)
                response = self.es_client.search(index=self.index_name, body=fallback_query)
                hits = response.get('hits', {}).get('hits', [])
                
                if hits:
                    current_slots = min(slots_per_source, remaining_slots)
                    for hit in hits[:current_slots]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "fallback_multi_db"
                        match.search_metadata["strategy_type"] = "ingredient_fallback"
                        match.search_metadata["fallback_source"] = f"{db_name}_{data_type}"
                        results.append(match)
                    
                    remaining_slots -= len(hits[:current_slots])
                    self.logger.info(f"Ingredient fallback ({db_name}_{data_type}): added {len(hits[:current_slots])} results")
            
            except Exception as e:
                self.logger.error(f"Error in ingredient fallback search ({db_name}_{data_type}): {e}")
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.score, reverse=True)
        
        self.logger.info(f"Strategic ingredient search completed: {len(results)} total results")
        return results

    def _build_strategic_query(self, search_term: str, target_db: str, data_type: str) -> Dict[str, Any]:
        """
        æˆ¦ç•¥çš„æ¤œç´¢ç”¨ã®Elasticsearchã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            search_term: æ¤œç´¢èªå½™
            target_db: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
            data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
            
        Returns:
            Elasticsearchã‚¯ã‚¨ãƒªè¾æ›¸
        """
        query = {
            "size": self.results_per_db,
            "query": {
                "bool": {
                    "must": [
                        {
                            "multi_match": {
                                "query": search_term,
                                "fields": [
                                    "search_name^3",
                                    "description^1"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ],
                    "filter": [
                        {"term": {"source_db": target_db}},
                        {"term": {"data_type": data_type}}
                    ]
                }
            },
            "sort": [
                {"_score": {"order": "desc"}}
            ]
        }
        
        return query
    
    async def _get_total_document_count(self) -> int:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…ã®ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å–å¾—"""
        try:
            stats = self.es_client.indices.stats(index=self.index_name)
            return stats["indices"][self.index_name]["total"]["docs"]["count"]
        except Exception as e:
            self.logger.warning(f"Failed to get document count: {e}")
            return 0
    
    def _build_elasticsearch_query(self, search_term: str, input_data: NutritionQueryInput) -> Dict[str, Any]:
        """
        Elasticsearchæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            search_term: æ¤œç´¢èªå½™
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            Elasticsearchã‚¯ã‚¨ãƒªè¾æ›¸
        """
        # åŸºæœ¬çš„ãªmulti_matchã‚¯ã‚¨ãƒª
        base_query = {
            "multi_match": {
                "query": search_term,
                "fields": [
                    "search_name^3",  # æ¤œç´¢åã«é«˜ã„é‡ã¿
                    "search_name.exact^5"  # å®Œå…¨ä¸€è‡´ã«æœ€é«˜ã®é‡ã¿
                ],
                "type": "best_fields",
                "fuzziness": "AUTO",
                "operator": "OR"
            }
        }
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
        filters = []
        
        # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
        if search_term in input_data.dish_names:
            filters.append({"term": {"data_type": "dish"}})
        # ingredient_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯é£Ÿæãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
        elif search_term in input_data.ingredient_names:
            filters.append({"term": {"data_type": "ingredient"}})
        
        # å„ªå…ˆã‚½ãƒ¼ã‚¹ã®è¨­å®š
        if input_data.preferred_source and input_data.preferred_source != "elasticsearch":
            source_mapping = {
                "yazio": "yazio",
                "mynetdiary": "mynetdiary", 
                "eatthismuch": "eatthismuch"
            }
            if input_data.preferred_source in source_mapping:
                filters.append({"term": {"source_db": source_mapping[input_data.preferred_source]}})
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãŒã‚ã‚‹å ´åˆã¯boolã‚¯ã‚¨ãƒªã§ãƒ©ãƒƒãƒ—
        if filters:
            query = {
                "bool": {
                    "must": [base_query],
                    "should": filters,  # shouldã§å„ªå…ˆåº¦ä»˜ã‘
                    "boost": 1.2
                }
            }
        else:
            query = base_query
        
        return {
            "query": query,
            "size": 5,  # ä¸Šä½5ä»¶ã‚’å–å¾—
            "_source": ["data_type", "id", "search_name", "nutrition", "weight", "source_db", "description"]
        }
    
    def _convert_es_hit_to_nutrition_match(self, hit: Dict[str, Any], search_term: str) -> NutritionMatch:
        """
        Elasticsearchãƒ’ãƒƒãƒˆã‚’NutritionMatchã«å¤‰æ›
        
        Args:
            hit: Elasticsearchãƒ’ãƒƒãƒˆ
            search_term: æ¤œç´¢èªå½™
            
        Returns:
            NutritionMatch
        """
        source = hit['_source']
        score = hit['_score']
        
        # ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
        source_db = source.get('source_db', 'unknown')
        search_name = source.get('search_name', search_term)
        final_source = f"elasticsearch_{source_db}"
        
        return NutritionMatch(
            id=source.get('id', 0),
            name=search_name,  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
            search_name=search_name,
            description=source.get('description'),
            data_type=source.get('data_type', 'unknown'),
            source_db=source_db,  # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
            source=final_source,
            nutrition=source.get('nutrition', {}),
            weight=source.get('weight'),
            score=score,
            search_metadata={
                "search_term": search_term,
                "elasticsearch_score": score,
                "search_method": "elasticsearch_multi_db" if self.multi_db_search_mode else "elasticsearch",
                "source_database": source_db,
                "index_name": self.index_name
            }
        ) 
```

================================================================================

ğŸ“„ FILE: app_v2/components/local_nutrition_search_component.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 21,683 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 12:20:02
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Local Nutrition Search Component

USDA database queryã‚’ nutrition_db_experiment ã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã«ç½®ãæ›ãˆã‚‹
"""

import os
import sys
import json
import asyncio
from typing import Optional, List, Dict, Any
from pathlib import Path

from .base import BaseComponent
from ..models.nutrition_search_models import (
    NutritionQueryInput, NutritionQueryOutput, NutritionMatch
)
from ..config import get_settings

# nutrition_db_experimentã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
NUTRITION_DB_EXPERIMENT_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "nutrition_db_experiment"
)
sys.path.append(NUTRITION_DB_EXPERIMENT_PATH)

class LocalNutritionSearchComponent(BaseComponent[NutritionQueryInput, NutritionQueryOutput]):
    """
    ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    
    nutrition_db_experimentã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦é£Ÿæåã‚’æ¤œç´¢ã—ã€
    ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ã§çµæœã‚’è¿”ã—ã¾ã™ã€‚
    """
    
    def __init__(self):
        super().__init__("LocalNutritionSearchComponent")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self._initialize_local_search_system()
        
        # unified_dbã®ã¿ã‚’ä½¿ç”¨
        self.unified_db_path = os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "unified_nutrition_db.json")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
        self.unified_database = self._load_unified_database()
        
        # nutrition_db_experimentã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.search_handler = None
        self.query_preprocessor = None
        
        self.logger.info(f"LocalNutritionSearchComponent initialized with {len(self.unified_database)} total items")
    
    def _initialize_local_search_system(self):
        """nutrition_db_experimentã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
        try:
            # æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            from api.search_handler import SearchHandler
            from api.query_preprocessor import QueryPreprocessor
            
            self.search_handler = SearchHandler()
            self.query_preprocessor = QueryPreprocessor()
            
            self.logger.info("Advanced local search system initialized")
        except ImportError as e:
            self.logger.warning(f"Advanced search system not available, will use direct database search: {e}")
        except Exception as e:
            self.logger.error(f"Failed to initialize advanced search system: {e}")
    
    def _load_unified_database(self) -> List[Dict[str, Any]]:
        """unified_nutrition_db.jsonã‚’èª­ã¿è¾¼ã¿"""
        try:
            if os.path.exists(self.unified_db_path):
                with open(self.unified_db_path, 'r', encoding='utf-8') as f:
                    database = json.load(f)
                self.logger.info(f"Loaded unified_db: {len(database)} items")
                return database
            else:
                self.logger.warning(f"Unified database file not found: {self.unified_db_path}")
                return []
        except Exception as e:
            self.logger.error(f"Error loading unified_db: {e}")
            return []
    
    async def process(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®ä¸»å‡¦ç†ï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰
        
        Args:
            input_data: NutritionQueryInput
            
        Returns:
            NutritionQueryOutput: ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢çµæœ
        """
        self.logger.info(f"Starting local nutrition search for {len(input_data.get_all_search_terms())} terms")
        
        # input_dataã‚’ä¿å­˜ã—ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—ã§ä½¿ç”¨
        self._current_input_data = input_data
        
        search_terms = input_data.get_all_search_terms()
        
        # æ¤œç´¢å¯¾è±¡ã®è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("ingredient_names", input_data.ingredient_names)
        self.log_processing_detail("dish_names", input_data.dish_names)
        self.log_processing_detail("total_search_terms", len(search_terms))
        self.log_processing_detail("search_method", "local_nutrition_database")
        self.log_processing_detail("preferred_source", input_data.preferred_source)
        
        matches = {}
        warnings = []
        errors = []
        
        successful_matches = 0
        total_searches = len(search_terms)
        
        # å„æ¤œç´¢èªå½™ã«ã¤ã„ã¦ç…§åˆã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Searching local database for: {search_term}")
            
            # æ¤œç´¢é–‹å§‹ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_term", search_term)
            self.log_processing_detail(f"search_{search_index}_start", f"Starting local search for '{search_term}'")
            
            try:
                # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®å®Ÿè¡Œ
                if self.search_handler and self.query_preprocessor:
                    # é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
                    match_result = await self._advanced_local_search(search_term, search_index, input_data)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°
                    match_result = await self._simple_local_search(search_term, search_index, input_data)
                
                if match_result:
                    matches[search_term] = match_result
                    successful_matches += 1
                    self.logger.debug(f"Found local match for '{search_term}': ID {match_result.id}")
                else:
                    self.log_reasoning(
                        f"no_match_{search_index}",
                        f"No local database match found for '{search_term}' - may not exist in local nutrition database"
                    )
                    self.logger.warning(f"No local match found for: {search_term}")
                    warnings.append(f"No local match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Local search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"search_error_{search_index}",
                    f"Local database search error for '{search_term}': {str(e)}"
                )
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "local_nutrition_database",
            "database_source": "nutrition_db_experiment",
            "preferred_source": input_data.preferred_source,
            "total_database_items": len(self.unified_database)
        }
        
        # å…¨ä½“çš„ãªæ¤œç´¢æˆåŠŸç‡ã‚’ãƒ­ã‚°
        overall_success_rate = successful_matches / total_searches if total_searches > 0 else 0
        self.log_processing_detail("search_summary", search_summary)
        
        # æ¤œç´¢å“è³ªã®è©•ä¾¡ã‚’ãƒ­ã‚°
        if overall_success_rate >= 0.8:
            self.log_reasoning("search_quality", "Excellent local search results with high match rate")
        elif overall_success_rate >= 0.6:
            self.log_reasoning("search_quality", "Good local search results with acceptable match rate")
        elif overall_success_rate >= 0.4:
            self.log_reasoning("search_quality", "Moderate local search results, some items may need manual review")
        else:
            self.log_reasoning("search_quality", "Poor local search results, many items not found in local database")
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Local nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%})")
        
        return result
    
    async def _advanced_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        nutrition_db_experimentã®é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        try:
            from api.search_handler import SearchRequest
            
            # æ¤œç´¢ã‚¿ã‚¤ãƒ—ã®æ±ºå®šï¼ˆæ–™ç†ã‹é£Ÿæã‹ã®æ¨å®šï¼‰
            db_type_filter = None  # å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢
            
            # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ã¨ã—ã¦å„ªå…ˆæ¤œç´¢
            if search_term in input_data.dish_names:
                db_type_filter = "dish"
                self.log_processing_detail(f"search_{search_index}_type", "dish")
            elif search_term in input_data.ingredient_names:
                db_type_filter = "ingredient"
                self.log_processing_detail(f"search_{search_index}_type", "ingredient")
            
            # æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
            request = SearchRequest(
                query=search_term,
                db_type_filter=db_type_filter,
                size=5  # ä¸Šä½5ä»¶ã‚’å–å¾—
            )
            
            # æ¤œç´¢å®Ÿè¡Œ
            response = self.search_handler.search(request)
            
            # æ¤œç´¢çµæœã®è©³ç´°ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_results_count", response.total_hits)
            self.log_processing_detail(f"search_{search_index}_processing_time_ms", response.took_ms)
            self.log_processing_detail(f"search_{search_index}_processed_query", response.query_info.get('processed_query'))
            
            if response.results:
                # nutrition_db_experimentã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ãŸå ´åˆã¯ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                best_result = response.results[0]
                
                # æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆIDãŒ123456ã®å ´åˆã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼‰
                if best_result.get('id') == 123456:
                    self.logger.warning(f"nutrition_db_experiment returned mock data for '{search_term}', falling back to direct database search")
                    return await self._direct_database_search(search_term, search_index, input_data)
                
                # ãƒãƒƒãƒé¸æŠã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_result['search_name']}' (ID: {best_result['id']}) for search term '{search_term}' based on local search algorithm (score: {best_result.get('_score', 'N/A')})"
                )
                
                # è©³ç´°ãªãƒãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_selected_id", best_result['id'])
                self.log_processing_detail(f"search_{search_index}_selected_name", best_result['search_name'])
                self.log_processing_detail(f"search_{search_index}_data_type", best_result.get('data_type', 'unknown'))
                self.log_processing_detail(f"search_{search_index}_score", best_result.get('_score'))
                
                # NutritionMatchå½¢å¼ã«å¤‰æ›
                return self._convert_to_nutrition_match(best_result, search_term)
            
            # çµæœãŒãªã„å ´åˆã¯ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._direct_database_search(search_term, search_index, input_data)
            
        except Exception as e:
            self.logger.error(f"Advanced local search failed for '{search_term}': {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._direct_database_search(search_term, search_index, input_data)
    
    async def _direct_database_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        try:
            self.log_processing_detail(f"search_{search_index}_method", "direct_database_search")
            
            search_term_lower = search_term.lower()
            best_match = None
            best_score = 0
            
            # unified_databaseã‹ã‚‰ç›´æ¥æ¤œç´¢
            for item in self.unified_database:
                # search_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§æ¤œç´¢
                if 'search_name' not in item:
                    continue
                    
                item_name = item['search_name'].lower()
                score = 0
                
                # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                if search_term_lower == item_name:
                    score = 1.0  # å®Œå…¨ä¸€è‡´
                elif search_term_lower in item_name:
                    # éƒ¨åˆ†ä¸€è‡´ï¼ˆèªé †è€ƒæ…®ï¼‰
                    if item_name.startswith(search_term_lower):
                        score = 0.9  # å‰æ–¹ä¸€è‡´
                    elif item_name.endswith(search_term_lower):
                        score = 0.8  # å¾Œæ–¹ä¸€è‡´
                    else:
                        score = 0.7  # ä¸­é–“ä¸€è‡´
                elif item_name in search_term_lower:
                    score = 0.6  # é€†éƒ¨åˆ†ä¸€è‡´
                else:
                    # å˜èªãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    search_words = search_term_lower.split()
                    item_words = item_name.split()
                    
                    common_words = set(search_words) & set(item_words)
                    if common_words:
                        score = len(common_words) / max(len(search_words), len(item_words)) * 0.5
                
                # data_typeå„ªå…ˆåº¦ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
                data_type = item.get('data_type', 'unknown')
                db_bonus = 1.0
                
                # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
                if search_term in input_data.dish_names and data_type == 'dish':
                    db_bonus = 1.2
                # ingredient_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯é£Ÿæãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ
                elif search_term in input_data.ingredient_names and data_type == 'ingredient':
                    db_bonus = 1.2
                
                final_score = score * db_bonus
                
                if final_score > best_score:
                    best_score = final_score
                    best_match = item.copy()
            
            if best_match and best_score > 0.1:  # æœ€ä½é–¾å€¤
                # ãƒãƒƒãƒã‚¹ã‚³ã‚¢æƒ…å ±ã‚’è¿½åŠ 
                best_match['_match_score'] = best_score
                
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_match['search_name']}' (ID: {best_match.get('id', 'N/A')}) for search term '{search_term}' using direct database search (score: {best_score:.3f})"
                )
                
                # è©³ç´°ãªãƒãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_selected_id", best_match.get('id', 'N/A'))
                self.log_processing_detail(f"search_{search_index}_selected_name", best_match['search_name'])
                self.log_processing_detail(f"search_{search_index}_data_type", best_match.get('data_type', 'unknown'))
                self.log_processing_detail(f"search_{search_index}_match_score", best_score)
                
                return self._convert_to_nutrition_match(best_match, search_term)
            
            return None
            
        except Exception as e:
            self.logger.error(f"Direct database search failed for '{search_term}': {e}")
            return None
    
    async def _simple_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½¿ç”¨ï¼‰
        
        Args:
            search_term: æ¤œç´¢èªå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        # é«˜åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã€ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨
        return await self._direct_database_search(search_term, search_index, input_data)
    
    def _convert_to_nutrition_match(self, local_item: Dict[str, Any], search_term: str) -> NutritionMatch:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã‚’NutritionMatchå½¢å¼ã«å¤‰æ›ï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        
        Args:
            local_item: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¤ãƒ†ãƒ 
            search_term: å…ƒã®æ¤œç´¢èªå½™
            
        Returns:
            NutritionMatch: å¤‰æ›ã•ã‚ŒãŸãƒãƒƒãƒçµæœï¼ˆç°¡ç´ åŒ–ã•ã‚ŒãŸãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰
        """
        # IDã®å–å¾—
        item_id = local_item.get('id', 0)
        
        # åŸºæœ¬æƒ…å ±ã®å–å¾—
        search_name = local_item.get('search_name', search_term)
        description = local_item.get('description')  # brandedã®å ´åˆã®ã¿å­˜åœ¨
        data_type = local_item.get('data_type', 'unknown')  # db_type â†’ data_typeã«å¤‰æ›´
        
        # æ „é¤Šãƒ‡ãƒ¼ã‚¿ï¼ˆ100gã‚ãŸã‚Šæ­£è¦åŒ–æ¸ˆã¿ï¼‰
        nutrition = local_item.get('nutrition', {})
        weight = local_item.get('weight')
        
        # ãƒãƒƒãƒã‚¹ã‚³ã‚¢
        score = local_item.get('_match_score') or local_item.get('_score') or 1.0
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ã®è©³ç´°åˆ†æ
        search_term_lower = search_term.lower()
        item_name_lower = search_name.lower()
        
        # åŸºæœ¬ãƒãƒƒãƒã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        match_type = "unknown"
        base_score = 0.0
        if search_term_lower == item_name_lower:
            match_type = "exact_match"
            base_score = 1.0
        elif search_term_lower in item_name_lower:
            if item_name_lower.startswith(search_term_lower):
                match_type = "prefix_match"
                base_score = 0.9
            elif item_name_lower.endswith(search_term_lower):
                match_type = "suffix_match"
                base_score = 0.8
            else:
                match_type = "contains_match"
                base_score = 0.7
        elif item_name_lower in search_term_lower:
            match_type = "reverse_contains"
            base_score = 0.6
        else:
            # å˜èªãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´
            search_words = set(search_term_lower.split())
            item_words = set(item_name_lower.split())
            common_words = search_words & item_words
            if common_words:
                match_type = "word_match"
                base_score = len(common_words) / max(len(search_words), len(item_words)) * 0.5
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ãƒœãƒ¼ãƒŠã‚¹ã®è¨ˆç®—
        type_bonus = 1.0
        if hasattr(self, '_current_input_data'):
            input_data = self._current_input_data
            if search_term in input_data.dish_names and data_type == 'dish':
                type_bonus = 1.2
            elif search_term in input_data.ingredient_names and data_type == 'ingredient':
                type_bonus = 1.2
        
        # æ¤œç´¢ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆè©³ç´°ãªè¨ˆç®—æƒ…å ±ã‚’å«ã‚€ï¼‰
        search_metadata = {
            "search_term": search_term,
            "match_score": score,
            "score_breakdown": {
                "match_type": match_type,
                "base_score": round(base_score, 3),
                "type_bonus": round(type_bonus, 3),
                "final_score": round(base_score * type_bonus, 3)
            },
            "calculation": f"{base_score:.3f} Ã— {type_bonus:.3f} = {score:.3f}"
        }
        
        # NutritionMatchã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆï¼ˆç°¡ç´ åŒ–ç‰ˆï¼‰
        return NutritionMatch(
            id=item_id,
            search_name=search_name,
            description=description,
            data_type=data_type,  # db_type â†’ data_typeã«å¤‰æ›´
            nutrition=nutrition,
            weight=weight,
            score=score,
            search_metadata=search_metadata
        ) 
```

================================================================================

ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å±¤
================================================================================

ğŸ“„ FILE: app_v2/models/__init__.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 680 bytes
æœ€çµ‚æ›´æ–°: 2025-06-09 12:06:09
å­˜åœ¨: âœ…

CONTENT:
```python
from .phase1_models import *
from .usda_models import *
from .phase2_models import *
from .nutrition_models import *
from .nutrition_search_models import *

__all__ = [
    # Phase1 models
    "Phase1Input", "Phase1Output", "Ingredient", "Dish",
    
    # USDA models
    "USDAQueryInput", "USDAQueryOutput", "USDAMatch", "USDANutrient",
    
    # Phase2 models
    "Phase2Input", "Phase2Output", "RefinedDish", "RefinedIngredient",
    
    # Nutrition models
    "NutritionInput", "NutritionOutput", "CalculatedNutrients", "TotalNutrients",
    
    # Nutrition Search models (ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼)
    "NutritionQueryInput", "NutritionQueryOutput", "NutritionMatch"
] 
```

================================================================================

ğŸ“„ FILE: app_v2/models/nutrition_search_models.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 7,863 bytes
æœ€çµ‚æ›´æ–°: 2025-06-11 10:13:59
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Nutrition Search Models

ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã§ä½¿ç”¨ã™ã‚‹ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ åŒ–å…¥åŠ›å¯¾å¿œï¼‰
"""

from typing import List, Dict, Optional, Any, Union
from pydantic import BaseModel, Field


class NutritionMatch(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆçµæœãƒ¢ãƒ‡ãƒ«ï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰"""
    id: Union[int, str] = Field(..., description="é£Ÿå“IDï¼ˆãƒ­ãƒ¼ã‚«ãƒ«IDï¼‰")
    name: str = Field(..., description="é£Ÿå“å")  # search_nameã‹ã‚‰å¤‰æ›´
    search_name: str = Field(..., description="æ¤œç´¢åï¼ˆç°¡æ½”ãªåç§°ï¼‰")
    description: Optional[str] = Field(None, description="è©³ç´°èª¬æ˜")
    data_type: str = Field(..., description="ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (dish, ingredient, branded)")
    source_db: str = Field(..., description="ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆyazio, mynetdiary, eatthismuchï¼‰")
    source: str = Field(default="local_database", description="ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ'local_database'ï¼‰")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«DBã®ç”Ÿã®æ „é¤Šãƒ‡ãƒ¼ã‚¿ï¼ˆ100gã‚ãŸã‚Šæ­£è¦åŒ–æ¸ˆã¿ï¼‰
    nutrition: Dict[str, float] = Field(default_factory=dict, description="ãƒ­ãƒ¼ã‚«ãƒ«DBã®æ „é¤Šãƒ‡ãƒ¼ã‚¿ï¼ˆ100gã‚ãŸã‚Šï¼‰")
    weight: Optional[float] = Field(None, description="å…ƒãƒ‡ãƒ¼ã‚¿ã®é‡é‡ï¼ˆgï¼‰")
    
    # æ¤œç´¢ã‚¹ã‚³ã‚¢
    score: Optional[float] = Field(None, description="æ¤œç´¢çµæœã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢")
    
    # æ¤œç´¢ã«é–¢ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    search_metadata: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ã«é–¢ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")


class AdvancedSearchOptions(BaseModel):
    """é«˜åº¦ãªæ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³"""
    enable_fuzzy_matching: bool = Field(default=True, description="ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    enable_two_stage_search: bool = Field(default=True, description="äºŒæ®µéšæ¤œç´¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    primary_term_boost: float = Field(default=3.0, description="ãƒ—ãƒ©ã‚¤ãƒãƒªç”¨èªã®ãƒ–ãƒ¼ã‚¹ãƒˆå€¤")
    brand_boost: float = Field(default=2.5, description="ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ã®ãƒ–ãƒ¼ã‚¹ãƒˆå€¤")
    ingredient_boost: float = Field(default=1.5, description="ææ–™æƒ…å ±ã®ãƒ–ãƒ¼ã‚¹ãƒˆå€¤")
    preparation_boost: float = Field(default=1.2, description="èª¿ç†æ³•æƒ…å ±ã®ãƒ–ãƒ¼ã‚¹ãƒˆå€¤")
    jaro_winkler_threshold: float = Field(default=0.8, description="Jaro-Winkleré¡ä¼¼åº¦ã®é–¾å€¤")
    levenshtein_threshold: float = Field(default=0.7, description="Levenshteiné¡ä¼¼åº¦ã®é–¾å€¤")
    first_stage_size: int = Field(default=50, description="ç¬¬ä¸€æ®µéšã§å–å¾—ã™ã‚‹å€™è£œæ•°")
    final_result_size: int = Field(default=10, description="æœ€çµ‚çµæœæ•°")


class NutritionQueryInput(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ åŒ–å…¥åŠ›å¯¾å¿œï¼‰"""
    ingredient_names: List[str] = Field(default_factory=list, description="é£Ÿæåã®ãƒªã‚¹ãƒˆ")
    dish_names: List[str] = Field(default_factory=list, description="æ–™ç†åã®ãƒªã‚¹ãƒˆ")
    search_options: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    preferred_source: str = Field(default="local_database", description="å„ªå…ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
    
    # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒ¼ãƒˆ
    structured_analysis: Optional[Dict[str, Any]] = Field(None, description="Phase1ã‹ã‚‰ã®æ§‹é€ åŒ–åˆ†æãƒ‡ãƒ¼ã‚¿")
    phase1_output: Optional[Any] = Field(None, description="Phase1Outputã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰")
    
    # é«˜åº¦ãªæ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    advanced_search_options: Optional[AdvancedSearchOptions] = Field(None, description="é«˜åº¦ãªæ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    
    # æ¤œç´¢æˆ¦ç•¥
    search_strategy: str = Field(default="basic", description="æ¤œç´¢æˆ¦ç•¥ï¼ˆbasic, strategic, advanced_structuredï¼‰")

    def get_all_search_terms(self) -> List[str]:
        """å…¨ã¦ã®æ¤œç´¢èªå½™ã‚’å–å¾—"""
        return list(set(self.ingredient_names + self.dish_names))
    
    def get_structured_search_terms(self) -> Optional[Dict[str, Any]]:
        """æ§‹é€ åŒ–ã•ã‚ŒãŸæ¤œç´¢ç”¨èªã‚’å–å¾—"""
        if self.phase1_output and hasattr(self.phase1_output, 'get_structured_search_terms'):
            return self.phase1_output.get_structured_search_terms()
        elif self.structured_analysis:
            return self.structured_analysis
        else:
            return None
    
    def get_primary_search_terms(self) -> List[str]:
        """ãƒ—ãƒ©ã‚¤ãƒãƒªæ¤œç´¢ç”¨èªã‚’å–å¾—ï¼ˆé«˜ä¿¡é ¼åº¦ã‚¢ã‚¤ãƒ†ãƒ ï¼‰"""
        if self.phase1_output and hasattr(self.phase1_output, 'get_primary_search_terms'):
            return self.phase1_output.get_primary_search_terms()
        else:
            return self.get_all_search_terms()
    
    def has_structured_data(self) -> bool:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return (
            self.structured_analysis is not None or
            (self.phase1_output and hasattr(self.phase1_output, 'detected_food_items'))
        )
    
    def is_advanced_search_enabled(self) -> bool:
        """é«˜åº¦ãªæ¤œç´¢ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        return (
            self.search_strategy in ["advanced_structured", "strategic"] or
            self.has_structured_data()
        )


class NutritionQueryOutput(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œï¼‰"""
    # ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢å¯¾å¿œï¼šå˜ä¸€çµæœã¾ãŸã¯ãƒªã‚¹ãƒˆçµæœã‚’å—ã‘å…¥ã‚Œã‚‹
    matches: Dict[str, Union[NutritionMatch, List[NutritionMatch]]] = Field(
        default_factory=dict, 
        description="æ¤œç´¢èªå½™ã¨å¯¾å¿œã™ã‚‹ç…§åˆçµæœã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå˜ä¸€çµæœã¾ãŸã¯ãƒãƒ«ãƒDBçµæœãƒªã‚¹ãƒˆï¼‰"
    )
    search_summary: Dict[str, Any] = Field(
        default_factory=dict, 
        description="æ¤œç´¢çµæœã®ã‚µãƒãƒªãƒ¼æƒ…å ±ï¼ˆæŸ”è»Ÿãªå‹å¯¾å¿œï¼‰"
    )
    warnings: Optional[List[str]] = Field(None, description="è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ")
    errors: Optional[List[str]] = Field(None, description="ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ")
    
    # é«˜åº¦ãªæ¤œç´¢çµæœã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    advanced_search_metadata: Optional[Dict[str, Any]] = Field(None, description="é«˜åº¦ãªæ¤œç´¢ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")

    def get_match_rate(self) -> float:
        """ç…§åˆæˆåŠŸç‡ã‚’è¨ˆç®—"""
        total_searches = self.search_summary.get("total_searches", 0)
        successful_matches = self.search_summary.get("successful_matches", 0)
        if total_searches == 0:
            return 0.0
        return successful_matches / total_searches

    def get_total_matches(self) -> int:
        """ç·ç…§åˆä»¶æ•°ã‚’å–å¾—ï¼ˆãƒãƒ«ãƒDBæ¤œç´¢å¯¾å¿œï¼‰"""
        total = 0
        for match_result in self.matches.values():
            if isinstance(match_result, list):
                total += len(match_result)
            else:
                total += 1
        return total
    
    def get_total_individual_results(self) -> int:
        """å€‹åˆ¥çµæœã®ç·æ•°ã‚’å–å¾—ï¼ˆãƒãƒ«ãƒDBæ¤œç´¢ç”¨ï¼‰"""
        return self.get_total_matches()
    
    def has_errors(self) -> bool:
        """ã‚¨ãƒ©ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.errors is not None and len(self.errors) > 0
    
    def has_warnings(self) -> bool:
        """è­¦å‘ŠãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.warnings is not None and len(self.warnings) > 0
    
    def get_search_method(self) -> str:
        """ä½¿ç”¨ã•ã‚ŒãŸæ¤œç´¢æ–¹æ³•ã‚’å–å¾—"""
        return self.search_summary.get("search_method", "unknown")
    
    def is_advanced_search_result(self) -> bool:
        """é«˜åº¦ãªæ¤œç´¢ã®çµæœã‹ãƒã‚§ãƒƒã‚¯"""
        search_method = self.get_search_method()
        return search_method in [
            "advanced_structured_elasticsearch", 
            "elasticsearch_strategic",
            "two_stage_search"
        ] 
```

================================================================================

ğŸ“„ FILE: app_v2/models/phase1_models.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 6,284 bytes
æœ€çµ‚æ›´æ–°: 2025-06-11 10:53:15
å­˜åœ¨: âœ…

CONTENT:
```python
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field
from enum import Enum


class AttributeType(str, Enum):
    """å±æ€§ã‚¿ã‚¤ãƒ—ã®åˆ—æŒ™"""
    INGREDIENT = "ingredient"
    PREPARATION = "preparation"
    COLOR = "color"
    TEXTURE = "texture"
    COOKING_METHOD = "cooking_method"
    SERVING_STYLE = "serving_style"
    ALLERGEN = "allergen"


class FoodAttribute(BaseModel):
    """é£Ÿå“å±æ€§ãƒ¢ãƒ‡ãƒ«ï¼ˆææ–™ã€èª¿ç†æ³•ãªã©ï¼‰"""
    type: AttributeType = Field(..., description="å±æ€§ã®ã‚¿ã‚¤ãƒ—")
    value: str = Field(..., description="å±æ€§ã®å€¤")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ã“ã®å±æ€§ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢")


class DetectedFoodItem(BaseModel):
    """æ¤œå‡ºã•ã‚ŒãŸé£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆæ§‹é€ åŒ–ï¼‰"""
    item_name: str = Field(..., description="é£Ÿå“åï¼ˆä¸»è¦ãªå€™è£œï¼‰")
    confidence: float = Field(..., ge=0.0, le=1.0, description="é£Ÿå“åã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢")
    attributes: List[FoodAttribute] = Field(default=[], description="é£Ÿå“ã®å±æ€§ãƒªã‚¹ãƒˆï¼ˆææ–™ã€èª¿ç†æ³•ãªã©ï¼‰")
    brand: Optional[str] = Field(None, description="èªè­˜ã•ã‚ŒãŸãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰")
    category_hints: List[str] = Field(default=[], description="æ¨å®šã•ã‚Œã‚‹é£Ÿå“ã‚«ãƒ†ã‚´ãƒª")
    negative_cues: List[str] = Field(default=[], description="ç”»åƒã‹ã‚‰åˆ¤æ–­ã§ãã‚‹ã€Œå«ã¾ã‚Œãªã„ã€è¦ç´ ")


class Ingredient(BaseModel):
    """é£Ÿææƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç”¨ãƒ»å¾“æ¥äº’æ›æ€§ï¼‰"""
    ingredient_name: str = Field(..., description="é£Ÿæã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ï¼‰")
    confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="é£Ÿæç‰¹å®šã®ä¿¡é ¼åº¦")
    detected_attributes: List[FoodAttribute] = Field(default=[], description="ã“ã®é£Ÿæã«é–¢é€£ã™ã‚‹å±æ€§")


class Dish(BaseModel):
    """æ–™ç†æƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç”¨ãƒ»å¾“æ¥äº’æ›æ€§ï¼‰"""
    dish_name: str = Field(..., description="ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ï¼‰")
    confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="æ–™ç†ç‰¹å®šã®ä¿¡é ¼åº¦")
    ingredients: List[Ingredient] = Field(..., description="ãã®æ–™ç†ã«å«ã¾ã‚Œã‚‹é£Ÿæã®ãƒªã‚¹ãƒˆ")
    detected_attributes: List[FoodAttribute] = Field(default=[], description="ã“ã®æ–™ç†ã«é–¢é€£ã™ã‚‹å±æ€§")


class Phase1Input(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    image_bytes: bytes = Field(..., description="ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆå½¢å¼ï¼‰")
    image_mime_type: str = Field(..., description="ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—")
    optional_text: Optional[str] = Field(None, description="ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±")

    class Config:
        arbitrary_types_allowed = True


class Phase1Output(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ åŒ–ãƒ»æ‹¡å¼µç‰ˆï¼‰"""
    # æ–°ã—ã„æ§‹é€ åŒ–å‡ºåŠ›
    detected_food_items: List[DetectedFoodItem] = Field(default=[], description="èªè­˜ã•ã‚ŒãŸé£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆï¼ˆæ§‹é€ åŒ–ï¼‰")
    
    # å¾“æ¥äº’æ›æ€§ã®ãŸã‚ã®å‡ºåŠ›
    dishes: List[Dish] = Field(..., description="ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆ")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    analysis_confidence: float = Field(..., ge=0.0, le=1.0, description="å…¨ä½“çš„ãªåˆ†æã®ä¿¡é ¼åº¦")
    processing_notes: List[str] = Field(default=[], description="å‡¦ç†ã«é–¢ã™ã‚‹æ³¨è¨˜")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_all_ingredient_names(self) -> List[str]:
        """å…¨ã¦ã®é£Ÿæåã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆUSDAæ¤œç´¢ç”¨ãƒ»å¾“æ¥äº’æ›æ€§ï¼‰"""
        ingredient_names = []
        for dish in self.dishes:
            for ingredient in dish.ingredients:
                ingredient_names.append(ingredient.ingredient_name)
        return ingredient_names

    def get_all_dish_names(self) -> List[str]:
        """å…¨ã¦ã®æ–™ç†åã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆUSDAæ¤œç´¢ç”¨ãƒ»å¾“æ¥äº’æ›æ€§ï¼‰"""
        return [dish.dish_name for dish in self.dishes]
    
    def get_structured_search_terms(self) -> Dict[str, Any]:
        """æ§‹é€ åŒ–ã•ã‚ŒãŸæ¤œç´¢ç”¨èªã‚’å–å¾—ï¼ˆæ–°ã—ã„æ¤œç´¢æˆ¦ç•¥ç”¨ï¼‰"""
        return {
            "high_confidence_items": [
                {
                    "item_name": item.item_name,
                    "confidence": item.confidence,
                    "brand": item.brand
                }
                for item in self.detected_food_items 
                if item.confidence >= 0.8
            ],
            "medium_confidence_items": [
                {
                    "item_name": item.item_name,
                    "confidence": item.confidence,
                    "brand": item.brand
                }
                for item in self.detected_food_items 
                if 0.5 <= item.confidence < 0.8
            ],
            "brands": [
                item.brand for item in self.detected_food_items 
                if item.brand is not None and item.brand != ""
            ],
            "ingredients": [
                attr.value for item in self.detected_food_items 
                for attr in item.attributes 
                if attr.type == AttributeType.INGREDIENT
            ],
            "cooking_methods": [
                attr.value for item in self.detected_food_items 
                for attr in item.attributes 
                if attr.type == AttributeType.PREPARATION
            ],
            "negative_cues": [
                cue for item in self.detected_food_items 
                for cue in item.negative_cues
            ]
        }
    
    def get_primary_search_terms(self) -> List[str]:
        """ãƒ—ãƒ©ã‚¤ãƒãƒªæ¤œç´¢ç”¨èªã‚’å–å¾—ï¼ˆé«˜ä¿¡é ¼åº¦ã‚¢ã‚¤ãƒ†ãƒ ï¼‰"""
        primary_terms = []
        
        # é«˜ä¿¡é ¼åº¦ã®æ¤œå‡ºã‚¢ã‚¤ãƒ†ãƒ 
        for item in self.detected_food_items:
            if item.confidence >= 0.7:
                primary_terms.append(item.item_name)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–™ç†åã¨é£Ÿæå
        if not primary_terms:
            primary_terms.extend(self.get_all_dish_names())
            primary_terms.extend(self.get_all_ingredient_names())
        
        return primary_terms 
```

================================================================================

ğŸ“ AI ã‚µãƒ¼ãƒ“ã‚¹å±¤
================================================================================

ğŸ“„ FILE: app_v2/services/__init__.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 228 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:00:07
å­˜åœ¨: âœ…

CONTENT:
```python
from .gemini_service import GeminiService
from .usda_service import USDAService  
from .nutrition_calculation_service import NutritionCalculationService

__all__ = ["GeminiService", "USDAService", "NutritionCalculationService"]

```

================================================================================

ğŸ“„ FILE: app_v2/services/gemini_service.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 22,141 bytes
æœ€çµ‚æ›´æ–°: 2025-06-11 10:49:17
å­˜åœ¨: âœ…

CONTENT:
```python
import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig, HarmCategory, HarmBlockThreshold
from typing import Dict, Optional
import json
import logging
from PIL import Image
import io

from ..config.prompts import Phase1Prompts, Phase2Prompts

logger = logging.getLogger(__name__)

# å¾“æ¥ã®JSONã‚¹ã‚­ãƒ¼ãƒï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
            "description": "ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰ã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰ã€‚"},
                    "ingredients": {
                        "type": "array",
                        "description": "ã“ã®æ–™ç†ã«å«ã¾ã‚Œã‚‹ã¨æ¨å®šã•ã‚Œã‚‹ææ–™ã®ãƒªã‚¹ãƒˆï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰ã€‚",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "ææ–™ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰ã€‚"}
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}

# æ–°ã—ã„æ§‹é€ åŒ–å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒ
STRUCTURED_MEAL_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "detected_food_items": {
            "type": "array",
            "description": "èªè­˜ã•ã‚ŒãŸé£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆï¼ˆæ§‹é€ åŒ–ï¼‰ã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "item_name": {"type": "string", "description": "é£Ÿå“åï¼ˆä¸»è¦ãªå€™è£œï¼‰"},
                    "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "é£Ÿå“åã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢"},
                    "attributes": {
                        "type": "array",
                        "description": "é£Ÿå“ã®å±æ€§ãƒªã‚¹ãƒˆï¼ˆææ–™ã€èª¿ç†æ³•ãªã©ï¼‰",
                        "items": {
                            "type": "object",
                            "properties": {
                                "type": {
                                    "type": "string", 
                                    "enum": ["ingredient", "preparation", "color", "texture", "cooking_method", "serving_style", "allergen"],
                                    "description": "å±æ€§ã®ã‚¿ã‚¤ãƒ—"
                                },
                                "value": {"type": "string", "description": "å±æ€§ã®å€¤"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "ã“ã®å±æ€§ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢"}
                            },
                            "required": ["type", "value", "confidence"]
                        }
                    },
                    "brand": {"type": "string", "description": "èªè­˜ã•ã‚ŒãŸãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè©²å½“ã™ã‚‹å ´åˆã€nullã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰"},
                    "category_hints": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "æ¨å®šã•ã‚Œã‚‹é£Ÿå“ã‚«ãƒ†ã‚´ãƒª"
                    },
                    "negative_cues": {
                        "type": "array", 
                        "items": {"type": "string"},
                        "description": "ç”»åƒã‹ã‚‰åˆ¤æ–­ã§ãã‚‹ã€Œå«ã¾ã‚Œãªã„ã€è¦ç´ "
                    }
                },
                "required": ["item_name", "confidence", "attributes"]
            }
        },
        "dishes": {
            "type": "array",
            "description": "å¾“æ¥äº’æ›æ€§ã®ãŸã‚ã®æ–™ç†ãƒªã‚¹ãƒˆ",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "æ–™ç†å"},
                    "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "æ–™ç†ç‰¹å®šã®ä¿¡é ¼åº¦"},
                    "ingredients": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "é£Ÿæå"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "é£Ÿæç‰¹å®šã®ä¿¡é ¼åº¦"},
                                "attributes": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "type": {"type": "string", "description": "å±æ€§ã‚¿ã‚¤ãƒ—"},
                                            "value": {"type": "string", "description": "å±æ€§å€¤"},
                                            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                                        },
                                        "required": ["type", "value", "confidence"]
                                    }
                                }
                            },
                            "required": ["ingredient_name"]
                        }
                    },
                    "attributes": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "type": {"type": "string"},
                                "value": {"type": "string"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                            },
                            "required": ["type", "value", "confidence"]
                        }
                    }
                },
                "required": ["dish_name", "ingredients"]
            }
        },
        "analysis_confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "å…¨ä½“çš„ãªåˆ†æã®ä¿¡é ¼åº¦"}
    },
    "required": ["detected_food_items", "dishes", "analysis_confidence"]
}

REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
            "description": "ç”»åƒã‹ã‚‰ç‰¹å®šãƒ»ç²¾ç·»åŒ–ã•ã‚ŒãŸæ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "ç‰¹å®šã•ã‚ŒãŸæ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®åç§°ã€‚"},
                    "type": {"type": "string", "description": "æ–™ç†ã®ç¨®é¡ï¼ˆä¾‹: ä¸»èœ, å‰¯èœ, å˜å“é£Ÿå“ï¼‰ã€‚"},
                    "quantity_on_plate": {"type": "string", "description": "çš¿ã®ä¸Šã®é‡ã€‚"},
                    "calculation_strategy": {
                        "type": "string",
                        "enum": ["dish_level", "ingredient_level"],
                        "description": "ã“ã®ã‚¢ã‚¤ãƒ†ãƒ ã®æ „é¤Šè¨ˆç®—æ–¹é‡ã€‚"
                    },
                    "fdc_id": {
                        "type": "integer",
                        "description": "calculation_strategyãŒ'dish_level'ã®å ´åˆã€ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“ã®FDC IDã€‚ãã‚Œä»¥å¤–ã¯nullã€‚"
                    },
                    "usda_source_description": {
                        "type": "string",
                        "description": "calculation_strategyãŒ'dish_level'ã®å ´åˆã€ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“ã®USDAå…¬å¼åç§°ã€‚ãã‚Œä»¥å¤–ã¯nullã€‚"
                    },
                    "ingredients": {
                        "type": "array",
                        "description": "ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã«å«ã¾ã‚Œã‚‹ã¨æ¨å®šã•ã‚Œã‚‹ææ–™ã®ãƒªã‚¹ãƒˆã€‚",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "ææ–™ã®åç§°ã€‚"},
                                "fdc_id": {
                                    "type": "integer",
                                    "description": "calculation_strategyãŒ'ingredient_level'ã®å ´åˆã€ã“ã®ææ–™ã®FDC IDã€‚ãã‚Œä»¥å¤–ã¯nullã¾ãŸã¯çœç•¥å¯ã€‚"
                                },
                                "usda_source_description": {
                                    "type": "string",
                                    "description": "calculation_strategyãŒ'ingredient_level'ã®å ´åˆã€ã“ã®ææ–™ã®USDAå…¬å¼åç§°ã€‚ãã‚Œä»¥å¤–ã¯nullã¾ãŸã¯çœç•¥å¯ã€‚"
                                }
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "type", "quantity_on_plate", "calculation_strategy", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}


class GeminiService:
    """Vertex AIçµŒç”±ã§Geminiã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_id: str, location: str, model_name: str = "gemini-2.5-flash-preview-05-20"):
        """
        åˆæœŸåŒ–
        
        Args:
            project_id: GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
            location: Vertex AIã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: us-central1ï¼‰
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        # Vertex AIã®åˆæœŸåŒ–
        vertexai.init(project=project_id, location=location)
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.model = GenerativeModel(model_name=model_name)
        
        # generation_configã‚’ä½œæˆ (Phase1ç”¨ - å¾“æ¥ç‰ˆ)
        self.generation_config = GenerationConfig(
            temperature=0.0,  # å®Œå…¨ã«deterministicã«
            top_p=1.0,       # nucleus samplingã‚’ç„¡åŠ¹åŒ–
            top_k=1,         # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é¸æŠè‚¢ã®ã¿
            max_output_tokens=8192,
            candidate_count=1,  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å€™è£œã‚’1ã¤ã«åˆ¶é™
            response_mime_type="application/json",
            response_schema=MEAL_ANALYSIS_GEMINI_SCHEMA
        )
        
        # æ§‹é€ åŒ–åˆ†æç”¨ã®generation_config
        self.structured_generation_config = GenerationConfig(
            temperature=0.1,  # ã‚ãšã‹ãªå¤‰å‹•ã‚’è¨±å¯ï¼ˆã‚ˆã‚Šè©³ç´°ãªåˆ†æã®ãŸã‚ï¼‰
            top_p=0.95,
            top_k=40,
            max_output_tokens=16384,  # ã‚ˆã‚Šå¤šãã®å‡ºåŠ›ã‚’è¨±å¯
            candidate_count=1,
            response_mime_type="application/json",
            response_schema=STRUCTURED_MEAL_ANALYSIS_SCHEMA
        )
        
        # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }
    
    async def analyze_phase1(
        self, 
        image_bytes: bytes, 
        image_mime_type: str, 
        optional_text: Optional[str] = None
    ) -> Dict:
        """
        Phase1: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã¦é£Ÿäº‹æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¾“æ¥ç‰ˆï¼‰
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜
            
        Returns:
            åˆ†æçµæœã®è¾æ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt = Phase1Prompts.get_system_prompt()
            user_prompt = Phase1Prompts.get_user_prompt(optional_text)
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase1 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e
    
    async def analyze_phase1_structured(
        self, 
        image_bytes: bytes, 
        image_mime_type: str, 
        optional_text: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> Dict:
        """
        Phase1: æ§‹é€ åŒ–ã•ã‚ŒãŸè©³ç´°ãªç”»åƒåˆ†æï¼ˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã€å±æ€§ã€ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ç­‰ã‚’å«ã‚€ï¼‰
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜
            system_prompt: ã‚«ã‚¹ã‚¿ãƒ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼‰
            
        Returns:
            æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æçµæœã®è¾æ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
            if system_prompt is None:
                system_prompt = Phase1Prompts.get_system_prompt()
            
            user_prompt = Phase1Prompts.get_user_prompt(optional_text)
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            logger.info("Starting structured Gemini Phase1 analysis...")
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—ï¼ˆæ§‹é€ åŒ–è¨­å®šä½¿ç”¨ï¼‰
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=self.structured_generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini structured analysis.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            # çµæœã®æ¤œè¨¼ã¨ä¿®æ­£
            result = self._validate_and_fix_structured_result(result)
            
            detected_items_count = len(result.get('detected_food_items', []))
            dishes_count = len(result.get('dishes', []))
            overall_confidence = result.get('analysis_confidence', 0.5)
            
            logger.info(f"Gemini Phase1 structured analysis completed: {detected_items_count} items, "
                       f"{dishes_count} dishes, confidence {overall_confidence:.2f}")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error in structured analysis: {e}")
            raise RuntimeError(f"Error processing structured response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini structured API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini structured API request failed: {e}") from e
    
    def _validate_and_fix_structured_result(self, result: Dict) -> Dict:
        """æ§‹é€ åŒ–åˆ†æçµæœã‚’æ¤œè¨¼ã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£"""
        # detected_food_itemsãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
        if 'detected_food_items' not in result:
            result['detected_food_items'] = []
        
        # dishesãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
        if 'dishes' not in result:
            result['dishes'] = []
        
        # analysis_confidenceãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
        if 'analysis_confidence' not in result:
            # å„ã‚¢ã‚¤ãƒ†ãƒ ã®å¹³å‡ä¿¡é ¼åº¦ã‚’è¨ˆç®—
            confidences = []
            for item in result['detected_food_items']:
                if 'confidence' in item:
                    confidences.append(item['confidence'])
            
            for dish in result['dishes']:
                if 'confidence' in dish and dish['confidence'] is not None:
                    confidences.append(dish['confidence'])
            
            result['analysis_confidence'] = sum(confidences) / len(confidences) if confidences else 0.5
        
        # å„detected_food_itemã®æ¤œè¨¼
        for item in result['detected_food_items']:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
            if 'confidence' not in item:
                item['confidence'] = 0.5
            if 'attributes' not in item:
                item['attributes'] = []
            if 'category_hints' not in item:
                item['category_hints'] = []
            if 'negative_cues' not in item:
                item['negative_cues'] = []
            
            # å±æ€§ã®æ¤œè¨¼
            for attr in item['attributes']:
                if 'confidence' not in attr:
                    attr['confidence'] = 0.5
                if 'type' not in attr:
                    attr['type'] = 'ingredient'
        
        return result
    
    async def analyze_phase2(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        usda_candidates_text: str,
        initial_analysis_data: str
    ) -> Dict:
        """
        Phase2: USDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’å†åˆ†æ
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            usda_candidates_text: USDAå€™è£œæƒ…å ±ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
            initial_analysis_data: Phase1ã®AIå‡ºåŠ›ï¼ˆJSONæ–‡å­—åˆ—ï¼‰
            
        Returns:
            ç²¾ç·»åŒ–ã•ã‚ŒãŸåˆ†æçµæœã®è¾æ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt = Phase2Prompts.get_system_prompt()
            user_prompt = Phase2Prompts.get_user_prompt(
                usda_candidates_text=usda_candidates_text,
                initial_analysis_data=initial_analysis_data
            )
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Phase2ç”¨ã®Generation Config (å‡ºåŠ›å®‰å®šåŒ–)
            phase2_generation_config = GenerationConfig(
                temperature=0.0,  # å®Œå…¨ã«deterministicã«
                top_p=1.0,       # nucleus samplingã‚’ç„¡åŠ¹åŒ–
                top_k=1,         # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é¸æŠè‚¢ã®ã¿
                max_output_tokens=8192,
                candidate_count=1,  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å€™è£œã‚’1ã¤ã«åˆ¶é™
                response_mime_type="application/json",
                response_schema=REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA
            )
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=phase2_generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase2 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e 
```

================================================================================

ğŸ“ è¨­å®šç®¡ç†
================================================================================

ğŸ“„ FILE: app_v2/config/__init__.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 85 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:46:54
å­˜åœ¨: âœ…

CONTENT:
```python
from .settings import Settings, get_settings

__all__ = ["Settings", "get_settings"] 
```

================================================================================

ğŸ“ Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†
================================================================================

ğŸ“„ FILE: create_elasticsearch_index.py
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 8,637 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 13:04:18
å­˜åœ¨: âœ…

CONTENT:
```python
#!/usr/bin/env python3
"""
Elasticsearch Index Creation Script

ç¾çŠ¶ã®JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹
"""

import json
import os
from elasticsearch import Elasticsearch
from typing import Dict, List, Any
import time


def create_index_mapping() -> Dict[str, Any]:
    """Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®šç¾©"""
    return {
        "mappings": {
            "properties": {
                "id": {
                    "type": "keyword"
                },
                "search_name": {
                    "type": "text",
                    "analyzer": "standard",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        },
                        "suggest": {
                            "type": "completion"
                        }
                    }
                },
                "description": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "data_type": {
                    "type": "keyword"
                },
                "nutrition": {
                    "type": "object",
                    "properties": {
                        "calories": {"type": "float"},
                        "protein": {"type": "float"},
                        "fat": {"type": "float"},
                        "carbs": {"type": "float"},
                        "carbohydrates": {"type": "float"},
                        "fiber": {"type": "float"},
                        "sugar": {"type": "float"},
                        "sodium": {"type": "float"}
                    }
                },
                "weight": {
                    "type": "float"
                },
                "source_db": {
                    "type": "keyword"
                }
            }
        },
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "food_analyzer": {
                        "type": "standard",
                        "stopwords": "_none_"
                    }
                }
            }
        }
    }


def load_json_databases() -> Dict[str, List[Dict[str, Any]]]:
    """JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    databases = {}
    
    db_configs = {
        "yazio": "db/yazio_db.json",
        "mynetdiary": "db/mynetdiary_db.json", 
        "eatthismuch": "db/eatthismuch_db.json"
    }
    
    for db_name, file_path in db_configs.items():
        try:
            if os.path.exists(file_path):
                print(f"Loading {db_name} from {file_path}...")
                with open(file_path, 'r', encoding='utf-8') as f:
                    database = json.load(f)
                    databases[db_name] = database
                    print(f"âœ… Loaded {db_name}: {len(database)} items")
            else:
                print(f"âš ï¸  File not found: {file_path}")
                databases[db_name] = []
        except Exception as e:
            print(f"âŒ Error loading {db_name}: {e}")
            databases[db_name] = []
    
    return databases


def prepare_document(item: Dict[str, Any], source_db: str) -> Dict[str, Any]:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Elasticsearchç”¨ã«æº–å‚™"""
    doc = {
        "id": item.get("id", 0),
        "search_name": item.get("search_name", ""),
        "description": item.get("description"),
        "data_type": item.get("data_type", "unknown"),
        "nutrition": item.get("nutrition", {}),
        "weight": item.get("weight"),
        "source_db": source_db
    }
    
    # ç©ºã®å€¤ã‚’é™¤å»
    return {k: v for k, v in doc.items() if v is not None}


def bulk_index_documents(es_client: Elasticsearch, index_name: str, documents: List[Dict[str, Any]], batch_size: int = 1000):
    """ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ """
    total_docs = len(documents)
    indexed_count = 0
    
    print(f"ğŸ“¥ Indexing {total_docs} documents in batches of {batch_size}...")
    
    for i in range(0, total_docs, batch_size):
        batch = documents[i:i + batch_size]
        
        # ãƒãƒ«ã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ§‹ç¯‰
        bulk_body = []
        for doc in batch:
            bulk_body.append({
                "index": {
                    "_index": index_name,
                    "_id": f"{doc['source_db']}_{doc['id']}"
                }
            })
            bulk_body.append(doc)
        
        try:
            response = es_client.bulk(body=bulk_body)
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if response.get("errors"):
                error_count = sum(1 for item in response["items"] if "error" in item.get("index", {}))
                print(f"âš ï¸  Batch {i//batch_size + 1}: {error_count} errors in batch")
            
            indexed_count += len(batch)
            print(f"   Progress: {indexed_count}/{total_docs} ({indexed_count/total_docs*100:.1f}%)")
            
        except Exception as e:
            print(f"âŒ Error indexing batch {i//batch_size + 1}: {e}")
    
    print(f"âœ… Indexing completed: {indexed_count} documents")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== Elasticsearch Index Creation ===")
    
    # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    print("\n1. Connecting to Elasticsearch...")
    es_client = Elasticsearch(["http://localhost:9200"])
    
    if not es_client.ping():
        print("âŒ Cannot connect to Elasticsearch. Make sure it's running on localhost:9200")
        return False
    
    print("âœ… Connected to Elasticsearch")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
    index_name = "nutrition_db"
    
    # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‰Šé™¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    print(f"\n2. Checking existing index '{index_name}'...")
    if es_client.indices.exists(index=index_name):
        print(f"   Index '{index_name}' already exists. Deleting...")
        es_client.indices.delete(index=index_name)
        print("   âœ… Deleted existing index")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    print(f"\n3. Creating index '{index_name}'...")
    mapping = create_index_mapping()
    es_client.indices.create(index=index_name, body=mapping)
    print("âœ… Index created with mapping")
    
    # JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
    print("\n4. Loading JSON databases...")
    databases = load_json_databases()
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æº–å‚™
    print("\n5. Preparing documents for indexing...")
    all_documents = []
    
    for db_name, items in databases.items():
        print(f"   Processing {db_name}: {len(items)} items")
        for item in items:
            if "search_name" in item:  # æœ‰åŠ¹ãªã‚¢ã‚¤ãƒ†ãƒ ã®ã¿
                doc = prepare_document(item, db_name)
                all_documents.append(doc)
    
    print(f"âœ… Prepared {len(all_documents)} documents for indexing")
    
    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    print("\n6. Bulk indexing documents...")
    start_time = time.time()
    bulk_index_documents(es_client, index_name, all_documents)
    end_time = time.time()
    
    print(f"âœ… Indexing completed in {end_time - start_time:.2f} seconds")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆã®è¡¨ç¤º
    print("\n7. Index statistics...")
    stats = es_client.indices.stats(index=index_name)
    doc_count = stats["indices"][index_name]["total"]["docs"]["count"]
    index_size = stats["indices"][index_name]["total"]["store"]["size_in_bytes"]
    
    print(f"   Total documents: {doc_count}")
    print(f"   Index size: {index_size / 1024 / 1024:.2f} MB")
    
    # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\n8. Testing sample search...")
    test_query = {
        "query": {
            "multi_match": {
                "query": "chicken",
                "fields": ["search_name", "search_name.exact"]
            }
        },
        "size": 3
    }
    
    response = es_client.search(index=index_name, body=test_query)
    hits = response["hits"]["hits"]
    
    print(f"   Sample search for 'chicken': {len(hits)} results")
    for hit in hits:
        source = hit["_source"]
        print(f"   - {source['search_name']} ({source['source_db']}) score: {hit['_score']:.2f}")
    
    print(f"\nğŸ‰ Elasticsearch index '{index_name}' successfully created!")
    print(f"   Ready for high-speed nutrition search")
    
    return True


if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… Index creation completed successfully!")
    else:
        print("\nâŒ Index creation failed!") 
```

================================================================================

ğŸ“ æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
================================================================================

ğŸ“„ FILE: db/yazio_db.json
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 504,128 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 11:55:48
å­˜åœ¨: âœ…

CONTENT (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ - æœ€åˆã®50è¡Œ):
```json
[
  {
    "data_type": "unified",
    "id": 1000000000,
    "search_name": "Cheese Coffeecake",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 339.0,
      "protein": 7.0,
      "fat": 15.2,
      "carbs": 44.3
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000001,
    "search_name": "Fruit Fried Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 316.0,
      "protein": 3.0,
      "fat": 16.1,
      "carbs": 42.6
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000002,
    "search_name": "Blueberry Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 245.0,
      "protein": 2.7,
      "fat": 11.9,
      "carbs": 33.5
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000003,
    "search_name": "Apple Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 265.0,
      "protein": 2.4,
      "fat": 12.5,
      "carbs": 37.1

... (23677 more lines)
```

================================================================================

ğŸ“„ FILE: db/mynetdiary_db.json
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 349,780 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 11:55:05
å­˜åœ¨: âœ…

CONTENT (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ - æœ€åˆã®50è¡Œ):
```json
[
  {
    "data_type": "unified",
    "id": 10000000000,
    "search_name": "Beans baked canned plain or vegetarian",
    "description": null,
    "nutrition": {
      "calories": 94.09448818897638,
      "protein": 4.724409448818898,
      "fat": 0.35433070866141736,
      "carbs": 21.25984251968504
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000001,
    "search_name": "Black beans boiled without salt",
    "description": null,
    "nutrition": {
      "calories": 131.97674418604652,
      "protein": 8.72093023255814,
      "fat": 0.5232558139534884,
      "carbs": 23.837209302325583
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000002,
    "search_name": "Black beans canned low sodium",
    "description": null,
    "nutrition": {
      "calories": 90.83333333333334,
      "protein": 5.833333333333334,
      "fat": 0.2916666666666667,
      "carbs": 16.666666666666668
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000003,
    "search_name": "Black beans canned no salt added",
    "description": null,
    "nutrition": {
      "calories": 84.61538461538461,
      "protein": 5.384615384615385,
      "fat": 0.0,
      "carbs": 16.153846153846153

... (14798 more lines)
```

================================================================================

ğŸ“„ FILE: db/eatthismuch_db.json
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,809,842 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 11:50:01
å­˜åœ¨: âœ…

CONTENT (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ - æœ€åˆã®50è¡Œ):
```json
[
  {
    "data_type": "dish",
    "id": 907072,
    "search_name": "Garlic and Cream Cheese Cauliflower \"Mashed Potatoes\"",
    "description": null,
    "nutrition": {
      "calories": 43.63999999999999,
      "protein": 2.5545171339563866,
      "fat": 1.6926272066458983,
      "carbs": 5.7632398753894085
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 905725,
    "search_name": "Poached Eggs on Toast",
    "description": null,
    "nutrition": {
      "calories": 211.81,
      "protein": 12.653061224489797,
      "fat": 6.530612244897958,
      "carbs": 24.89795918367347
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 3267515,
    "search_name": "Caprese Salad",
    "description": null,
    "nutrition": {
      "calories": 174.96289228159455,
      "protein": 6.530958439355386,
      "fat": 15.436810856658182,
      "carbs": 3.307888040712468
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 906392,
    "search_name": "Spinach and Pear Omelet",
    "description": null,
    "nutrition": {
      "calories": 105.17,
      "protein": 5.143651529193697,
      "fat": 6.580166821130676,
      "carbs": 7.298424467099165

... (115366 more lines)
```

================================================================================

ğŸ“ ãƒ†ã‚¹ãƒˆç”»åƒ
================================================================================

ğŸ“„ FILE: test_images/food3.jpg
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 133,358 bytes
æœ€çµ‚æ›´æ–°: 2025-05-30 16:00:37
å­˜åœ¨: âœ…

CONTENT: [ç”»åƒãƒ•ã‚¡ã‚¤ãƒ« - ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿]
ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«: food3.jpg
ç”¨é€”: test_advanced_elasticsearch_search.py ã®å…¥åŠ›ç”»åƒ

================================================================================

ğŸ“ ä¾å­˜é–¢ä¿‚ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
================================================================================

ğŸ“„ FILE: requirements.txt
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 274 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 13:01:13
å­˜åœ¨: âœ…

CONTENT:
```python
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
google-cloud-aiplatform==1.94.0
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
httpx
pytest==7.4.3
pytest-asyncio==0.21.1
python-dotenv==1.0.0
Pillow==11.2.1
elasticsearch==8.15.1 
```

================================================================================

ğŸ“„ FILE: README.md
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 15,369 bytes
æœ€çµ‚æ›´æ–°: 2025-06-10 14:55:33
å­˜åœ¨: âœ…

CONTENT:
```python
# é£Ÿäº‹åˆ†æ API (Meal Analysis API) v2.0

## æ¦‚è¦

ã“ã® API ã¯ã€**Google Gemini AI** ã¨ **Elasticsearch ãƒ™ãƒ¼ã‚¹ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ **ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªé£Ÿäº‹ç”»åƒåˆ†æã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚**å‹•çš„æ „é¤Šè¨ˆç®—æ©Ÿèƒ½**ã«ã‚ˆã‚Šã€æ–™ç†ã®ç‰¹æ€§ã«å¿œã˜ã¦æœ€é©ãªæ „é¤Šè¨ˆç®—æˆ¦ç•¥ã‚’è‡ªå‹•é¸æŠã—ã€æ­£ç¢ºãªæ „é¤Šä¾¡æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸŒŸ ä¸»ãªæ©Ÿèƒ½

### **ğŸ”¥ æ–°æ©Ÿèƒ½: Elasticsearch ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ v2.0**

- **âš¡ Elasticsearch é«˜é€Ÿæ¤œç´¢**: é«˜æ€§èƒ½ãª Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ˆã‚‹å¤§è¦æ¨¡æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
- **ğŸ“Š 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆæ¤œç´¢**: 1 ã¤ã®ã‚¯ã‚¨ãƒªã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰åŒ…æ‹¬çš„ãªæ „é¤Šæƒ…å ±ã‚’å–å¾—
  - **YAZIO**: 1,825 é …ç›® - ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé£Ÿå“ã‚«ãƒ†ã‚´ãƒª
  - **MyNetDiary**: 1,142 é …ç›® - ç§‘å­¦çš„/æ „é¤Šå­¦çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
  - **EatThisMuch**: 8,878 é …ç›® - æœ€å¤§ã‹ã¤æœ€ã‚‚åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- **ğŸ” ãƒãƒ«ãƒ DB æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰**: å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€å¤§ 5 ä»¶ãšã¤ã€ç·åˆçš„ãªæ¤œç´¢çµæœã‚’æä¾›
- **ğŸ¯ é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°**: 90.9%ã®æˆåŠŸç‡ã€å„ DB ã‹ã‚‰å‡ç­‰ãªçµæœåˆ†æ•£
- **ğŸ’¾ è©³ç´°çµæœä¿å­˜**: JSONãƒ»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®æ¤œç´¢çµæœè‡ªå‹•ä¿å­˜

### **å¾“æ¥æ©Ÿèƒ½: å‹•çš„æ „é¤Šè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ **

- **ğŸ§  AI é§†å‹•ã®è¨ˆç®—æˆ¦ç•¥æ±ºå®š**: Gemini AI ãŒå„æ–™ç†ã«å¯¾ã—ã¦æœ€é©ãªæ „é¤Šè¨ˆç®—æ–¹æ³•ã‚’è‡ªå‹•é¸æŠ
- **ğŸ¯ é«˜ç²¾åº¦æ „é¤Šè¨ˆç®—**: é£Ÿæé‡é‡ Ã— 100g ã‚ãŸã‚Šæ „é¤Šä¾¡ã§æ­£ç¢ºãªå®Ÿæ „é¤Šä¾¡ã‚’ç®—å‡º
- **ğŸ“Š 3 å±¤é›†è¨ˆã‚·ã‚¹ãƒ†ãƒ **: é£Ÿæ â†’ æ–™ç† â†’ é£Ÿäº‹å…¨ä½“ã®è‡ªå‹•æ „é¤Šé›†è¨ˆ

### **ã‚³ã‚¢æ©Ÿèƒ½**

- **ãƒ•ã‚§ãƒ¼ã‚º 1**: Gemini AI ã«ã‚ˆã‚‹é£Ÿäº‹ç”»åƒã®åˆ†æï¼ˆæ–™ç†è­˜åˆ¥ã€é£ŸææŠ½å‡ºã€é‡é‡æ¨å®šï¼‰
- **ãƒãƒ«ãƒ DB æ¤œç´¢**: 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®åŒ…æ‹¬çš„æ „é¤Šæƒ…å ±å–å¾—
- **è¤‡æ•°æ–™ç†å¯¾å¿œ**: 1 æšã®ç”»åƒã§è¤‡æ•°ã®æ–™ç†ã‚’åŒæ™‚åˆ†æ
- **è‹±èªãƒ»æ—¥æœ¬èªå¯¾å¿œ**: å¤šè¨€èªã§ã®é£Ÿæãƒ»æ–™ç†èªè­˜
- **OpenAPI 3.0 æº–æ‹ **: å®Œå…¨ãª API æ–‡æ›¸åŒ–ã¨ã‚¿ã‚¤ãƒ—å®‰å…¨æ€§

## ğŸ— ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
meal_analysis_api_2/
â”œâ”€â”€ db/                                   # ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
â”‚   â”œâ”€â”€ yazio_db.json                     # YAZIOæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ1,825é …ç›®ï¼‰
â”‚   â”œâ”€â”€ mynetdiary_db.json                # MyNetDiaryæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ1,142é …ç›®ï¼‰
â”‚   â””â”€â”€ eatthismuch_db.json               # EatThisMuchæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ8,878é …ç›®ï¼‰
â”œâ”€â”€ app_v2/                               # æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆ
â”‚   â”œâ”€â”€ components/                       # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ™ãƒ¼ã‚¹è¨­è¨ˆ
â”‚   â”‚   â”œâ”€â”€ local_nutrition_search_component.py  # ãƒãƒ«ãƒDBæ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â”œâ”€â”€ phase1_component.py           # ç”»åƒåˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â””â”€â”€ base.py                       # ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”œâ”€â”€ pipeline/                         # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ orchestrator.py               # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
â”‚   â”‚   â””â”€â”€ result_manager.py             # çµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
â”‚   â”œâ”€â”€ models/                           # ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â”œâ”€â”€ nutrition_search_models.py    # æ „é¤Šæ¤œç´¢ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â””â”€â”€ phase1_models.py              # Phase1ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ app.py                        # FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â””â”€â”€ config/                           # è¨­å®šç®¡ç†
â”œâ”€â”€ test_multi_db_nutrition_search.py     # ãƒãƒ«ãƒDBæ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰
â”œâ”€â”€ test_local_nutrition_search_v2.py     # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ test_images/                          # ãƒ†ã‚¹ãƒˆç”¨ç”»åƒ
â””â”€â”€ requirements.txt                      # Pythonä¾å­˜é–¢ä¿‚
```

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python -m venv venv

# ä»®æƒ³ç’°å¢ƒã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source venv/bin/activate  # macOS/Linux
# ã¾ãŸã¯
venv\Scripts\activate     # Windows

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

### 2. Google Cloud è¨­å®š

#### Google Cloud SDK ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ã¾ã ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ãªã„å ´åˆã¯ã€ä»¥ä¸‹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š
https://cloud.google.com/sdk/docs/install

#### Google Cloud èªè¨¼ã®è¨­å®š

é–‹ç™ºç’°å¢ƒã§ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§èªè¨¼ã‚’è¨­å®šï¼š

```bash
# Google Cloudã«ãƒ­ã‚°ã‚¤ãƒ³
gcloud auth login

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼æƒ…å ±ã‚’è¨­å®š
gcloud auth application-default login

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’è¨­å®š
gcloud config set project YOUR_PROJECT_ID
```

æœ¬ç•ªç’°å¢ƒã§ã¯ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼š

```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your-service-account-key.json"
```

#### Vertex AI API ã®æœ‰åŠ¹åŒ–

```bash
# Vertex AI APIã‚’æœ‰åŠ¹åŒ–
gcloud services enable aiplatform.googleapis.com
```

### 3. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š

```bash
# USDA APIè¨­å®š
export USDA_API_KEY="your-usda-api-key"

# Vertex AIè¨­å®š
export GOOGLE_APPLICATION_CREDENTIALS="path/to/service-account-key.json"
export GEMINI_PROJECT_ID="your-gcp-project-id"
export GEMINI_LOCATION="us-central1"
export GEMINI_MODEL_NAME="gemini-2.5-flash-preview-05-20"
```

## ğŸ–¥ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•

### app_v2 ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ï¼ˆãƒãƒ«ãƒ DB å¯¾å¿œï¼‰

```bash
# app_v2ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
python -m app_v2.main.app
```

**âš ï¸ æ³¨æ„**: ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚ã€å¿…ãšãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å½¢å¼ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã™ã‚‹ã¨ã€ä»¥ä¸‹ã® URL ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ãªã‚Šã¾ã™ï¼š

- **API**: http://localhost:8000
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: http://localhost:8000/docs
- **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯**: http://localhost:8000/health

## ğŸ§ª ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

### ğŸ”¥ ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆæœ€æ–°æ©Ÿèƒ½ï¼‰

**é‡è¦**: ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹çŠ¶æ…‹ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```bash
# åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œ
python test_multi_db_nutrition_search.py
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

- **æ¤œç´¢é€Ÿåº¦**: 11 ã‚¯ã‚¨ãƒªã‚’ 0.10 ç§’ã§å‡¦ç†
- **ãƒãƒƒãƒç‡**: å„ DB90.9%ã®ã‚¯ã‚¨ãƒªã§çµæœç™ºè¦‹
- **ç·ãƒãƒƒãƒæ•°**: 87 ä»¶ï¼ˆå¹³å‡ 7.9 ä»¶/ã‚¯ã‚¨ãƒªï¼‰
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ**:
  - YAZIO: 1,825 é …ç›®
  - MyNetDiary: 1,142 é …ç›®
  - EatThisMuch: 8,878 é …ç›®

**ãƒ†ã‚¹ãƒˆçµæœä¾‹**:

```
ğŸ“ˆ Multi-Database Search Results Summary:
- Total queries: 11
- Total matches found: 87
- Average matches per query: 7.9
- Search time: 0.10s

ğŸ” Detailed Query Results:
1. 'Roasted Potatoes' (dish)
   EatThisMuch: 3 matches
     Best: 'Roasted Potatoes' (score: 1.000)
     Nutrition: 91.0 kcal, 1.9g protein
```

### ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆ

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµ±åˆãƒ†ã‚¹ãƒˆ
python test_local_nutrition_search_v2.py
```

### åŸºæœ¬ãƒ†ã‚¹ãƒˆï¼ˆãƒ•ã‚§ãƒ¼ã‚º 1 ã®ã¿ï¼‰

```bash
python test_phase1_only.py
```

## ğŸš€ ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  v2.0

### **æ–°æ©Ÿèƒ½: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ**

ã‚·ã‚¹ãƒ†ãƒ ãŒ USDA API ä¾å­˜ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«å¯¾å¿œã—ã¾ã—ãŸï¼š

- **ğŸ” BM25F + ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ¤œç´¢**: é«˜ç²¾åº¦ãªé£Ÿæãƒãƒƒãƒãƒ³ã‚°
- **ğŸ“Š 8,878 é …ç›®ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ „é¤Šè¨ˆç®—å¯¾å¿œ
- **âš¡ 90.9%ãƒãƒƒãƒç‡**: å®Ÿæ¸¬å€¤ã«ã‚ˆã‚‹é«˜ã„æˆåŠŸç‡
- **ğŸ”„ USDA äº’æ›æ€§**: æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®å®Œå…¨äº’æ›æ€§ç¶­æŒ

### ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆv2.0 å¯¾å¿œï¼‰

```bash
# app_v2ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
python -m app_v2.main.app
```

### ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆ

**é‡è¦**: ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹çŠ¶æ…‹ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµ±åˆãƒ†ã‚¹ãƒˆ
python test_local_nutrition_search_v2.py
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

- **ãƒãƒƒãƒç‡**: 90.9% (10/11 æ¤œç´¢æˆåŠŸ)
- **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: ~11 ç§’
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ (8,878 é …ç›®)
- **æ¤œç´¢æ–¹æ³•**: BM25F + ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°

**ãƒ†ã‚¹ãƒˆçµæœä¾‹**:

```
ğŸ” Local Nutrition Search Results:
- Matches found: 10
- Match rate: 90.9%
- Search method: local_nutrition_database
- Total searches: 11
- Successful matches: 10

ğŸ½ Final Meal Nutrition:
- Calories: 400.00 kcal
- Protein: 60.00 g
- Carbohydrates: 220.00 g
- Fat: 120.00 g
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©³ç´°

**ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹æˆ**:

- `dish_db.json`: 4,583 æ–™ç†ãƒ‡ãƒ¼ã‚¿
- `ingredient_db.json`: 1,473 é£Ÿæãƒ‡ãƒ¼ã‚¿
- `branded_db.json`: 2,822 ãƒ–ãƒ©ãƒ³ãƒ‰é£Ÿå“
- `unified_nutrition_db.json`: 8,878 çµ±åˆãƒ‡ãƒ¼ã‚¿

## ğŸ“¡ API ä½¿ç”¨æ–¹æ³•

### ğŸ”¥ å®Œå…¨åˆ†æ (æ¨å¥¨): å…¨ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆ

**1 ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å…¨ã¦ã®åˆ†æã‚’å®Ÿè¡Œ**

```bash
curl -X POST "http://localhost:8000/api/v1/meal-analyses/complete" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg"
```

ã“ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã‚’è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ï¼š

- ãƒ•ã‚§ãƒ¼ã‚º 1: ç”»åƒåˆ†æ
- USDA ç…§åˆ: é£Ÿæãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
- ãƒ•ã‚§ãƒ¼ã‚º 2: è¨ˆç®—æˆ¦ç•¥æ±ºå®š
- æ „é¤Šè¨ˆç®—: æœ€çµ‚æ „é¤Šä¾¡ç®—å‡º
- çµæœä¿å­˜: è‡ªå‹•çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

**ä¿å­˜ã•ã‚ŒãŸçµæœã®å–å¾—**

```bash
# å…¨çµæœä¸€è¦§
curl "http://localhost:8000/api/v1/meal-analyses/results"

# ç‰¹å®šã®çµæœå–å¾—
curl "http://localhost:8000/api/v1/meal-analyses/results/{analysis_id}"
```

### ãƒ•ã‚§ãƒ¼ã‚º 1: åŸºæœ¬åˆ†æ

```bash
curl -X POST "http://localhost:8000/api/v1/meal-analyses" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg"
```

### ãƒ•ã‚§ãƒ¼ã‚º 2: å‹•çš„æ „é¤Šè¨ˆç®—

```bash
# æœ€åˆã«ãƒ•ã‚§ãƒ¼ã‚º1ã®çµæœã‚’å–å¾—
initial_result=$(curl -X POST "http://localhost:8000/api/v1/meal-analyses" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg")

# ãƒ•ã‚§ãƒ¼ã‚º2ã§å‹•çš„æ „é¤Šè¨ˆç®—
curl -X POST "http://localhost:8000/api/v1/meal-analyses/refine" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg" \
  -F "initial_analysis_data=$initial_result"
```

## ğŸ“‹ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

### ãƒ•ã‚§ãƒ¼ã‚º 1 ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "dishes": [
    {
      "dish_name": "Fried Fish with Spaghetti and Tomato Sauce",
      "type": "Main Dish",
      "quantity_on_plate": "2 pieces of fish, 1 small serving of spaghetti",
      "ingredients": [
        {
          "ingredient_name": "White Fish Fillet",
          "weight_g": 150.0
        },
        {
          "ingredient_name": "Spaghetti (cooked)",
          "weight_g": 80.0
        }
      ]
    }
  ]
}
```

### ãƒ•ã‚§ãƒ¼ã‚º 2 ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆå‹•çš„æ „é¤Šè¨ˆç®—ï¼‰

```json
{
  "dishes": [
    {
      "dish_name": "Spinach and Daikon Radish Aemono",
      "type": "Side Dish",
      "calculation_strategy": "ingredient_level",
      "fdc_id": null,
      "ingredients": [
        {
          "ingredient_name": "Spinach",
          "weight_g": 80.0,
          "fdc_id": 1905313,
          "usda_source_description": "SPINACH",
          "key_nutrients_per_100g": {
            "calories_kcal": 24.0,
            "protein_g": 3.53,
            "carbohydrates_g": 3.53,
            "fat_g": 0.0
          },
          "actual_nutrients": {
            "calories_kcal": 19.2,
            "protein_g": 2.82,
            "carbohydrates_g": 2.82,
            "fat_g": 0.0
          }
        }
      ],
      "dish_total_actual_nutrients": {
        "calories_kcal": 57.45,
        "protein_g": 3.85,
        "carbohydrates_g": 4.57,
        "fat_g": 3.31
      }
    },
    {
      "dish_name": "Green Tea",
      "type": "Drink",
      "calculation_strategy": "dish_level",
      "fdc_id": 1810668,
      "usda_source_description": "GREEN TEA",
      "key_nutrients_per_100g": {
        "calories_kcal": 0.0,
        "protein_g": 0.0,
        "carbohydrates_g": 0.0,
        "fat_g": 0.0
      },
      "dish_total_actual_nutrients": {
        "calories_kcal": 0.0,
        "protein_g": 0.0,
        "carbohydrates_g": 0.0,
        "fat_g": 0.0
      }
    }
  ],
  "total_meal_nutrients": {
    "calories_kcal": 337.95,
    "protein_g": 13.32,
    "carbohydrates_g": 56.19,
    "fat_g": 6.67
  },
  "warnings": null,
  "errors": null
}
```

## ğŸ”§ æŠ€è¡“ä»•æ§˜

### å‹•çš„è¨ˆç®—æˆ¦ç•¥ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯

**Dish Level (`dish_level`)**:

- ã‚·ãƒ³ãƒ—ãƒ«ãªå˜å“é£Ÿå“ï¼ˆæœç‰©ã€é£²ã¿ç‰©ã€åŸºæœ¬é£Ÿæï¼‰
- æ¨™æº–åŒ–ã•ã‚ŒãŸæ—¢è£½å“ã§é©åˆ‡ãª USDA ID ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
- ä¾‹: ç·‘èŒ¶ã€ã‚Šã‚“ã”ã€ç™½ç±³

**Ingredient Level (`ingredient_level`)**:

- è¤‡é›‘ãªèª¿ç†æ¸ˆã¿æ–™ç†ï¼ˆç‚’ã‚ç‰©ã€ã‚µãƒ©ãƒ€ã€ã‚¹ãƒ¼ãƒ—ï¼‰
- è¤‡æ•°é£Ÿæã®çµ„ã¿åˆã‚ã›ã§æ–™ç†å…¨ä½“ã® USDA ID ãŒä¸é©åˆ‡ãªå ´åˆ
- ä¾‹: é‡èœç‚’ã‚ã€æ‰‹ä½œã‚Šã‚µãƒ©ãƒ€ã€å‘³å™Œæ±

### æ „é¤Šè¨ˆç®—å¼

```
å®Ÿæ „é¤Šä¾¡ = (100gã‚ãŸã‚Šæ „é¤Šä¾¡ Ã· 100) Ã— æ¨å®šé‡é‡(g)
```

### é›†è¨ˆéšå±¤

1. **é£Ÿæãƒ¬ãƒ™ãƒ«**: å€‹åˆ¥é£Ÿæã®é‡é‡ Ã— 100g æ „é¤Šä¾¡
2. **æ–™ç†ãƒ¬ãƒ™ãƒ«**: é£Ÿæãƒ¬ãƒ™ãƒ«ã®åˆè¨ˆ ã¾ãŸã¯ æ–™ç†å…¨ä½“è¨ˆç®—
3. **é£Ÿäº‹ãƒ¬ãƒ™ãƒ«**: å…¨æ–™ç†ã®æ „é¤Šä¾¡åˆè¨ˆ

## âš ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

API ã¯ä»¥ä¸‹ã® HTTP ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã—ã¾ã™ï¼š

- `200 OK`: æ­£å¸¸ãªåˆ†æå®Œäº†
- `400 Bad Request`: ä¸æ­£ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆç”»åƒå½¢å¼ã‚¨ãƒ©ãƒ¼ãªã©ï¼‰
- `422 Unprocessable Entity`: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼
- `503 Service Unavailable`: å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆUSDA/Geminiï¼‰ã‚¨ãƒ©ãƒ¼
- `500 Internal Server Error`: ã‚µãƒ¼ãƒãƒ¼å†…éƒ¨ã‚¨ãƒ©ãƒ¼

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### èªè¨¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆ

```bash
# ç¾åœ¨ã®èªè¨¼çŠ¶æ…‹ã‚’ç¢ºèª
gcloud auth list

# ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã‚’ç¢ºèª
gcloud config list

# å¿…è¦ã«å¿œã˜ã¦å†åº¦èªè¨¼
gcloud auth application-default login
```

### Vertex AI API ãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ãªã„å ´åˆ

```bash
# APIã®æœ‰åŠ¹çŠ¶æ³ã‚’ç¢ºèª
gcloud services list --enabled | grep aiplatform

# æœ‰åŠ¹ã§ãªã„å ´åˆã¯æœ‰åŠ¹åŒ–
gcloud services enable aiplatform.googleapis.com
```

### USDA API ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆ

- API ã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
- ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆï¼ˆ3,600 ä»¶/æ™‚ï¼‰ã«é”ã—ã¦ã„ãªã„ã‹ç¢ºèª
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª

## ğŸ’» é–‹ç™ºæƒ…å ±

- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: FastAPI 0.104+
- **AI ã‚µãƒ¼ãƒ“ã‚¹**: Google Vertex AI (Gemini 2.5 Flash)
- **æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: USDA FoodData Central API
- **èªè¨¼**: Google Cloud ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
- **Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 3.9+
- **ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
  - `google-cloud-aiplatform` (Vertex AI)
  - `httpx` (éåŒæœŸ HTTP)
  - `pydantic` (ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³)
  - `pillow` (ç”»åƒå‡¦ç†)

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ MIT ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## æ³¨æ„äº‹é …

**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: API ã‚­ãƒ¼ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã¯çµ¶å¯¾ã«ãƒªãƒã‚¸ãƒˆãƒªã«ã‚³ãƒŸãƒƒãƒˆã—ãªã„ã§ãã ã•ã„ã€‚ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦å®‰å…¨ã«ç®¡ç†ã—ã¦ãã ã•ã„ã€‚

```

================================================================================

ğŸ“„ FILE: service-account-key.json
------------------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,421 bytes
æœ€çµ‚æ›´æ–°: 2025-05-16 17:45:05
å­˜åœ¨: âœ…

CONTENT (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«):
```json
{
  "type": "service_account",
  "project_id": "recording-diet-ai-3e7cf",
  "private_key_id": "614eb51641f7bfa861054c72d1ec20a94cbfe9c8",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDSKPu5UHQ9MCkL\nB8AL/OvuOPQmD9y9pYPZa32/z/aqpeYrG3lg70yIxroCtPFlsThGQPLP1LD1LsTq\nA64+fpq0Oh+a5/90eDnEw3a527yLzfexpgg59gFQ6e4apxglHP8iS8xokLFqz5ud\n8AoYtotgPWxmIXMLNGHQx7szptRSyao/SKDVJfpmhmlO2LpGCYvPJqO9DXBzIcp6\ngZIwn2cj6hr34Y/t4fHf6QtThCAVmL+x5NYMUb8Eox1zpjUQ15PMDdfnilUkj23i\nZJW7mLcZ5KIaTWPP4tAgYSsKr6HgkWO2fQQEVnOrBWeO1JZR3dK6YAtESnb4A+QU\nUwgH7oOVAgMBAAECggEACsHLRKyADqO47hYa9+Idx2dF4+7a6yAeNBJC8P75jrmI\nIvu4hkQpZDTbz4iodH6SNVJcu1OVLS+UJ7BBinRbgc8Z0AmA81q2BkLFAHVlXLiR\nrZekgyb64UPgCqAz5a5qxrwffdiuJBv1SKvBMIujba0nL8EVjOTE/vVCOT4YTxZo\nnH75Ob1+jdNzBKSfr2uMvN9BRdPkYJ8Yi9+c/yUH2rkdNdTrAqUp/To8EKvAjOR2\npmC0T2m+WcdYKGYwWtLgxuPk6GLYuWF3s3hxSd5+u9GL6pbxBiiLSl+emuiBoQi9\nj/LFJY57Bp4rOM2G5pGc/HWrm3zahapoDqigENCfsQKBgQD7ZEtSnt7hyNitCHR5\nJkWXvtQrOhimPjRpHYmL704NRexz/5pJAabgQY7QmQ8XfECRHOcyl/qXqyR+RDGU\nT0NlouFChNyL1wxsHK1y3C2C7Ny0M6+X7VsBe4tiidn33hS2XGXqnZCSB2ABYHrn\nu9lOD+Py1Q4r5zDmDQaojU8UKQKBgQDWAzOCvSX9sLgWjI2EFmYcjhcpoh5T9Hsf\nlOVWSlInBZARgqDiJLqeqF7wSjYOdtlN6+zVsv3ErA2QvkDd4kMgLLWzFsOHHrc9\nEvlIKsv2wZh9qy1zlJGxpnuUfgCgd5pbQqLTUal8XHeVf678sACB6bz/H7fFUk7p\nV5F1nXJBjQKBgQCRtqGeQy4Hi2ZkbWktq8xc16SdZbBR8+5nG1LVxKDmOqiC2B4y\nwP1cMUO5j25a+49lTW6JOeRrsyyU76wZPhRfvhh5eQ9pEv5FUB4NXKgYoniPDwJx\nuoeshVLWi/bGoHg697WvVyMsMZApXCYBWjXr9HP5Fht/wSLrxZMdccLreQKBgEuE\nuaqKwFsy/uLGGjHgDYxJ/5ZrZLRPcxsD2aGHfFHUvq/PHqJuP4Q4+bdlGIomGixK\n8jm+fZnm9Kp82Drz2qgB3uQhRkHp7tMYXOrAX1Tln7/IpbNBW+AKVVVi2SnGyqsl\nanuTN3Fw16njcoYsPSGar1x/fsOcdcgVZHjSZD0JAoGAIQglGBuSJGe+o8qBfcVX\nWZDArPREsCVc6ZknlNKpI6nL412TuLvvTNwpWDrvcC4A3icIeQWdqONE8VMH+Ta/\nzBG0P/G7d98N1WOX5eP4UbBnK4TWwreCZjKQqTsTn4WwpnCcIcntWxzypzOyAAyh\nmbONwHWdsxwL/QNu3dPf3F4=\n-----END PRIVATE KEY-----\n",
  "client_email": "vertex-ai-service-account-596@recording-diet-ai-3e7cf.iam.gserviceaccount.com",
  "client_id": "116391166725593063876",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/vertex-ai-service-account-596%40recording-diet-ai-3e7cf.iam.gserviceaccount.com",
  "universe_domain": "googleapis.com"
}
```

================================================================================

ğŸ“Š DETAILED EXECUTION RESULTS ANALYSIS
================================================================================

ğŸ—ƒï¸ DATABASE SOURCE DISTRIBUTION:
   elasticsearch_eatthismuch: 20 results (66.7%)
   elasticsearch_yazio: 5 results (16.7%)
   elasticsearch_mynetdiary: 5 results (16.7%)

ğŸ¯ STRATEGIC BREAKDOWN:
   dish_primary: 5 results (16.7%)
   ingredient_primary: 10 results (33.3%)
   ingredient_fallback: 15 results (50.0%)

ğŸ“‹ QUERY ANALYSIS:
   'mixed greens' (ingredient): 5 results
      Top: Mixed Greens (score: 34.975)
   'corn' (ingredient): 5 results
      Top: Corn (score: 18.406)
   'Glazed Chicken Thighs with Roasted Baby Potatoes and Mixed Green Salad' (dish): 5 results
      Top: Mustard Glazed Chicken Thighs (score: 38.529)
   'tomato' (ingredient): 5 results
      Top: Tomatoes in Tomato Juice, canned (score: 17.471)
   'baby potato' (ingredient): 5 results
      Top: Baby carrots (score: 22.453)
   'chicken thigh' (ingredient): 5 results
      Top: Chicken thigh (score: 27.961)

================================================================================

ğŸ¯ ADVANCED STRATEGIC ELASTICSEARCH SEARCH ANALYSIS SUMMARY
----------------------------------------------------------------------
ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 25
å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 25
åˆ†æå®Œäº†æ™‚åˆ»: 2025-06-11 12:41:22

ğŸ“Š LATEST EXECUTION PERFORMANCE:
   âœ… Match Rate: 100.0%
   âš¡ Search Time: 383ms
   ğŸ“‹ Total Results: 30

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€test_advanced_elasticsearch_search.pyå®Ÿè¡Œæ™‚ã«
é–¢ã‚ã‚‹é«˜åº¦æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
ãƒ•ã‚¡ã‚¤ãƒ«ã¨æœ€æ–°å®Ÿè¡Œçµæœã®å®Œå…¨ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

ğŸ”¥ ADVANCED STRATEGIC SEARCH SYSTEM HIGHLIGHTS:
- ğŸ§  AIæ§‹é€ åŒ–åˆ†æ: Gemini 2.5 Flash DetectedFoodItemsæŠ½å‡º
- ğŸ½ï¸  Advanced Dishæˆ¦ç•¥: EatThisMuch dish â†’ branded fallback
- ğŸ¥• Advanced Ingredientæˆ¦ç•¥: EatThisMuch ingredient â†’ Multi-DB fallback
- ğŸ“ˆ å‹•çš„å“è³ªä¿è¨¼: ã‚¹ã‚³ã‚¢é–¾å€¤20.0ã«ã‚ˆã‚‹è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ğŸ¯ æˆ¦ç•¥çš„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: å®Œå…¨ãªæ¤œç´¢ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡
- âš¡ é«˜æ€§èƒ½å®Ÿè¡Œ: Sub-second strategic search
- ğŸ“Š åŒ…æ‹¬çš„åˆ†æ: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆ†å¸ƒãƒ»æˆ¦ç•¥çµ±è¨ˆ
- ğŸ’¾ ãƒ‡ãƒ¥ã‚¢ãƒ«å‡ºåŠ›: JSON + Markdown ãƒ¬ãƒãƒ¼ãƒˆ
- ğŸ”§ Production-Ready: èªè¨¼ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Œå‚™
- ğŸš€ æ‹¡å¼µå¯èƒ½è¨­è¨ˆ: Component-based Strategic Architecture
