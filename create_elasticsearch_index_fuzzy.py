#!/usr/bin/env python3
"""
é«˜åº¦ãªãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ç”¨Elasticsearch Index Creation Script

ä»•æ§˜æ›¸ã«å¾“ã£ãŸå¤šå±¤å‹æ¤œç´¢ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ç”¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®š:
- ãƒãƒ«ãƒãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
- ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ (normalized_sorted, edge_ngram)
- ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢æœ€é©åŒ–
"""

import json
import os
from elasticsearch import Elasticsearch
from typing import Dict, List, Any
import time

# è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from app_v2.utils.lemmatization import lemmatize_term
    LEMMATIZATION_AVAILABLE = True
    print("âœ… è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âš ï¸ è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“: {e}")
    print("   åŸºæœ¬æ©Ÿèƒ½ã®ã¿ã§å®Ÿè¡Œã—ã¾ã™")
    LEMMATIZATION_AVAILABLE = False
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
    def lemmatize_term(term: str) -> str:
        return term.lower() if term else ""


def create_fuzzy_index_mapping() -> Dict[str, Any]:
    """ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ç”¨Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®šç¾©"""
    return {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "standard_analyzer_with_synonyms": {
                        "type": "standard",
                        "stopwords": "_none_"
                    },
                    "normalized_sorted_analyzer": {
                        "tokenizer": "standard",
                        "filter": [
                            "lowercase",
                            "asciifolding",
                            "sort_tokens_filter"
                        ]
                    },
                    "edge_ngram_analyzer": {
                        "tokenizer": "standard",
                        "filter": [
                            "lowercase",
                            "asciifolding",
                            "edge_ngram_filter"
                        ]
                    }
                },
                "filter": {
                    "sort_tokens_filter": {
                        "type": "fingerprint",
                        "separator": " "
                    },
                    "edge_ngram_filter": {
                        "type": "edge_ngram",
                        "min_gram": 2,
                        "max_gram": 15
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "id": {
                    "type": "keyword"
                },
                "search_name": {
                    "type": "text",
                    "analyzer": "standard_analyzer_with_synonyms",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        },
                        "normalized_sorted": {
                            "type": "text",
                            "analyzer": "normalized_sorted_analyzer"
                        },
                        "edge_ngram": {
                            "type": "text",
                            "analyzer": "edge_ngram_analyzer"
                        },
                        "suggest": {
                            "type": "completion"
                        }
                    }
                },
                "search_name_lemmatized": {
                    "type": "text",
                    "analyzer": "standard",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        }
                    }
                },
                "search_name_normalized": {
                    "type": "text",
                    "analyzer": "normalized_sorted_analyzer",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        }
                    }
                },
                "description": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "data_type": {
                    "type": "keyword"
                },
                "nutrition": {
                    "type": "object",
                    "properties": {
                        "calories": {"type": "float"},
                        "protein": {"type": "float"},
                        "fat": {"type": "float"},
                        "carbs": {"type": "float"},
                        "carbohydrates": {"type": "float"},
                        "fiber": {"type": "float"},
                        "sugar": {"type": "float"},
                        "sodium": {"type": "float"}
                    }
                },
                "weight": {
                    "type": "float"
                },
                "source_db": {
                    "type": "keyword"
                }
            }
        }
    }


def normalize_and_sort_string(query_string: str) -> str:
    """æ­£è¦åŒ–å‡¦ç†: å°æ–‡å­—åŒ–ã€å¥èª­ç‚¹é™¤å»ã€å˜èªã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã‚½ãƒ¼ãƒˆ"""
    if not query_string:
        return ""
    
    import re
    cleaned_string = re.sub(r'[^\w\s]', '', query_string.lower())
    sorted_words = sorted(cleaned_string.split())
    return " ".join(sorted_words)


def load_json_databases() -> Dict[str, List[Dict[str, Any]]]:
    """JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    databases = {}
    
    db_configs = {
        "yazio": "db/yazio_db.json",
        "mynetdiary": "db/mynetdiary_db.json", 
        "eatthismuch": "db/eatthismuch_db.json"
    }
    
    for db_name, file_path in db_configs.items():
        try:
            if os.path.exists(file_path):
                print(f"Loading {db_name} from {file_path}...")
                with open(file_path, 'r', encoding='utf-8') as f:
                    database = json.load(f)
                    databases[db_name] = database
                    print(f"âœ… Loaded {db_name}: {len(database)} items")
            else:
                print(f"âš ï¸  File not found: {file_path}")
                databases[db_name] = []
        except Exception as e:
            print(f"âŒ Error loading {db_name}: {e}")
            databases[db_name] = []
    
    return databases


def prepare_fuzzy_document(item: Dict[str, Any], source_db: str) -> Dict[str, Any]:
    """ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Elasticsearchç”¨ã«æº–å‚™"""
    search_name = item.get("search_name", "")
    
    # è¦‹å‡ºã—èªåŒ–ã•ã‚ŒãŸsearch_nameã‚’ç”Ÿæˆ
    search_name_lemmatized = ""
    if search_name and LEMMATIZATION_AVAILABLE:
        try:
            search_name_lemmatized = lemmatize_term(search_name)
        except Exception as e:
            print(f"âš ï¸ è¦‹å‡ºã—èªåŒ–ã‚¨ãƒ©ãƒ¼ for '{search_name}': {e}")
            search_name_lemmatized = search_name.lower()
    else:
        search_name_lemmatized = search_name.lower()
    
    # æ­£è¦åŒ–ã•ã‚ŒãŸsearch_nameã‚’ç”Ÿæˆ
    search_name_normalized = normalize_and_sort_string(search_name)
    
    doc = {
        "id": item.get("id", 0),
        "search_name": search_name,
        "search_name_lemmatized": search_name_lemmatized,
        "search_name_normalized": search_name_normalized,
        "description": item.get("description"),
        "data_type": item.get("data_type", "unknown"),
        "nutrition": item.get("nutrition", {}),
        "weight": item.get("weight"),
        "source_db": source_db
    }
    
    # ç©ºã®å€¤ã‚’é™¤å»
    return {k: v for k, v in doc.items() if v is not None}


def bulk_index_documents(es_client: Elasticsearch, index_name: str, documents: List[Dict[str, Any]], batch_size: int = 1000):
    """ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ """
    total_docs = len(documents)
    indexed_count = 0
    
    print(f"ğŸ“¥ Indexing {total_docs} documents in batches of {batch_size}...")
    
    for i in range(0, total_docs, batch_size):
        batch = documents[i:i + batch_size]
        
        # ãƒãƒ«ã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ§‹ç¯‰
        bulk_body = []
        for doc in batch:
            bulk_body.append({
                "index": {
                    "_index": index_name,
                    "_id": f"{doc['source_db']}_{doc['id']}"
                }
            })
            bulk_body.append(doc)
        
        try:
            response = es_client.bulk(body=bulk_body)
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if response.get("errors"):
                error_count = sum(1 for item in response["items"] if "error" in item.get("index", {}))
                print(f"âš ï¸  Batch {i//batch_size + 1}: {error_count} errors in batch")
            
            indexed_count += len(batch)
            print(f"   Progress: {indexed_count}/{total_docs} ({indexed_count/total_docs*100:.1f}%)")
            
        except Exception as e:
            print(f"âŒ Error indexing batch {i//batch_size + 1}: {e}")
    
    print(f"âœ… Indexing completed: {indexed_count} documents")


def test_fuzzy_search_queries(es_client: Elasticsearch, index_name: str):
    """ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ§ª ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    test_queries = [
        "Onion raw",
        "raw onion", 
        "onio raw",  # ã‚¿ã‚¤ãƒ
        "Tortilla white flour",
        "white flour tortilla",  # é †åºé•ã„
        "tomat",  # éƒ¨åˆ†ä¸€è‡´
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Testing query: '{query}'")
        
        # Tier 3ç›¸å½“ã®ã‚¯ã‚¨ãƒª
        search_query = {
            "min_score": 3.0,
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["search_name"],
                    "type": "best_fields",
                    "fuzziness": "AUTO",
                    "prefix_length": 1,
                    "max_expansions": 10
                }
            },
            "size": 3
        }
        
        try:
            response = es_client.search(index=index_name, body=search_query)
            hits = response['hits']['hits']
            
            if hits:
                print(f"   âœ… Found {len(hits)} matches:")
                for hit in hits:
                    print(f"      - {hit['_source']['search_name']} (score: {hit['_score']:.2f})")
            else:
                print("   âŒ No matches found")
                
        except Exception as e:
            print(f"   âŒ Search error: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== é«˜åº¦ãªãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ç”¨Elasticsearch Index Creation ===")
    
    # Elasticsearchã«æ¥ç¶š
    try:
        es_client = Elasticsearch(
            hosts=["http://localhost:9200"],
            timeout=30,
            max_retries=3,
            retry_on_timeout=True
        )
        
        if not es_client.ping():
            print("âŒ Elasticsearchã«æ¥ç¶šã§ãã¾ã›ã‚“")
            return
        
        print("âœ… Elasticsearchæ¥ç¶šæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ Elasticsearchæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return
    
    index_name = "nutrition_fuzzy_search"
    
    # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
    if es_client.indices.exists(index=index_name):
        print(f"ğŸ—‘ï¸  æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã‚’å‰Šé™¤ä¸­...")
        es_client.indices.delete(index=index_name)
    
    # æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    print(f"ğŸ—ï¸  æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ '{index_name}' ã‚’ä½œæˆä¸­...")
    mapping = create_fuzzy_index_mapping()
    es_client.indices.create(index=index_name, body=mapping)
    print("âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
    print("\nğŸ“š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿ä¸­...")
    databases = load_json_databases()
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™
    print("\nğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™ä¸­...")
    all_documents = []
    
    for db_name, items in databases.items():
        for item in items:
            doc = prepare_fuzzy_document(item, db_name)
            all_documents.append(doc)
    
    print(f"âœ… æº–å‚™å®Œäº†: {len(all_documents)} documents")
    
    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    if all_documents:
        bulk_index_documents(es_client, index_name, all_documents)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥
        print("ğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ä¸­...")
        es_client.indices.refresh(index=index_name)
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        stats = es_client.indices.stats(index=index_name)
        doc_count = stats['indices'][index_name]['total']['docs']['count']
        print(f"ğŸ“Š ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ: {doc_count} documents indexed")
        
        # ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ
        test_fuzzy_search_queries(es_client, index_name)
        
    else:
        print("âš ï¸  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
    
    print("\nâœ… ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†!")


if __name__ == "__main__":
    main() 