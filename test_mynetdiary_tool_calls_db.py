#!/usr/bin/env python3
"""
MyNetDiary Tool Calls DB Test Script

mynetdiary_tool_calls_dbã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ãŸå®Œå…¨ãªã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
å…¨ã¦ã®æ¤œç´¢çµæœã‚’è©³ç´°ã«å‡ºåŠ›
"""

import requests
import json
from datetime import datetime
from typing import List, Dict, Any

# Elasticsearchç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ç”¨
ELASTICSEARCH_URL = "http://localhost:9200"
INDEX_NAME = "mynetdiary_tool_calls_db"

def elasticsearch_search(query: str, size: int = 10) -> Dict[str, Any]:
    """Elasticsearchã«ç›´æ¥ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
    search_body = {
        "query": {
            "multi_match": {
                "query": query,
                "fields": ["search_name^3", "description^2", "original_name"]
            }
        },
        "size": size
    }
    
    try:
        response = requests.post(
            f"{ELASTICSEARCH_URL}/{INDEX_NAME}/_search",
            headers={"Content-Type": "application/json"},
            data=json.dumps(search_body)
        )
        return response.json()
    except Exception as e:
        return {"error": str(e)}

def format_nutrition_info(nutrition: Dict[str, float]) -> str:
    """æ „é¤Šæƒ…å ±ã‚’è¦‹ã‚„ã™ã„å½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    return f"ğŸ½ {nutrition.get('calories', 0):.1f}kcal | ğŸ¥© {nutrition.get('protein', 0):.1f}g | ğŸ {nutrition.get('carbs', 0):.1f}g | ğŸ§ˆ {nutrition.get('fat', 0):.1f}g"

def test_single_query(query: str, expected_description: str = "") -> Dict[str, Any]:
    """å˜ä¸€ã‚¯ã‚¨ãƒªã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print(f"\n{'='*80}")
    print(f"ğŸ” ã‚¯ã‚¨ãƒª: '{query}'")
    if expected_description:
        print(f"ğŸ“ æœŸå¾…ã™ã‚‹çµæœ: {expected_description}")
    print(f"{'='*80}")
    
    # Elasticsearchæ¤œç´¢ã‚’å®Ÿè¡Œ
    result = elasticsearch_search(query, size=15)
    
    if "error" in result:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        return {"query": query, "error": result["error"], "results": []}
    
    hits = result.get("hits", {}).get("hits", [])
    total_hits = result.get("hits", {}).get("total", {}).get("value", 0)
    
    print(f"ğŸ“Š ç·æ¤œç´¢çµæœæ•°: {total_hits}")
    print(f"ğŸ“‹ è¡¨ç¤ºçµæœæ•°: {len(hits)}")
    
    processed_results = []
    
    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]
        
        search_name = source.get("search_name", "Unknown")
        description = source.get("description", "No description")
        original_name = source.get("original_name", "No original name")
        nutrition = source.get("nutrition", {})
        processing_method = source.get("processing_method", "Unknown")
        
        print(f"\nğŸ† çµæœ #{i} (ã‚¹ã‚³ã‚¢: {score:.2f})")
        print(f"   ğŸ”¸ æ¤œç´¢å: '{search_name}'")
        print(f"   ğŸ”¸ èª¬æ˜: '{description}'")
        print(f"   ğŸ”¸ å…ƒã®åå‰: '{original_name}'")
        print(f"   ğŸ”¸ æ „é¤Šæƒ…å ±: {format_nutrition_info(nutrition)}")
        print(f"   ğŸ”¸ å‡¦ç†æ–¹æ³•: {processing_method}")
        
        processed_results.append({
            "rank": i,
            "score": score,
            "search_name": search_name,
            "description": description,
            "original_name": original_name,
            "nutrition": nutrition,
            "processing_method": processing_method
        })
    
    return {
        "query": query,
        "total_hits": total_hits,
        "displayed_results": len(hits),
        "results": processed_results
    }

def run_comprehensive_test() -> Dict[str, Any]:
    """åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print("ğŸš€ MyNetDiary Tool Calls DB å®Œå…¨ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆé–‹å§‹")
    print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ—„ï¸ å¯¾è±¡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {INDEX_NAME}")
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆ
    test_queries = [
        # åŸºæœ¬çš„ãªé£Ÿæ
        ("beans", "ã‚·ãƒ³ãƒ—ãƒ«ãªbeansæ¤œç´¢"),
        ("chicken", "ãƒã‚­ãƒ³é–¢é€£ã®æ¤œç´¢"),
        ("tomato", "ãƒˆãƒãƒˆé–¢é€£ã®æ¤œç´¢"),
        ("rice", "ç±³é–¢é€£ã®æ¤œç´¢"),
        
        # è¤‡åˆèª
        ("chicken breast", "ãƒã‚­ãƒ³ãƒ–ãƒ¬ã‚¹ãƒˆã®æ¤œç´¢"),
        ("brown rice", "ç„ç±³ã®æ¤œç´¢"),
        ("black beans", "é»’è±†ã®æ¤œç´¢"),
        
        # èª¿ç†æ–¹æ³•è¾¼ã¿
        ("fried chicken", "ãƒ•ãƒ©ã‚¤ãƒ‰ãƒã‚­ãƒ³ã®æ¤œç´¢"),
        ("baked potato", "ãƒ™ã‚¤ã‚¯ãƒ‰ãƒãƒ†ãƒˆã®æ¤œç´¢"),
        ("steamed broccoli", "è’¸ã—ãƒ–ãƒ­ãƒƒã‚³ãƒªãƒ¼ã®æ¤œç´¢"),
        
        # å¤§æ–‡å­—å°æ–‡å­—ãƒ†ã‚¹ãƒˆ
        ("BEANS", "å¤§æ–‡å­—ã§ã®beansæ¤œç´¢"),
        ("Chicken", "ã‚¿ã‚¤ãƒˆãƒ«ã‚±ãƒ¼ã‚¹ã§ã®chickenæ¤œç´¢"),
        
        # éƒ¨åˆ†ä¸€è‡´ãƒ†ã‚¹ãƒˆ
        ("bean", "beanã®éƒ¨åˆ†ä¸€è‡´æ¤œç´¢"),
        ("chick", "chickã®éƒ¨åˆ†ä¸€è‡´æ¤œç´¢"),
        
        # å…·ä½“çš„ãªæ–™ç†å
        ("soup", "ã‚¹ãƒ¼ãƒ—é–¢é€£ã®æ¤œç´¢"),
        ("salad", "ã‚µãƒ©ãƒ€é–¢é€£ã®æ¤œç´¢"),
        ("pasta", "ãƒ‘ã‚¹ã‚¿é–¢é€£ã®æ¤œç´¢")
    ]
    
    all_results = []
    
    for query, description in test_queries:
        result = test_single_query(query, description)
        all_results.append(result)
    
    # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
    print(f"\n{'='*80}")
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*80}")
    
    total_queries = len(all_results)
    queries_with_results = len([r for r in all_results if r.get("total_hits", 0) > 0])
    avg_results_per_query = sum(r.get("total_hits", 0) for r in all_results) / total_queries
    
    print(f"ğŸ”¢ ç·ã‚¯ã‚¨ãƒªæ•°: {total_queries}")
    print(f"âœ… çµæœã‚ã‚Šã‚¯ã‚¨ãƒªæ•°: {queries_with_results}")
    print(f"âŒ çµæœãªã—ã‚¯ã‚¨ãƒªæ•°: {total_queries - queries_with_results}")
    print(f"ğŸ“ˆ ã‚¯ã‚¨ãƒªã‚ãŸã‚Šå¹³å‡çµæœæ•°: {avg_results_per_query:.1f}")
    
    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"mynetdiary_tool_calls_db_test_results_{timestamp}.json"
    
    test_summary = {
        "timestamp": timestamp,
        "index_name": INDEX_NAME,
        "total_queries": total_queries,
        "queries_with_results": queries_with_results,
        "avg_results_per_query": avg_results_per_query,
        "detailed_results": all_results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(test_summary, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è©³ç´°çµæœã‚’ä¿å­˜: {output_file}")
    
    return test_summary

def check_index_stats():
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®çµ±è¨ˆæƒ…å ±ã‚’ç¢ºèª"""
    print(f"\nğŸ—„ï¸ {INDEX_NAME} ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆæƒ…å ±")
    print("="*60)
    
    try:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ
        stats_response = requests.get(f"{ELASTICSEARCH_URL}/{INDEX_NAME}/_stats")
        stats = stats_response.json()
        
        doc_count = stats.get("_all", {}).get("total", {}).get("docs", {}).get("count", 0)
        index_size = stats.get("_all", {}).get("total", {}).get("store", {}).get("size_in_bytes", 0)
        
        print(f"ğŸ“„ ç·ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {doc_count:,}")
        print(f"ğŸ’½ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚º: {index_size / (1024*1024):.2f} MB")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        sample_response = requests.get(f"{ELASTICSEARCH_URL}/{INDEX_NAME}/_search?size=3")
        sample_data = sample_response.json()
        
        print(f"\nğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿:")
        for i, hit in enumerate(sample_data.get("hits", {}).get("hits", []), 1):
            source = hit["_source"]
            print(f"  {i}. '{source.get('search_name', 'Unknown')}' - {source.get('description', 'No description')}")
            
    except Exception as e:
        print(f"âŒ çµ±è¨ˆæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    print("ğŸ” MyNetDiary Tool Calls DB ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ")
    print("="*80)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆæƒ…å ±
    check_index_stats()
    
    # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = run_comprehensive_test()
    
    print(f"\nğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†!")
    print(f"ğŸ“Š {results['queries_with_results']}/{results['total_queries']} ã‚¯ã‚¨ãƒªã§çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")