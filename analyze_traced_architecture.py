#!/usr/bin/env python3
"""
ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ (ä¾å­˜é–¢ä¿‚è¿½è·¡ç‰ˆ)

å®Ÿéš›ã«test_local_nutrition_search_v2.pyã‹ã‚‰è¿½è·¡ã•ã‚ŒãŸä¾å­˜é–¢ä¿‚ã«åŸºã¥ã„ã¦
æ­£ç¢ºãªãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import os
import datetime
from pathlib import Path
from typing import Dict, List

def get_file_content(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å®‰å…¨ã«èª­ã¿å–ã‚‹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        return f"ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {str(e)}"

def analyze_traced_dependencies():
    """è¿½è·¡ã•ã‚ŒãŸä¾å­˜é–¢ä¿‚ã«åŸºã¥ãã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ"""
    
    # å®Ÿéš›ã«è¿½è·¡ã•ã‚ŒãŸä¾å­˜é–¢ä¿‚
    traced_dependencies = {'project_python_files': ['test_local_nutrition_search_v2.py', 'venv/lib/python3.9/site-packages/requests/__init__.py'], 'server_files': ['app_v2/__init__.py', 'app_v2/api/__init__.py', 'app_v2/api/v1/__init__.py', 'app_v2/api/v1/endpoints/__init__.py', 'app_v2/api/v1/endpoints/meal_analysis.py', 'app_v2/components/__init__.py', 'app_v2/components/base.py', 'app_v2/components/local_nutrition_search_component.py', 'app_v2/components/phase1_component.py', 'app_v2/components/usda_query_component.py', 'app_v2/config/__init__.py', 'app_v2/config/prompts/__init__.py', 'app_v2/config/prompts/phase1_prompts.py', 'app_v2/config/prompts/phase2_prompts.py', 'app_v2/config/settings.py', 'app_v2/main/app.py', 'app_v2/models/__init__.py', 'app_v2/models/nutrition_models.py', 'app_v2/models/nutrition_search_models.py', 'app_v2/models/phase1_models.py', 'app_v2/models/phase2_models.py', 'app_v2/models/usda_models.py', 'app_v2/pipeline/__init__.py', 'app_v2/pipeline/orchestrator.py', 'app_v2/pipeline/result_manager.py', 'app_v2/services/__init__.py', 'app_v2/services/gemini_service.py', 'app_v2/services/nutrition_calculation_service.py', 'app_v2/services/usda_service.py', 'nutrition_db_experiment/search_service/api/query_builder.py', 'nutrition_db_experiment/search_service/api/search_handler.py', 'nutrition_db_experiment/search_service/nlp/query_preprocessor.py', 'nutrition_db_experiment/search_service/tests/test_search_algorithm.py'], 'data_files': [], 'config_files': ['nutrition_db_experiment/nutrition_database_specification.md'], 'external_files': ['/Users/odasoya/.pyenv/versions/3.9.6/lib/python3.9/datetime.py', '/Users/odasoya/.pyenv/versions/3.9.6/lib/python3.9/json/__init__.py', '/Users/odasoya/.pyenv/versions/3.9.6/lib/python3.9/os.py'], 'missing_files': [], 'import_errors': []}
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ©Ÿèƒ½åˆ¥ã«åˆ†é¡
    files_to_analyze = {
        "ğŸ¯ å®Ÿè¡Œèµ·ç‚¹ãƒ•ã‚¡ã‚¤ãƒ«": [
            "test_local_nutrition_search_v2.py"
        ],
        "ğŸ—ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…Pythonãƒ•ã‚¡ã‚¤ãƒ«": traced_dependencies["project_python_files"],
        "ğŸŒ ã‚µãƒ¼ãƒãƒ¼å´ãƒ•ã‚¡ã‚¤ãƒ«": traced_dependencies["server_files"],
        "âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«": traced_dependencies["config_files"]
    }
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"traced_nutrition_architecture_{timestamp}.txt"
    
    with open(output_file, 'w', encoding='utf-8') as out_f:
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        out_f.write("=" * 80 + "\n")
        out_f.write("MEAL ANALYSIS API v2.0 - ä¾å­˜é–¢ä¿‚è¿½è·¡ç‰ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ\n")
        out_f.write("=" * 80 + "\n")
        out_f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        out_f.write(f"è¿½è·¡èµ·ç‚¹: test_local_nutrition_search_v2.py\n")
        out_f.write(f"ç·è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(traced_dependencies['project_python_files']) + len(traced_dependencies['server_files']) + len(traced_dependencies['config_files'])}\n")
        out_f.write("=" * 80 + "\n\n")
        
        # ä¾å­˜é–¢ä¿‚æ¦‚è¦
        out_f.write("ğŸ“Š DEPENDENCY TRACE SUMMARY\n")
        out_f.write("-" * 40 + "\n")
        out_f.write(f"âœ… Project Python Files: {len(traced_dependencies['project_python_files'])}\n")
        out_f.write(f"ğŸŒ Server Files: {len(traced_dependencies['server_files'])}\n")
        out_f.write(f"âš™ï¸ Config Files: {len(traced_dependencies['config_files'])}\n")
        out_f.write(f"ğŸ—ƒï¸ Data Files (excluded): {len(traced_dependencies['data_files'])}\n")
        out_f.write(f"ğŸŒ External Files: {len(traced_dependencies['external_files'])}\n")
        out_f.write(f"âŒ Missing Files: {len(traced_dependencies['missing_files'])}\n")
        if traced_dependencies['import_errors']:
            out_f.write(f"âš ï¸ Import Errors: {len(traced_dependencies['import_errors'])}\n")
        out_f.write("\n")
        
        # ã‚¨ãƒ©ãƒ¼ã¨æ¬ æãƒ•ã‚¡ã‚¤ãƒ«ã®å ±å‘Š
        if traced_dependencies['missing_files']:
            out_f.write("âŒ MISSING FILES\n")
            out_f.write("-" * 20 + "\n")
            for missing in traced_dependencies['missing_files']:
                out_f.write(f"- {missing}\n")
            out_f.write("\n")
        
        if traced_dependencies['import_errors']:
            out_f.write("âš ï¸ IMPORT ERRORS\n")
            out_f.write("-" * 20 + "\n")
            for error in traced_dependencies['import_errors']:
                out_f.write(f"- {error}\n")
            out_f.write("\n")
        
        # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        for category, file_list in files_to_analyze.items():
            if not file_list:
                continue
                
            out_f.write(f"{category}\n")
            out_f.write("=" * 60 + "\n\n")
            
            for file_path in file_list:
                out_f.write(f"ğŸ“„ FILE: {file_path}\n")
                out_f.write("-" * 50 + "\n")
                
                if os.path.exists(file_path):
                    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
                    file_stat = os.stat(file_path)
                    file_size = file_stat.st_size
                    mod_time = datetime.datetime.fromtimestamp(file_stat.st_mtime)
                    
                    out_f.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes\n")
                    out_f.write(f"æœ€çµ‚æ›´æ–°: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    out_f.write(f"å­˜åœ¨: âœ…\n\n")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯åˆ¶é™ï¼‰
                    if file_path.endswith('.json') and file_size > 1024*1024:  # 1MBä»¥ä¸Šã®JSONã¯æ¦‚è¦ã®ã¿
                        out_f.write("CONTENT: (Large JSON file - showing structure only)\n")
                        try:
                            import json
                            with open(file_path, 'r') as f:
                                data = json.load(f)
                            if isinstance(data, list):
                                out_f.write(f"Array with {len(data)} items\n")
                                if data:
                                    out_f.write(f"Sample item keys: {list(data[0].keys()) if isinstance(data[0], dict) else 'Non-dict items'}\n")
                            elif isinstance(data, dict):
                                out_f.write(f"Object with keys: {list(data.keys())}\n")
                        except:
                            out_f.write("Could not parse JSON structure\n")
                    else:
                        out_f.write("CONTENT:\n")
                        out_f.write("```\n")
                        content = get_file_content(file_path)
                        out_f.write(content)
                        out_f.write("\n```\n")
                    out_f.write("\n")
                else:
                    out_f.write("å­˜åœ¨: âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n\n")
                
                out_f.write("=" * 60 + "\n\n")
        
        # çµ±è¨ˆæƒ…å ±
        out_f.write("ğŸ“Š ANALYSIS STATISTICS\n")
        out_f.write("-" * 40 + "\n")
        total_files = sum(len(files) for files in files_to_analyze.values() if files)
        existing_files = sum(1 for files in files_to_analyze.values() if files for f in files if os.path.exists(f))
        
        out_f.write(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}\n")
        out_f.write(f"å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {existing_files}\n")
        out_f.write(f"åˆ†æå®Œäº†æ™‚åˆ»: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        out_f.write("\nã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€test_local_nutrition_search_v2.pyå®Ÿè¡Œæ™‚ã«\n")
        out_f.write("å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n")
        
    return output_file, total_files, existing_files

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” ä¾å­˜é–¢ä¿‚è¿½è·¡ç‰ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æé–‹å§‹...")
    print("-" * 60)
    
    try:
        output_file, total_files, existing_files = analyze_traced_dependencies()
        
        print(f"âœ… åˆ†æå®Œäº†!")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
        print(f"ğŸ“Š ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
        print(f"âœ… å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {existing_files}")
        
        if existing_files < total_files:
            missing = total_files - existing_files
            print(f"âš ï¸  è¦‹ã¤ã‹ã‚‰ãªã„ãƒ•ã‚¡ã‚¤ãƒ«: {missing}å€‹")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’è¡¨ç¤º
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file)
            print(f"ğŸ“„ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes")
        
        print("\nğŸ‰ ä¾å­˜é–¢ä¿‚è¿½è·¡ç‰ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ!")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
