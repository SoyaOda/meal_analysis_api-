====================================================================================================
MEAL ANALYSIS API v2.0 - 高度戦略的Elasticsearch検索システム アーキテクチャ分析
====================================================================================================
生成日時: 2025-06-11 12:41:22
分析対象: test_advanced_elasticsearch_search.py実行時に呼び出される全ファイル
====================================================================================================

🎯 LATEST EXECUTION RESULTS SUMMARY
------------------------------------------------------------
📊 Analysis ID: fc58aeb4
🕒 Execution Time: 20250611_105809
✅ Total Searches: 6
🎯 Successful Matches: 6
📈 Match Rate: 100.0%
⚡ Search Time: 383ms
📋 Total Results: 30
🗃️ Total Indexed Documents: 11,845

🎯 STRATEGIC APPROACH:
   🍽️  Dish Strategy: eatthismuch_dish_primary + eatthismuch_branded_fallback
   🥕 Ingredient Strategy: eatthismuch_ingredient_primary + multi_db_fallback

📝 INPUT QUERIES ANALYSIS:
   📋 Total Queries: 6
   🍽️  Dish Queries: 1
   🥕 Ingredient Queries: 5
   🍽️  Dishes: Glazed Chicken Thighs with Roasted Baby Potatoes and Mixed Green Salad
   🥕 Ingredients: mixed greens, corn, tomato, baby potato, chicken thigh

====================================================================================================

🚀 ADVANCED STRATEGIC ELASTICSEARCH SEARCH ARCHITECTURE OVERVIEW
--------------------------------------------------------------------------------

🔄 ADVANCED STRATEGIC SEARCH EXECUTION FLOW:
1. test_advanced_elasticsearch_search.py → JPG画像アップロード
2. FastAPI /api/v1/meal-analyses/complete → 完全分析エンドポイント
3. Phase1: Gemini AI 2.5 Flash 画像分析 → 構造化データ抽出
   📋 DetectedFoodItems: 信頼度付き食品識別
   🏷️  Attributes: 材料・調理法・ブランド・ネガティブキュー
4. 高度戦略的Elasticsearch検索:
   📍 DISH戦略: EatThisMuch dish (primary) → EatThisMuch branded (fallback, score<20.0)
   🥕 INGREDIENT戦略: EatThisMuch ingredient (primary) → MyNetDiary/YAZIO/branded (fallback)
   🔧 Strategic Features:
      - Min Score Threshold: 20.0 (動的フォールバック)
      - Strategic Metadata: 戦略フェーズ・タイプ追跡
      - Multi-DB Fallback: 段階的品質保証
5. 結果統合・保存: JSON + Markdown レポート生成

🏗️ ADVANCED COMPONENT-BASED ARCHITECTURE v2.0:
├── Advanced Test Layer
│   └── test_advanced_elasticsearch_search.py (高度戦略的検索テスト)
├── FastAPI Application Layer (app_v2)
│   ├── main/app.py (Server, CORS, health endpoints)
│   └── api/v1/endpoints/meal_analysis.py (Complete analysis API)
├── Pipeline Management Layer
│   ├── orchestrator.py (MealAnalysisPipeline - 全フェーズ統制・構造化データ対応)
│   └── result_manager.py (ResultManager - 結果保存・履歴管理)
├── AI Component Layer
│   ├── base.py (BaseComponent - 共通インターフェース)
│   ├── phase1_component.py (Phase1Component - Gemini構造化分析)
│   └── elasticsearch_nutrition_search_component.py (高度戦略的検索エンジン)
├── AI Service Layer
│   └── gemini_service.py (GeminiService - Vertex AI統合・構造化スキーマ)
├── Data Model Layer
│   ├── nutrition_search_models.py (NutritionMatch, strategic metadata)
│   └── phase1_models.py (DetectedFoodItem, FoodAttribute, 構造化出力)
├── Elasticsearch Infrastructure
│   ├── create_elasticsearch_index.py (11,845ドキュメント インデックス管理)
│   └── elasticsearch-8.10.4/ (高性能検索エンジン)
└── Strategic Data Layer
    ├── yazio_db.json (1,825項目 - バランス食品・25カテゴリ)
    ├── mynetdiary_db.json (1,142項目 - 科学的データ・統一型)
    └── eatthismuch_db.json (8,878項目 - 最大・3データ型対応)

🎯 ADVANCED STRATEGIC SEARCH FEATURES:
- 🔥 Enhanced Dish検索戦略:
  * Primary: EatThisMuch data_type=dish (高関連性料理データ)
  * Fallback: EatThisMuch data_type=branded (スコア<20.0時の自動切替)
  * Strategy Metadata: dish_primary, dish_fallback tracking
- 🥕 Enhanced Ingredient検索戦略:
  * Primary: EatThisMuch data_type=ingredient (メイン食材データ)
  * Multi-DB Fallback: MyNetDiary(科学的) → YAZIO(分類済) → EatThisMuch branded
  * Strategy Metadata: ingredient_primary, ingredient_fallback tracking
- ⚡ Performance Optimization:
  * Strategic Filtering: 関連性重視の絞り込み
  * Dynamic Fallback: スコア閾値ベース自動切替
  * Results Per DB: 5件制限による効率化
- 📊 Advanced Analytics:
  * Strategic Distribution Tracking
  * Database Source Analysis
  * Query Type Classification (dish vs ingredient)
  * Execution Time Monitoring
- 💾 Comprehensive Metadata:
  * strategic_phase: main_dish, main_ingredient, fallback_multi_db
  * strategy_type: dish_primary, dish_fallback, ingredient_primary, ingredient_fallback
  * fallback_source: 補助DB詳細情報
  * elasticsearch_score: 生スコア保持

🔧 TECHNICAL SPECIFICATIONS:
- Search Engine: Elasticsearch 8.10.4 (BM25F + Multi-Signal Boosting)
- AI Service: Google Vertex AI Gemini 2.5 Flash (構造化出力対応)
- Web Framework: FastAPI 0.104+ (async/await, multipart/form-data)
- Architecture Pattern: Strategic Component Pipeline
- Data Format: JSON (100g正規化栄養データ + 戦略メタデータ)
- Search Strategy: Strategic Multi-Stage Filtering + Score-based Fallback
- Authentication: Google Cloud Service Account
- Performance: Sub-second response times, 100% match rates

🚀 ADVANCED IMPROVEMENTS vs BASIC MULTI-DB:
- 🧠 構造化AI分析: DetectedFoodItems + Attributes抽出
- 🎯 戦略的データベース選択: EatThisMuchを中心とした最適化
- 📈 動的品質保証: スコア閾値ベースフォールバック
- 🔍 高度メタデータ追跡: 検索プロセス完全可視化
- ⚡ 効率的リソース使用: 戦略的結果数制限
- 📊 包括的分析: データベース分布・戦略分析
- 🔧 拡張可能設計: 新戦略・DB追加容易
- 💾 永続化対応: JSON + Markdown デュアル出力

🎖️ STRATEGIC SEARCH EXCELLENCE:
- Phase1-to-Search Pipeline: シームレスなデータ流れ
- Multi-Modal Input: JPG画像 → 構造化クエリ
- Intelligent Fallback: 品質保証付き多段階検索
- Real-time Analytics: 実行時戦略分析
- Production-Ready: 認証・エラーハンドリング完備

====================================================================================================

📁 Advanced Strategic Search テスト実行ファイル
================================================================================

📄 FILE: test_advanced_elasticsearch_search.py
------------------------------------------------------------
ファイルサイズ: 28,907 bytes
最終更新: 2025-06-11 12:28:54
存在: ✅

CONTENT:
```python
#!/usr/bin/env python3
"""
Advanced Elasticsearch Search Test v1.0 - Strategic Search Edition (Multi-Image)

ElasticsearchNutritionSearchComponentの戦略的検索機能をテストするスクリプト
test_images内の全JPG画像を対象とし、Phase1解析結果から抽出したクエリで
高度なElasticsearch検索戦略（dish/ingredient戦略的検索）をテスト
"""

import requests
import json
import time
import os
import glob
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional

# Elasticsearch Nutrition Search Component
from app_v2.components.elasticsearch_nutrition_search_component import ElasticsearchNutritionSearchComponent
from app_v2.models.nutrition_search_models import NutritionQueryInput

# API設定
BASE_URL = "http://localhost:8000/api/v1"

# テスト画像のパス（全てのfood*.jpgファイル）
test_images_dir = "test_images"
image_files = sorted(glob.glob(os.path.join(test_images_dir, "food*.jpg")))

async def test_single_image_advanced_elasticsearch_search(image_path: str, main_results_dir: str) -> Optional[Dict[str, Any]]:
    """単一画像でAdvanced Elasticsearch戦略的検索をテスト"""
    
    print(f"\n{'='*60}")
    print(f"🖼️  Testing image: {os.path.basename(image_path)}")
    print(f"{'='*60}")
    
    try:
        # 完全分析エンドポイントを呼び出してPhase1結果を取得
        with open(image_path, "rb") as f:
            files = {"image": (os.path.basename(image_path), f, "image/jpeg")}
            data = {"save_results": True}
            
            print("Starting complete analysis to get Phase1 results...")
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/meal-analyses/complete", files=files, data=data)
            end_time = time.time()
        
        print(f"Status Code: {response.status_code}")
        print(f"Response Time: {end_time - start_time:.2f}s")
        
        if response.status_code != 200:
            print("❌ Failed to get Phase1 results!")
            print(f"Error: {response.text}")
            return None
        
        result = response.json()
        analysis_id = result.get("analysis_id")
        print(f"Analysis ID: {analysis_id}")
        
        # Phase1結果から検索クエリを抽出
        phase1_result = result.get("phase1_result", {})
        dishes = phase1_result.get("dishes", [])
        
        all_queries = []
        dish_names = []
        ingredient_names = []
        
        for dish in dishes:
            dish_name = dish.get("dish_name")
            if dish_name:
                dish_names.append(dish_name)
                all_queries.append(dish_name)
            
            ingredients = dish.get("ingredients", [])
            for ingredient in ingredients:
                ingredient_name = ingredient.get("ingredient_name")
                if ingredient_name:
                    ingredient_names.append(ingredient_name)
                    all_queries.append(ingredient_name)
        
        # 重複を除去
        all_queries = list(set(all_queries))
        dish_names = list(set(dish_names))
        ingredient_names = list(set(ingredient_names))
        
        print(f"\n📊 Extracted Search Queries from Phase1:")
        print(f"- Total dishes: {len(dish_names)}")
        print(f"- Total ingredients: {len(ingredient_names)}")
        print(f"- Total unique queries: {len(all_queries)}")
        
        if len(all_queries) == 0:
            print("❌ No search queries extracted from Phase1 results!")
            return None
        
        # ElasticsearchNutritionSearchComponentを戦略的検索モードで初期化
        print(f"\n🔧 Initializing ElasticsearchNutritionSearchComponent (Strategic Search Mode)...")
        es_component = ElasticsearchNutritionSearchComponent(
            multi_db_search_mode=True,    # 戦略的検索モードを有効化
            results_per_db=5,             # 各データベースから5つずつ結果を取得
            enable_advanced_features=False # 仕様書通りの戦略的検索を実行するため無効化
        )
        
        # 検索入力データを作成
        nutrition_query_input = NutritionQueryInput(
            ingredient_names=ingredient_names,
            dish_names=dish_names,
            preferred_source="elasticsearch"
        )
        
        print(f"📝 Strategic Query Input:")
        print(f"- Ingredient names: {len(ingredient_names)} items")
        print(f"- Dish names: {len(dish_names)} items")
        print(f"- Total search terms: {len(nutrition_query_input.get_all_search_terms())}")
        
        # Advanced Elasticsearch戦略検索を実行
        print(f"\n🔍 Starting Advanced Elasticsearch strategic search...")
        search_start_time = time.time()
        
        search_results = await es_component.execute(nutrition_query_input)
        
        search_end_time = time.time()
        search_time = search_end_time - search_start_time
        
        print(f"✅ Advanced Elasticsearch strategic search completed in {search_time:.3f}s")
        
        # 結果の分析
        matches = search_results.matches
        search_summary = search_results.search_summary
        
        print(f"\n📈 Advanced Elasticsearch Strategic Search Results Summary:")
        print(f"- Total queries: {search_summary.get('total_searches', 0)}")
        print(f"- Successful matches: {search_summary.get('successful_matches', 0)}")
        print(f"- Failed searches: {search_summary.get('failed_searches', 0)}")
        print(f"- Match rate: {search_summary.get('match_rate_percent', 0):.1f}%")
        print(f"- Search time: {search_summary.get('search_time_ms', 0)}ms")
        print(f"- Total results: {search_summary.get('total_results', 0)}")
        
        # 結果を保存
        await save_advanced_elasticsearch_results(
            analysis_id, search_results, all_queries, dish_names, ingredient_names, 
            image_filename=os.path.basename(image_path), main_results_dir=main_results_dir
        )
        
        # この画像の結果をサマリー用に返す
        summary_result = {
            "image_name": os.path.basename(image_path),
            "analysis_id": analysis_id,
            "total_queries": search_summary.get('total_searches', 0),
            "successful_matches": search_summary.get('successful_matches', 0),
            "failed_searches": search_summary.get('failed_searches', 0),
            "match_rate_percent": search_summary.get('match_rate_percent', 0),
            "search_time_ms": search_summary.get('search_time_ms', 0),
            "total_results": search_summary.get('total_results', 0),
            "dish_names": dish_names,
            "ingredient_names": ingredient_names,
            "all_queries": all_queries
        }
        
        # 詳細結果も含めて返す
        detailed_result = {
            "analysis_id": analysis_id,
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "image_filename": os.path.basename(image_path),
            "input_queries": {
                "all_queries": all_queries,
                "dish_names": dish_names,
                "ingredient_names": ingredient_names
            },
            "search_summary": search_results.search_summary,
            "matches": {},
            "warnings": search_results.warnings,
            "errors": search_results.errors
        }
        
        # 検索結果を辞書形式に変換
        for query, match_results in search_results.matches.items():
            if isinstance(match_results, list):
                detailed_result["matches"][query] = [
                    {
                        "id": match.id,
                        "search_name": match.search_name,
                        "description": match.description,
                        "data_type": match.data_type,
                        "source": match.source,
                        "nutrition": match.nutrition,
                        "weight": match.weight,
                        "score": match.score,
                        "search_metadata": match.search_metadata
                    } for match in match_results
                ]
            else:
                detailed_result["matches"][query] = {
                    "id": match_results.id,
                    "search_name": match_results.search_name,
                    "description": match_results.description,
                    "data_type": match_results.data_type,
                    "source": match_results.source,
                    "nutrition": match_results.nutrition,
                    "weight": match_results.weight,
                    "score": match_results.score,
                    "search_metadata": match_results.search_metadata
                }
        
        return summary_result, detailed_result
        
    except Exception as e:
        print(f"❌ Error testing {os.path.basename(image_path)}: {str(e)}")
        return None

async def test_advanced_elasticsearch_search():
    """全画像でAdvanced Elasticsearch戦略的検索をテスト"""
    
    print("🚀 Starting Advanced Elasticsearch Strategic Search Test (Multi-Image)")
    print("=== Advanced Elasticsearch Search Test v1.0 - Strategic Search Edition ===")
    print(f"📁 Testing {len(image_files)} images: {[os.path.basename(f) for f in image_files]}")
    print("🔍 Testing Advanced Elasticsearch strategic search (dish/ingredient optimization)")
    print("📊 Strategic database targeting: EatThisMuch dishes/ingredients + fallback optimization")
    
    if not image_files:
        print("❌ No food*.jpg images found in test_images directory!")
        return False
    
    # 実行用のメインディレクトリを作成
    main_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    main_results_dir = f"analysis_results/multi_image_test_{main_timestamp}"
    os.makedirs(main_results_dir, exist_ok=True)
    print(f"📁 Created main results directory: {main_results_dir}")
    
    # 全体のサマリー用変数
    all_results = []
    all_detailed_results = []  # 詳細検索結果を保存
    total_queries = 0
    total_successful = 0
    total_failed = 0
    total_search_time = 0
    total_results_count = 0
    
    # 各画像でテストを実行
    for image_path in image_files:
        result = await test_single_image_advanced_elasticsearch_search(image_path, main_results_dir)
        if result:
            summary_result, detailed_result = result
            all_results.append(summary_result)
            all_detailed_results.append(detailed_result)
            
            total_queries += summary_result["total_queries"]
            total_successful += summary_result["successful_matches"]
            total_failed += summary_result["failed_searches"]
            total_search_time += summary_result["search_time_ms"]
            total_results_count += summary_result["total_results"]
    
    # 全体のサマリーを表示
    print(f"\n{'='*80}")
    print(f"🎯 OVERALL MULTI-IMAGE TEST SUMMARY")
    print(f"{'='*80}")
    print(f"📊 Images tested: {len(all_results)}/{len(image_files)}")
    print(f"📈 Overall Statistics:")
    print(f"   - Total queries across all images: {total_queries}")
    print(f"   - Total successful matches: {total_successful}")
    print(f"   - Total failed searches: {total_failed}")
    print(f"   - Overall match rate: {(total_successful/total_queries*100) if total_queries > 0 else 0:.1f}%")
    print(f"   - Total search time: {total_search_time}ms")
    print(f"   - Average search time per image: {total_search_time/len(all_results) if all_results else 0:.1f}ms")
    print(f"   - Total results found: {total_results_count}")
    print(f"   - Average results per image: {total_results_count/len(all_results) if all_results else 0:.1f}")
    
    print(f"\n📋 Per-Image Results Breakdown:")
    for i, result in enumerate(all_results, 1):
        print(f"   {i}. {result['image_name']}:")
        print(f"      - Queries: {result['total_queries']} | Matches: {result['successful_matches']} | Success: {result['match_rate_percent']:.1f}%")
        print(f"      - Time: {result['search_time_ms']}ms | Results: {result['total_results']}")
        print(f"      - Dishes: {len(result['dish_names'])} | Ingredients: {len(result['ingredient_names'])}")
    
    # 集約結果を保存（詳細結果も含める）
    await save_multi_image_summary(all_results, total_queries, total_successful, total_failed, total_search_time, total_results_count, all_detailed_results, main_results_dir)
    
    print(f"\n✅ Multi-image Advanced Elasticsearch strategic search test completed!")
    print(f"🎯 Overall success rate: {(total_successful/total_queries*100) if total_queries > 0 else 0:.1f}%")
    
    return len(all_results) > 0

async def save_advanced_elasticsearch_results(analysis_id: str, search_results, all_queries: List[str], dish_names: List[str], ingredient_names: List[str], image_filename: str, main_results_dir: str):
    """Advanced Elasticsearch戦略的検索結果をファイルに保存"""
    
    # メインディレクトリ内にサブディレクトリを作成
    image_base = os.path.splitext(image_filename)[0]  # food1, food2, etc.
    results_dir = f"{main_results_dir}/{image_base}_{analysis_id}"
    os.makedirs(results_dir, exist_ok=True)
    
    # 検索結果を辞書形式に変換
    matches_dict = {}
    for query, match_results in search_results.matches.items():
        if isinstance(match_results, list):
            matches_dict[query] = [
                {
                    "id": match.id,
                    "search_name": match.search_name,
                    "description": match.description,
                    "data_type": match.data_type,
                    "source": match.source,
                    "nutrition": match.nutrition,
                    "weight": match.weight,
                    "score": match.score,
                    "search_metadata": match.search_metadata
                } for match in match_results
            ]
        else:
            matches_dict[query] = {
                "id": match_results.id,
                "search_name": match_results.search_name,
                "description": match_results.description,
                "data_type": match_results.data_type,
                "source": match_results.source,
                "nutrition": match_results.nutrition,
                "weight": match_results.weight,
                "score": match_results.score,
                "search_metadata": match_results.search_metadata
            }
    
    # 1. 全検索結果をJSONで保存
    results_file = os.path.join(results_dir, "advanced_elasticsearch_search_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "analysis_id": analysis_id,
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "image_filename": image_filename,
            "search_method": "elasticsearch_strategic",
            "input_queries": {
                "all_queries": all_queries,
                "dish_names": dish_names,
                "ingredient_names": ingredient_names
            },
            "search_summary": search_results.search_summary,
            "matches": matches_dict,
            "warnings": search_results.warnings,
            "errors": search_results.errors
        }, f, indent=2, ensure_ascii=False)
    
    # 2. 検索サマリーをマークダウンで保存
    summary_file = os.path.join(results_dir, "advanced_elasticsearch_summary.md")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"# Advanced Elasticsearch Strategic Search Results\n\n")
        f.write(f"**Analysis ID:** {analysis_id}\n")
        f.write(f"**Image:** {image_filename}\n")
        f.write(f"**Timestamp:** {datetime.now().strftime('%Y%m%d_%H%M%S')}\n")
        f.write(f"**Search Method:** Advanced Elasticsearch Strategic Search\n")
        f.write(f"**Total Queries:** {len(all_queries)}\n\n")
        
        # 検索サマリー
        summary = search_results.search_summary
        f.write(f"## Search Summary\n\n")
        f.write(f"- **Total searches:** {summary.get('total_searches', 0)}\n")
        f.write(f"- **Successful matches:** {summary.get('successful_matches', 0)}\n")
        f.write(f"- **Failed searches:** {summary.get('failed_searches', 0)}\n")
        f.write(f"- **Match rate:** {summary.get('match_rate_percent', 0):.1f}%\n")
        f.write(f"- **Search time:** {summary.get('search_time_ms', 0)}ms\n")
        f.write(f"- **Total results:** {summary.get('total_results', 0)}\n\n")
    
    print(f"   💾 Results saved: {results_dir}/")

async def save_multi_image_summary(all_results: List[Dict[str, Any]], total_queries: int, total_successful: int, total_failed: int, total_search_time: int, total_results_count: int, detailed_results: List[Dict[str, Any]], main_results_dir: str):
    """マルチ画像テストの全体サマリーと詳細結果を1つのファイルに保存"""
    
    # メインディレクトリに直接保存
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 1. 全体サマリーと詳細結果をJSONで保存
    comprehensive_file = os.path.join(main_results_dir, "comprehensive_multi_image_results.json")
    with open(comprehensive_file, 'w', encoding='utf-8') as f:
        json.dump({
            "timestamp": timestamp,
            "test_type": "comprehensive_multi_image_advanced_elasticsearch_strategic",
            "overall_summary": {
                "images_tested": len(all_results),
                "total_queries": total_queries,
                "total_successful": total_successful,
                "total_failed": total_failed,
                "overall_match_rate_percent": (total_successful/total_queries*100) if total_queries > 0 else 0,
                "total_search_time_ms": total_search_time,
                "average_search_time_per_image_ms": total_search_time/len(all_results) if all_results else 0,
                "total_results_found": total_results_count,
                "average_results_per_image": total_results_count/len(all_results) if all_results else 0
            },
            "per_image_summary": all_results,
            "detailed_search_results": detailed_results or []
        }, f, indent=2, ensure_ascii=False)
    
    # 2. 包括的マークダウンレポートを保存
    comprehensive_md_file = os.path.join(main_results_dir, "comprehensive_multi_image_results.md")
    with open(comprehensive_md_file, 'w', encoding='utf-8') as f:
        f.write(f"# Comprehensive Multi-Image Advanced Elasticsearch Strategic Search Results\n\n")
        f.write(f"**Test Date:** {timestamp}\n")
        f.write(f"**Test Type:** Comprehensive Multi-Image Advanced Elasticsearch Strategic Search\n")
        f.write(f"**Images Tested:** {len(all_results)}\n\n")
        
        # 全体パフォーマンス
        f.write(f"## Overall Performance Summary\n\n")
        f.write(f"- **Total Queries:** {total_queries}\n")
        f.write(f"- **Successful Matches:** {total_successful}\n")
        f.write(f"- **Failed Searches:** {total_failed}\n")
        f.write(f"- **Overall Success Rate:** {(total_successful/total_queries*100) if total_queries > 0 else 0:.1f}%\n")
        f.write(f"- **Total Search Time:** {total_search_time}ms\n")
        f.write(f"- **Average Time per Image:** {total_search_time/len(all_results) if all_results else 0:.1f}ms\n")
        f.write(f"- **Total Results Found:** {total_results_count}\n")
        f.write(f"- **Average Results per Image:** {total_results_count/len(all_results) if all_results else 0:.1f}\n\n")
        
        # パフォーマンス比較表
        f.write(f"## Performance Comparison Table\n\n")
        f.write(f"| Image | Queries | Matches | Success % | Time (ms) | Results | Dishes | Ingredients |\n")
        f.write(f"|-------|---------|---------|-----------|-----------|---------|--------|-------------|\n")
        for result in all_results:
            f.write(f"| {result['image_name']} | {result['total_queries']} | {result['successful_matches']} | {result['match_rate_percent']:.1f}% | {result['search_time_ms']} | {result['total_results']} | {len(result['dish_names'])} | {len(result['ingredient_names'])} |\n")
        
        f.write(f"\n**Average Performance:** {(total_successful/total_queries*100) if total_queries > 0 else 0:.1f}% success rate, {total_search_time/len(all_results) if all_results else 0:.1f}ms per image\n\n")
        
        # 各画像の詳細結果
        if detailed_results:
            f.write(f"## Detailed Search Results by Image\n\n")
            
            for i, detail in enumerate(detailed_results):
                image_info = all_results[i]
                f.write(f"### {i+1}. {image_info['image_name']}\n\n")
                f.write(f"- **Analysis ID:** {image_info['analysis_id']}\n")
                f.write(f"- **Success Rate:** {image_info['match_rate_percent']:.1f}% ({image_info['successful_matches']}/{image_info['total_queries']})\n")
                f.write(f"- **Search Time:** {image_info['search_time_ms']}ms\n")
                f.write(f"- **Total Results:** {image_info['total_results']}\n\n")
                
                # 検出された料理と材料
                f.write(f"#### Detected Items\n\n")
                if image_info['dish_names']:
                    f.write(f"**Dishes ({len(image_info['dish_names'])}):** {', '.join(image_info['dish_names'])}\n\n")
                if image_info['ingredient_names']:
                    f.write(f"**Ingredients ({len(image_info['ingredient_names'])}):** {', '.join(image_info['ingredient_names'])}\n\n")
                
                # 検索結果詳細
                f.write(f"#### Search Results Detail\n\n")
                matches = detail.get('matches', {})
                dish_names = image_info['dish_names']
                
                # 料理結果を先に表示
                dish_results = {k: v for k, v in matches.items() if k in dish_names}
                ingredient_results = {k: v for k, v in matches.items() if k not in dish_names}
                
                if dish_results:
                    f.write(f"##### Dish Search Results\n\n")
                    for j, (query, match_results) in enumerate(dish_results.items(), 1):
                        f.write(f"**{j}. {query} (dish)**\n\n")
                        if isinstance(match_results, list):
                            f.write(f"Found {len(match_results)} results:\n\n")
                            for k, match in enumerate(match_results[:3], 1):  # 上位3件のみ表示
                                f.write(f"   {k}. **{match.get('search_name', 'Unknown')}** (score: {match.get('score', 0):.2f})\n")
                                f.write(f"      - Source: {match.get('source', 'Unknown')}\n")
                                f.write(f"      - Data Type: {match.get('data_type', 'Unknown')}\n")
                                if match.get('nutrition'):
                                    nutrition = match['nutrition']
                                    calories = nutrition.get('calories', 0)
                                    protein = nutrition.get('protein', 0)
                                    fat = nutrition.get('fat', 0)
                                    carbs = nutrition.get('carbs', nutrition.get('carbohydrates', 0))
                                    f.write(f"      - Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g\n")
                                f.write(f"\n")
                            if len(match_results) > 3:
                                f.write(f"   ... and {len(match_results) - 3} more results\n\n")
                        f.write(f"\n")
                
                if ingredient_results:
                    f.write(f"##### Ingredient Search Results\n\n")
                    for j, (query, match_results) in enumerate(ingredient_results.items(), 1):
                        f.write(f"**{j}. {query} (ingredient)**\n\n")
                        if isinstance(match_results, list):
                            f.write(f"Found {len(match_results)} results:\n\n")
                            for k, match in enumerate(match_results[:2], 1):  # 上位2件のみ表示
                                f.write(f"   {k}. **{match.get('search_name', 'Unknown')}** (score: {match.get('score', 0):.2f})\n")
                                f.write(f"      - Source: {match.get('source', 'Unknown')}\n")
                                f.write(f"      - Data Type: {match.get('data_type', 'Unknown')}\n")
                                if match.get('nutrition'):
                                    nutrition = match['nutrition']
                                    calories = nutrition.get('calories', 0)
                                    protein = nutrition.get('protein', 0)
                                    fat = nutrition.get('fat', 0)
                                    carbs = nutrition.get('carbs', nutrition.get('carbohydrates', 0))
                                    f.write(f"      - Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g\n")
                                f.write(f"\n")
                            if len(match_results) > 2:
                                f.write(f"   ... and {len(match_results) - 2} more results\n\n")
                        f.write(f"\n")
                
                f.write(f"---\n\n")
        
        # 戦略的検索統計（全画像統合）
        if detailed_results:
            f.write(f"## Strategic Search Statistics (All Images)\n\n")
            
            # データベース分布統計
            total_db_stats = {"elasticsearch_eatthismuch": 0, "elasticsearch_yazio": 0, "elasticsearch_mynetdiary": 0}
            strategy_stats = {"dish_primary": 0, "dish_fallback": 0, "ingredient_primary": 0, "ingredient_fallback": 0}
            total_individual_results = 0
            
            for detail in detailed_results:
                matches = detail.get('matches', {})
                dish_names = set(detail.get('input_queries', {}).get('dish_names', []))
                
                for query, match_results in matches.items():
                    if isinstance(match_results, list):
                        total_individual_results += len(match_results)
                        for match in match_results:
                            source = match.get('source', '')
                            if source in total_db_stats:
                                total_db_stats[source] += 1
                            
                            # 戦略統計
                            metadata = match.get('search_metadata', {})
                            strategy_type = metadata.get('strategy_type', '')
                            if strategy_type in strategy_stats:
                                strategy_stats[strategy_type] += 1
            
            f.write(f"### Database Distribution\n\n")
            for db, count in total_db_stats.items():
                if count > 0:
                    percentage = (count / total_individual_results) * 100 if total_individual_results > 0 else 0
                    db_name = db.replace('elasticsearch_', '').title()
                    f.write(f"- **{db_name}:** {count} results ({percentage:.1f}%)\n")
            
            f.write(f"\n### Strategy Distribution\n\n")
            total_strategy_results = sum(strategy_stats.values())
            for strategy, count in strategy_stats.items():
                if count > 0:
                    percentage = (count / total_strategy_results) * 100 if total_strategy_results > 0 else 0
                    strategy_name = strategy.replace('_', ' ').title()
                    f.write(f"- **{strategy_name}:** {count} results ({percentage:.1f}%)\n")
    
    print(f"\n📊 Comprehensive multi-image results saved to:")
    print(f"   📁 {main_results_dir}/")
    print(f"   📄 comprehensive_multi_image_results.json")
    print(f"   📄 comprehensive_multi_image_results.md")

if __name__ == "__main__":
    print("🚀 Starting Advanced Elasticsearch Strategic Search Test")
    success = asyncio.run(test_advanced_elasticsearch_search())
    
    if success:
        print("\n✅ Advanced Elasticsearch strategic search test completed successfully!")
        print("🎯 Strategic search optimization: dish/ingredient targeting with fallback strategies")
    else:
        print("\n❌ Advanced Elasticsearch strategic search test failed!") 
```

================================================================================

📁 FastAPI アプリケーション層 (app_v2)
================================================================================

📄 FILE: app_v2/main/app.py
------------------------------------------------------------
ファイルサイズ: 2,030 bytes
最終更新: 2025-06-05 12:55:53
存在: ✅

CONTENT:
```python
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ..api.v1.endpoints import meal_analysis
from ..config import get_settings

# 環境変数の設定（既存のappと同じ）
os.environ.setdefault("USDA_API_KEY", "vSWtKJ3jYD0Cn9LRyVJUFkuyCt9p8rEtVXz74PZg")
os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", "/Users/odasoya/meal_analysis_api /service-account-key.json")
os.environ.setdefault("GEMINI_PROJECT_ID", "recording-diet-ai-3e7cf")
os.environ.setdefault("GEMINI_LOCATION", "us-central1")
os.environ.setdefault("GEMINI_MODEL_NAME", "gemini-2.5-flash-preview-05-20")

# ロギング設定
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# FastAPIアプリの作成
app = FastAPI(
    title="食事分析 API v2.0",
    description="コンポーネント化された食事分析システム",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS設定
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ルーターの登録
app.include_router(
    meal_analysis.router,
    prefix="/api/v1/meal-analyses",
    tags=["Complete Meal Analysis v2.0"]
)

# ルートエンドポイント
@app.get("/")
async def root():
    """ルートエンドポイント"""
    return {
        "message": "食事分析 API v2.0 - コンポーネント化版",
        "version": "2.0.0",
        "architecture": "Component-based Pipeline",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """ヘルスチェック"""
    return {
        "status": "healthy",
        "version": "v2.0",
        "components": ["Phase1Component", "USDAQueryComponent"]
    }

if __name__ == "__main__":
    import uvicorn
    settings = get_settings()
    uvicorn.run(
        "app_v2.main.app:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=True
    ) 
```

================================================================================

📄 FILE: app_v2/api/v1/endpoints/meal_analysis.py
------------------------------------------------------------
ファイルサイズ: 2,696 bytes
最終更新: 2025-06-09 11:27:28
存在: ✅

CONTENT:
```python
from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from typing import Optional
import logging

from ....pipeline import MealAnalysisPipeline

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/complete")
async def complete_meal_analysis(
    image: UploadFile = File(...),
    save_results: bool = Form(True),
    save_detailed_logs: bool = Form(True)
):
    """
    完全な食事分析を実行（v2.0 コンポーネント化版）
    
    - Phase 1: Gemini AIによる画像分析
    - USDA Query: 食材のUSDAデータベース照合
    - Phase 2: 計算戦略決定と栄養価精緻化 (TODO)
    - Nutrition Calculation: 最終栄養価計算 (TODO)
    
    Args:
        image: 分析対象の食事画像
        save_results: 結果を保存するかどうか (デフォルト: True)
        save_detailed_logs: 詳細ログを保存するかどうか (デフォルト: True)
    
    Returns:
        完全な分析結果と栄養価計算、詳細ログファイルパス
    """
    
    try:
        # 画像の検証
        if not image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="アップロードされたファイルは画像である必要があります")
        
        # 画像データの読み込み
        image_data = await image.read()
        logger.info(f"Starting complete meal analysis pipeline v2.0 (detailed_logs: {save_detailed_logs})")
        
        # パイプラインの実行
        pipeline = MealAnalysisPipeline()
        result = await pipeline.execute_complete_analysis(
            image_bytes=image_data,
            image_mime_type=image.content_type,
            save_results=save_results,
            save_detailed_logs=save_detailed_logs
        )
        
        logger.info(f"Complete analysis pipeline v2.0 finished successfully")
        
        return JSONResponse(
            status_code=200,
            content=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Complete analysis failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Complete analysis failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """ヘルスチェック"""
    return {"status": "healthy", "version": "v2.0", "message": "食事分析API v2.0 - コンポーネント化版"}


@router.get("/pipeline-info")
async def get_pipeline_info():
    """パイプライン情報の取得"""
    pipeline = MealAnalysisPipeline()
    return pipeline.get_pipeline_info() 
```

================================================================================

📁 パイプライン統制層
================================================================================

📄 FILE: app_v2/pipeline/__init__.py
------------------------------------------------------------
ファイルサイズ: 142 bytes
最終更新: 2025-06-05 13:08:07
存在: ✅

CONTENT:
```python
from .orchestrator import MealAnalysisPipeline
from .result_manager import ResultManager

__all__ = ["MealAnalysisPipeline", "ResultManager"] 
```

================================================================================

📄 FILE: app_v2/pipeline/orchestrator.py
------------------------------------------------------------
ファイルサイズ: 16,426 bytes
最終更新: 2025-06-11 10:51:22
存在: ✅

CONTENT:
```python
import uuid
import json
from datetime import datetime
from typing import Optional, Dict, Any
import logging

from ..components import Phase1Component, USDAQueryComponent, LocalNutritionSearchComponent, ElasticsearchNutritionSearchComponent
from ..models import (
    Phase1Input, Phase1Output,
    USDAQueryInput, USDAQueryOutput,
    NutritionQueryInput
)
from ..config import get_settings
from .result_manager import ResultManager

logger = logging.getLogger(__name__)


class MealAnalysisPipeline:
    """
    食事分析パイプラインのオーケストレーター
    
    4つのフェーズを統合して完全な分析を実行します。
    """
    
    def __init__(self, use_local_nutrition_search: Optional[bool] = None, use_elasticsearch_search: Optional[bool] = None):
        """
        パイプラインの初期化
        
        Args:
            use_local_nutrition_search: ローカル栄養データベース検索を使用するかどうか（レガシー）
            use_elasticsearch_search: Elasticsearch栄養データベース検索を使用するかどうか
                                    None: 設定ファイルから自動取得
                                    True: ElasticsearchNutritionSearchComponent使用（推奨）
                                    False: 従来のUSDAQueryComponent使用
        """
        self.pipeline_id = str(uuid.uuid4())[:8]
        self.settings = get_settings()
        
        # Elasticsearch検索優先度の決定
        if use_elasticsearch_search is not None:
            self.use_elasticsearch_search = use_elasticsearch_search
        elif hasattr(self.settings, 'USE_ELASTICSEARCH_SEARCH'):
            self.use_elasticsearch_search = self.settings.USE_ELASTICSEARCH_SEARCH
        else:
            # デフォルトはElasticsearch使用
            self.use_elasticsearch_search = True
        
        # レガシー互換性の処理
        if use_local_nutrition_search is not None and use_elasticsearch_search is None:
            # 旧パラメータが指定された場合はそちらを優先
            if use_local_nutrition_search:
                self.use_elasticsearch_search = False
                self.use_local_nutrition_search = True
            else:
                self.use_elasticsearch_search = False
                self.use_local_nutrition_search = False
        else:
            self.use_local_nutrition_search = not self.use_elasticsearch_search and (
                use_local_nutrition_search or getattr(self.settings, 'USE_LOCAL_NUTRITION_SEARCH', False)
            )
        
        # コンポーネントの初期化
        self.phase1_component = Phase1Component()
        
        # 栄養データベース検索コンポーネントの選択
        if self.use_elasticsearch_search:
            self.nutrition_search_component = ElasticsearchNutritionSearchComponent(
                multi_db_search_mode=True,
                results_per_db=5
            )
            self.search_component_name = "ElasticsearchNutritionSearchComponent"
            logger.info("Using Elasticsearch nutrition database search (high-performance, multi-DB mode)")
        elif self.use_local_nutrition_search:
            self.nutrition_search_component = LocalNutritionSearchComponent()
            self.search_component_name = "LocalNutritionSearchComponent"
            logger.info("Using local nutrition database search (nutrition_db_experiment)")
        else:
            self.nutrition_search_component = USDAQueryComponent()
            self.search_component_name = "USDAQueryComponent"
            logger.info("Using traditional USDA API search")
            
        # TODO: Phase2ComponentとNutritionCalculationComponentを追加
        
        self.logger = logging.getLogger(f"{__name__}.{self.pipeline_id}")
        
    async def execute_complete_analysis(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        optional_text: Optional[str] = None,
        save_results: bool = True,
        save_detailed_logs: bool = True
    ) -> Dict[str, Any]:
        """
        完全な食事分析を実行
        
        Args:
            image_bytes: 画像データ
            image_mime_type: 画像のMIMEタイプ
            optional_text: オプションのテキスト
            save_results: 結果を保存するかどうか
            save_detailed_logs: 詳細ログを保存するかどうか
            
        Returns:
            完全な分析結果
        """
        analysis_id = str(uuid.uuid4())[:8]
        start_time = datetime.now()
        
        # ResultManagerの初期化
        result_manager = ResultManager(analysis_id) if save_detailed_logs else None
        
        self.logger.info(f"[{analysis_id}] Starting complete meal analysis pipeline")
        if self.use_elasticsearch_search:
            self.logger.info(f"[{analysis_id}] Nutrition search method: Elasticsearch (high-performance)")
        elif self.use_local_nutrition_search:
            self.logger.info(f"[{analysis_id}] Nutrition search method: Local Database")
        else:
            self.logger.info(f"[{analysis_id}] Nutrition search method: USDA API")
        
        try:
            # === Phase 1: 画像分析 ===
            self.logger.info(f"[{analysis_id}] Phase 1: Image analysis")
            
            phase1_input = Phase1Input(
                image_bytes=image_bytes,
                image_mime_type=image_mime_type,
                optional_text=optional_text
            )
            
            # Phase1の詳細ログを作成
            phase1_log = result_manager.create_execution_log("Phase1Component", f"{analysis_id}_phase1") if result_manager else None
            
            phase1_result = await self.phase1_component.execute(phase1_input, phase1_log)
            
            self.logger.info(f"[{analysis_id}] Phase 1 completed - Detected {len(phase1_result.dishes)} dishes")
            
            # === Nutrition Search Phase: データベース照合 ===
            if self.use_elasticsearch_search:
                search_phase_name = "Elasticsearch Search"
            elif self.use_local_nutrition_search:
                search_phase_name = "Local Nutrition Search"
            else:
                search_phase_name = "USDA Query"
                
            self.logger.info(f"[{analysis_id}] {search_phase_name} Phase: Database matching")
            
            # === 統一された栄養検索入力を作成 ===
            if self.use_elasticsearch_search or self.use_local_nutrition_search:
                # Elasticsearch検索またはローカル検索の場合はNutritionQueryInputを使用
                preferred_source = "elasticsearch" if self.use_elasticsearch_search else "local_database"
                nutrition_search_input = NutritionQueryInput(
                    ingredient_names=phase1_result.get_all_ingredient_names(),
                    dish_names=phase1_result.get_all_dish_names(),
                    preferred_source=preferred_source
                )
            else:
                # USDA検索の場合はUSDAQueryInputを使用（レガシー互換性）
                nutrition_search_input = USDAQueryInput(
                    ingredient_names=phase1_result.get_all_ingredient_names(),
                    dish_names=phase1_result.get_all_dish_names()
                )
            
            # Nutrition Searchの詳細ログを作成
            search_log = result_manager.create_execution_log(self.search_component_name, f"{analysis_id}_nutrition_search") if result_manager else None
            
            nutrition_search_result = await self.nutrition_search_component.execute(nutrition_search_input, search_log)
            
            self.logger.info(f"[{analysis_id}] {search_phase_name} completed - {nutrition_search_result.get_match_rate():.1%} match rate")
            
            # === 暫定的な結果の構築 (Phase2とNutritionは後で追加) ===
            
            # Phase1の結果を辞書形式に変換（構造化データを含む）
            phase1_dict = {
                "detected_food_items": [
                    {
                        "item_name": item.item_name,
                        "confidence": item.confidence,
                        "attributes": [
                            {
                                "type": attr.type.value if hasattr(attr.type, 'value') else str(attr.type),
                                "value": attr.value,
                                "confidence": attr.confidence
                            }
                            for attr in item.attributes
                        ],
                        "brand": item.brand or "",
                        "category_hints": item.category_hints,
                        "negative_cues": item.negative_cues
                    }
                    for item in phase1_result.detected_food_items
                ],
                "dishes": [
                    {
                        "dish_name": dish.dish_name,
                        "confidence": dish.confidence,
                        "ingredients": [
                            {
                                "ingredient_name": ing.ingredient_name,
                                "confidence": ing.confidence
                            }
                            for ing in dish.ingredients
                        ],
                        "attributes": [
                            {
                                "type": attr.type.value if hasattr(attr.type, 'value') else str(attr.type),
                                "value": attr.value,
                                "confidence": attr.confidence
                            }
                            for attr in dish.detected_attributes
                        ]
                    }
                    for dish in phase1_result.dishes
                ],
                "analysis_confidence": phase1_result.analysis_confidence,
                "processing_notes": phase1_result.processing_notes
            }
            
            # 簡単な栄養計算（暫定）
            total_calories = sum(
                len(dish.ingredients) * 50  # 仮の計算
                for dish in phase1_result.dishes
            )
            
            # 検索方法の特定
            if self.use_elasticsearch_search:
                search_method = "elasticsearch"
                search_api_method = "elasticsearch"
            elif self.use_local_nutrition_search:
                search_method = "local_nutrition_database"
                search_api_method = "local_database"
            else:
                search_method = "usda_api"
                search_api_method = "usda_api"
            
            # 完全分析結果の構築
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            complete_result = {
                "analysis_id": analysis_id,
                "phase1_result": phase1_dict,
                "nutrition_search_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary,
                    "search_method": search_method
                },
                # レガシー互換性のため、usdaキーも残す
                "usda_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary
                },
                "processing_summary": {
                    "total_dishes": len(phase1_result.dishes),
                    "total_ingredients": len(phase1_result.get_all_ingredient_names()),
                    "nutrition_search_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",
                    "usda_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",  # レガシー互換性
                    "total_calories": total_calories,
                    "pipeline_status": "completed",
                    "processing_time_seconds": processing_time,
                    "search_method": search_method
                },
                # 暫定的な最終結果
                "final_nutrition_result": {
                    "dishes": phase1_dict["dishes"],
                    "total_meal_nutrients": {
                        "calories_kcal": total_calories,
                        "protein_g": total_calories * 0.15,  # 仮の値
                        "carbohydrates_g": total_calories * 0.55,  # 仮の値
                        "fat_g": total_calories * 0.30,  # 仮の値
                    }
                },
                "metadata": {
                    "pipeline_version": "v2.0",
                    "timestamp": datetime.now().isoformat(),
                    "components_used": ["Phase1Component", self.search_component_name],
                    "nutrition_search_method": search_api_method
                }
            }
            
            # ResultManagerに最終結果を設定
            if result_manager:
                result_manager.set_final_result(complete_result)
                result_manager.finalize_pipeline()
            
            # 結果の保存
            saved_files = {}
            if save_detailed_logs and result_manager:
                # 新しいフェーズ別保存方式
                saved_files = result_manager.save_phase_results()
                complete_result["analysis_folder"] = result_manager.get_analysis_folder_path()
                complete_result["saved_files"] = saved_files
                
                logger.info(f"[{analysis_id}] Detailed logs saved to folder: {result_manager.get_analysis_folder_path()}")
                logger.info(f"[{analysis_id}] Saved {len(saved_files)} files across all phases")
            
            if save_results:
                # 通常の結果保存（互換性維持）
                saved_file = f"analysis_results/meal_analysis_{analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                complete_result["legacy_saved_to"] = saved_file
            
            self.logger.info(f"[{analysis_id}] Complete analysis pipeline finished successfully in {processing_time:.2f}s")
            
            return complete_result
            
        except Exception as e:
            self.logger.error(f"[{analysis_id}] Complete analysis failed: {str(e)}", exc_info=True)
            
            # エラー時もResultManagerを保存
            if result_manager:
                result_manager.set_final_result({"error": str(e), "timestamp": datetime.now().isoformat()})
                result_manager.finalize_pipeline()
                error_saved_files = result_manager.save_phase_results()
                self.logger.info(f"[{analysis_id}] Error analysis logs saved to folder: {result_manager.get_analysis_folder_path()}")
            
            raise
    
    def get_pipeline_info(self) -> Dict[str, Any]:
        """パイプライン情報を取得"""
        if self.use_elasticsearch_search:
            search_method = "elasticsearch"
        elif self.use_local_nutrition_search:
            search_method = "local_database"
        else:
            search_method = "usda_api"
            
        return {
            "pipeline_id": self.pipeline_id,
            "version": "v2.0",
            "nutrition_search_method": search_method,
            "components": [
                {
                    "component_name": "Phase1Component",
                    "component_type": "analysis",
                    "execution_count": 0
                },
                {
                    "component_name": self.search_component_name,
                    "component_type": "nutrition_search",
                    "execution_count": 0
                }
            ]
        } 
```

================================================================================

📄 FILE: app_v2/pipeline/result_manager.py
------------------------------------------------------------
ファイルサイズ: 36,336 bytes
最終更新: 2025-06-10 14:06:44
存在: ✅

CONTENT:
```python
import json
import os
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class DetailedExecutionLog:
    """各コンポーネントの詳細実行ログ"""
    
    def __init__(self, component_name: str, execution_id: str):
        self.component_name = component_name
        self.execution_id = execution_id
        self.execution_start_time = datetime.now()
        self.execution_end_time = None
        self.input_data = {}
        self.output_data = {}
        self.processing_details = {}
        self.prompts_used = {}
        self.reasoning = {}
        self.confidence_scores = {}
        self.warnings = []
        self.errors = []
        
    def set_input(self, input_data: Dict[str, Any]):
        """入力データを記録（機密情報は除外）"""
        # 画像データは大きすぎるので、メタデータのみ保存
        safe_input = {}
        for key, value in input_data.items():
            if key == 'image_bytes':
                safe_input[key] = {
                    "size_bytes": len(value) if value else 0,
                    "type": "binary_image_data"
                }
            else:
                safe_input[key] = value
        self.input_data = safe_input
    
    def set_output(self, output_data: Dict[str, Any]):
        """出力データを記録"""
        self.output_data = output_data
        
    def add_prompt(self, prompt_name: str, prompt_content: str, variables: Dict[str, Any] = None):
        """使用されたプロンプトを記録"""
        self.prompts_used[prompt_name] = {
            "content": prompt_content,
            "variables": variables or {},
            "timestamp": datetime.now().isoformat()
        }
    
    def add_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """推論理由を記録"""
        self.reasoning[decision_point] = {
            "reason": reason,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat()
        }
    
    def add_processing_detail(self, detail_key: str, detail_value: Any):
        """処理詳細を記録"""
        self.processing_details[detail_key] = detail_value
    
    def add_confidence_score(self, metric_name: str, score: float):
        """信頼度スコアを記録"""
        self.confidence_scores[metric_name] = score
    
    def add_warning(self, warning: str):
        """警告を記録"""
        self.warnings.append({
            "message": warning,
            "timestamp": datetime.now().isoformat()
        })
    
    def add_error(self, error: str):
        """エラーを記録"""
        self.errors.append({
            "message": error,
            "timestamp": datetime.now().isoformat()
        })
    
    def finalize(self):
        """実行完了時の最終処理"""
        self.execution_end_time = datetime.now()
    
    def get_execution_time(self) -> float:
        """実行時間を取得（秒）"""
        if self.execution_end_time:
            return (self.execution_end_time - self.execution_start_time).total_seconds()
        return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """辞書形式で取得"""
        return {
            "component_name": self.component_name,
            "execution_id": self.execution_id,
            "execution_start_time": self.execution_start_time.isoformat(),
            "execution_end_time": self.execution_end_time.isoformat() if self.execution_end_time else None,
            "execution_time_seconds": self.get_execution_time(),
            "input_data": self.input_data,
            "output_data": self.output_data,
            "processing_details": self.processing_details,
            "prompts_used": self.prompts_used,
            "reasoning": self.reasoning,
            "confidence_scores": self.confidence_scores,
            "warnings": self.warnings,
            "errors": self.errors
        }


class ResultManager:
    """解析結果と詳細ログの管理クラス（フェーズ別整理版）"""
    
    def __init__(self, analysis_id: str, save_directory: str = "analysis_results"):
        self.analysis_id = analysis_id
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # 実行ごとのフォルダを作成
        self.analysis_folder_name = f"analysis_{self.timestamp}_{self.analysis_id}"
        self.analysis_dir = Path(save_directory) / self.analysis_folder_name
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # 各フェーズのフォルダを作成
        self.phase1_dir = self.analysis_dir / "phase1"
        self.nutrition_search_dir = self.analysis_dir / "nutrition_search_query"
        self.phase2_dir = self.analysis_dir / "phase2"
        self.nutrition_dir = self.analysis_dir / "nutrition_calculation"
        
        for phase_dir in [self.phase1_dir, self.nutrition_search_dir, self.phase2_dir, self.nutrition_dir]:
            phase_dir.mkdir(exist_ok=True)
        
        self.pipeline_start_time = datetime.now()
        self.pipeline_end_time = None
        self.execution_logs: List[DetailedExecutionLog] = []
        self.final_result = {}
        self.pipeline_metadata = {
            "analysis_id": analysis_id,
            "version": "v2.0",
            "analysis_folder": self.analysis_folder_name,
            "pipeline_start_time": self.pipeline_start_time.isoformat()
        }
        
    def create_execution_log(self, component_name: str, execution_id: str) -> DetailedExecutionLog:
        """新しい実行ログを作成"""
        log = DetailedExecutionLog(component_name, execution_id)
        self.execution_logs.append(log)
        return log
    
    def set_final_result(self, result: Dict[str, Any]):
        """最終結果を設定"""
        self.final_result = result
        
    def finalize_pipeline(self):
        """パイプライン完了時の最終処理"""
        self.pipeline_end_time = datetime.now()
        self.pipeline_metadata["pipeline_end_time"] = self.pipeline_end_time.isoformat()
        self.pipeline_metadata["total_execution_time_seconds"] = (
            self.pipeline_end_time - self.pipeline_start_time
        ).total_seconds()
    
    def save_phase_results(self) -> Dict[str, str]:
        """フェーズ別に結果を保存"""
        saved_files = {}
        
        # 実行されたコンポーネントのログを処理
        executed_components = set()
        for log in self.execution_logs:
            if log.component_name == "Phase1Component":
                files = self._save_phase1_results(log)
                saved_files.update(files)
                executed_components.add("Phase1Component")
            elif log.component_name in ["USDAQueryComponent", "LocalNutritionSearchComponent", "ElasticsearchNutritionSearchComponent"]:
                files = self._save_nutrition_search_results(log)
                saved_files.update(files)
                executed_components.add(log.component_name)
            elif log.component_name == "Phase2Component":
                files = self._save_phase2_results(log)
                saved_files.update(files)
                executed_components.add("Phase2Component")
            elif log.component_name == "NutritionCalculationComponent":
                files = self._save_nutrition_results(log)
                saved_files.update(files)
                executed_components.add("NutritionCalculationComponent")
        
        # 未実装/未実行のコンポーネントにプレースホルダーファイルを作成
        if "Phase2Component" not in executed_components:
            placeholder_log = DetailedExecutionLog("Phase2Component", f"{self.analysis_id}_phase2_placeholder")
            placeholder_log.input_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.output_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.finalize()
            files = self._save_phase2_results(placeholder_log)
            saved_files.update(files)
        
        if "NutritionCalculationComponent" not in executed_components:
            placeholder_log = DetailedExecutionLog("NutritionCalculationComponent", f"{self.analysis_id}_nutrition_placeholder")
            placeholder_log.input_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.output_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.finalize()
            files = self._save_nutrition_results(placeholder_log)
            saved_files.update(files)
        
        # パイプライン全体のサマリーを保存
        summary_files = self._save_pipeline_summary()
        saved_files.update(summary_files)
        
        return saved_files
    
    def _save_phase1_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase1の結果を保存"""
        files = {}
        
        # 1. JSON形式の入出力データ
        input_output_file = self.phase1_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time()
            }, f, indent=2, ensure_ascii=False)
        files["phase1_input_output"] = str(input_output_file)
        
        # 2. プロンプトと推論理由のマークダウン
        prompts_md_file = self.phase1_dir / "prompts_and_reasoning.md"
        prompts_content = self._generate_phase1_prompts_md(log)
        with open(prompts_md_file, 'w', encoding='utf-8') as f:
            f.write(prompts_content)
        files["phase1_prompts_md"] = str(prompts_md_file)
        
        # 3. 検出された料理・食材のテキスト
        detected_items_file = self.phase1_dir / "detected_items.txt"
        detected_content = self._generate_phase1_detected_items_txt(log)
        with open(detected_items_file, 'w', encoding='utf-8') as f:
            f.write(detected_content)
        files["phase1_detected_txt"] = str(detected_items_file)
        
        return files
    
    def _save_nutrition_search_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """栄養データベース検索の結果を保存（USDAQueryComponent、LocalNutritionSearchComponent、ElasticsearchNutritionSearchComponent対応）"""
        files = {}
        
        # 検索方法の判定
        search_method = "unknown"
        db_source = "unknown"
        
        if log.component_name == "USDAQueryComponent":
            search_method = "usda_api"
            db_source = "usda_database"
        elif log.component_name == "LocalNutritionSearchComponent":
            search_method = "local_search"
            db_source = "local_nutrition_database"
        elif log.component_name == "ElasticsearchNutritionSearchComponent":
            search_method = "elasticsearch"
            db_source = "elasticsearch_nutrition_db"
        
        # 1. JSON形式の入出力データ（検索方法情報を含む）
        input_output_file = self.nutrition_search_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "search_metadata": {
                    "component_name": log.component_name,
                    "search_method": search_method,
                    "database_source": db_source,
                    "timestamp": log.execution_end_time.isoformat() if log.execution_end_time else None
                }
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_search_input_output"] = str(input_output_file)
        
        # 2. 検索結果の詳細マークダウン
        search_results_md_file = self.nutrition_search_dir / "search_results.md"
        search_content = self._generate_nutrition_search_results_md(log, search_method, db_source)
        with open(search_results_md_file, 'w', encoding='utf-8') as f:
            f.write(search_content)
        files["nutrition_search_results_md"] = str(search_results_md_file)
        
        # 3. マッチ詳細のテキスト
        match_details_file = self.nutrition_search_dir / "match_details.txt"
        match_content = self._generate_nutrition_match_details_txt(log, search_method, db_source)
        with open(match_details_file, 'w', encoding='utf-8') as f:
            f.write(match_content)
        files["nutrition_search_match_details"] = str(match_details_file)
        
        return files
    
    def _save_phase2_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase2の結果を保存（将来実装用）"""
        files = {}
        
        # 1. JSON形式の入出力データ
        input_output_file = self.phase2_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "Phase2Component is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["phase2_input_output"] = str(input_output_file)
        
        # 2. 戦略決定のマークダウン
        strategy_md_file = self.phase2_dir / "strategy_decisions.md"
        with open(strategy_md_file, 'w', encoding='utf-8') as f:
            f.write("# Phase2 Strategy Decisions\n\n*Phase2Component is not yet implemented*\n")
        files["phase2_strategy_md"] = str(strategy_md_file)
        
        # 3. 選択項目のテキスト
        selected_items_file = self.phase2_dir / "selected_items.txt"
        with open(selected_items_file, 'w', encoding='utf-8') as f:
            f.write("Phase2Component is not yet implemented\n")
        files["phase2_items_txt"] = str(selected_items_file)
        
        return files
    
    def _save_nutrition_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """栄養計算の結果を保存（将来実装用）"""
        files = {}
        
        # 1. JSON形式の入出力データ
        input_output_file = self.nutrition_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "NutritionCalculationComponent is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_input_output"] = str(input_output_file)
        
        # 2. 計算式のマークダウン
        formulas_md_file = self.nutrition_dir / "calculation_formulas.md"
        with open(formulas_md_file, 'w', encoding='utf-8') as f:
            f.write("# Nutrition Calculation Formulas\n\n*NutritionCalculationComponent is not yet implemented*\n")
        files["nutrition_formulas_md"] = str(formulas_md_file)
        
        # 3. 栄養サマリーのテキスト
        summary_txt_file = self.nutrition_dir / "nutrition_summary.txt"
        with open(summary_txt_file, 'w', encoding='utf-8') as f:
            f.write("NutritionCalculationComponent is not yet implemented\n")
        files["nutrition_summary_txt"] = str(summary_txt_file)
        
        return files
    
    def _save_pipeline_summary(self) -> Dict[str, str]:
        """パイプライン全体のサマリーを保存"""
        files = {}
        
        # 1. パイプラインサマリーJSON
        summary_file = self.analysis_dir / "pipeline_summary.json"
        summary_data = {
            "analysis_id": self.analysis_id,
            "timestamp": self.timestamp,
            "pipeline_metadata": self.pipeline_metadata,
            "execution_summary": {
                log.component_name: {
                    "execution_time": log.get_execution_time(),
                    "success": len(log.errors) == 0,
                    "warnings_count": len(log.warnings),
                    "errors_count": len(log.errors)
                }
                for log in self.execution_logs
            },
            "final_result": self.final_result
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        files["pipeline_summary"] = str(summary_file)
        
        # 2. 完全な詳細ログJSON
        complete_log_file = self.analysis_dir / "complete_analysis_log.json"
        complete_data = {
            "pipeline_metadata": self.pipeline_metadata,
            "execution_logs": [log.to_dict() for log in self.execution_logs],
            "final_result": self.final_result,
            "summary": {
                "total_components": len(self.execution_logs),
                "total_warnings": sum(len(log.warnings) for log in self.execution_logs),
                "total_errors": sum(len(log.errors) for log in self.execution_logs)
            }
        }
        
        with open(complete_log_file, 'w', encoding='utf-8') as f:
            json.dump(complete_data, f, indent=2, ensure_ascii=False)
        files["complete_log"] = str(complete_log_file)
        
        return files
    
    def _generate_phase1_prompts_md(self, log: DetailedExecutionLog) -> str:
        """Phase1のプロンプトと推論理由のマークダウンを生成"""
        content = f"""# Phase1: 画像分析 - プロンプトと推論

## 実行情報
- 実行ID: {log.execution_id}
- 開始時刻: {log.execution_start_time.isoformat()}
- 終了時刻: {log.execution_end_time.isoformat() if log.execution_end_time else 'N/A'}
- 実行時間: {log.get_execution_time():.2f}秒

## 使用されたプロンプト

"""
        
        # プロンプト情報
        for prompt_name, prompt_data in log.prompts_used.items():
            content += f"### {prompt_name.replace('_', ' ').title()}\n\n"
            content += f"**タイムスタンプ**: {prompt_data['timestamp']}\n\n"
            content += f"```\n{prompt_data['content']}\n```\n\n"
            
            if prompt_data.get('variables'):
                content += f"**変数**:\n"
                for var_name, var_value in prompt_data['variables'].items():
                    content += f"- {var_name}: {var_value}\n"
                content += "\n"
        
        # 推論理由
        content += "## AI推論の詳細\n\n"
        
        # 料理識別の推論
        dish_reasoning = [r for r in log.reasoning.items() if r[0].startswith('dish_identification_')]
        if dish_reasoning:
            content += "### 料理識別の推論\n\n"
            for decision_point, reasoning_data in dish_reasoning:
                dish_num = decision_point.split('_')[-1]
                content += f"**料理 {dish_num}**:\n"
                content += f"- 推論: {reasoning_data['reason']}\n"
                content += f"- タイムスタンプ: {reasoning_data['timestamp']}\n\n"
        
        # 食材選択の推論
        ingredient_reasoning = [r for r in log.reasoning.items() if r[0].startswith('ingredient_selection_')]
        if ingredient_reasoning:
            content += "### 食材選択の推論\n\n"
            for decision_point, reasoning_data in ingredient_reasoning:
                content += f"**{decision_point.replace('_', ' ').title()}**:\n"
                content += f"- 推論: {reasoning_data['reason']}\n"
                content += f"- タイムスタンプ: {reasoning_data['timestamp']}\n\n"
        
        # 警告とエラー
        if log.warnings:
            content += "## 警告\n\n"
            for warning in log.warnings:
                content += f"- {warning['message']} (at {warning['timestamp']})\n"
            content += "\n"
        
        if log.errors:
            content += "## エラー\n\n"
            for error in log.errors:
                content += f"- {error['message']} (at {error['timestamp']})\n"
            content += "\n"
        
        return content
    
    def _generate_phase1_detected_items_txt(self, log: DetailedExecutionLog) -> str:
        """Phase1で検出された料理・食材のテキストを生成（USDA検索特化）"""
        content = f"Phase1 検出結果 - {log.execution_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += "=" * 60 + "\n\n"
        
        if 'output_data' in log.output_data and 'dishes' in log.output_data['output_data']:
            dishes = log.output_data['output_data']['dishes']
            content += f"検出された料理数: {len(dishes)}\n\n"
            
            for i, dish in enumerate(dishes, 1):
                content += f"料理 {i}: {dish['dish_name']}\n"
                content += f"  食材数: {len(dish['ingredients'])}\n"
                content += "  食材詳細:\n"
                
                for j, ingredient in enumerate(dish['ingredients'], 1):
                    content += f"    {j}. {ingredient['ingredient_name']}\n"
                content += "\n"
        
        # USDA検索準備情報
        if 'usda_search_terms' in log.processing_details:
            search_terms = log.processing_details['usda_search_terms']
            content += f"USDA検索語彙 ({len(search_terms)}個):\n"
            for i, term in enumerate(search_terms, 1):
                content += f"  {i}. {term}\n"
            content += "\n"
        
        # 処理詳細
        if log.processing_details:
            content += "処理詳細:\n"
            for detail_key, detail_value in log.processing_details.items():
                if detail_key == 'usda_search_terms':
                    continue  # 既に上で表示済み
                if isinstance(detail_value, (dict, list)):
                    content += f"  {detail_key}: {json.dumps(detail_value, ensure_ascii=False)}\n"
                else:
                    content += f"  {detail_key}: {detail_value}\n"
        
        return content
    
    def _generate_nutrition_search_results_md(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """栄養データベース検索結果のマークダウンを生成（USDA/ローカル対応）"""
        content = []
        
        content.append(f"# Nutrition Database Search Results")
        content.append(f"")
        content.append(f"**Search Method:** {search_method}")
        content.append(f"**Database Source:** {db_source}")
        content.append(f"**Component:** {log.component_name}")
        content.append(f"**Execution Time:** {log.get_execution_time():.3f} seconds")
        content.append(f"**Timestamp:** {log.execution_start_time.isoformat()}")
        content.append(f"")
        
        # 入力データの表示
        if log.input_data:
            content.append(f"## Input Data")
            if 'ingredient_names' in log.input_data:
                ingredients = log.input_data['ingredient_names']
                content.append(f"**Ingredients ({len(ingredients)}):** {', '.join(ingredients)}")
            
            if 'dish_names' in log.input_data:
                dishes = log.input_data['dish_names']
                content.append(f"**Dishes ({len(dishes)}):** {', '.join(dishes)}")
            content.append(f"")
        
        # 出力データの表示
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            content.append(f"## Search Results")
            content.append(f"**Total Matches:** {len(matches)}")
            content.append(f"")
            
            for i, (search_term, match_data) in enumerate(matches.items(), 1):
                content.append(f"### {i}. {search_term}")
                if isinstance(match_data, dict):
                    content.append(f"**ID:** {match_data.get('id', 'N/A')}")
                    
                    # search_name と description を適切に表示
                    search_name = match_data.get('search_name', 'N/A')
                    description = match_data.get('description', None)
                    content.append(f"**Search Name:** {search_name}")
                    if description:
                        content.append(f"**Description:** {description}")
                    else:
                        content.append(f"**Description:** None")
                    
                    content.append(f"**Data Type:** {match_data.get('data_type', 'N/A')}")
                    content.append(f"**Source:** {match_data.get('source', 'N/A')}")
                    
                    # スコア情報を改善
                    score = match_data.get('score', 'N/A')
                    if score != 'N/A' and 'search_metadata' in match_data:
                        metadata = match_data['search_metadata']
                        score_breakdown = metadata.get('score_breakdown', {})
                        calculation = metadata.get('calculation', '')
                        match_type = score_breakdown.get('match_type', 'unknown')
                        
                        if calculation:
                            content.append(f"**Score:** {score} *({match_type}: {calculation})*")
                        else:
                            content.append(f"**Score:** {score} *(text similarity + data type priority)*")
                    else:
                        content.append(f"**Score:** {score}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        content.append(f"**Nutrients ({len(match_data['nutrients'])}):**")
                        for nutrient in match_data['nutrients'][:5]:  # 最初の5つだけ表示
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                content.append(f"  - {name}: {amount} {unit}")
                        if len(match_data['nutrients']) > 5:
                            content.append(f"  - ... and {len(match_data['nutrients']) - 5} more nutrients")
                content.append(f"")
        
        # 検索サマリー
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            content.append(f"## Search Summary")
            content.append(f"**Total Searches:** {summary.get('total_searches', 0)}")
            content.append(f"**Successful Matches:** {summary.get('successful_matches', 0)}")
            content.append(f"**Failed Searches:** {summary.get('failed_searches', 0)}")
            content.append(f"**Match Rate:** {summary.get('match_rate_percent', 0)}%")
            content.append(f"**Search Method:** {summary.get('search_method', 'unknown')}")
            content.append(f"")
        
        # 推論理由があれば表示
        if log.reasoning:
            content.append(f"## Search Reasoning")
            for decision_point, reason_data in log.reasoning.items():
                reason = reason_data.get('reason', '') if isinstance(reason_data, dict) else str(reason_data)
                content.append(f"**{decision_point}:** {reason}")
            content.append(f"")
        
        # 警告・エラーがあれば表示
        if log.warnings:
            content.append(f"## Warnings")
            for warning in log.warnings:
                content.append(f"- {warning}")
            content.append(f"")
        
        if log.errors:
            content.append(f"## Errors")
            for error in log.errors:
                content.append(f"- {error}")
            content.append(f"")
        
        return "\n".join(content)
    
    def _generate_nutrition_match_details_txt(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """栄養データベース検索のマッチ詳細テキストを生成（USDA/ローカル/マルチDB対応）"""
        lines = []
        
        lines.append(f"Nutrition Database Search Match Details")
        lines.append(f"=" * 50)
        lines.append(f"Search Method: {search_method}")
        lines.append(f"Database Source: {db_source}")
        lines.append(f"Component: {log.component_name}")
        lines.append(f"Execution Time: {log.get_execution_time():.3f} seconds")
        lines.append(f"Timestamp: {log.execution_start_time.isoformat()}")
        lines.append(f"")
        
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            
            # 総マッチ数を計算（単一結果とリスト結果両方に対応）
            total_matches = 0
            for match_data in matches.values():
                if isinstance(match_data, list):
                    total_matches += len(match_data)
                elif isinstance(match_data, dict):
                    total_matches += 1
            
            lines.append(f"Total Matches: {total_matches}")
            lines.append(f"")
            
            for search_term, match_data in matches.items():
                lines.append(f"Query: {search_term}")
                lines.append(f"-" * 30)
                
                # マルチDB検索結果（リスト形式）への対応
                if isinstance(match_data, list):
                    lines.append(f"  Found {len(match_data)} results from multiple databases:")
                    lines.append(f"")
                    
                    for i, match_item in enumerate(match_data, 1):
                        lines.append(f"  Result {i}:")
                        lines.append(f"    ID: {match_item.get('id', 'N/A')}")
                        
                        search_name = match_item.get('search_name', 'N/A')
                        description = match_item.get('description', None)
                        lines.append(f"    Search Name: {search_name}")
                        if description:
                            lines.append(f"    Description: {description}")
                        else:
                            lines.append(f"    Description: None")
                        
                        lines.append(f"    Data Type: {match_item.get('data_type', 'N/A')}")
                        lines.append(f"    Source: {match_item.get('source', 'N/A')}")
                        
                        # スコア情報
                        score = match_item.get('score', 'N/A')
                        if score != 'N/A' and 'search_metadata' in match_item:
                            metadata = match_item['search_metadata']
                            source_db = metadata.get('source_database', 'unknown')
                            lines.append(f"    Score: {score:.3f} (from {source_db})")
                        else:
                            lines.append(f"    Score: {score}")
                        
                        # 栄養情報（簡略版）
                        if 'nutrition' in match_item and match_item['nutrition']:
                            nutrition = match_item['nutrition']
                            calories = nutrition.get('calories', 0)
                            protein = nutrition.get('protein', 0)
                            fat = nutrition.get('fat', 0)
                            carbs = nutrition.get('carbs', 0)
                            lines.append(f"    Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g")
                        
                        lines.append(f"")
                
                # 単一結果（辞書形式）への対応（従来の方式）
                elif isinstance(match_data, dict):
                    lines.append(f"  ID: {match_data.get('id', 'N/A')}")
                    
                    search_name = match_data.get('search_name', 'N/A')
                    description = match_data.get('description', None)
                    lines.append(f"  Search Name: {search_name}")
                    if description:
                        lines.append(f"  Description: {description}")
                    else:
                        lines.append(f"  Description: None")
                    
                    lines.append(f"  Data Type: {match_data.get('data_type', 'N/A')}")
                    lines.append(f"  Source: {match_data.get('source', 'N/A')}")
                    
                    # スコア情報を改善
                    score = match_data.get('score', 'N/A')
                    if score != 'N/A' and 'search_metadata' in match_data:
                        metadata = match_data['search_metadata']
                        score_breakdown = metadata.get('score_breakdown', {})
                        calculation = metadata.get('calculation', '')
                        match_type = score_breakdown.get('match_type', 'unknown')
                        
                        if calculation:
                            lines.append(f"  Score: {score} ({match_type}: {calculation})")
                        else:
                            lines.append(f"  Score: {score} (text similarity + data type priority)")
                    else:
                        lines.append(f"  Score: {score}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        lines.append(f"  Nutrients ({len(match_data['nutrients'])}):")
                        for nutrient in match_data['nutrients']:
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                lines.append(f"    - {name}: {amount} {unit}")
                    
                    if 'original_data' in match_data:
                        original_data = match_data['original_data']
                        if isinstance(original_data, dict):
                            lines.append(f"  Original Data Source: {original_data.get('source', 'Unknown')}")
                            if search_method == "local_search":
                                lines.append(f"  Local DB Source: {original_data.get('db_source', 'Unknown')}")
                    
                    lines.append(f"")
        
        # 検索統計
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            lines.append(f"Search Statistics:")
            lines.append(f"  Total Searches: {summary.get('total_searches', 0)}")
            lines.append(f"  Successful Matches: {summary.get('successful_matches', 0)}")
            lines.append(f"  Failed Searches: {summary.get('failed_searches', 0)}")
            lines.append(f"  Match Rate: {summary.get('match_rate_percent', 0)}%")
            
            # マルチDB検索の場合の追加情報
            if 'target_databases' in summary:
                lines.append(f"  Target Databases: {', '.join(summary['target_databases'])}")
                lines.append(f"  Results per Database: {summary.get('results_per_db', 'N/A')}")
                lines.append(f"  Total Results: {summary.get('total_results', 'N/A')}")
            
            if search_method == "local_search":
                lines.append(f"  Total Database Items: {summary.get('total_database_items', 0)}")
        
        return "\n".join(lines)
    
    def get_analysis_folder_path(self) -> str:
        """解析フォルダパスを取得"""
        return str(self.analysis_dir) 
```

================================================================================

📁 コンポーネント層 - Phase1 AI分析
================================================================================

📄 FILE: app_v2/components/__init__.py
------------------------------------------------------------
ファイルサイズ: 713 bytes
最終更新: 2025-06-10 13:00:58
存在: ✅

CONTENT:
```python
from .base import BaseComponent
from .phase1_component import Phase1Component
from .usda_query_component import USDAQueryComponent
from .local_nutrition_search_component import LocalNutritionSearchComponent
from .elasticsearch_nutrition_search_component import ElasticsearchNutritionSearchComponent
# TODO: Phase2ComponentとNutritionCalculationComponentを実装
# from .phase2_component import Phase2Component
# from .nutrition_calc_component import NutritionCalculationComponent

__all__ = [
    "BaseComponent",
    "Phase1Component", 
    "USDAQueryComponent",
    "LocalNutritionSearchComponent",
    "ElasticsearchNutritionSearchComponent",
    # "Phase2Component",
    # "NutritionCalculationComponent"
] 
```

================================================================================

📄 FILE: app_v2/components/base.py
------------------------------------------------------------
ファイルサイズ: 6,824 bytes
最終更新: 2025-06-05 13:08:38
存在: ✅

CONTENT:
```python
from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Any, Optional
import logging
from datetime import datetime

# 型変数の定義
InputType = TypeVar('InputType')
OutputType = TypeVar('OutputType')


class BaseComponent(ABC, Generic[InputType, OutputType]):
    """
    食事分析パイプラインのベースコンポーネント抽象クラス
    
    全てのコンポーネントはこのクラスを継承し、process メソッドを実装する必要があります。
    """
    
    def __init__(self, component_name: str, logger: Optional[logging.Logger] = None):
        """
        ベースコンポーネントの初期化
        
        Args:
            component_name: コンポーネント名
            logger: ロガーインスタンス（指定しない場合は自動生成）
        """
        self.component_name = component_name
        self.logger = logger or logging.getLogger(f"{__name__}.{component_name}")
        self.created_at = datetime.now()
        self.execution_count = 0
        self.current_execution_log = None  # 詳細ログ
        
    @abstractmethod
    async def process(self, input_data: InputType) -> OutputType:
        """
        メイン処理メソッド（抽象メソッド）
        
        Args:
            input_data: 入力データ
            
        Returns:
            OutputType: 処理結果
            
        Raises:
            ComponentError: 処理エラーが発生した場合
        """
        pass
    
    async def execute(self, input_data: InputType, execution_log: Optional['DetailedExecutionLog'] = None) -> OutputType:
        """
        ラップされた実行メソッド（ログ記録、エラーハンドリング付き）
        
        Args:
            input_data: 入力データ
            execution_log: 詳細実行ログ（オプション）
            
        Returns:
            OutputType: 処理結果
        """
        self.execution_count += 1
        execution_id = f"{self.component_name}_{self.execution_count}"
        
        # 詳細ログの設定
        if execution_log:
            self.current_execution_log = execution_log
            # 入力データを記録
            self.current_execution_log.set_input(self._safe_serialize_input(input_data))
        
        self.logger.info(f"[{execution_id}] Starting {self.component_name} processing")
        
        try:
            start_time = datetime.now()
            result = await self.process(input_data)
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            self.logger.info(f"[{execution_id}] {self.component_name} completed in {processing_time:.2f}s")
            
            # 詳細ログに出力データを記録
            if self.current_execution_log:
                self.current_execution_log.set_output(self._safe_serialize_output(result))
                self.current_execution_log.finalize()
            
            return result
            
        except Exception as e:
            self.logger.error(f"[{execution_id}] {self.component_name} failed: {str(e)}", exc_info=True)
            
            # 詳細ログにエラーを記録
            if self.current_execution_log:
                self.current_execution_log.add_error(str(e))
                self.current_execution_log.finalize()
            
            raise ComponentError(f"{self.component_name} processing failed: {str(e)}") from e
        finally:
            self.current_execution_log = None
    
    def log_prompt(self, prompt_name: str, prompt_content: str, variables: dict = None):
        """プロンプトをログに記録"""
        if self.current_execution_log:
            self.current_execution_log.add_prompt(prompt_name, prompt_content, variables)
    
    def log_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """推論理由をログに記録"""
        if self.current_execution_log:
            self.current_execution_log.add_reasoning(decision_point, reason, confidence)
    
    def log_processing_detail(self, detail_key: str, detail_value: Any):
        """処理詳細をログに記録"""
        if self.current_execution_log:
            self.current_execution_log.add_processing_detail(detail_key, detail_value)
    
    def log_confidence_score(self, metric_name: str, score: float):
        """信頼度スコアをログに記録"""
        if self.current_execution_log:
            self.current_execution_log.add_confidence_score(metric_name, score)
    
    def log_warning(self, warning: str):
        """警告をログに記録"""
        if self.current_execution_log:
            self.current_execution_log.add_warning(warning)
    
    def _safe_serialize_input(self, input_data: InputType) -> dict:
        """入力データを安全にシリアライズ"""
        try:
            if hasattr(input_data, 'model_dump'):
                return input_data.model_dump()
            elif hasattr(input_data, '__dict__'):
                return input_data.__dict__
            else:
                return {"data": str(input_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def _safe_serialize_output(self, output_data: OutputType) -> dict:
        """出力データを安全にシリアライズ"""
        try:
            if hasattr(output_data, 'model_dump'):
                return output_data.model_dump()
            elif hasattr(output_data, '__dict__'):
                return output_data.__dict__
            else:
                return {"data": str(output_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def get_component_info(self) -> dict:
        """コンポーネント情報を取得"""
        return {
            "component_name": self.component_name,
            "created_at": self.created_at.isoformat(),
            "execution_count": self.execution_count,
            "component_type": self.__class__.__name__
        }


class ComponentError(Exception):
    """コンポーネント処理エラー"""
    
    def __init__(self, message: str, component_name: str = None, original_error: Exception = None):
        super().__init__(message)
        self.component_name = component_name
        self.original_error = original_error
        self.timestamp = datetime.now()
    
    def to_dict(self) -> dict:
        """エラー情報を辞書形式で取得"""
        return {
            "error_message": str(self),
            "component_name": self.component_name,
            "timestamp": self.timestamp.isoformat(),
            "original_error": str(self.original_error) if self.original_error else None
        } 
```

================================================================================

📄 FILE: app_v2/components/phase1_component.py
------------------------------------------------------------
ファイルサイズ: 16,155 bytes
最終更新: 2025-06-11 10:52:34
存在: ✅

CONTENT:
```python
import json
from typing import Optional

from .base import BaseComponent
from ..models.phase1_models import (
    Phase1Input, Phase1Output, Dish, Ingredient, 
    DetectedFoodItem, FoodAttribute, AttributeType
)
from ..services.gemini_service import GeminiService
from ..config import get_settings
from ..config.prompts import Phase1Prompts


class Phase1Component(BaseComponent[Phase1Input, Phase1Output]):
    """
    Phase1: 画像分析コンポーネント（構造化出力対応・USDA検索特化）
    
    Gemini AIを使用して食事画像を分析し、構造化された詳細情報
    （信頼度スコア、属性、ブランド情報等）を含むUSDA検索に適した出力を生成します。
    """
    
    def __init__(self, gemini_service: Optional[GeminiService] = None):
        super().__init__("Phase1Component")
        
        # GeminiServiceの初期化
        if gemini_service is None:
            settings = get_settings()
            self.gemini_service = GeminiService(
                project_id=settings.GEMINI_PROJECT_ID,
                location=settings.GEMINI_LOCATION,
                model_name=settings.GEMINI_MODEL_NAME
            )
        else:
            self.gemini_service = gemini_service
    
    async def process(self, input_data: Phase1Input) -> Phase1Output:
        """
        Phase1の主処理: 構造化画像分析（USDA検索特化）
        
        Args:
            input_data: Phase1Input (image_bytes, image_mime_type, optional_text)
            
        Returns:
            Phase1Output: 構造化された分析結果（信頼度スコア、属性、ブランド情報等を含む）
        """
        self.logger.info(f"Starting Phase1 structured image analysis for enhanced USDA query generation")
        
        # プロンプト生成と記録（構造化出力用に拡張）
        system_prompt = self._get_structured_system_prompt()
        user_prompt = Phase1Prompts.get_user_prompt(input_data.optional_text)
        
        self.log_prompt("structured_system_prompt", system_prompt)
        self.log_prompt("user_prompt", user_prompt, {
            "optional_text": input_data.optional_text,
            "image_mime_type": input_data.image_mime_type
        })
        
        # 画像情報のログ記録
        self.log_processing_detail("image_size_bytes", len(input_data.image_bytes))
        self.log_processing_detail("image_mime_type", input_data.image_mime_type)
        
        try:
            # Gemini AIによる構造化画像分析
            self.log_processing_detail("gemini_structured_api_call_start", "Calling Gemini API for structured image analysis")
            
            gemini_result = await self.gemini_service.analyze_phase1_structured(
                image_bytes=input_data.image_bytes,
                image_mime_type=input_data.image_mime_type,
                optional_text=input_data.optional_text,
                system_prompt=system_prompt
            )
            
            self.log_processing_detail("gemini_structured_response", gemini_result)
            
            # 構造化データを処理
            detected_food_items = []
            if "detected_food_items" in gemini_result:
                for item_index, item_data in enumerate(gemini_result["detected_food_items"]):
                    # 属性を処理
                    attributes = []
                    for attr_data in item_data.get("attributes", []):
                        # AttributeTypeに存在しない場合はPREPARATIONにフォールバック
                        attr_type_str = attr_data.get("type", "ingredient")
                        try:
                            attr_type = AttributeType(attr_type_str)
                        except ValueError:
                            # 未知の属性タイプの場合は最も近い既存タイプにマッピング
                            if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                attr_type = AttributeType.PREPARATION
                            elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                attr_type = AttributeType.COOKING_METHOD
                            elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                attr_type = AttributeType.TEXTURE
                            else:
                                attr_type = AttributeType.PREPARATION  # デフォルト
                        
                        attribute = FoodAttribute(
                            type=attr_type,
                            value=attr_data["value"],
                            confidence=attr_data.get("confidence", 0.5)
                        )
                        attributes.append(attribute)
                    
                    # DetectedFoodItemを作成
                    detected_item = DetectedFoodItem(
                        item_name=item_data["item_name"],
                        confidence=item_data.get("confidence", 0.5),
                        attributes=attributes,
                        brand=item_data.get("brand"),
                        category_hints=item_data.get("category_hints", []),
                        negative_cues=item_data.get("negative_cues", [])
                    )
                    detected_food_items.append(detected_item)
                    
                    # 構造化アイテム識別の推論理由をログ
                    self.log_reasoning(
                        f"structured_item_identification_{item_index}",
                        f"Structured identification: '{item_data['item_name']}' (confidence: {item_data.get('confidence', 0.5):.2f}, "
                        f"attributes: {len(attributes)}, brand: {item_data.get('brand', 'N/A')})"
                    )
            
            # 従来互換性のためのdishesも生成
            dishes = []
            if "dishes" in gemini_result:
                for dish_index, dish_data in enumerate(gemini_result.get("dishes", [])):
                    ingredients = []
                    for ingredient_index, ingredient_data in enumerate(dish_data.get("ingredients", [])):
                        # 構造化属性を従来形式に変換
                        ingredient_attributes = []
                        if "attributes" in ingredient_data:
                            for attr_data in ingredient_data["attributes"]:
                                # AttributeTypeに存在しない場合の処理
                                attr_type_str = attr_data.get("type", "ingredient")
                                try:
                                    attr_type = AttributeType(attr_type_str)
                                except ValueError:
                                    # 未知の属性タイプの場合はマッピング
                                    if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                        attr_type = AttributeType.PREPARATION
                                    elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                        attr_type = AttributeType.COOKING_METHOD
                                    elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                        attr_type = AttributeType.TEXTURE
                                    else:
                                        attr_type = AttributeType.PREPARATION
                                
                                attr = FoodAttribute(
                                    type=attr_type,
                                    value=attr_data["value"],
                                    confidence=attr_data.get("confidence", 0.5)
                                )
                                ingredient_attributes.append(attr)
                        
                        ingredient = Ingredient(
                            ingredient_name=ingredient_data["ingredient_name"],
                            confidence=ingredient_data.get("confidence"),
                            detected_attributes=ingredient_attributes
                        )
                        ingredients.append(ingredient)
                    
                    # 料理レベルの属性
                    dish_attributes = []
                    if "attributes" in dish_data:
                        for attr_data in dish_data["attributes"]:
                            # AttributeTypeに存在しない場合の処理
                            attr_type_str = attr_data.get("type", "preparation")
                            try:
                                attr_type = AttributeType(attr_type_str)
                            except ValueError:
                                # 未知の属性タイプの場合はマッピング
                                if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                    attr_type = AttributeType.PREPARATION
                                elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                    attr_type = AttributeType.COOKING_METHOD
                                elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                    attr_type = AttributeType.TEXTURE
                                else:
                                    attr_type = AttributeType.PREPARATION
                            
                            attr = FoodAttribute(
                                type=attr_type,
                                value=attr_data["value"],
                                confidence=attr_data.get("confidence", 0.5)
                            )
                            dish_attributes.append(attr)
                    
                    dish = Dish(
                        dish_name=dish_data["dish_name"],
                        confidence=dish_data.get("confidence"),
                        ingredients=ingredients,
                        detected_attributes=dish_attributes
                    )
                    dishes.append(dish)
            
            # フォールバック: 構造化データから従来形式を生成
            if not dishes and detected_food_items:
                dishes = self._convert_structured_to_legacy(detected_food_items)
            
            # 分析統計の記録
            self.log_processing_detail("detected_structured_items_count", len(detected_food_items))
            self.log_processing_detail("detected_dishes_count", len(dishes))
            self.log_processing_detail("total_ingredients_count", sum(len(dish.ingredients) for dish in dishes))
            
            # 全体的な分析信頼度を計算
            overall_confidence = self._calculate_overall_confidence(detected_food_items, dishes)
            
            # 処理ノートを生成
            processing_notes = [
                f"Structured analysis generated {len(detected_food_items)} food items",
                f"Overall confidence: {overall_confidence:.2f}",
                f"Legacy compatibility: {len(dishes)} dishes generated"
            ]
            
            result = Phase1Output(
                detected_food_items=detected_food_items,
                dishes=dishes,
                analysis_confidence=overall_confidence,
                processing_notes=processing_notes,
                warnings=[]
            )
            
            self.log_processing_detail("structured_search_terms", result.get_structured_search_terms())
            self.log_reasoning(
                "structured_analysis_completion",
                f"Phase1 structured analysis completed: {len(detected_food_items)} structured items, "
                f"overall confidence {overall_confidence:.2f}"
            )
            
            self.logger.info(f"Phase1 structured analysis completed: {len(detected_food_items)} items, "
                           f"confidence {overall_confidence:.2f}")
            return result
            
        except Exception as e:
            self.logger.error(f"Phase1 structured processing failed: {str(e)}")
            raise
    
    def _get_structured_system_prompt(self) -> str:
        """構造化出力用の詳細なシステムプロンプトを生成"""
        return """You are an advanced food recognition AI that analyzes food images and provides detailed structured output.

Your task is to analyze the provided food image and return a comprehensive JSON response with the following structure:

{
  "detected_food_items": [
    {
      "item_name": "Primary food item name (e.g., 'Spaghetti Carbonara')",
      "confidence": 0.85,
      "attributes": [
        {"type": "ingredient", "value": "pasta", "confidence": 0.9},
        {"type": "ingredient", "value": "egg", "confidence": 0.7},
        {"type": "preparation", "value": "creamy", "confidence": 0.8},
        {"type": "cooking_method", "value": "boiled", "confidence": 0.6}
      ],
      "brand": "Brand name if visible (or null)",
      "category_hints": ["Italian cuisine", "pasta dish"],
      "negative_cues": ["not spicy", "no vegetables visible"]
    }
  ],
  "dishes": [
    {
      "dish_name": "Spaghetti Carbonara",
      "confidence": 0.85,
      "ingredients": [
        {
          "ingredient_name": "pasta",
          "confidence": 0.9,
          "attributes": [
            {"type": "ingredient", "value": "spaghetti", "confidence": 0.95}
          ]
        }
      ],
      "attributes": [
        {"type": "preparation", "value": "creamy", "confidence": 0.8}
      ]
    }
  ],
  "analysis_confidence": 0.85
}

Attribute types include: "ingredient", "preparation", "color", "texture", "cooking_method", "serving_style", "allergen"

Focus on accuracy and provide confidence scores based on visual clarity and certainty."""
    
    def _convert_structured_to_legacy(self, detected_items: list) -> list:
        """構造化データを従来形式に変換（フォールバック用）"""
        dishes = []
        
        for item in detected_items:
            # 食材属性を抽出
            ingredients = []
            ingredient_attrs = [attr for attr in item.attributes if attr.type == AttributeType.INGREDIENT]
            
            if ingredient_attrs:
                for attr in ingredient_attrs:
                    ingredient = Ingredient(
                        ingredient_name=attr.value,
                        confidence=attr.confidence,
                        detected_attributes=[attr]
                    )
                    ingredients.append(ingredient)
            else:
                # フォールバック: アイテム名を食材として使用
                ingredient = Ingredient(
                    ingredient_name=item.item_name,
                    confidence=item.confidence,
                    detected_attributes=item.attributes
                )
                ingredients.append(ingredient)
            
            dish = Dish(
                dish_name=item.item_name,
                confidence=item.confidence,
                ingredients=ingredients,
                detected_attributes=item.attributes
            )
            dishes.append(dish)
        
        return dishes
    
    def _calculate_overall_confidence(self, structured_items: list, dishes: list) -> float:
        """全体的な分析信頼度を計算"""
        if not structured_items and not dishes:
            return 0.0
        
        total_confidence = 0.0
        count = 0
        
        # 構造化アイテムの信頼度
        for item in structured_items:
            total_confidence += item.confidence
            count += 1
        
        # 料理の信頼度
        for dish in dishes:
            if dish.confidence is not None:
                total_confidence += dish.confidence
                count += 1
        
        return total_confidence / count if count > 0 else 0.5 
```

================================================================================

📁 コンポーネント層 - 高度戦略的Elasticsearch検索
================================================================================

📄 FILE: app_v2/components/elasticsearch_nutrition_search_component.py
------------------------------------------------------------
ファイルサイズ: 44,187 bytes
最終更新: 2025-06-11 10:16:41
存在: ✅

CONTENT:
```python
#!/usr/bin/env python3
"""
Elasticsearch Nutrition Search Component

高度なクエリ戦略対応版：構造化入力、bool query、function_score、文字列類似度、二段階検索
"""

import os
import json
import asyncio
from typing import Optional, List, Dict, Any, Tuple
from datetime import datetime

from .base import BaseComponent
from ..models.nutrition_search_models import (
    NutritionQueryInput, NutritionQueryOutput, NutritionMatch
)
from ..models.phase1_models import Phase1Output, DetectedFoodItem, FoodAttribute, AttributeType
from ..config import get_settings

# 文字列類似度アルゴリズム
try:
    from rapidfuzz.distance import JaroWinkler, Levenshtein
    from rapidfuzz.fuzz import ratio, partial_ratio
    RAPIDFUZZ_AVAILABLE = True
except ImportError:
    RAPIDFUZZ_AVAILABLE = False

# Elasticsearchクライアント
try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False


class ElasticsearchNutritionSearchComponent(BaseComponent[NutritionQueryInput, NutritionQueryOutput]):
    """
    Elasticsearch栄養データベース検索コンポーネント（高度なクエリ戦略対応）
    
    構造化入力対応、bool query、function_score、文字列類似度、二段階検索を実装
    """
    
    def __init__(
        self, 
        elasticsearch_url: str = "http://localhost:9200", 
        multi_db_search_mode: bool = False, 
        results_per_db: int = 3,
        enable_advanced_features: bool = True
    ):
        super().__init__("ElasticsearchNutritionSearchComponent")
        
        self.elasticsearch_url = elasticsearch_url
        self.es_client = None
        self.index_name = "nutrition_db"
        self.multi_db_search_mode = multi_db_search_mode
        self.results_per_db = results_per_db
        self.target_databases = ["yazio", "mynetdiary", "eatthismuch"]
        
        # 高度な機能フラグ
        self.enable_advanced_features = enable_advanced_features
        self.enable_fuzzy_matching = RAPIDFUZZ_AVAILABLE and enable_advanced_features
        self.enable_two_stage_search = enable_advanced_features
        
        # 二段階検索のパラメータ
        self.first_stage_size = 50  # 第一段階で取得する候補数
        self.final_result_size = 10  # 最終的な結果数
        
        # ブースティングとスコアリングのパラメータ
        self.primary_term_boost = 3.0
        self.brand_boost = 2.5
        self.ingredient_boost = 1.5
        self.preparation_boost = 1.2
        self.jaro_winkler_threshold = 0.8
        self.levenshtein_threshold = 0.7
        
        # Elasticsearchクライアントの初期化
        self._initialize_elasticsearch()
        
        self.logger.info(f"ElasticsearchNutritionSearchComponent initialized")
        self.logger.info(f"Advanced features enabled: {self.enable_advanced_features}")
        self.logger.info(f"Fuzzy matching available: {self.enable_fuzzy_matching}")
        self.logger.info(f"Two-stage search enabled: {self.enable_two_stage_search}")
        
    def _initialize_elasticsearch(self):
        """Elasticsearchクライアントを初期化"""
        if not ELASTICSEARCH_AVAILABLE:
            self.logger.error("Elasticsearch library not available. Please install: pip install elasticsearch")
            return
        
        try:
            self.es_client = Elasticsearch([self.elasticsearch_url])
            
            # 接続テスト
            if self.es_client.ping():
                self.logger.info(f"Successfully connected to Elasticsearch at {self.elasticsearch_url}")
                
                # インデックス存在確認
                if self.es_client.indices.exists(index=self.index_name):
                    self.logger.info(f"Index '{self.index_name}' exists and ready")
                else:
                    self.logger.error(f"Index '{self.index_name}' does not exist. Please run create_elasticsearch_index.py first.")
                    self.es_client = None
            else:
                self.logger.error("Elasticsearch ping failed. Please ensure Elasticsearch is running.")
                self.es_client = None
                
        except Exception as e:
            self.logger.error(f"Failed to connect to Elasticsearch: {e}")
            self.es_client = None
    
    async def process(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        高度なElasticsearch検索の主処理
        
        Args:
            input_data: NutritionQueryInput (構造化入力対応)
            
        Returns:
            NutritionQueryOutput: 高度なElasticsearch検索結果
            
        Raises:
            RuntimeError: Elasticsearchが利用できない場合
        """
        # Elasticsearch利用可能性チェック
        if not ELASTICSEARCH_AVAILABLE or not self.es_client:
            return self._create_error_response("Elasticsearch not available")
        
        start_time = datetime.now()
        
        # 構造化入力の検出と処理
        structured_data = self._extract_structured_data(input_data)
        search_terms = input_data.get_all_search_terms()
        
        self.logger.info(f"Starting advanced Elasticsearch search for {len(search_terms)} terms")
        self.log_processing_detail("structured_data_available", structured_data is not None)
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("advanced_features_enabled", self.enable_advanced_features)
        
        if self.enable_advanced_features and structured_data:
            # 構造化データを使用した高度な検索
            return await self._advanced_structured_search(input_data, structured_data, search_terms, start_time)
        elif self.multi_db_search_mode:
            # 従来の戦略的検索
            return await self._elasticsearch_strategic_search(input_data, search_terms)
        else:
            # 基本検索
            return await self._elasticsearch_search(input_data, search_terms)
    
    def _extract_structured_data(self, input_data: NutritionQueryInput) -> Optional[Dict[str, Any]]:
        """入力データから構造化データを抽出"""
        # NutritionQueryInputが構造化データを含んでいるかチェック
        if hasattr(input_data, 'structured_analysis') and input_data.structured_analysis:
            return input_data.structured_analysis
        
        # 代替: Phase1Outputから構造化データを抽出
        if hasattr(input_data, 'phase1_output') and input_data.phase1_output:
            phase1_output = input_data.phase1_output
            if hasattr(phase1_output, 'get_structured_search_terms'):
                return phase1_output.get_structured_search_terms()
        
        return None
    
    async def _advanced_structured_search(
        self, 
        input_data: NutritionQueryInput, 
        structured_data: Dict[str, Any], 
        search_terms: List[str],
        start_time: datetime
    ) -> NutritionQueryOutput:
        """構造化データを使用した高度な検索"""
        self.log_processing_detail("search_method", "advanced_structured")
        
        matches = {}
        successful_matches = 0
        total_searches = len(search_terms)
        
        # 高信頼度アイテムの処理
        high_confidence_items = structured_data.get('high_confidence_items', [])
        medium_confidence_items = structured_data.get('medium_confidence_items', [])
        brands = structured_data.get('brands', [])
        ingredients = structured_data.get('ingredients', [])
        cooking_methods = structured_data.get('cooking_methods', [])
        negative_cues = structured_data.get('negative_cues', [])
        
        self.log_processing_detail("high_confidence_items_count", len(high_confidence_items))
        self.log_processing_detail("brands_detected", brands)
        self.log_processing_detail("negative_cues", negative_cues)
        
        # 各検索語彙に対して高度な検索を実行
        for search_index, search_term in enumerate(search_terms):
            try:
                # 構造化データを使用したクエリ構築
                advanced_query = self._build_advanced_structured_query(
                    search_term, 
                    structured_data, 
                    input_data
                )
                
                if self.enable_two_stage_search:
                    # 二段階検索
                    nutrition_matches = await self._two_stage_search(
                        advanced_query, 
                        search_term, 
                        structured_data
                    )
                else:
                    # 単段階高度検索
                    nutrition_matches = await self._single_stage_advanced_search(
                        advanced_query, 
                        search_term
                    )
                
                if nutrition_matches:
                    matches[search_term] = nutrition_matches[0]  # ベストマッチを選択
                    successful_matches += 1
                    
                    self.log_reasoning(
                        f"advanced_match_{search_index}",
                        f"Advanced structured match for '{search_term}': {nutrition_matches[0].name} "
                        f"(score: {nutrition_matches[0].score:.3f}, db: {nutrition_matches[0].source_db})"
                    )
                
            except Exception as e:
                self.logger.error(f"Advanced search failed for '{search_term}': {e}")
                continue
        
        # 結果の統計処理
        search_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
        match_rate = (successful_matches / total_searches * 100) if total_searches > 0 else 0
        
        return NutritionQueryOutput(
            matches=matches,
            search_summary={
                "total_searches": total_searches,
                "successful_matches": successful_matches,
                "failed_searches": total_searches - successful_matches,
                "match_rate_percent": round(match_rate, 1),
                "search_method": "advanced_structured_elasticsearch",
                "search_time_ms": search_time_ms,
                "high_confidence_items": len(high_confidence_items),
                "brands_used": len(brands),
                "negative_cues_applied": len(negative_cues),
                "fuzzy_matching_enabled": self.enable_fuzzy_matching,
                "two_stage_search_enabled": self.enable_two_stage_search
            },
            errors=[]
        )
    
    def _build_advanced_structured_query(
        self, 
        search_term: str, 
        structured_data: Dict[str, Any], 
        input_data: NutritionQueryInput
    ) -> Dict[str, Any]:
        """構造化データを使用した高度なElasticsearchクエリを構築"""
        
        # 構造化データから関連情報を抽出
        high_confidence_items = structured_data.get('high_confidence_items', [])
        brands = structured_data.get('brands', [])
        ingredients = structured_data.get('ingredients', [])
        cooking_methods = structured_data.get('cooking_methods', [])
        negative_cues = structured_data.get('negative_cues', [])
        
        # 現在の検索語句が高信頼度アイテムに含まれるかチェック
        is_high_confidence = any(
            item.item_name.lower() == search_term.lower() 
            for item in high_confidence_items 
            if hasattr(item, 'item_name')
        )
        
        # Bool クエリの構築
        bool_query = {
            "bool": {
                "must": [],
                "should": [],
                "must_not": []
            }
        }
        
        # Must句: 高信頼度の場合は必須条件として使用
        if is_high_confidence:
            bool_query["bool"]["must"].append({
                "match_phrase": {
                    "search_name": {
                        "query": search_term,
                        "boost": self.primary_term_boost
                    }
                }
            })
        else:
            # 中・低信頼度の場合はshouldで柔軟に
            bool_query["bool"]["should"].append({
                "match_phrase": {
                    "search_name": {
                        "query": search_term,
                        "boost": self.primary_term_boost,
                        "slop": 1  # 単語間の距離を1つまで許可
                    }
                }
            })
            
            # 代替マッチングオプション
            bool_query["bool"]["should"].append({
                "match": {
                    "search_name": {
                        "query": search_term,
                        "boost": self.primary_term_boost * 0.8
                    }
                }
            })
        
        # Should句: ブランド情報によるブースト
        for brand in brands:
            if brand:
                bool_query["bool"]["should"].append({
                    "match": {
                        "search_name": {
                            "query": brand,
                            "boost": self.brand_boost
                        }
                    }
                })
        
        # Should句: 材料情報によるブースト
        for ingredient in ingredients:
            if ingredient:
                bool_query["bool"]["should"].append({
                    "match": {
                        "search_name": {
                            "query": ingredient,
                            "boost": self.ingredient_boost
                        }
                    }
                })
        
        # Should句: 調理法情報によるブースト
        for cooking_method in cooking_methods:
            if cooking_method:
                bool_query["bool"]["should"].append({
                    "multi_match": {
                        "query": cooking_method,
                        "fields": ["search_name", "source_description"],
                        "boost": self.preparation_boost
                    }
                })
        
        # Must_not句: ネガティブキュー（除外条件）
        for negative_cue in negative_cues:
            if negative_cue:
                bool_query["bool"]["must_not"].append({
                    "match": {
                        "search_name": negative_cue
                    }
                })
        
        # Function_score クエリでラップ（文字列類似度適用）
        # Painlessスクリプトの問題のため、RapidFuzzベースの二段階検索を使用
        if False:  # self.enable_fuzzy_matching:
            try:
                query = {
                    "function_score": {
                        "query": bool_query,
                        "functions": [
                            {
                                "script_score": {
                                    "script": {
                                        "source": self._get_similarity_script(),
                                        "params": {
                                            "search_term": search_term
                                        }
                                    }
                                }
                            }
                        ],
                        "score_mode": "max",
                        "boost_mode": "multiply"
                    }
                }
            except Exception as script_error:
                # スクリプトエラーの場合はbool queryにフォールバック
                self.logger.warning(f"Script score error, falling back to bool query: {script_error}")
                query = bool_query
        else:
            query = bool_query
        
        return {
            "query": query,
            "size": self.first_stage_size if self.enable_two_stage_search else self.final_result_size,
            "_source": True,
            "sort": [
                {"_score": {"order": "desc"}}
            ]
        }
    
    def _get_similarity_script(self) -> str:
        """文字列類似度計算用のPainlessスクリプト（シンプル版）"""
        return """
        if (doc['search_name.keyword'].size() == 0) {
            return _score;
        }
        String dbName = doc['search_name.keyword'].value;
        String searchTerm = params.search_term;
        
        if (dbName == null || searchTerm == null) {
            return _score;
        }
        
        String dbLower = dbName.toLowerCase();
        String searchLower = searchTerm.toLowerCase();
        
        // 完全一致
        if (dbLower.equals(searchLower)) {
            return _score * 2.0;
        }
        
        // 部分一致
        if (dbLower.contains(searchLower) || searchLower.contains(dbLower)) {
            return _score * 1.5;
        }
        
        return _score;
        """
    
    async def _two_stage_search(
        self, 
        first_stage_query: Dict[str, Any], 
        search_term: str, 
        structured_data: Dict[str, Any]
    ) -> List[NutritionMatch]:
        """二段階検索の実行"""
        
        # 第一段階: 広めに候補を取得
        response = self.es_client.search(
            index=self.index_name,
            body=first_stage_query
        )
        
        first_stage_hits = response.get('hits', {}).get('hits', [])
        self.log_processing_detail("first_stage_hits_count", len(first_stage_hits))
        
        if not first_stage_hits:
            return []
        
        # 第二段階: 詳細な再ランキング
        candidates = []
        for hit in first_stage_hits:
            nutrition_match = self._convert_es_hit_to_nutrition_match(hit, search_term)
            
            # 詳細な類似度スコアを計算（RapidFuzzを使用）
            if self.enable_fuzzy_matching:
                enhanced_score = self._calculate_enhanced_similarity_score(
                    nutrition_match, 
                    search_term, 
                    structured_data
                )
                nutrition_match.score = enhanced_score
            
            candidates.append(nutrition_match)
        
        # スコア順でソート
        candidates.sort(key=lambda x: x.score, reverse=True)
        
        # 上位結果を返す
        final_results = candidates[:self.final_result_size]
        
        self.log_processing_detail("second_stage_results_count", len(final_results))
        if final_results:
            self.log_processing_detail("top_result_score", final_results[0].score)
        
        return final_results
    
    def _calculate_enhanced_similarity_score(
        self, 
        nutrition_match: NutritionMatch, 
        search_term: str, 
        structured_data: Dict[str, Any]
    ) -> float:
        """RapidFuzzを使用した詳細な類似度スコア計算"""
        if not RAPIDFUZZ_AVAILABLE:
            return nutrition_match.score
        
        base_score = nutrition_match.score
        db_name = nutrition_match.name.lower()
        search_lower = search_term.lower()
        
        # Jaro-Winkler 類似度
        jaro_winkler_score = JaroWinkler.similarity(db_name, search_lower)
        
        # Levenshtein 類似度（正規化）
        levenshtein_score = 1.0 - (Levenshtein.distance(db_name, search_lower) / max(len(db_name), len(search_lower)))
        
        # FuzzyWuzzy Ratio
        fuzzy_ratio = ratio(db_name, search_lower) / 100.0
        
        # FuzzyWuzzy Partial Ratio
        partial_ratio_score = partial_ratio(db_name, search_lower) / 100.0
        
        # 重み付き組み合わせ
        similarity_boost = (
            jaro_winkler_score * 0.4 +
            levenshtein_score * 0.3 +
            fuzzy_ratio * 0.2 +
            partial_ratio_score * 0.1
        )
        
        # ブランド一致ボーナス
        brands = structured_data.get('brands', [])
        brand_bonus = 0.0
        for brand in brands:
            if brand and brand.lower() in db_name:
                brand_bonus = 0.3
                break
        
        # 最終スコア計算
        enhanced_score = base_score * (1.0 + similarity_boost + brand_bonus)
        
        return enhanced_score
    
    async def _single_stage_advanced_search(
        self, 
        advanced_query: Dict[str, Any], 
        search_term: str
    ) -> List[NutritionMatch]:
        """単段階高度検索の実行"""
        
        response = self.es_client.search(
            index=self.index_name,
            body=advanced_query
        )
        
        hits = response.get('hits', {}).get('hits', [])
        
        results = []
        for hit in hits:
            nutrition_match = self._convert_es_hit_to_nutrition_match(hit, search_term)
            results.append(nutrition_match)
        
        return results
    
    def _create_error_response(self, error_message: str) -> NutritionQueryOutput:
        """エラーレスポンスを作成"""
        return NutritionQueryOutput(
            matches={},
            search_summary={
                "total_searches": 0,
                "successful_matches": 0,
                "failed_searches": 0,
                "match_rate_percent": 0,
                "search_method": "error",
                "search_time_ms": 0
            },
            errors=[error_message]
        )

    async def _elasticsearch_search(self, input_data: NutritionQueryInput, search_terms: List[str]) -> NutritionQueryOutput:
        """
        Elasticsearchを使用した単一結果検索（従来の方式）
        
        Args:
            input_data: 入力データ
            search_terms: 検索語彙リスト
            
        Returns:
            NutritionQueryOutput: Elasticsearch検索結果
        """
        matches = {}
        warnings = []
        errors = []
        successful_matches = 0
        total_searches = len(search_terms)
        
        start_time = datetime.now()
        
        # 各検索語彙についてElasticsearch検索を実行
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Elasticsearch search for: {search_term}")
            
            self.log_processing_detail(f"es_search_{search_index}_term", search_term)
            
            try:
                # Elasticsearchクエリの構築
                es_query = self._build_elasticsearch_query(search_term, input_data)
                
                # 検索実行
                response = self.es_client.search(
                    index=self.index_name,
                    body=es_query
                )
                
                # 結果処理
                hits = response.get('hits', {}).get('hits', [])
                
                if hits:
                    # 最良のマッチを選択
                    best_hit = hits[0]
                    source = best_hit['_source']
                    score = best_hit['_score']
                    
                    match_result = self._convert_es_hit_to_nutrition_match(best_hit, search_term)
                    matches[search_term] = match_result
                    successful_matches += 1
                    
                    self.log_reasoning(
                        f"es_match_{search_index}",
                        f"Found Elasticsearch match for '{search_term}': {source.get('search_name', 'N/A')} (score: {score:.3f}, db: {source.get('source_db', 'N/A')})"
                    )
                    
                    self.logger.debug(f"ES match for '{search_term}': {source.get('search_name', 'N/A')} from {source.get('source_db', 'N/A')}")
                else:
                    self.log_reasoning(
                        f"es_no_match_{search_index}",
                        f"No Elasticsearch match found for '{search_term}'"
                    )
                    warnings.append(f"No Elasticsearch match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Elasticsearch search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                self.log_reasoning(
                    f"es_search_error_{search_index}",
                    f"Elasticsearch search error for '{search_term}': {str(e)}"
                )
        
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        # 検索サマリーを作成
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "elasticsearch",
            "database_source": "elasticsearch_nutrition_db",
            "preferred_source": input_data.preferred_source,
            "search_time_ms": search_time_ms,
            "index_name": self.index_name,
            "total_indexed_documents": await self._get_total_document_count()
        }
        
        self.log_processing_detail("elasticsearch_search_summary", search_summary)
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Elasticsearch nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%}) in {search_time_ms}ms")
        
        return result
    
    async def _elasticsearch_strategic_search(self, input_data: NutritionQueryInput, search_terms: List[str]) -> NutritionQueryOutput:
        """
        戦略的Elasticsearch検索（dish/ingredient別の最適化された検索）
        
        Dish戦略:
        - メイン: EatThisMuch data_type=dish
        - 補助: EatThisMuch data_type=branded (スコアが低い場合)
        
        Ingredient戦略:
        - メイン: EatThisMuch data_type=ingredient  
        - 補助: MyNetDiary, YAZIO, EatThisMuch branded
        
        Args:
            input_data: 入力データ
            search_terms: 検索語彙リスト
            
        Returns:
            NutritionQueryOutput: 戦略的検索結果
        """
        matches = {}
        warnings = []
        errors = []
        successful_matches = 0
        total_searches = len(search_terms)
        
        start_time = datetime.now()
        
        # 各検索語彙について戦略的検索を実行
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Strategic Elasticsearch search for: {search_term}")
            
            self.log_processing_detail(f"strategic_search_{search_index}_term", search_term)
            
            try:
                # クエリタイプを判定
                query_type = "dish" if search_term in input_data.dish_names else "ingredient"
                self.log_processing_detail(f"search_{search_index}_type", query_type)
                
                # 戦略的検索を実行
                if query_type == "dish":
                    strategic_results = await self._strategic_dish_search(search_term, input_data)
                else:
                    strategic_results = await self._strategic_ingredient_search(search_term, input_data)
                
                if strategic_results:
                    matches[search_term] = strategic_results
                    successful_matches += 1
                    
                    self.log_reasoning(
                        f"strategic_match_{search_index}",
                        f"Found strategic matches for '{search_term}' ({query_type}): {len(strategic_results)} results"
                    )
                else:
                    self.log_reasoning(
                        f"strategic_no_results_{search_index}",
                        f"No strategic results for '{search_term}' ({query_type})"
                    )
                    warnings.append(f"No strategic results found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Strategic Elasticsearch search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                self.log_reasoning(
                    f"strategic_search_error_{search_index}",
                    f"Strategic Elasticsearch search error for '{search_term}': {str(e)}"
                )
        
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        # 戦略的検索サマリーを作成
        total_results = sum(len(result_list) if isinstance(result_list, list) else 1 for result_list in matches.values())
        
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "elasticsearch_strategic",
            "database_source": "elasticsearch_nutrition_db",
            "preferred_source": input_data.preferred_source,
            "search_time_ms": search_time_ms,
            "index_name": self.index_name,
            "total_indexed_documents": await self._get_total_document_count(),
            "strategic_approach": {
                "dish_strategy": "eatthismuch_dish_primary + eatthismuch_branded_fallback",
                "ingredient_strategy": "eatthismuch_ingredient_primary + multi_db_fallback"
            },
            "total_results": total_results
        }
        
        self.log_processing_detail("elasticsearch_strategic_search_summary", search_summary)
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Strategic Elasticsearch nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%}) with {total_results} total results in {search_time_ms}ms")
        
        return result

    async def _strategic_dish_search(self, search_term: str, input_data: NutritionQueryInput) -> List[NutritionMatch]:
        """
        Dish検索戦略を実行
        
        戦略:
        1. EatThisMuch data_type=dish をメイン検索
        2. スコアが低い場合は EatThisMuch data_type=branded を補助検索
        
        Args:
            search_term: 検索語彙
            input_data: 入力データ
            
        Returns:
            List[NutritionMatch]: 戦略的検索結果
        """
        results = []
        MIN_SCORE_THRESHOLD = 20.0  # スコア閾値
        
        self.logger.info(f"Strategic dish search for '{search_term}': EatThisMuch dish -> branded fallback")
        
        # Step 1: EatThisMuch dish をメイン検索
        main_query = self._build_strategic_query(search_term, "eatthismuch", "dish")
        
        try:
            response = self.es_client.search(index=self.index_name, body=main_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                best_score = hits[0].get('_score', 0)
                self.logger.info(f"Dish main search: found {len(hits)} results, best score: {best_score}")
                
                if best_score >= MIN_SCORE_THRESHOLD:
                    # 高スコア: メイン結果のみ使用
                    for hit in hits[:self.results_per_db]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "main_dish"
                        match.search_metadata["strategy_type"] = "dish_primary"
                        results.append(match)
                    
                    self.logger.info(f"High score dish results: using {len(results)} main results")
                    return results
                else:
                    # 低スコア: メイン結果を保持して補助検索も実行
                    for hit in hits[:max(1, self.results_per_db // 2)]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "main_dish_low_score"
                        match.search_metadata["strategy_type"] = "dish_primary"
                        results.append(match)
        
        except Exception as e:
            self.logger.error(f"Error in dish main search: {e}")
        
        # Step 2: EatThisMuch branded を補助検索
        fallback_query = self._build_strategic_query(search_term, "eatthismuch", "branded")
        
        try:
            response = self.es_client.search(index=self.index_name, body=fallback_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                remaining_slots = self.results_per_db - len(results)
                for hit in hits[:remaining_slots]:
                    match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                    match.search_metadata["strategic_phase"] = "fallback_branded"
                    match.search_metadata["strategy_type"] = "dish_fallback"
                    results.append(match)
                
                self.logger.info(f"Dish fallback search: added {min(len(hits), remaining_slots)} branded results")
        
        except Exception as e:
            self.logger.error(f"Error in dish fallback search: {e}")
        
        # スコア順でソート
        results.sort(key=lambda x: x.score, reverse=True)
        
        self.logger.info(f"Strategic dish search completed: {len(results)} total results")
        return results

    async def _strategic_ingredient_search(self, search_term: str, input_data: NutritionQueryInput) -> List[NutritionMatch]:
        """
        Ingredient検索戦略を実行
        
        戦略:
        1. EatThisMuch data_type=ingredient をメイン検索
        2. MyNetDiary, YAZIO, EatThisMuch branded を補助検索
        
        Args:
            search_term: 検索語彙
            input_data: 入力データ
            
        Returns:
            List[NutritionMatch]: 戦略的検索結果
        """
        results = []
        
        self.logger.info(f"Strategic ingredient search for '{search_term}': EatThisMuch ingredient -> multi-DB fallback")
        
        # Step 1: EatThisMuch ingredient をメイン検索
        main_query = self._build_strategic_query(search_term, "eatthismuch", "ingredient")
        
        try:
            response = self.es_client.search(index=self.index_name, body=main_query)
            hits = response.get('hits', {}).get('hits', [])
            
            if hits:
                # メイン結果を追加（最大半分のスロット使用）
                main_slots = max(1, self.results_per_db // 2)
                for hit in hits[:main_slots]:
                    match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                    match.search_metadata["strategic_phase"] = "main_ingredient"
                    match.search_metadata["strategy_type"] = "ingredient_primary"
                    results.append(match)
                
                self.logger.info(f"Ingredient main search: added {len(results)} primary results")
        
        except Exception as e:
            self.logger.error(f"Error in ingredient main search: {e}")
        
        # Step 2: 補助データベース検索
        fallback_sources = [
            ("mynetdiary", "unified"),
            ("yazio", "unified"), 
            ("eatthismuch", "branded")
        ]
        
        remaining_slots = self.results_per_db - len(results)
        slots_per_source = max(1, remaining_slots // len(fallback_sources))
        
        for db_name, data_type in fallback_sources:
            if remaining_slots <= 0:
                break
                
            try:
                fallback_query = self._build_strategic_query(search_term, db_name, data_type)
                response = self.es_client.search(index=self.index_name, body=fallback_query)
                hits = response.get('hits', {}).get('hits', [])
                
                if hits:
                    current_slots = min(slots_per_source, remaining_slots)
                    for hit in hits[:current_slots]:
                        match = self._convert_es_hit_to_nutrition_match(hit, search_term)
                        match.search_metadata["strategic_phase"] = "fallback_multi_db"
                        match.search_metadata["strategy_type"] = "ingredient_fallback"
                        match.search_metadata["fallback_source"] = f"{db_name}_{data_type}"
                        results.append(match)
                    
                    remaining_slots -= len(hits[:current_slots])
                    self.logger.info(f"Ingredient fallback ({db_name}_{data_type}): added {len(hits[:current_slots])} results")
            
            except Exception as e:
                self.logger.error(f"Error in ingredient fallback search ({db_name}_{data_type}): {e}")
        
        # スコア順でソート
        results.sort(key=lambda x: x.score, reverse=True)
        
        self.logger.info(f"Strategic ingredient search completed: {len(results)} total results")
        return results

    def _build_strategic_query(self, search_term: str, target_db: str, data_type: str) -> Dict[str, Any]:
        """
        戦略的検索用のElasticsearchクエリを構築
        
        Args:
            search_term: 検索語彙
            target_db: ターゲットデータベース
            data_type: データタイプ
            
        Returns:
            Elasticsearchクエリ辞書
        """
        query = {
            "size": self.results_per_db,
            "query": {
                "bool": {
                    "must": [
                        {
                            "multi_match": {
                                "query": search_term,
                                "fields": [
                                    "search_name^3",
                                    "description^1"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ],
                    "filter": [
                        {"term": {"source_db": target_db}},
                        {"term": {"data_type": data_type}}
                    ]
                }
            },
            "sort": [
                {"_score": {"order": "desc"}}
            ]
        }
        
        return query
    
    async def _get_total_document_count(self) -> int:
        """インデックス内の総ドキュメント数を取得"""
        try:
            stats = self.es_client.indices.stats(index=self.index_name)
            return stats["indices"][self.index_name]["total"]["docs"]["count"]
        except Exception as e:
            self.logger.warning(f"Failed to get document count: {e}")
            return 0
    
    def _build_elasticsearch_query(self, search_term: str, input_data: NutritionQueryInput) -> Dict[str, Any]:
        """
        Elasticsearch検索クエリを構築
        
        Args:
            search_term: 検索語彙
            input_data: 入力データ（検索タイプ判定用）
            
        Returns:
            Elasticsearchクエリ辞書
        """
        # 基本的なmulti_matchクエリ
        base_query = {
            "multi_match": {
                "query": search_term,
                "fields": [
                    "search_name^3",  # 検索名に高い重み
                    "search_name.exact^5"  # 完全一致に最高の重み
                ],
                "type": "best_fields",
                "fuzziness": "AUTO",
                "operator": "OR"
            }
        }
        
        # データタイプとソースデータベースフィルタ
        filters = []
        
        # dish_namesに含まれる場合は料理データを優先
        if search_term in input_data.dish_names:
            filters.append({"term": {"data_type": "dish"}})
        # ingredient_namesに含まれる場合は食材データを優先
        elif search_term in input_data.ingredient_names:
            filters.append({"term": {"data_type": "ingredient"}})
        
        # 優先ソースの設定
        if input_data.preferred_source and input_data.preferred_source != "elasticsearch":
            source_mapping = {
                "yazio": "yazio",
                "mynetdiary": "mynetdiary", 
                "eatthismuch": "eatthismuch"
            }
            if input_data.preferred_source in source_mapping:
                filters.append({"term": {"source_db": source_mapping[input_data.preferred_source]}})
        
        # フィルタがある場合はboolクエリでラップ
        if filters:
            query = {
                "bool": {
                    "must": [base_query],
                    "should": filters,  # shouldで優先度付け
                    "boost": 1.2
                }
            }
        else:
            query = base_query
        
        return {
            "query": query,
            "size": 5,  # 上位5件を取得
            "_source": ["data_type", "id", "search_name", "nutrition", "weight", "source_db", "description"]
        }
    
    def _convert_es_hit_to_nutrition_match(self, hit: Dict[str, Any], search_term: str) -> NutritionMatch:
        """
        ElasticsearchヒットをNutritionMatchに変換
        
        Args:
            hit: Elasticsearchヒット
            search_term: 検索語彙
            
        Returns:
            NutritionMatch
        """
        source = hit['_source']
        score = hit['_score']
        
        # ソースデータベース情報を取得
        source_db = source.get('source_db', 'unknown')
        search_name = source.get('search_name', search_term)
        final_source = f"elasticsearch_{source_db}"
        
        return NutritionMatch(
            id=source.get('id', 0),
            name=search_name,  # 必須フィールド追加
            search_name=search_name,
            description=source.get('description'),
            data_type=source.get('data_type', 'unknown'),
            source_db=source_db,  # 必須フィールド追加
            source=final_source,
            nutrition=source.get('nutrition', {}),
            weight=source.get('weight'),
            score=score,
            search_metadata={
                "search_term": search_term,
                "elasticsearch_score": score,
                "search_method": "elasticsearch_multi_db" if self.multi_db_search_mode else "elasticsearch",
                "source_database": source_db,
                "index_name": self.index_name
            }
        ) 
```

================================================================================

📄 FILE: app_v2/components/local_nutrition_search_component.py
------------------------------------------------------------
ファイルサイズ: 21,683 bytes
最終更新: 2025-06-10 12:20:02
存在: ✅

CONTENT:
```python
#!/usr/bin/env python3
"""
Local Nutrition Search Component

USDA database queryを nutrition_db_experiment で実装したローカル検索システムに置き換える
"""

import os
import sys
import json
import asyncio
from typing import Optional, List, Dict, Any
from pathlib import Path

from .base import BaseComponent
from ..models.nutrition_search_models import (
    NutritionQueryInput, NutritionQueryOutput, NutritionMatch
)
from ..config import get_settings

# nutrition_db_experimentのパスを追加
NUTRITION_DB_EXPERIMENT_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "nutrition_db_experiment"
)
sys.path.append(NUTRITION_DB_EXPERIMENT_PATH)

class LocalNutritionSearchComponent(BaseComponent[NutritionQueryInput, NutritionQueryOutput]):
    """
    ローカル栄養データベース検索コンポーネント
    
    nutrition_db_experimentで実装したローカル検索システムを使用して食材名を検索し、
    純粋なローカル形式で結果を返します。
    """
    
    def __init__(self):
        super().__init__("LocalNutritionSearchComponent")
        
        # ローカル検索システムの初期化
        self._initialize_local_search_system()
        
        # unified_dbのみを使用
        self.unified_db_path = os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "unified_nutrition_db.json")
        
        # ローカルデータベースの読み込み
        self.unified_database = self._load_unified_database()
        
        # nutrition_db_experimentのコンポーネント
        self.search_handler = None
        self.query_preprocessor = None
        
        self.logger.info(f"LocalNutritionSearchComponent initialized with {len(self.unified_database)} total items")
    
    def _initialize_local_search_system(self):
        """nutrition_db_experimentの検索システムを初期化"""
        try:
            # 検索システムのインポート（オプション）
            from api.search_handler import SearchHandler
            from api.query_preprocessor import QueryPreprocessor
            
            self.search_handler = SearchHandler()
            self.query_preprocessor = QueryPreprocessor()
            
            self.logger.info("Advanced local search system initialized")
        except ImportError as e:
            self.logger.warning(f"Advanced search system not available, will use direct database search: {e}")
        except Exception as e:
            self.logger.error(f"Failed to initialize advanced search system: {e}")
    
    def _load_unified_database(self) -> List[Dict[str, Any]]:
        """unified_nutrition_db.jsonを読み込み"""
        try:
            if os.path.exists(self.unified_db_path):
                with open(self.unified_db_path, 'r', encoding='utf-8') as f:
                    database = json.load(f)
                self.logger.info(f"Loaded unified_db: {len(database)} items")
                return database
            else:
                self.logger.warning(f"Unified database file not found: {self.unified_db_path}")
                return []
        except Exception as e:
            self.logger.error(f"Error loading unified_db: {e}")
            return []
    
    async def process(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        ローカル検索の主処理（純粋なローカル形式）
        
        Args:
            input_data: NutritionQueryInput
            
        Returns:
            NutritionQueryOutput: 純粋なローカル検索結果
        """
        self.logger.info(f"Starting local nutrition search for {len(input_data.get_all_search_terms())} terms")
        
        # input_dataを保存してスコア計算で使用
        self._current_input_data = input_data
        
        search_terms = input_data.get_all_search_terms()
        
        # 検索対象の詳細をログに記録
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("ingredient_names", input_data.ingredient_names)
        self.log_processing_detail("dish_names", input_data.dish_names)
        self.log_processing_detail("total_search_terms", len(search_terms))
        self.log_processing_detail("search_method", "local_nutrition_database")
        self.log_processing_detail("preferred_source", input_data.preferred_source)
        
        matches = {}
        warnings = []
        errors = []
        
        successful_matches = 0
        total_searches = len(search_terms)
        
        # 各検索語彙について照合を実行
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Searching local database for: {search_term}")
            
            # 検索開始をログ
            self.log_processing_detail(f"search_{search_index}_term", search_term)
            self.log_processing_detail(f"search_{search_index}_start", f"Starting local search for '{search_term}'")
            
            try:
                # ローカル検索の実行
                if self.search_handler and self.query_preprocessor:
                    # 高度な検索システムを使用
                    match_result = await self._advanced_local_search(search_term, search_index, input_data)
                else:
                    # フォールバック: シンプルな文字列マッチング
                    match_result = await self._simple_local_search(search_term, search_index, input_data)
                
                if match_result:
                    matches[search_term] = match_result
                    successful_matches += 1
                    self.logger.debug(f"Found local match for '{search_term}': ID {match_result.id}")
                else:
                    self.log_reasoning(
                        f"no_match_{search_index}",
                        f"No local database match found for '{search_term}' - may not exist in local nutrition database"
                    )
                    self.logger.warning(f"No local match found for: {search_term}")
                    warnings.append(f"No local match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Local search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                # エラーの詳細をログ
                self.log_reasoning(
                    f"search_error_{search_index}",
                    f"Local database search error for '{search_term}': {str(e)}"
                )
        
        # 検索サマリーを作成（純粋なローカル形式）
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "local_nutrition_database",
            "database_source": "nutrition_db_experiment",
            "preferred_source": input_data.preferred_source,
            "total_database_items": len(self.unified_database)
        }
        
        # 全体的な検索成功率をログ
        overall_success_rate = successful_matches / total_searches if total_searches > 0 else 0
        self.log_processing_detail("search_summary", search_summary)
        
        # 検索品質の評価をログ
        if overall_success_rate >= 0.8:
            self.log_reasoning("search_quality", "Excellent local search results with high match rate")
        elif overall_success_rate >= 0.6:
            self.log_reasoning("search_quality", "Good local search results with acceptable match rate")
        elif overall_success_rate >= 0.4:
            self.log_reasoning("search_quality", "Moderate local search results, some items may need manual review")
        else:
            self.log_reasoning("search_quality", "Poor local search results, many items not found in local database")
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Local nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%})")
        
        return result
    
    async def _advanced_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        nutrition_db_experimentの高度な検索システムを使用したローカル検索
        
        Args:
            search_term: 検索語彙
            search_index: 検索インデックス（ログ用）
            input_data: 入力データ（検索タイプ判定用）
            
        Returns:
            NutritionMatch または None
        """
        try:
            from api.search_handler import SearchRequest
            
            # 検索タイプの決定（料理か食材かの推定）
            db_type_filter = None  # 全データベースを検索
            
            # dish_namesに含まれる場合は料理として優先検索
            if search_term in input_data.dish_names:
                db_type_filter = "dish"
                self.log_processing_detail(f"search_{search_index}_type", "dish")
            elif search_term in input_data.ingredient_names:
                db_type_filter = "ingredient"
                self.log_processing_detail(f"search_{search_index}_type", "ingredient")
            
            # 検索リクエストの作成
            request = SearchRequest(
                query=search_term,
                db_type_filter=db_type_filter,
                size=5  # 上位5件を取得
            )
            
            # 検索実行
            response = self.search_handler.search(request)
            
            # 検索結果の詳細をログ
            self.log_processing_detail(f"search_{search_index}_results_count", response.total_hits)
            self.log_processing_detail(f"search_{search_index}_processing_time_ms", response.took_ms)
            self.log_processing_detail(f"search_{search_index}_processed_query", response.query_info.get('processed_query'))
            
            if response.results:
                # nutrition_db_experimentの検索システムが模擬データを返した場合は、実際のデータベース検索にフォールバック
                best_result = response.results[0]
                
                # 模擬データかどうかをチェック（IDが123456の場合は模擬データ）
                if best_result.get('id') == 123456:
                    self.logger.warning(f"nutrition_db_experiment returned mock data for '{search_term}', falling back to direct database search")
                    return await self._direct_database_search(search_term, search_index, input_data)
                
                # マッチ選択の推論理由をログ
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_result['search_name']}' (ID: {best_result['id']}) for search term '{search_term}' based on local search algorithm (score: {best_result.get('_score', 'N/A')})"
                )
                
                # 詳細なマッチ情報をログ
                self.log_processing_detail(f"search_{search_index}_selected_id", best_result['id'])
                self.log_processing_detail(f"search_{search_index}_selected_name", best_result['search_name'])
                self.log_processing_detail(f"search_{search_index}_data_type", best_result.get('data_type', 'unknown'))
                self.log_processing_detail(f"search_{search_index}_score", best_result.get('_score'))
                
                # NutritionMatch形式に変換
                return self._convert_to_nutrition_match(best_result, search_term)
            
            # 結果がない場合は直接データベース検索にフォールバック
            return await self._direct_database_search(search_term, search_index, input_data)
            
        except Exception as e:
            self.logger.error(f"Advanced local search failed for '{search_term}': {e}")
            # エラーの場合も直接データベース検索にフォールバック
            return await self._direct_database_search(search_term, search_index, input_data)
    
    async def _direct_database_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ローカルデータベースファイルを直接検索
        
        Args:
            search_term: 検索語彙
            search_index: 検索インデックス（ログ用）
            input_data: 入力データ（検索タイプ判定用）
            
        Returns:
            NutritionMatch または None
        """
        try:
            self.log_processing_detail(f"search_{search_index}_method", "direct_database_search")
            
            search_term_lower = search_term.lower()
            best_match = None
            best_score = 0
            
            # unified_databaseから直接検索
            for item in self.unified_database:
                # search_nameフィールドで検索
                if 'search_name' not in item:
                    continue
                    
                item_name = item['search_name'].lower()
                score = 0
                
                # スコアリングアルゴリズム
                if search_term_lower == item_name:
                    score = 1.0  # 完全一致
                elif search_term_lower in item_name:
                    # 部分一致（語順考慮）
                    if item_name.startswith(search_term_lower):
                        score = 0.9  # 前方一致
                    elif item_name.endswith(search_term_lower):
                        score = 0.8  # 後方一致
                    else:
                        score = 0.7  # 中間一致
                elif item_name in search_term_lower:
                    score = 0.6  # 逆部分一致
                else:
                    # 単語レベルの一致をチェック
                    search_words = search_term_lower.split()
                    item_words = item_name.split()
                    
                    common_words = set(search_words) & set(item_words)
                    if common_words:
                        score = len(common_words) / max(len(search_words), len(item_words)) * 0.5
                
                # data_type優先度によるボーナス
                data_type = item.get('data_type', 'unknown')
                db_bonus = 1.0
                
                # dish_namesに含まれる場合は料理データを優先
                if search_term in input_data.dish_names and data_type == 'dish':
                    db_bonus = 1.2
                # ingredient_namesに含まれる場合は食材データを優先
                elif search_term in input_data.ingredient_names and data_type == 'ingredient':
                    db_bonus = 1.2
                
                final_score = score * db_bonus
                
                if final_score > best_score:
                    best_score = final_score
                    best_match = item.copy()
            
            if best_match and best_score > 0.1:  # 最低閾値
                # マッチスコア情報を追加
                best_match['_match_score'] = best_score
                
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_match['search_name']}' (ID: {best_match.get('id', 'N/A')}) for search term '{search_term}' using direct database search (score: {best_score:.3f})"
                )
                
                # 詳細なマッチ情報をログ
                self.log_processing_detail(f"search_{search_index}_selected_id", best_match.get('id', 'N/A'))
                self.log_processing_detail(f"search_{search_index}_selected_name", best_match['search_name'])
                self.log_processing_detail(f"search_{search_index}_data_type", best_match.get('data_type', 'unknown'))
                self.log_processing_detail(f"search_{search_index}_match_score", best_score)
                
                return self._convert_to_nutrition_match(best_match, search_term)
            
            return None
            
        except Exception as e:
            self.logger.error(f"Direct database search failed for '{search_term}': {e}")
            return None
    
    async def _simple_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        シンプルな文字列マッチングによるフォールバック検索（実際のデータベース使用）
        
        Args:
            search_term: 検索語彙
            search_index: 検索インデックス（ログ用）
            input_data: 入力データ（検索タイプ判定用）
            
        Returns:
            NutritionMatch または None
        """
        # 高度検索システムが利用できない場合は、直接データベース検索を使用
        return await self._direct_database_search(search_term, search_index, input_data)
    
    def _convert_to_nutrition_match(self, local_item: Dict[str, Any], search_term: str) -> NutritionMatch:
        """
        ローカルデータベースアイテムをNutritionMatch形式に変換（簡素化版）
        
        Args:
            local_item: ローカルデータベースのアイテム
            search_term: 元の検索語彙
            
        Returns:
            NutritionMatch: 変換されたマッチ結果（簡素化されたローカル形式）
        """
        # IDの取得
        item_id = local_item.get('id', 0)
        
        # 基本情報の取得
        search_name = local_item.get('search_name', search_term)
        description = local_item.get('description')  # brandedの場合のみ存在
        data_type = local_item.get('data_type', 'unknown')  # db_type → data_typeに変更
        
        # 栄養データ（100gあたり正規化済み）
        nutrition = local_item.get('nutrition', {})
        weight = local_item.get('weight')
        
        # マッチスコア
        score = local_item.get('_match_score') or local_item.get('_score') or 1.0
        
        # スコア計算の詳細分析
        search_term_lower = search_term.lower()
        item_name_lower = search_name.lower()
        
        # 基本マッチタイプの判定
        match_type = "unknown"
        base_score = 0.0
        if search_term_lower == item_name_lower:
            match_type = "exact_match"
            base_score = 1.0
        elif search_term_lower in item_name_lower:
            if item_name_lower.startswith(search_term_lower):
                match_type = "prefix_match"
                base_score = 0.9
            elif item_name_lower.endswith(search_term_lower):
                match_type = "suffix_match"
                base_score = 0.8
            else:
                match_type = "contains_match"
                base_score = 0.7
        elif item_name_lower in search_term_lower:
            match_type = "reverse_contains"
            base_score = 0.6
        else:
            # 単語レベルの一致
            search_words = set(search_term_lower.split())
            item_words = set(item_name_lower.split())
            common_words = search_words & item_words
            if common_words:
                match_type = "word_match"
                base_score = len(common_words) / max(len(search_words), len(item_words)) * 0.5
        
        # データタイプボーナスの計算
        type_bonus = 1.0
        if hasattr(self, '_current_input_data'):
            input_data = self._current_input_data
            if search_term in input_data.dish_names and data_type == 'dish':
                type_bonus = 1.2
            elif search_term in input_data.ingredient_names and data_type == 'ingredient':
                type_bonus = 1.2
        
        # 検索メタデータ（詳細な計算情報を含む）
        search_metadata = {
            "search_term": search_term,
            "match_score": score,
            "score_breakdown": {
                "match_type": match_type,
                "base_score": round(base_score, 3),
                "type_bonus": round(type_bonus, 3),
                "final_score": round(base_score * type_bonus, 3)
            },
            "calculation": f"{base_score:.3f} × {type_bonus:.3f} = {score:.3f}"
        }
        
        # NutritionMatchオブジェクトの作成（簡素化版）
        return NutritionMatch(
            id=item_id,
            search_name=search_name,
            description=description,
            data_type=data_type,  # db_type → data_typeに変更
            nutrition=nutrition,
            weight=weight,
            score=score,
            search_metadata=search_metadata
        ) 
```

================================================================================

📁 データモデル層
================================================================================

📄 FILE: app_v2/models/__init__.py
------------------------------------------------------------
ファイルサイズ: 680 bytes
最終更新: 2025-06-09 12:06:09
存在: ✅

CONTENT:
```python
from .phase1_models import *
from .usda_models import *
from .phase2_models import *
from .nutrition_models import *
from .nutrition_search_models import *

__all__ = [
    # Phase1 models
    "Phase1Input", "Phase1Output", "Ingredient", "Dish",
    
    # USDA models
    "USDAQueryInput", "USDAQueryOutput", "USDAMatch", "USDANutrient",
    
    # Phase2 models
    "Phase2Input", "Phase2Output", "RefinedDish", "RefinedIngredient",
    
    # Nutrition models
    "NutritionInput", "NutritionOutput", "CalculatedNutrients", "TotalNutrients",
    
    # Nutrition Search models (純粋なローカル形式)
    "NutritionQueryInput", "NutritionQueryOutput", "NutritionMatch"
] 
```

================================================================================

📄 FILE: app_v2/models/nutrition_search_models.py
------------------------------------------------------------
ファイルサイズ: 7,863 bytes
最終更新: 2025-06-11 10:13:59
存在: ✅

CONTENT:
```python
#!/usr/bin/env python3
"""
Nutrition Search Models

ローカル栄養データベース検索で使用する純粋なローカル形式のモデル（構造化入力対応）
"""

from typing import List, Dict, Optional, Any, Union
from pydantic import BaseModel, Field


class NutritionMatch(BaseModel):
    """栄養データベース照合結果モデル（純粋なローカル形式）"""
    id: Union[int, str] = Field(..., description="食品ID（ローカルID）")
    name: str = Field(..., description="食品名")  # search_nameから変更
    search_name: str = Field(..., description="検索名（簡潔な名称）")
    description: Optional[str] = Field(None, description="詳細説明")
    data_type: str = Field(..., description="データタイプ (dish, ingredient, branded)")
    source_db: str = Field(..., description="ソースデータベース（yazio, mynetdiary, eatthismuch）")
    source: str = Field(default="local_database", description="データソース（'local_database'）")
    
    # ローカルDBの生の栄養データ（100gあたり正規化済み）
    nutrition: Dict[str, float] = Field(default_factory=dict, description="ローカルDBの栄養データ（100gあたり）")
    weight: Optional[float] = Field(None, description="元データの重量（g）")
    
    # 検索スコア
    score: Optional[float] = Field(None, description="検索結果の関連度スコア")
    
    # 検索に関するメタデータ
    search_metadata: Optional[Dict[str, Any]] = Field(None, description="検索に関するメタデータ")


class AdvancedSearchOptions(BaseModel):
    """高度な検索オプション"""
    enable_fuzzy_matching: bool = Field(default=True, description="ファジーマッチングを有効にする")
    enable_two_stage_search: bool = Field(default=True, description="二段階検索を有効にする")
    primary_term_boost: float = Field(default=3.0, description="プライマリ用語のブースト値")
    brand_boost: float = Field(default=2.5, description="ブランド情報のブースト値")
    ingredient_boost: float = Field(default=1.5, description="材料情報のブースト値")
    preparation_boost: float = Field(default=1.2, description="調理法情報のブースト値")
    jaro_winkler_threshold: float = Field(default=0.8, description="Jaro-Winkler類似度の閾値")
    levenshtein_threshold: float = Field(default=0.7, description="Levenshtein類似度の閾値")
    first_stage_size: int = Field(default=50, description="第一段階で取得する候補数")
    final_result_size: int = Field(default=10, description="最終結果数")


class NutritionQueryInput(BaseModel):
    """栄養データベース検索入力モデル（構造化入力対応）"""
    ingredient_names: List[str] = Field(default_factory=list, description="食材名のリスト")
    dish_names: List[str] = Field(default_factory=list, description="料理名のリスト")
    search_options: Optional[Dict[str, Any]] = Field(None, description="検索オプション")
    preferred_source: str = Field(default="local_database", description="優先データソース")
    
    # 構造化データのサポート
    structured_analysis: Optional[Dict[str, Any]] = Field(None, description="Phase1からの構造化分析データ")
    phase1_output: Optional[Any] = Field(None, description="Phase1Outputオブジェクト（構造化データ含む）")
    
    # 高度な検索オプション
    advanced_search_options: Optional[AdvancedSearchOptions] = Field(None, description="高度な検索オプション")
    
    # 検索戦略
    search_strategy: str = Field(default="basic", description="検索戦略（basic, strategic, advanced_structured）")

    def get_all_search_terms(self) -> List[str]:
        """全ての検索語彙を取得"""
        return list(set(self.ingredient_names + self.dish_names))
    
    def get_structured_search_terms(self) -> Optional[Dict[str, Any]]:
        """構造化された検索用語を取得"""
        if self.phase1_output and hasattr(self.phase1_output, 'get_structured_search_terms'):
            return self.phase1_output.get_structured_search_terms()
        elif self.structured_analysis:
            return self.structured_analysis
        else:
            return None
    
    def get_primary_search_terms(self) -> List[str]:
        """プライマリ検索用語を取得（高信頼度アイテム）"""
        if self.phase1_output and hasattr(self.phase1_output, 'get_primary_search_terms'):
            return self.phase1_output.get_primary_search_terms()
        else:
            return self.get_all_search_terms()
    
    def has_structured_data(self) -> bool:
        """構造化データが利用可能かチェック"""
        return (
            self.structured_analysis is not None or
            (self.phase1_output and hasattr(self.phase1_output, 'detected_food_items'))
        )
    
    def is_advanced_search_enabled(self) -> bool:
        """高度な検索が有効かチェック"""
        return (
            self.search_strategy in ["advanced_structured", "strategic"] or
            self.has_structured_data()
        )


class NutritionQueryOutput(BaseModel):
    """栄養データベース検索結果モデル（構造化出力対応）"""
    # マルチデータベース検索対応：単一結果またはリスト結果を受け入れる
    matches: Dict[str, Union[NutritionMatch, List[NutritionMatch]]] = Field(
        default_factory=dict, 
        description="検索語彙と対応する照合結果のマッピング（単一結果またはマルチDB結果リスト）"
    )
    search_summary: Dict[str, Any] = Field(
        default_factory=dict, 
        description="検索結果のサマリー情報（柔軟な型対応）"
    )
    warnings: Optional[List[str]] = Field(None, description="警告メッセージのリスト")
    errors: Optional[List[str]] = Field(None, description="エラーメッセージのリスト")
    
    # 高度な検索結果のメタデータ
    advanced_search_metadata: Optional[Dict[str, Any]] = Field(None, description="高度な検索のメタデータ")

    def get_match_rate(self) -> float:
        """照合成功率を計算"""
        total_searches = self.search_summary.get("total_searches", 0)
        successful_matches = self.search_summary.get("successful_matches", 0)
        if total_searches == 0:
            return 0.0
        return successful_matches / total_searches

    def get_total_matches(self) -> int:
        """総照合件数を取得（マルチDB検索対応）"""
        total = 0
        for match_result in self.matches.values():
            if isinstance(match_result, list):
                total += len(match_result)
            else:
                total += 1
        return total
    
    def get_total_individual_results(self) -> int:
        """個別結果の総数を取得（マルチDB検索用）"""
        return self.get_total_matches()
    
    def has_errors(self) -> bool:
        """エラーが存在するかチェック"""
        return self.errors is not None and len(self.errors) > 0
    
    def has_warnings(self) -> bool:
        """警告が存在するかチェック"""
        return self.warnings is not None and len(self.warnings) > 0
    
    def get_search_method(self) -> str:
        """使用された検索方法を取得"""
        return self.search_summary.get("search_method", "unknown")
    
    def is_advanced_search_result(self) -> bool:
        """高度な検索の結果かチェック"""
        search_method = self.get_search_method()
        return search_method in [
            "advanced_structured_elasticsearch", 
            "elasticsearch_strategic",
            "two_stage_search"
        ] 
```

================================================================================

📄 FILE: app_v2/models/phase1_models.py
------------------------------------------------------------
ファイルサイズ: 6,284 bytes
最終更新: 2025-06-11 10:53:15
存在: ✅

CONTENT:
```python
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field
from enum import Enum


class AttributeType(str, Enum):
    """属性タイプの列挙"""
    INGREDIENT = "ingredient"
    PREPARATION = "preparation"
    COLOR = "color"
    TEXTURE = "texture"
    COOKING_METHOD = "cooking_method"
    SERVING_STYLE = "serving_style"
    ALLERGEN = "allergen"


class FoodAttribute(BaseModel):
    """食品属性モデル（材料、調理法など）"""
    type: AttributeType = Field(..., description="属性のタイプ")
    value: str = Field(..., description="属性の値")
    confidence: float = Field(..., ge=0.0, le=1.0, description="この属性の信頼度スコア")


class DetectedFoodItem(BaseModel):
    """検出された食品アイテム（構造化）"""
    item_name: str = Field(..., description="食品名（主要な候補）")
    confidence: float = Field(..., ge=0.0, le=1.0, description="食品名の信頼度スコア")
    attributes: List[FoodAttribute] = Field(default=[], description="食品の属性リスト（材料、調理法など）")
    brand: Optional[str] = Field(None, description="認識されたブランド名（該当する場合）")
    category_hints: List[str] = Field(default=[], description="推定される食品カテゴリ")
    negative_cues: List[str] = Field(default=[], description="画像から判断できる「含まれない」要素")


class Ingredient(BaseModel):
    """食材情報モデル（USDA検索用・従来互換性）"""
    ingredient_name: str = Field(..., description="食材の名称（USDA検索で使用）")
    confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="食材特定の信頼度")
    detected_attributes: List[FoodAttribute] = Field(default=[], description="この食材に関連する属性")


class Dish(BaseModel):
    """料理情報モデル（USDA検索用・従来互換性）"""
    dish_name: str = Field(..., description="特定された料理の名称（USDA検索で使用）")
    confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="料理特定の信頼度")
    ingredients: List[Ingredient] = Field(..., description="その料理に含まれる食材のリスト")
    detected_attributes: List[FoodAttribute] = Field(default=[], description="この料理に関連する属性")


class Phase1Input(BaseModel):
    """Phase1コンポーネントの入力モデル"""
    image_bytes: bytes = Field(..., description="画像データ（バイト形式）")
    image_mime_type: str = Field(..., description="画像のMIMEタイプ")
    optional_text: Optional[str] = Field(None, description="オプションのテキスト情報")

    class Config:
        arbitrary_types_allowed = True


class Phase1Output(BaseModel):
    """Phase1コンポーネントの出力モデル（構造化・拡張版）"""
    # 新しい構造化出力
    detected_food_items: List[DetectedFoodItem] = Field(default=[], description="認識された食品アイテムのリスト（構造化）")
    
    # 従来互換性のための出力
    dishes: List[Dish] = Field(..., description="画像から特定された料理のリスト")
    
    # メタデータ
    analysis_confidence: float = Field(..., ge=0.0, le=1.0, description="全体的な分析の信頼度")
    processing_notes: List[str] = Field(default=[], description="処理に関する注記")
    warnings: Optional[List[str]] = Field(None, description="処理中の警告メッセージ")

    def get_all_ingredient_names(self) -> List[str]:
        """全ての食材名のリストを取得（USDA検索用・従来互換性）"""
        ingredient_names = []
        for dish in self.dishes:
            for ingredient in dish.ingredients:
                ingredient_names.append(ingredient.ingredient_name)
        return ingredient_names

    def get_all_dish_names(self) -> List[str]:
        """全ての料理名のリストを取得（USDA検索用・従来互換性）"""
        return [dish.dish_name for dish in self.dishes]
    
    def get_structured_search_terms(self) -> Dict[str, Any]:
        """構造化された検索用語を取得（新しい検索戦略用）"""
        return {
            "high_confidence_items": [
                {
                    "item_name": item.item_name,
                    "confidence": item.confidence,
                    "brand": item.brand
                }
                for item in self.detected_food_items 
                if item.confidence >= 0.8
            ],
            "medium_confidence_items": [
                {
                    "item_name": item.item_name,
                    "confidence": item.confidence,
                    "brand": item.brand
                }
                for item in self.detected_food_items 
                if 0.5 <= item.confidence < 0.8
            ],
            "brands": [
                item.brand for item in self.detected_food_items 
                if item.brand is not None and item.brand != ""
            ],
            "ingredients": [
                attr.value for item in self.detected_food_items 
                for attr in item.attributes 
                if attr.type == AttributeType.INGREDIENT
            ],
            "cooking_methods": [
                attr.value for item in self.detected_food_items 
                for attr in item.attributes 
                if attr.type == AttributeType.PREPARATION
            ],
            "negative_cues": [
                cue for item in self.detected_food_items 
                for cue in item.negative_cues
            ]
        }
    
    def get_primary_search_terms(self) -> List[str]:
        """プライマリ検索用語を取得（高信頼度アイテム）"""
        primary_terms = []
        
        # 高信頼度の検出アイテム
        for item in self.detected_food_items:
            if item.confidence >= 0.7:
                primary_terms.append(item.item_name)
        
        # フォールバック: 従来の料理名と食材名
        if not primary_terms:
            primary_terms.extend(self.get_all_dish_names())
            primary_terms.extend(self.get_all_ingredient_names())
        
        return primary_terms 
```

================================================================================

📁 AI サービス層
================================================================================

📄 FILE: app_v2/services/__init__.py
------------------------------------------------------------
ファイルサイズ: 228 bytes
最終更新: 2025-06-05 13:00:07
存在: ✅

CONTENT:
```python
from .gemini_service import GeminiService
from .usda_service import USDAService  
from .nutrition_calculation_service import NutritionCalculationService

__all__ = ["GeminiService", "USDAService", "NutritionCalculationService"]

```

================================================================================

📄 FILE: app_v2/services/gemini_service.py
------------------------------------------------------------
ファイルサイズ: 22,141 bytes
最終更新: 2025-06-11 10:49:17
存在: ✅

CONTENT:
```python
import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig, HarmCategory, HarmBlockThreshold
from typing import Dict, Optional
import json
import logging
from PIL import Image
import io

from ..config.prompts import Phase1Prompts, Phase2Prompts

logger = logging.getLogger(__name__)

# 従来のJSONスキーマ（USDA検索特化）
MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
            "description": "画像から特定された料理のリスト（USDA検索用）。",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "特定された料理の名称（USDA検索で使用される）。"},
                    "ingredients": {
                        "type": "array",
                        "description": "この料理に含まれると推定される材料のリスト（USDA検索用）。",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "材料の名称（USDA検索で使用される）。"}
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}

# 新しい構造化出力スキーマ
STRUCTURED_MEAL_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "detected_food_items": {
            "type": "array",
            "description": "認識された食品アイテムのリスト（構造化）。",
            "items": {
                "type": "object",
                "properties": {
                    "item_name": {"type": "string", "description": "食品名（主要な候補）"},
                    "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "食品名の信頼度スコア"},
                    "attributes": {
                        "type": "array",
                        "description": "食品の属性リスト（材料、調理法など）",
                        "items": {
                            "type": "object",
                            "properties": {
                                "type": {
                                    "type": "string", 
                                    "enum": ["ingredient", "preparation", "color", "texture", "cooking_method", "serving_style", "allergen"],
                                    "description": "属性のタイプ"
                                },
                                "value": {"type": "string", "description": "属性の値"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "この属性の信頼度スコア"}
                            },
                            "required": ["type", "value", "confidence"]
                        }
                    },
                    "brand": {"type": "string", "description": "認識されたブランド名（該当する場合、nullの場合は空文字列）"},
                    "category_hints": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "推定される食品カテゴリ"
                    },
                    "negative_cues": {
                        "type": "array", 
                        "items": {"type": "string"},
                        "description": "画像から判断できる「含まれない」要素"
                    }
                },
                "required": ["item_name", "confidence", "attributes"]
            }
        },
        "dishes": {
            "type": "array",
            "description": "従来互換性のための料理リスト",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "料理名"},
                    "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "料理特定の信頼度"},
                    "ingredients": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "食材名"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "食材特定の信頼度"},
                                "attributes": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "type": {"type": "string", "description": "属性タイプ"},
                                            "value": {"type": "string", "description": "属性値"},
                                            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                                        },
                                        "required": ["type", "value", "confidence"]
                                    }
                                }
                            },
                            "required": ["ingredient_name"]
                        }
                    },
                    "attributes": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "type": {"type": "string"},
                                "value": {"type": "string"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                            },
                            "required": ["type", "value", "confidence"]
                        }
                    }
                },
                "required": ["dish_name", "ingredients"]
            }
        },
        "analysis_confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "全体的な分析の信頼度"}
    },
    "required": ["detected_food_items", "dishes", "analysis_confidence"]
}

REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
            "description": "画像から特定・精緻化された料理/食品アイテムのリスト。",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "特定された料理/食品アイテムの名称。"},
                    "type": {"type": "string", "description": "料理の種類（例: 主菜, 副菜, 単品食品）。"},
                    "quantity_on_plate": {"type": "string", "description": "皿の上の量。"},
                    "calculation_strategy": {
                        "type": "string",
                        "enum": ["dish_level", "ingredient_level"],
                        "description": "このアイテムの栄養計算方針。"
                    },
                    "fdc_id": {
                        "type": "integer",
                        "description": "calculation_strategyが'dish_level'の場合、この料理/食品アイテム全体のFDC ID。それ以外はnull。"
                    },
                    "usda_source_description": {
                        "type": "string",
                        "description": "calculation_strategyが'dish_level'の場合、この料理/食品アイテム全体のUSDA公式名称。それ以外はnull。"
                    },
                    "ingredients": {
                        "type": "array",
                        "description": "この料理/食品アイテムに含まれると推定される材料のリスト。",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "材料の名称。"},
                                "fdc_id": {
                                    "type": "integer",
                                    "description": "calculation_strategyが'ingredient_level'の場合、この材料のFDC ID。それ以外はnullまたは省略可。"
                                },
                                "usda_source_description": {
                                    "type": "string",
                                    "description": "calculation_strategyが'ingredient_level'の場合、この材料のUSDA公式名称。それ以外はnullまたは省略可。"
                                }
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "type", "quantity_on_plate", "calculation_strategy", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}


class GeminiService:
    """Vertex AI経由でGeminiを使用して食事画像を分析するサービスクラス"""
    
    def __init__(self, project_id: str, location: str, model_name: str = "gemini-2.5-flash-preview-05-20"):
        """
        初期化
        
        Args:
            project_id: GCPプロジェクトID
            location: Vertex AIのロケーション（例: us-central1）
            model_name: 使用するモデル名
        """
        # Vertex AIの初期化
        vertexai.init(project=project_id, location=location)
        
        # モデルの初期化
        self.model = GenerativeModel(model_name=model_name)
        
        # generation_configを作成 (Phase1用 - 従来版)
        self.generation_config = GenerationConfig(
            temperature=0.0,  # 完全にdeterministicに
            top_p=1.0,       # nucleus samplingを無効化
            top_k=1,         # 最も確率の高い選択肢のみ
            max_output_tokens=8192,
            candidate_count=1,  # レスポンス候補を1つに制限
            response_mime_type="application/json",
            response_schema=MEAL_ANALYSIS_GEMINI_SCHEMA
        )
        
        # 構造化分析用のgeneration_config
        self.structured_generation_config = GenerationConfig(
            temperature=0.1,  # わずかな変動を許可（より詳細な分析のため）
            top_p=0.95,
            top_k=40,
            max_output_tokens=16384,  # より多くの出力を許可
            candidate_count=1,
            response_mime_type="application/json",
            response_schema=STRUCTURED_MEAL_ANALYSIS_SCHEMA
        )
        
        # セーフティ設定
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }
    
    async def analyze_phase1(
        self, 
        image_bytes: bytes, 
        image_mime_type: str, 
        optional_text: Optional[str] = None
    ) -> Dict:
        """
        Phase1: 画像とテキストを分析して食事情報を抽出（従来版）
        
        Args:
            image_bytes: 画像のバイトデータ
            image_mime_type: 画像のMIMEタイプ
            optional_text: オプションのテキスト説明
            
        Returns:
            分析結果の辞書
            
        Raises:
            RuntimeError: Gemini APIエラー時
        """
        try:
            # プロンプトを取得
            system_prompt = Phase1Prompts.get_system_prompt()
            user_prompt = Phase1Prompts.get_user_prompt(optional_text)
            
            # 完全なプロンプトを構築
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # コンテンツリストを作成
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Gemini APIを呼び出し
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            # レスポンスのテキストを取得
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONレスポンスをパース
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase1 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e
    
    async def analyze_phase1_structured(
        self, 
        image_bytes: bytes, 
        image_mime_type: str, 
        optional_text: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> Dict:
        """
        Phase1: 構造化された詳細な画像分析（信頼度スコア、属性、ブランド情報等を含む）
        
        Args:
            image_bytes: 画像のバイトデータ
            image_mime_type: 画像のMIMEタイプ
            optional_text: オプションのテキスト説明
            system_prompt: カスタムシステムプロンプト（指定されない場合はデフォルト使用）
            
        Returns:
            構造化された分析結果の辞書
            
        Raises:
            RuntimeError: Gemini APIエラー時
        """
        try:
            # プロンプトを準備
            if system_prompt is None:
                system_prompt = Phase1Prompts.get_system_prompt()
            
            user_prompt = Phase1Prompts.get_user_prompt(optional_text)
            
            # 完全なプロンプトを構築
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # コンテンツリストを作成
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            logger.info("Starting structured Gemini Phase1 analysis...")
            
            # Gemini APIを呼び出し（構造化設定使用）
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=self.structured_generation_config,
                safety_settings=self.safety_settings
            )
            
            # レスポンスのテキストを取得
            if not response.text:
                raise ValueError("No response returned from Gemini structured analysis.")
            
            # JSONレスポンスをパース
            result = json.loads(response.text)
            
            # 結果の検証と修正
            result = self._validate_and_fix_structured_result(result)
            
            detected_items_count = len(result.get('detected_food_items', []))
            dishes_count = len(result.get('dishes', []))
            overall_confidence = result.get('analysis_confidence', 0.5)
            
            logger.info(f"Gemini Phase1 structured analysis completed: {detected_items_count} items, "
                       f"{dishes_count} dishes, confidence {overall_confidence:.2f}")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error in structured analysis: {e}")
            raise RuntimeError(f"Error processing structured response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini structured API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini structured API request failed: {e}") from e
    
    def _validate_and_fix_structured_result(self, result: Dict) -> Dict:
        """構造化分析結果を検証し、必要に応じて修正"""
        # detected_food_itemsが存在しない場合の処理
        if 'detected_food_items' not in result:
            result['detected_food_items'] = []
        
        # dishesが存在しない場合の処理
        if 'dishes' not in result:
            result['dishes'] = []
        
        # analysis_confidenceが存在しない場合の処理
        if 'analysis_confidence' not in result:
            # 各アイテムの平均信頼度を計算
            confidences = []
            for item in result['detected_food_items']:
                if 'confidence' in item:
                    confidences.append(item['confidence'])
            
            for dish in result['dishes']:
                if 'confidence' in dish and dish['confidence'] is not None:
                    confidences.append(dish['confidence'])
            
            result['analysis_confidence'] = sum(confidences) / len(confidences) if confidences else 0.5
        
        # 各detected_food_itemの検証
        for item in result['detected_food_items']:
            # 必須フィールドのデフォルト値設定
            if 'confidence' not in item:
                item['confidence'] = 0.5
            if 'attributes' not in item:
                item['attributes'] = []
            if 'category_hints' not in item:
                item['category_hints'] = []
            if 'negative_cues' not in item:
                item['negative_cues'] = []
            
            # 属性の検証
            for attr in item['attributes']:
                if 'confidence' not in attr:
                    attr['confidence'] = 0.5
                if 'type' not in attr:
                    attr['type'] = 'ingredient'
        
        return result
    
    async def analyze_phase2(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        usda_candidates_text: str,
        initial_analysis_data: str
    ) -> Dict:
        """
        Phase2: USDAコンテキストを使用して画像を再分析
        
        Args:
            image_bytes: 画像のバイトデータ
            image_mime_type: 画像のMIMEタイプ
            usda_candidates_text: USDA候補情報のフォーマット済みテキスト
            initial_analysis_data: Phase1のAI出力（JSON文字列）
            
        Returns:
            精緻化された分析結果の辞書
            
        Raises:
            RuntimeError: Gemini APIエラー時
        """
        try:
            # プロンプトを取得
            system_prompt = Phase2Prompts.get_system_prompt()
            user_prompt = Phase2Prompts.get_user_prompt(
                usda_candidates_text=usda_candidates_text,
                initial_analysis_data=initial_analysis_data
            )
            
            # 完全なプロンプトを構築
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # コンテンツリストを作成
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Phase2用のGeneration Config (出力安定化)
            phase2_generation_config = GenerationConfig(
                temperature=0.0,  # 完全にdeterministicに
                top_p=1.0,       # nucleus samplingを無効化
                top_k=1,         # 最も確率の高い選択肢のみ
                max_output_tokens=8192,
                candidate_count=1,  # レスポンス候補を1つに制限
                response_mime_type="application/json",
                response_schema=REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA
            )
            
            # Gemini APIを呼び出し
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=phase2_generation_config,
                safety_settings=self.safety_settings
            )
            
            # レスポンスのテキストを取得
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONレスポンスをパース
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase2 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e 
```

================================================================================

📁 設定管理
================================================================================

📄 FILE: app_v2/config/__init__.py
------------------------------------------------------------
ファイルサイズ: 85 bytes
最終更新: 2025-06-05 12:46:54
存在: ✅

CONTENT:
```python
from .settings import Settings, get_settings

__all__ = ["Settings", "get_settings"] 
```

================================================================================

📁 Elasticsearch インデックス管理
================================================================================

📄 FILE: create_elasticsearch_index.py
------------------------------------------------------------
ファイルサイズ: 8,637 bytes
最終更新: 2025-06-10 13:04:18
存在: ✅

CONTENT:
```python
#!/usr/bin/env python3
"""
Elasticsearch Index Creation Script

現状のJSONデータベースからElasticsearchインデックスを作成する
"""

import json
import os
from elasticsearch import Elasticsearch
from typing import Dict, List, Any
import time


def create_index_mapping() -> Dict[str, Any]:
    """Elasticsearchインデックスのマッピングを定義"""
    return {
        "mappings": {
            "properties": {
                "id": {
                    "type": "keyword"
                },
                "search_name": {
                    "type": "text",
                    "analyzer": "standard",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        },
                        "suggest": {
                            "type": "completion"
                        }
                    }
                },
                "description": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "data_type": {
                    "type": "keyword"
                },
                "nutrition": {
                    "type": "object",
                    "properties": {
                        "calories": {"type": "float"},
                        "protein": {"type": "float"},
                        "fat": {"type": "float"},
                        "carbs": {"type": "float"},
                        "carbohydrates": {"type": "float"},
                        "fiber": {"type": "float"},
                        "sugar": {"type": "float"},
                        "sodium": {"type": "float"}
                    }
                },
                "weight": {
                    "type": "float"
                },
                "source_db": {
                    "type": "keyword"
                }
            }
        },
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "food_analyzer": {
                        "type": "standard",
                        "stopwords": "_none_"
                    }
                }
            }
        }
    }


def load_json_databases() -> Dict[str, List[Dict[str, Any]]]:
    """JSONデータベースファイルを読み込み"""
    databases = {}
    
    db_configs = {
        "yazio": "db/yazio_db.json",
        "mynetdiary": "db/mynetdiary_db.json", 
        "eatthismuch": "db/eatthismuch_db.json"
    }
    
    for db_name, file_path in db_configs.items():
        try:
            if os.path.exists(file_path):
                print(f"Loading {db_name} from {file_path}...")
                with open(file_path, 'r', encoding='utf-8') as f:
                    database = json.load(f)
                    databases[db_name] = database
                    print(f"✅ Loaded {db_name}: {len(database)} items")
            else:
                print(f"⚠️  File not found: {file_path}")
                databases[db_name] = []
        except Exception as e:
            print(f"❌ Error loading {db_name}: {e}")
            databases[db_name] = []
    
    return databases


def prepare_document(item: Dict[str, Any], source_db: str) -> Dict[str, Any]:
    """ドキュメントをElasticsearch用に準備"""
    doc = {
        "id": item.get("id", 0),
        "search_name": item.get("search_name", ""),
        "description": item.get("description"),
        "data_type": item.get("data_type", "unknown"),
        "nutrition": item.get("nutrition", {}),
        "weight": item.get("weight"),
        "source_db": source_db
    }
    
    # 空の値を除去
    return {k: v for k, v in doc.items() if v is not None}


def bulk_index_documents(es_client: Elasticsearch, index_name: str, documents: List[Dict[str, Any]], batch_size: int = 1000):
    """バルクインデックスでドキュメントを追加"""
    total_docs = len(documents)
    indexed_count = 0
    
    print(f"📥 Indexing {total_docs} documents in batches of {batch_size}...")
    
    for i in range(0, total_docs, batch_size):
        batch = documents[i:i + batch_size]
        
        # バルクリクエストの構築
        bulk_body = []
        for doc in batch:
            bulk_body.append({
                "index": {
                    "_index": index_name,
                    "_id": f"{doc['source_db']}_{doc['id']}"
                }
            })
            bulk_body.append(doc)
        
        try:
            response = es_client.bulk(body=bulk_body)
            
            # エラーチェック
            if response.get("errors"):
                error_count = sum(1 for item in response["items"] if "error" in item.get("index", {}))
                print(f"⚠️  Batch {i//batch_size + 1}: {error_count} errors in batch")
            
            indexed_count += len(batch)
            print(f"   Progress: {indexed_count}/{total_docs} ({indexed_count/total_docs*100:.1f}%)")
            
        except Exception as e:
            print(f"❌ Error indexing batch {i//batch_size + 1}: {e}")
    
    print(f"✅ Indexing completed: {indexed_count} documents")


def main():
    """メイン処理"""
    print("=== Elasticsearch Index Creation ===")
    
    # Elasticsearchクライアントの初期化
    print("\n1. Connecting to Elasticsearch...")
    es_client = Elasticsearch(["http://localhost:9200"])
    
    if not es_client.ping():
        print("❌ Cannot connect to Elasticsearch. Make sure it's running on localhost:9200")
        return False
    
    print("✅ Connected to Elasticsearch")
    
    # インデックス名
    index_name = "nutrition_db"
    
    # 既存インデックスの削除（必要に応じて）
    print(f"\n2. Checking existing index '{index_name}'...")
    if es_client.indices.exists(index=index_name):
        print(f"   Index '{index_name}' already exists. Deleting...")
        es_client.indices.delete(index=index_name)
        print("   ✅ Deleted existing index")
    
    # インデックスの作成
    print(f"\n3. Creating index '{index_name}'...")
    mapping = create_index_mapping()
    es_client.indices.create(index=index_name, body=mapping)
    print("✅ Index created with mapping")
    
    # JSONデータベースの読み込み
    print("\n4. Loading JSON databases...")
    databases = load_json_databases()
    
    # ドキュメントの準備
    print("\n5. Preparing documents for indexing...")
    all_documents = []
    
    for db_name, items in databases.items():
        print(f"   Processing {db_name}: {len(items)} items")
        for item in items:
            if "search_name" in item:  # 有効なアイテムのみ
                doc = prepare_document(item, db_name)
                all_documents.append(doc)
    
    print(f"✅ Prepared {len(all_documents)} documents for indexing")
    
    # バルクインデックス
    print("\n6. Bulk indexing documents...")
    start_time = time.time()
    bulk_index_documents(es_client, index_name, all_documents)
    end_time = time.time()
    
    print(f"✅ Indexing completed in {end_time - start_time:.2f} seconds")
    
    # インデックス統計の表示
    print("\n7. Index statistics...")
    stats = es_client.indices.stats(index=index_name)
    doc_count = stats["indices"][index_name]["total"]["docs"]["count"]
    index_size = stats["indices"][index_name]["total"]["store"]["size_in_bytes"]
    
    print(f"   Total documents: {doc_count}")
    print(f"   Index size: {index_size / 1024 / 1024:.2f} MB")
    
    # サンプル検索テスト
    print("\n8. Testing sample search...")
    test_query = {
        "query": {
            "multi_match": {
                "query": "chicken",
                "fields": ["search_name", "search_name.exact"]
            }
        },
        "size": 3
    }
    
    response = es_client.search(index=index_name, body=test_query)
    hits = response["hits"]["hits"]
    
    print(f"   Sample search for 'chicken': {len(hits)} results")
    for hit in hits:
        source = hit["_source"]
        print(f"   - {source['search_name']} ({source['source_db']}) score: {hit['_score']:.2f}")
    
    print(f"\n🎉 Elasticsearch index '{index_name}' successfully created!")
    print(f"   Ready for high-speed nutrition search")
    
    return True


if __name__ == "__main__":
    success = main()
    if success:
        print("\n✅ Index creation completed successfully!")
    else:
        print("\n❌ Index creation failed!") 
```

================================================================================

📁 栄養データベース
================================================================================

📄 FILE: db/yazio_db.json
------------------------------------------------------------
ファイルサイズ: 504,128 bytes
最終更新: 2025-06-10 11:55:48
存在: ✅

CONTENT (データベース - 最初の50行):
```json
[
  {
    "data_type": "unified",
    "id": 1000000000,
    "search_name": "Cheese Coffeecake",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 339.0,
      "protein": 7.0,
      "fat": 15.2,
      "carbs": 44.3
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000001,
    "search_name": "Fruit Fried Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 316.0,
      "protein": 3.0,
      "fat": 16.1,
      "carbs": 42.6
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000002,
    "search_name": "Blueberry Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 245.0,
      "protein": 2.7,
      "fat": 11.9,
      "carbs": 33.5
    },
    "source": "YAZIO"
  },
  {
    "data_type": "unified",
    "id": 1000000003,
    "search_name": "Apple Pie",
    "description": "Cakes & Pies",
    "nutrition": {
      "calories": 265.0,
      "protein": 2.4,
      "fat": 12.5,
      "carbs": 37.1

... (23677 more lines)
```

================================================================================

📄 FILE: db/mynetdiary_db.json
------------------------------------------------------------
ファイルサイズ: 349,780 bytes
最終更新: 2025-06-10 11:55:05
存在: ✅

CONTENT (データベース - 最初の50行):
```json
[
  {
    "data_type": "unified",
    "id": 10000000000,
    "search_name": "Beans baked canned plain or vegetarian",
    "description": null,
    "nutrition": {
      "calories": 94.09448818897638,
      "protein": 4.724409448818898,
      "fat": 0.35433070866141736,
      "carbs": 21.25984251968504
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000001,
    "search_name": "Black beans boiled without salt",
    "description": null,
    "nutrition": {
      "calories": 131.97674418604652,
      "protein": 8.72093023255814,
      "fat": 0.5232558139534884,
      "carbs": 23.837209302325583
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000002,
    "search_name": "Black beans canned low sodium",
    "description": null,
    "nutrition": {
      "calories": 90.83333333333334,
      "protein": 5.833333333333334,
      "fat": 0.2916666666666667,
      "carbs": 16.666666666666668
    },
    "source": "MyNetDiary"
  },
  {
    "data_type": "unified",
    "id": 10000000003,
    "search_name": "Black beans canned no salt added",
    "description": null,
    "nutrition": {
      "calories": 84.61538461538461,
      "protein": 5.384615384615385,
      "fat": 0.0,
      "carbs": 16.153846153846153

... (14798 more lines)
```

================================================================================

📄 FILE: db/eatthismuch_db.json
------------------------------------------------------------
ファイルサイズ: 2,809,842 bytes
最終更新: 2025-06-10 11:50:01
存在: ✅

CONTENT (データベース - 最初の50行):
```json
[
  {
    "data_type": "dish",
    "id": 907072,
    "search_name": "Garlic and Cream Cheese Cauliflower \"Mashed Potatoes\"",
    "description": null,
    "nutrition": {
      "calories": 43.63999999999999,
      "protein": 2.5545171339563866,
      "fat": 1.6926272066458983,
      "carbs": 5.7632398753894085
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 905725,
    "search_name": "Poached Eggs on Toast",
    "description": null,
    "nutrition": {
      "calories": 211.81,
      "protein": 12.653061224489797,
      "fat": 6.530612244897958,
      "carbs": 24.89795918367347
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 3267515,
    "search_name": "Caprese Salad",
    "description": null,
    "nutrition": {
      "calories": 174.96289228159455,
      "protein": 6.530958439355386,
      "fat": 15.436810856658182,
      "carbs": 3.307888040712468
    },
    "source": "EatThisMuch"
  },
  {
    "data_type": "dish",
    "id": 906392,
    "search_name": "Spinach and Pear Omelet",
    "description": null,
    "nutrition": {
      "calories": 105.17,
      "protein": 5.143651529193697,
      "fat": 6.580166821130676,
      "carbs": 7.298424467099165

... (115366 more lines)
```

================================================================================

📁 テスト画像
================================================================================

📄 FILE: test_images/food3.jpg
------------------------------------------------------------
ファイルサイズ: 133,358 bytes
最終更新: 2025-05-30 16:00:37
存在: ✅

CONTENT: [画像ファイル - バイナリデータ]
画像ファイル: food3.jpg
用途: test_advanced_elasticsearch_search.py の入力画像

================================================================================

📁 依存関係・設定ファイル
================================================================================

📄 FILE: requirements.txt
------------------------------------------------------------
ファイルサイズ: 274 bytes
最終更新: 2025-06-10 13:01:13
存在: ✅

CONTENT:
```python
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
google-cloud-aiplatform==1.94.0
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
httpx
pytest==7.4.3
pytest-asyncio==0.21.1
python-dotenv==1.0.0
Pillow==11.2.1
elasticsearch==8.15.1 
```

================================================================================

📄 FILE: README.md
------------------------------------------------------------
ファイルサイズ: 15,369 bytes
最終更新: 2025-06-10 14:55:33
存在: ✅

CONTENT:
```python
# 食事分析 API (Meal Analysis API) v2.0

## 概要

この API は、**Google Gemini AI** と **Elasticsearch ベースマルチデータベース栄養検索システム**を使用した高度な食事画像分析システムです。**動的栄養計算機能**により、料理の特性に応じて最適な栄養計算戦略を自動選択し、正確な栄養価情報を提供します。

## 🌟 主な機能

### **🔥 新機能: Elasticsearch マルチデータベース栄養検索 v2.0**

- **⚡ Elasticsearch 高速検索**: 高性能な Elasticsearch インデックスによる大規模栄養データベース検索
- **📊 3 つのデータベース統合検索**: 1 つのクエリで複数のデータベースから包括的な栄養情報を取得
  - **YAZIO**: 1,825 項目 - バランスの取れた食品カテゴリ
  - **MyNetDiary**: 1,142 項目 - 科学的/栄養学的アプローチ
  - **EatThisMuch**: 8,878 項目 - 最大かつ最も包括的なデータベース
- **🔍 マルチ DB 検索モード**: 各データベースから最大 5 件ずつ、総合的な検索結果を提供
- **🎯 高精度マッチング**: 90.9%の成功率、各 DB から均等な結果分散
- **💾 詳細結果保存**: JSON・マークダウン・テキスト形式での検索結果自動保存

### **従来機能: 動的栄養計算システム**

- **🧠 AI 駆動の計算戦略決定**: Gemini AI が各料理に対して最適な栄養計算方法を自動選択
- **🎯 高精度栄養計算**: 食材重量 × 100g あたり栄養価で正確な実栄養価を算出
- **📊 3 層集計システム**: 食材 → 料理 → 食事全体の自動栄養集計

### **コア機能**

- **フェーズ 1**: Gemini AI による食事画像の分析（料理識別、食材抽出、重量推定）
- **マルチ DB 検索**: 3 つのデータベースからの包括的栄養情報取得
- **複数料理対応**: 1 枚の画像で複数の料理を同時分析
- **英語・日本語対応**: 多言語での食材・料理認識
- **OpenAPI 3.0 準拠**: 完全な API 文書化とタイプ安全性

## 🏗 プロジェクト構造

```
meal_analysis_api_2/
├── db/                                   # マルチデータベース（新機能）
│   ├── yazio_db.json                     # YAZIO栄養データベース（1,825項目）
│   ├── mynetdiary_db.json                # MyNetDiary栄養データベース（1,142項目）
│   └── eatthismuch_db.json               # EatThisMuch栄養データベース（8,878項目）
├── app_v2/                               # 新アーキテクチャ版
│   ├── components/                       # コンポーネントベース設計
│   │   ├── local_nutrition_search_component.py  # マルチDB検索コンポーネント
│   │   ├── phase1_component.py           # 画像分析コンポーネント
│   │   └── base.py                       # ベースコンポーネント
│   ├── pipeline/                         # パイプライン管理
│   │   ├── orchestrator.py               # メイン処理オーケストレーター
│   │   └── result_manager.py             # 結果管理システム
│   ├── models/                           # データモデル
│   │   ├── nutrition_search_models.py    # 栄養検索モデル
│   │   └── phase1_models.py              # Phase1モデル
│   ├── main/
│   │   └── app.py                        # FastAPIアプリケーション
│   └── config/                           # 設定管理
├── test_multi_db_nutrition_search.py     # マルチDB検索テストスクリプト（新機能）
├── test_local_nutrition_search_v2.py     # ローカル検索テストスクリプト
├── test_images/                          # テスト用画像
└── requirements.txt                      # Python依存関係
```

## 🚀 セットアップ

### 1. 依存関係のインストール

```bash
# 仮想環境の作成
python -m venv venv

# 仮想環境のアクティベート
source venv/bin/activate  # macOS/Linux
# または
venv\Scripts\activate     # Windows

# 依存関係のインストール
pip install -r requirements.txt
```

### 2. Google Cloud 設定

#### Google Cloud SDK のインストール

まだインストールしていない場合は、以下からインストールしてください：
https://cloud.google.com/sdk/docs/install

#### Google Cloud 認証の設定

開発環境では以下のコマンドで認証を設定：

```bash
# Google Cloudにログイン
gcloud auth login

# アプリケーションのデフォルト認証情報を設定
gcloud auth application-default login

# プロジェクトIDを設定
gcloud config set project YOUR_PROJECT_ID
```

本番環境ではサービスアカウントキーを使用：

```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your-service-account-key.json"
```

#### Vertex AI API の有効化

```bash
# Vertex AI APIを有効化
gcloud services enable aiplatform.googleapis.com
```

### 3. 環境変数の設定

以下の環境変数を設定してください：

```bash
# USDA API設定
export USDA_API_KEY="your-usda-api-key"

# Vertex AI設定
export GOOGLE_APPLICATION_CREDENTIALS="path/to/service-account-key.json"
export GEMINI_PROJECT_ID="your-gcp-project-id"
export GEMINI_LOCATION="us-central1"
export GEMINI_MODEL_NAME="gemini-2.5-flash-preview-05-20"
```

## 🖥 サーバー起動

### app_v2 サーバーの起動（マルチ DB 対応）

```bash
# app_v2サーバーの起動
python -m app_v2.main.app
```

**⚠️ 注意**: 相対インポートエラーを回避するため、必ずモジュール形式で実行してください。

サーバーが起動すると、以下の URL でアクセス可能になります：

- **API**: http://localhost:8000
- **ドキュメント**: http://localhost:8000/docs
- **ヘルスチェック**: http://localhost:8000/health

## 🧪 テストの実行

### 🔥 マルチデータベース栄養検索テスト（最新機能）

**重要**: サーバーが起動している状態で実行してください。

```bash
# 別のターミナルで実行
python test_multi_db_nutrition_search.py
```

**期待される結果**:

- **検索速度**: 11 クエリを 0.10 秒で処理
- **マッチ率**: 各 DB90.9%のクエリで結果発見
- **総マッチ数**: 87 件（平均 7.9 件/クエリ）
- **データベース統計**:
  - YAZIO: 1,825 項目
  - MyNetDiary: 1,142 項目
  - EatThisMuch: 8,878 項目

**テスト結果例**:

```
📈 Multi-Database Search Results Summary:
- Total queries: 11
- Total matches found: 87
- Average matches per query: 7.9
- Search time: 0.10s

🔍 Detailed Query Results:
1. 'Roasted Potatoes' (dish)
   EatThisMuch: 3 matches
     Best: 'Roasted Potatoes' (score: 1.000)
     Nutrition: 91.0 kcal, 1.9g protein
```

### ローカル栄養検索テスト

```bash
# ローカル栄養データベース検索の統合テスト
python test_local_nutrition_search_v2.py
```

### 基本テスト（フェーズ 1 のみ）

```bash
python test_phase1_only.py
```

## 🚀 ローカル栄養データベース検索システム v2.0

### **新機能: ローカルデータベース統合**

システムが USDA API 依存からローカル栄養データベース検索に対応しました：

- **🔍 BM25F + マルチシグナルブースティング検索**: 高精度な食材マッチング
- **📊 8,878 項目のローカルデータベース**: オフライン栄養計算対応
- **⚡ 90.9%マッチ率**: 実測値による高い成功率
- **🔄 USDA 互換性**: 既存システムとの完全互換性維持

### サーバー起動（v2.0 対応）

```bash
# app_v2サーバーの起動
python -m app_v2.main.app
```

### ローカル栄養検索テスト

**重要**: サーバーが起動している状態で実行してください。

```bash
# ローカル栄養データベース検索の統合テスト
python test_local_nutrition_search_v2.py
```

**期待される結果**:

- **マッチ率**: 90.9% (10/11 検索成功)
- **レスポンス時間**: ~11 秒
- **データベース**: ローカル栄養データ (8,878 項目)
- **検索方法**: BM25F + マルチシグナルブースティング

**テスト結果例**:

```
🔍 Local Nutrition Search Results:
- Matches found: 10
- Match rate: 90.9%
- Search method: local_nutrition_database
- Total searches: 11
- Successful matches: 10

🍽 Final Meal Nutrition:
- Calories: 400.00 kcal
- Protein: 60.00 g
- Carbohydrates: 220.00 g
- Fat: 120.00 g
```

### データベース詳細

**ローカル栄養データベース構成**:

- `dish_db.json`: 4,583 料理データ
- `ingredient_db.json`: 1,473 食材データ
- `branded_db.json`: 2,822 ブランド食品
- `unified_nutrition_db.json`: 8,878 統合データ

## 📡 API 使用方法

### 🔥 完全分析 (推奨): 全フェーズ統合

**1 つのリクエストで全ての分析を実行**

```bash
curl -X POST "http://localhost:8000/api/v1/meal-analyses/complete" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg"
```

このエンドポイントは以下を自動実行します：

- フェーズ 1: 画像分析
- USDA 照合: 食材データベース検索
- フェーズ 2: 計算戦略決定
- 栄養計算: 最終栄養価算出
- 結果保存: 自動的にファイル保存

**保存された結果の取得**

```bash
# 全結果一覧
curl "http://localhost:8000/api/v1/meal-analyses/results"

# 特定の結果取得
curl "http://localhost:8000/api/v1/meal-analyses/results/{analysis_id}"
```

### フェーズ 1: 基本分析

```bash
curl -X POST "http://localhost:8000/api/v1/meal-analyses" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg"
```

### フェーズ 2: 動的栄養計算

```bash
# 最初にフェーズ1の結果を取得
initial_result=$(curl -X POST "http://localhost:8000/api/v1/meal-analyses" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg")

# フェーズ2で動的栄養計算
curl -X POST "http://localhost:8000/api/v1/meal-analyses/refine" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg" \
  -F "initial_analysis_data=$initial_result"
```

## 📋 レスポンス例

### フェーズ 1 レスポンス

```json
{
  "dishes": [
    {
      "dish_name": "Fried Fish with Spaghetti and Tomato Sauce",
      "type": "Main Dish",
      "quantity_on_plate": "2 pieces of fish, 1 small serving of spaghetti",
      "ingredients": [
        {
          "ingredient_name": "White Fish Fillet",
          "weight_g": 150.0
        },
        {
          "ingredient_name": "Spaghetti (cooked)",
          "weight_g": 80.0
        }
      ]
    }
  ]
}
```

### フェーズ 2 レスポンス（動的栄養計算）

```json
{
  "dishes": [
    {
      "dish_name": "Spinach and Daikon Radish Aemono",
      "type": "Side Dish",
      "calculation_strategy": "ingredient_level",
      "fdc_id": null,
      "ingredients": [
        {
          "ingredient_name": "Spinach",
          "weight_g": 80.0,
          "fdc_id": 1905313,
          "usda_source_description": "SPINACH",
          "key_nutrients_per_100g": {
            "calories_kcal": 24.0,
            "protein_g": 3.53,
            "carbohydrates_g": 3.53,
            "fat_g": 0.0
          },
          "actual_nutrients": {
            "calories_kcal": 19.2,
            "protein_g": 2.82,
            "carbohydrates_g": 2.82,
            "fat_g": 0.0
          }
        }
      ],
      "dish_total_actual_nutrients": {
        "calories_kcal": 57.45,
        "protein_g": 3.85,
        "carbohydrates_g": 4.57,
        "fat_g": 3.31
      }
    },
    {
      "dish_name": "Green Tea",
      "type": "Drink",
      "calculation_strategy": "dish_level",
      "fdc_id": 1810668,
      "usda_source_description": "GREEN TEA",
      "key_nutrients_per_100g": {
        "calories_kcal": 0.0,
        "protein_g": 0.0,
        "carbohydrates_g": 0.0,
        "fat_g": 0.0
      },
      "dish_total_actual_nutrients": {
        "calories_kcal": 0.0,
        "protein_g": 0.0,
        "carbohydrates_g": 0.0,
        "fat_g": 0.0
      }
    }
  ],
  "total_meal_nutrients": {
    "calories_kcal": 337.95,
    "protein_g": 13.32,
    "carbohydrates_g": 56.19,
    "fat_g": 6.67
  },
  "warnings": null,
  "errors": null
}
```

## 🔧 技術仕様

### 動的計算戦略の決定ロジック

**Dish Level (`dish_level`)**:

- シンプルな単品食品（果物、飲み物、基本食材）
- 標準化された既製品で適切な USDA ID が存在する場合
- 例: 緑茶、りんご、白米

**Ingredient Level (`ingredient_level`)**:

- 複雑な調理済み料理（炒め物、サラダ、スープ）
- 複数食材の組み合わせで料理全体の USDA ID が不適切な場合
- 例: 野菜炒め、手作りサラダ、味噌汁

### 栄養計算式

```
実栄養価 = (100gあたり栄養価 ÷ 100) × 推定重量(g)
```

### 集計階層

1. **食材レベル**: 個別食材の重量 × 100g 栄養価
2. **料理レベル**: 食材レベルの合計 または 料理全体計算
3. **食事レベル**: 全料理の栄養価合計

## ⚠️ エラーハンドリング

API は以下の HTTP ステータスコードを返します：

- `200 OK`: 正常な分析完了
- `400 Bad Request`: 不正なリクエスト（画像形式エラーなど）
- `422 Unprocessable Entity`: バリデーションエラー
- `503 Service Unavailable`: 外部サービス（USDA/Gemini）エラー
- `500 Internal Server Error`: サーバー内部エラー

## 🔍 トラブルシューティング

### 認証エラーが発生する場合

```bash
# 現在の認証状態を確認
gcloud auth list

# 現在のプロジェクト設定を確認
gcloud config list

# 必要に応じて再度認証
gcloud auth application-default login
```

### Vertex AI API が有効になっていない場合

```bash
# APIの有効状況を確認
gcloud services list --enabled | grep aiplatform

# 有効でない場合は有効化
gcloud services enable aiplatform.googleapis.com
```

### USDA API エラーが発生する場合

- API キーが正しく設定されているか確認
- レートリミット（3,600 件/時）に達していないか確認
- ネットワーク接続を確認

## 💻 開発情報

- **フレームワーク**: FastAPI 0.104+
- **AI サービス**: Google Vertex AI (Gemini 2.5 Flash)
- **栄養データベース**: USDA FoodData Central API
- **認証**: Google Cloud サービスアカウント
- **Python バージョン**: 3.9+
- **主要ライブラリ**:
  - `google-cloud-aiplatform` (Vertex AI)
  - `httpx` (非同期 HTTP)
  - `pydantic` (データバリデーション)
  - `pillow` (画像処理)

## 📄 ライセンス

このプロジェクトは MIT ライセンスの下で公開されています。

## 注意事項

**セキュリティ**: API キーやサービスアカウントキーは絶対にリポジトリにコミットしないでください。環境変数として安全に管理してください。

```

================================================================================

📄 FILE: service-account-key.json
------------------------------------------------------------
ファイルサイズ: 2,421 bytes
最終更新: 2025-05-16 17:45:05
存在: ✅

CONTENT (設定ファイル):
```json
{
  "type": "service_account",
  "project_id": "recording-diet-ai-3e7cf",
  "private_key_id": "614eb51641f7bfa861054c72d1ec20a94cbfe9c8",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDSKPu5UHQ9MCkL\nB8AL/OvuOPQmD9y9pYPZa32/z/aqpeYrG3lg70yIxroCtPFlsThGQPLP1LD1LsTq\nA64+fpq0Oh+a5/90eDnEw3a527yLzfexpgg59gFQ6e4apxglHP8iS8xokLFqz5ud\n8AoYtotgPWxmIXMLNGHQx7szptRSyao/SKDVJfpmhmlO2LpGCYvPJqO9DXBzIcp6\ngZIwn2cj6hr34Y/t4fHf6QtThCAVmL+x5NYMUb8Eox1zpjUQ15PMDdfnilUkj23i\nZJW7mLcZ5KIaTWPP4tAgYSsKr6HgkWO2fQQEVnOrBWeO1JZR3dK6YAtESnb4A+QU\nUwgH7oOVAgMBAAECggEACsHLRKyADqO47hYa9+Idx2dF4+7a6yAeNBJC8P75jrmI\nIvu4hkQpZDTbz4iodH6SNVJcu1OVLS+UJ7BBinRbgc8Z0AmA81q2BkLFAHVlXLiR\nrZekgyb64UPgCqAz5a5qxrwffdiuJBv1SKvBMIujba0nL8EVjOTE/vVCOT4YTxZo\nnH75Ob1+jdNzBKSfr2uMvN9BRdPkYJ8Yi9+c/yUH2rkdNdTrAqUp/To8EKvAjOR2\npmC0T2m+WcdYKGYwWtLgxuPk6GLYuWF3s3hxSd5+u9GL6pbxBiiLSl+emuiBoQi9\nj/LFJY57Bp4rOM2G5pGc/HWrm3zahapoDqigENCfsQKBgQD7ZEtSnt7hyNitCHR5\nJkWXvtQrOhimPjRpHYmL704NRexz/5pJAabgQY7QmQ8XfECRHOcyl/qXqyR+RDGU\nT0NlouFChNyL1wxsHK1y3C2C7Ny0M6+X7VsBe4tiidn33hS2XGXqnZCSB2ABYHrn\nu9lOD+Py1Q4r5zDmDQaojU8UKQKBgQDWAzOCvSX9sLgWjI2EFmYcjhcpoh5T9Hsf\nlOVWSlInBZARgqDiJLqeqF7wSjYOdtlN6+zVsv3ErA2QvkDd4kMgLLWzFsOHHrc9\nEvlIKsv2wZh9qy1zlJGxpnuUfgCgd5pbQqLTUal8XHeVf678sACB6bz/H7fFUk7p\nV5F1nXJBjQKBgQCRtqGeQy4Hi2ZkbWktq8xc16SdZbBR8+5nG1LVxKDmOqiC2B4y\nwP1cMUO5j25a+49lTW6JOeRrsyyU76wZPhRfvhh5eQ9pEv5FUB4NXKgYoniPDwJx\nuoeshVLWi/bGoHg697WvVyMsMZApXCYBWjXr9HP5Fht/wSLrxZMdccLreQKBgEuE\nuaqKwFsy/uLGGjHgDYxJ/5ZrZLRPcxsD2aGHfFHUvq/PHqJuP4Q4+bdlGIomGixK\n8jm+fZnm9Kp82Drz2qgB3uQhRkHp7tMYXOrAX1Tln7/IpbNBW+AKVVVi2SnGyqsl\nanuTN3Fw16njcoYsPSGar1x/fsOcdcgVZHjSZD0JAoGAIQglGBuSJGe+o8qBfcVX\nWZDArPREsCVc6ZknlNKpI6nL412TuLvvTNwpWDrvcC4A3icIeQWdqONE8VMH+Ta/\nzBG0P/G7d98N1WOX5eP4UbBnK4TWwreCZjKQqTsTn4WwpnCcIcntWxzypzOyAAyh\nmbONwHWdsxwL/QNu3dPf3F4=\n-----END PRIVATE KEY-----\n",
  "client_email": "vertex-ai-service-account-596@recording-diet-ai-3e7cf.iam.gserviceaccount.com",
  "client_id": "116391166725593063876",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/vertex-ai-service-account-596%40recording-diet-ai-3e7cf.iam.gserviceaccount.com",
  "universe_domain": "googleapis.com"
}
```

================================================================================

📊 DETAILED EXECUTION RESULTS ANALYSIS
================================================================================

🗃️ DATABASE SOURCE DISTRIBUTION:
   elasticsearch_eatthismuch: 20 results (66.7%)
   elasticsearch_yazio: 5 results (16.7%)
   elasticsearch_mynetdiary: 5 results (16.7%)

🎯 STRATEGIC BREAKDOWN:
   dish_primary: 5 results (16.7%)
   ingredient_primary: 10 results (33.3%)
   ingredient_fallback: 15 results (50.0%)

📋 QUERY ANALYSIS:
   'mixed greens' (ingredient): 5 results
      Top: Mixed Greens (score: 34.975)
   'corn' (ingredient): 5 results
      Top: Corn (score: 18.406)
   'Glazed Chicken Thighs with Roasted Baby Potatoes and Mixed Green Salad' (dish): 5 results
      Top: Mustard Glazed Chicken Thighs (score: 38.529)
   'tomato' (ingredient): 5 results
      Top: Tomatoes in Tomato Juice, canned (score: 17.471)
   'baby potato' (ingredient): 5 results
      Top: Baby carrots (score: 22.453)
   'chicken thigh' (ingredient): 5 results
      Top: Chicken thigh (score: 27.961)

================================================================================

🎯 ADVANCED STRATEGIC ELASTICSEARCH SEARCH ANALYSIS SUMMARY
----------------------------------------------------------------------
総ファイル数: 25
存在ファイル数: 25
分析完了時刻: 2025-06-11 12:41:22

📊 LATEST EXECUTION PERFORMANCE:
   ✅ Match Rate: 100.0%
   ⚡ Search Time: 383ms
   📋 Total Results: 30

このファイルには、test_advanced_elasticsearch_search.py実行時に
関わる高度戦略的Elasticsearch検索システムの全アプリケーション
ファイルと最新実行結果の完全な内容が含まれています。

🔥 ADVANCED STRATEGIC SEARCH SYSTEM HIGHLIGHTS:
- 🧠 AI構造化分析: Gemini 2.5 Flash DetectedFoodItems抽出
- 🍽️  Advanced Dish戦略: EatThisMuch dish → branded fallback
- 🥕 Advanced Ingredient戦略: EatThisMuch ingredient → Multi-DB fallback
- 📈 動的品質保証: スコア閾値20.0による自動フォールバック
- 🎯 戦略的メタデータ: 完全な検索プロセス追跡
- ⚡ 高性能実行: Sub-second strategic search
- 📊 包括的分析: データベース分布・戦略統計
- 💾 デュアル出力: JSON + Markdown レポート
- 🔧 Production-Ready: 認証・エラーハンドリング完備
- 🚀 拡張可能設計: Component-based Strategic Architecture
