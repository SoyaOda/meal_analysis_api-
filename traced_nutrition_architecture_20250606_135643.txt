================================================================================
MEAL ANALYSIS API v2.0 - ä¾å­˜é–¢ä¿‚è¿½è·¡ç‰ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æž
================================================================================
ç”Ÿæˆæ—¥æ™‚: 2025-06-06 13:56:43
è¿½è·¡èµ·ç‚¹: test_local_nutrition_search_v2.py
ç·è¿½è·¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 36
================================================================================

ðŸ“Š DEPENDENCY TRACE SUMMARY
----------------------------------------
âœ… Project Python Files: 2
ðŸŒ Server Files: 33
âš™ï¸ Config Files: 1
ðŸ—ƒï¸ Data Files (excluded): 0
ðŸŒ External Files: 3
âŒ Missing Files: 0

ðŸŽ¯ å®Ÿè¡Œèµ·ç‚¹ãƒ•ã‚¡ã‚¤ãƒ«
============================================================

ðŸ“„ FILE: test_local_nutrition_search_v2.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 13,113 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:49:32
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Local Nutrition Search System Test v2.0

nutrition_db_experimentã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¨çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ†ã‚¹ãƒˆ
"""

import requests
import json
import time
import os
from datetime import datetime

# APIè¨­å®šï¼ˆæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆï¼‰
BASE_URL = "http://localhost:8000/api/v1"

# ãƒ†ã‚¹ãƒˆç”»åƒã®ãƒ‘ã‚¹
image_path = "test_images/food3.jpg"

def test_local_nutrition_search_complete_analysis():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸå®Œå…¨åˆ†æžã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("=== Local Nutrition Search Complete Analysis Test v2.0 ===")
    print(f"Using image: {image_path}")
    print("ðŸ” Testing nutrition_db_experiment integration")
    
    try:
        # å®Œå…¨åˆ†æžã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
        with open(image_path, "rb") as f:
            files = {"image": ("food3.jpg", f, "image/jpeg")}
            data = {"save_results": True}  # çµæžœã‚’ä¿å­˜
            
            print("Starting complete analysis with local nutrition search...")
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/meal-analyses/complete", files=files, data=data)
            end_time = time.time()
        
        print(f"Status Code: {response.status_code}")
        print(f"Response Time: {end_time - start_time:.2f}s")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… Local nutrition search analysis successful!")
            
            # åˆ†æžID
            analysis_id = result.get("analysis_id")
            print(f"Analysis ID: {analysis_id}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢æ–¹æ³•ã®ç¢ºèªï¼‰
            metadata = result.get("metadata", {})
            print(f"\nðŸ“Š Pipeline Info:")
            print(f"- Version: {metadata.get('pipeline_version')}")
            print(f"- Components: {', '.join(metadata.get('components_used', []))}")
            print(f"- Nutrition Search Method: {metadata.get('nutrition_search_method')}")
            print(f"- Timestamp: {metadata.get('timestamp')}")
            
            # å‡¦ç†ã‚µãƒžãƒªãƒ¼
            summary = result.get("processing_summary", {})
            print(f"\nðŸ“ˆ Processing Summary:")
            print(f"- Total dishes: {summary.get('total_dishes')}")
            print(f"- Total ingredients: {summary.get('total_ingredients')}")
            print(f"- Search method: {summary.get('search_method')}")
            
            # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢çµæžœ
            nutrition_search_result = result.get("nutrition_search_result", {})
            print(f"\nðŸ” Local Nutrition Search Results:")
            print(f"- Matches found: {nutrition_search_result.get('matches_count', 0)}")
            print(f"- Match rate: {nutrition_search_result.get('match_rate', 0):.1%}")
            print(f"- Search method: {nutrition_search_result.get('search_method', 'unknown')}")
            
            search_summary = nutrition_search_result.get('search_summary', {})
            if search_summary:
                print(f"- Database source: {search_summary.get('database_source', 'unknown')}")
                print(f"- Total searches: {search_summary.get('total_searches', 0)}")
                print(f"- Successful matches: {search_summary.get('successful_matches', 0)}")
                print(f"- Failed searches: {search_summary.get('failed_searches', 0)}")
            
            # Phase1 çµæžœ
            phase1_result = result.get("phase1_result", {})
            phase1_dishes = len(phase1_result.get("dishes", []))
            print(f"\nðŸ” Phase1 Results:")
            print(f"- Detected dishes: {phase1_dishes}")
            
            if phase1_dishes > 0:
                print("- Dish details:")
                for i, dish in enumerate(phase1_result.get("dishes", [])[:3], 1):  # æœ€åˆã®3æ–™ç†ã®ã¿è¡¨ç¤º
                    print(f"  {i}. {dish.get('dish_name', 'Unknown')}")
                    ingredients = dish.get('ingredients', [])
                    print(f"     Ingredients ({len(ingredients)}): {', '.join([ing.get('ingredient_name', 'Unknown') for ing in ingredients[:5]])}")
                    if len(ingredients) > 5:
                        print(f"     ... and {len(ingredients) - 5} more")
            
            # æœ€çµ‚æ „é¤Šä¾¡çµæžœï¼ˆæš«å®šï¼‰
            final_nutrition = result.get("final_nutrition_result", {})
            total_nutrients = final_nutrition.get("total_meal_nutrients", {})
            
            print(f"\nðŸ½ Final Meal Nutrition (Preliminary):")
            print(f"- Calories: {total_nutrients.get('calories_kcal', 0):.2f} kcal")
            print(f"- Protein: {total_nutrients.get('protein_g', 0):.2f} g")
            print(f"- Carbohydrates: {total_nutrients.get('carbohydrates_g', 0):.2f} g")
            print(f"- Fat: {total_nutrients.get('fat_g', 0):.2f} g")
            
            # ä¿å­˜ã•ã‚ŒãŸè©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
            analysis_folder = result.get("analysis_folder")
            saved_files = result.get("saved_files", {})
            
            if analysis_folder:
                print(f"\nðŸ“ Analysis Folder:")
                print(f"- Path: {analysis_folder}")
                print(f"- Contains organized phase-by-phase results")
            
            if saved_files:
                print(f"\nðŸ’¾ Saved Files by Phase ({len(saved_files)} total):")
                
                # Phase1 files
                phase1_files = [k for k in saved_files.keys() if k.startswith('phase1_')]
                if phase1_files:
                    print("  ðŸ“Š Phase1 (Image Analysis):")
                    for file_key in phase1_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
                
                # Local search files  
                search_files = [k for k in saved_files.keys() if 'nutrition_search' in k or 'local' in k.lower()]
                if search_files:
                    print("  ðŸ” Local Nutrition Search:")
                    for file_key in search_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
                
                # Pipeline files
                pipeline_files = [k for k in saved_files.keys() if k in ['pipeline_summary', 'complete_log']]
                if pipeline_files:
                    print("  ðŸ“‹ Pipeline Summary:")
                    for file_key in pipeline_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
            
            return True, analysis_id
            
        else:
            print("âŒ Local nutrition search analysis failed!")
            print(f"Error: {response.text}")
            return False, None
            
    except Exception as e:
        print(f"âŒ Error during local nutrition search analysis: {e}")
        return False, None

def test_pipeline_info_local():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Local Nutrition Search Pipeline Info ===")
    
    try:
        response = requests.get(f"{BASE_URL}/meal-analyses/pipeline-info")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… Pipeline info retrieved!")
            print(f"Pipeline ID: {result.get('pipeline_id')}")
            print(f"Version: {result.get('version')}")
            print(f"Nutrition Search Method: {result.get('nutrition_search_method')}")
            
            components = result.get('components', [])
            print(f"\nðŸ”§ Components ({len(components)}):")
            for i, comp in enumerate(components, 1):
                print(f"  {i}. {comp.get('component_name')} ({comp.get('component_type')})")
                print(f"     Executions: {comp.get('execution_count')}")
        else:
            print(f"âŒ Pipeline info failed: {response.status_code}")
            
    except Exception as e:
        print(f"âŒ Error getting pipeline info: {e}")

def test_nutrition_db_experiment_availability():
    """nutrition_db_experimentã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Nutrition DB Experiment Availability Test ===")
    
    try:
        # nutrition_db_experimentãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
        nutrition_db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "nutrition_db_experiment")
        
        print(f"ðŸ” Checking nutrition_db_experiment path: {nutrition_db_path}")
        
        if os.path.exists(nutrition_db_path):
            print("âœ… nutrition_db_experiment directory found")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªï¼ˆæ­£ã—ã„ãƒ‘ã‚¹ã«ä¿®æ­£ï¼‰
            db_files = [
                "nutrition_db/dish_db.json",
                "nutrition_db/ingredient_db.json", 
                "nutrition_db/branded_db.json",
                "nutrition_db/unified_nutrition_db.json"
            ]
            
            print("ðŸ“Š Database Files:")
            for db_file in db_files:
                full_path = os.path.join(nutrition_db_path, db_file)
                if os.path.exists(full_path):
                    try:
                        with open(full_path, 'r', encoding='utf-8') as f:
                            # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ä¸€éƒ¨ã ã‘èª­ã¿è¾¼ã¿
                            if os.path.getsize(full_path) > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                                f.seek(0)
                                first_chunk = f.read(1024)
                                if first_chunk.strip().startswith('['):
                                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‹ã‚‰æŽ¨å®šã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’è¨ˆç®—
                                    file_size_mb = os.path.getsize(full_path) / (1024 * 1024)
                                    estimated_items = int(file_size_mb * 1000)  # å¤§ã¾ã‹ãªæŽ¨å®š
                                    print(f"  âœ… {db_file}: ~{estimated_items} items (file size: {file_size_mb:.1f}MB)")
                                else:
                                    print(f"  âœ… {db_file}: Large file ({file_size_mb:.1f}MB)")
                            else:
                                data = json.load(f)
                                print(f"  âœ… {db_file}: {len(data)} items")
                    except Exception as e:
                        print(f"  âŒ {db_file}: Error reading - {e}")
                else:
                    print(f"  âŒ {db_file}: Not found")
            
            # æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
            print("ðŸ”§ Search Components:")
            
            search_service_path = os.path.join(nutrition_db_path, "search_service")
            if os.path.exists(search_service_path):
                print(f"  âœ… search_service directory found: {search_service_path}")
                
                # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                component_files = [
                    "nlp/query_preprocessor.py",
                    "api/search_handler.py", 
                    "api/query_builder.py"
                ]
                
                for comp_file in component_files:
                    full_path = os.path.join(search_service_path, comp_file)
                    if os.path.exists(full_path):
                        print(f"    âœ… {comp_file}")
                    else:
                        print(f"    âŒ {comp_file}: Not found")
            else:
                print(f"  âŒ search_service directory not found")
                
        else:
            print("âŒ nutrition_db_experiment directory not found")
            print("ðŸ’¡ Please ensure nutrition_db_experiment is in the same directory as this script")
            
    except Exception as e:
        print(f"âŒ Error checking nutrition_db_experiment: {e}")

def compare_search_methods():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã¨USDAæ¤œç´¢ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    print("\n=== Search Methods Comparison ===")
    print("ðŸ”¬ This would compare local search vs USDA API search")
    print("ðŸ“ TODO: Implement when both methods are available")

if __name__ == "__main__":
    print("Testing Local Nutrition Search Integration v2.0")
    print("=" * 70)
    
    # nutrition_db_experimentã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
    test_nutrition_db_experiment_availability()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±
    test_pipeline_info_local()
    
    # ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ã‚’ä½¿ã£ãŸå®Œå…¨åˆ†æžã®ãƒ†ã‚¹ãƒˆ
    success, analysis_id = test_local_nutrition_search_complete_analysis()
    
    if success:
        print("\nðŸŽ‰ Local nutrition search integration test completed successfully!")
        print("ðŸš€ nutrition_db_experiment search system is working with the meal analysis pipeline!")
        print(f"ðŸ“‹ Analysis ID: {analysis_id}")
    else:
        print("\nðŸ’¥ Local nutrition search integration test failed!")
        print("ðŸ”§ Check the local search system setup and logs.")
        
    # æ¯”è¼ƒãƒ†ã‚¹ãƒˆï¼ˆå°†æ¥å®Ÿè£…äºˆå®šï¼‰
    compare_search_methods() 
```

============================================================

ðŸ—ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…Pythonãƒ•ã‚¡ã‚¤ãƒ«
============================================================

ðŸ“„ FILE: test_local_nutrition_search_v2.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 13,113 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:49:32
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Local Nutrition Search System Test v2.0

nutrition_db_experimentã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¨çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ†ã‚¹ãƒˆ
"""

import requests
import json
import time
import os
from datetime import datetime

# APIè¨­å®šï¼ˆæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆï¼‰
BASE_URL = "http://localhost:8000/api/v1"

# ãƒ†ã‚¹ãƒˆç”»åƒã®ãƒ‘ã‚¹
image_path = "test_images/food3.jpg"

def test_local_nutrition_search_complete_analysis():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã—ãŸå®Œå…¨åˆ†æžã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("=== Local Nutrition Search Complete Analysis Test v2.0 ===")
    print(f"Using image: {image_path}")
    print("ðŸ” Testing nutrition_db_experiment integration")
    
    try:
        # å®Œå…¨åˆ†æžã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
        with open(image_path, "rb") as f:
            files = {"image": ("food3.jpg", f, "image/jpeg")}
            data = {"save_results": True}  # çµæžœã‚’ä¿å­˜
            
            print("Starting complete analysis with local nutrition search...")
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/meal-analyses/complete", files=files, data=data)
            end_time = time.time()
        
        print(f"Status Code: {response.status_code}")
        print(f"Response Time: {end_time - start_time:.2f}s")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… Local nutrition search analysis successful!")
            
            # åˆ†æžID
            analysis_id = result.get("analysis_id")
            print(f"Analysis ID: {analysis_id}")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢æ–¹æ³•ã®ç¢ºèªï¼‰
            metadata = result.get("metadata", {})
            print(f"\nðŸ“Š Pipeline Info:")
            print(f"- Version: {metadata.get('pipeline_version')}")
            print(f"- Components: {', '.join(metadata.get('components_used', []))}")
            print(f"- Nutrition Search Method: {metadata.get('nutrition_search_method')}")
            print(f"- Timestamp: {metadata.get('timestamp')}")
            
            # å‡¦ç†ã‚µãƒžãƒªãƒ¼
            summary = result.get("processing_summary", {})
            print(f"\nðŸ“ˆ Processing Summary:")
            print(f"- Total dishes: {summary.get('total_dishes')}")
            print(f"- Total ingredients: {summary.get('total_ingredients')}")
            print(f"- Search method: {summary.get('search_method')}")
            
            # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢çµæžœ
            nutrition_search_result = result.get("nutrition_search_result", {})
            print(f"\nðŸ” Local Nutrition Search Results:")
            print(f"- Matches found: {nutrition_search_result.get('matches_count', 0)}")
            print(f"- Match rate: {nutrition_search_result.get('match_rate', 0):.1%}")
            print(f"- Search method: {nutrition_search_result.get('search_method', 'unknown')}")
            
            search_summary = nutrition_search_result.get('search_summary', {})
            if search_summary:
                print(f"- Database source: {search_summary.get('database_source', 'unknown')}")
                print(f"- Total searches: {search_summary.get('total_searches', 0)}")
                print(f"- Successful matches: {search_summary.get('successful_matches', 0)}")
                print(f"- Failed searches: {search_summary.get('failed_searches', 0)}")
            
            # Phase1 çµæžœ
            phase1_result = result.get("phase1_result", {})
            phase1_dishes = len(phase1_result.get("dishes", []))
            print(f"\nðŸ” Phase1 Results:")
            print(f"- Detected dishes: {phase1_dishes}")
            
            if phase1_dishes > 0:
                print("- Dish details:")
                for i, dish in enumerate(phase1_result.get("dishes", [])[:3], 1):  # æœ€åˆã®3æ–™ç†ã®ã¿è¡¨ç¤º
                    print(f"  {i}. {dish.get('dish_name', 'Unknown')}")
                    ingredients = dish.get('ingredients', [])
                    print(f"     Ingredients ({len(ingredients)}): {', '.join([ing.get('ingredient_name', 'Unknown') for ing in ingredients[:5]])}")
                    if len(ingredients) > 5:
                        print(f"     ... and {len(ingredients) - 5} more")
            
            # æœ€çµ‚æ „é¤Šä¾¡çµæžœï¼ˆæš«å®šï¼‰
            final_nutrition = result.get("final_nutrition_result", {})
            total_nutrients = final_nutrition.get("total_meal_nutrients", {})
            
            print(f"\nðŸ½ Final Meal Nutrition (Preliminary):")
            print(f"- Calories: {total_nutrients.get('calories_kcal', 0):.2f} kcal")
            print(f"- Protein: {total_nutrients.get('protein_g', 0):.2f} g")
            print(f"- Carbohydrates: {total_nutrients.get('carbohydrates_g', 0):.2f} g")
            print(f"- Fat: {total_nutrients.get('fat_g', 0):.2f} g")
            
            # ä¿å­˜ã•ã‚ŒãŸè©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
            analysis_folder = result.get("analysis_folder")
            saved_files = result.get("saved_files", {})
            
            if analysis_folder:
                print(f"\nðŸ“ Analysis Folder:")
                print(f"- Path: {analysis_folder}")
                print(f"- Contains organized phase-by-phase results")
            
            if saved_files:
                print(f"\nðŸ’¾ Saved Files by Phase ({len(saved_files)} total):")
                
                # Phase1 files
                phase1_files = [k for k in saved_files.keys() if k.startswith('phase1_')]
                if phase1_files:
                    print("  ðŸ“Š Phase1 (Image Analysis):")
                    for file_key in phase1_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
                
                # Local search files  
                search_files = [k for k in saved_files.keys() if 'nutrition_search' in k or 'local' in k.lower()]
                if search_files:
                    print("  ðŸ” Local Nutrition Search:")
                    for file_key in search_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
                
                # Pipeline files
                pipeline_files = [k for k in saved_files.keys() if k in ['pipeline_summary', 'complete_log']]
                if pipeline_files:
                    print("  ðŸ“‹ Pipeline Summary:")
                    for file_key in pipeline_files:
                        print(f"    - {file_key}: {saved_files[file_key]}")
            
            return True, analysis_id
            
        else:
            print("âŒ Local nutrition search analysis failed!")
            print(f"Error: {response.text}")
            return False, None
            
    except Exception as e:
        print(f"âŒ Error during local nutrition search analysis: {e}")
        return False, None

def test_pipeline_info_local():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Local Nutrition Search Pipeline Info ===")
    
    try:
        response = requests.get(f"{BASE_URL}/meal-analyses/pipeline-info")
        
        if response.status_code == 200:
            result = response.json()
            print("âœ… Pipeline info retrieved!")
            print(f"Pipeline ID: {result.get('pipeline_id')}")
            print(f"Version: {result.get('version')}")
            print(f"Nutrition Search Method: {result.get('nutrition_search_method')}")
            
            components = result.get('components', [])
            print(f"\nðŸ”§ Components ({len(components)}):")
            for i, comp in enumerate(components, 1):
                print(f"  {i}. {comp.get('component_name')} ({comp.get('component_type')})")
                print(f"     Executions: {comp.get('execution_count')}")
        else:
            print(f"âŒ Pipeline info failed: {response.status_code}")
            
    except Exception as e:
        print(f"âŒ Error getting pipeline info: {e}")

def test_nutrition_db_experiment_availability():
    """nutrition_db_experimentã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
    print("\n=== Nutrition DB Experiment Availability Test ===")
    
    try:
        # nutrition_db_experimentãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
        nutrition_db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "nutrition_db_experiment")
        
        print(f"ðŸ” Checking nutrition_db_experiment path: {nutrition_db_path}")
        
        if os.path.exists(nutrition_db_path):
            print("âœ… nutrition_db_experiment directory found")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªï¼ˆæ­£ã—ã„ãƒ‘ã‚¹ã«ä¿®æ­£ï¼‰
            db_files = [
                "nutrition_db/dish_db.json",
                "nutrition_db/ingredient_db.json", 
                "nutrition_db/branded_db.json",
                "nutrition_db/unified_nutrition_db.json"
            ]
            
            print("ðŸ“Š Database Files:")
            for db_file in db_files:
                full_path = os.path.join(nutrition_db_path, db_file)
                if os.path.exists(full_path):
                    try:
                        with open(full_path, 'r', encoding='utf-8') as f:
                            # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ä¸€éƒ¨ã ã‘èª­ã¿è¾¼ã¿
                            if os.path.getsize(full_path) > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                                f.seek(0)
                                first_chunk = f.read(1024)
                                if first_chunk.strip().startswith('['):
                                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‹ã‚‰æŽ¨å®šã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’è¨ˆç®—
                                    file_size_mb = os.path.getsize(full_path) / (1024 * 1024)
                                    estimated_items = int(file_size_mb * 1000)  # å¤§ã¾ã‹ãªæŽ¨å®š
                                    print(f"  âœ… {db_file}: ~{estimated_items} items (file size: {file_size_mb:.1f}MB)")
                                else:
                                    print(f"  âœ… {db_file}: Large file ({file_size_mb:.1f}MB)")
                            else:
                                data = json.load(f)
                                print(f"  âœ… {db_file}: {len(data)} items")
                    except Exception as e:
                        print(f"  âŒ {db_file}: Error reading - {e}")
                else:
                    print(f"  âŒ {db_file}: Not found")
            
            # æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
            print("ðŸ”§ Search Components:")
            
            search_service_path = os.path.join(nutrition_db_path, "search_service")
            if os.path.exists(search_service_path):
                print(f"  âœ… search_service directory found: {search_service_path}")
                
                # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                component_files = [
                    "nlp/query_preprocessor.py",
                    "api/search_handler.py", 
                    "api/query_builder.py"
                ]
                
                for comp_file in component_files:
                    full_path = os.path.join(search_service_path, comp_file)
                    if os.path.exists(full_path):
                        print(f"    âœ… {comp_file}")
                    else:
                        print(f"    âŒ {comp_file}: Not found")
            else:
                print(f"  âŒ search_service directory not found")
                
        else:
            print("âŒ nutrition_db_experiment directory not found")
            print("ðŸ’¡ Please ensure nutrition_db_experiment is in the same directory as this script")
            
    except Exception as e:
        print(f"âŒ Error checking nutrition_db_experiment: {e}")

def compare_search_methods():
    """ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã¨USDAæ¤œç´¢ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    print("\n=== Search Methods Comparison ===")
    print("ðŸ”¬ This would compare local search vs USDA API search")
    print("ðŸ“ TODO: Implement when both methods are available")

if __name__ == "__main__":
    print("Testing Local Nutrition Search Integration v2.0")
    print("=" * 70)
    
    # nutrition_db_experimentã®åˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
    test_nutrition_db_experiment_availability()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±
    test_pipeline_info_local()
    
    # ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ã‚’ä½¿ã£ãŸå®Œå…¨åˆ†æžã®ãƒ†ã‚¹ãƒˆ
    success, analysis_id = test_local_nutrition_search_complete_analysis()
    
    if success:
        print("\nðŸŽ‰ Local nutrition search integration test completed successfully!")
        print("ðŸš€ nutrition_db_experiment search system is working with the meal analysis pipeline!")
        print(f"ðŸ“‹ Analysis ID: {analysis_id}")
    else:
        print("\nðŸ’¥ Local nutrition search integration test failed!")
        print("ðŸ”§ Check the local search system setup and logs.")
        
    # æ¯”è¼ƒãƒ†ã‚¹ãƒˆï¼ˆå°†æ¥å®Ÿè£…äºˆå®šï¼‰
    compare_search_methods() 
```

============================================================

ðŸ“„ FILE: venv/lib/python3.9/site-packages/requests/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 5,072 bytes
æœ€çµ‚æ›´æ–°: 2025-05-27 15:05:29
å­˜åœ¨: âœ…

CONTENT:
```
#   __
#  /__)  _  _     _   _ _/   _
# / (   (- (/ (/ (- _)  /  _)
#          /

"""
Requests HTTP Library
~~~~~~~~~~~~~~~~~~~~~

Requests is an HTTP library, written in Python, for human beings.
Basic GET usage:

   >>> import requests
   >>> r = requests.get('https://www.python.org')
   >>> r.status_code
   200
   >>> b'Python is a programming language' in r.content
   True

... or POST:

   >>> payload = dict(key1='value1', key2='value2')
   >>> r = requests.post('https://httpbin.org/post', data=payload)
   >>> print(r.text)
   {
     ...
     "form": {
       "key1": "value1",
       "key2": "value2"
     },
     ...
   }

The other HTTP methods are supported - see `requests.api`. Full documentation
is at <https://requests.readthedocs.io>.

:copyright: (c) 2017 by Kenneth Reitz.
:license: Apache 2.0, see LICENSE for more details.
"""

import warnings

import urllib3

from .exceptions import RequestsDependencyWarning

try:
    from charset_normalizer import __version__ as charset_normalizer_version
except ImportError:
    charset_normalizer_version = None

try:
    from chardet import __version__ as chardet_version
except ImportError:
    chardet_version = None


def check_compatibility(urllib3_version, chardet_version, charset_normalizer_version):
    urllib3_version = urllib3_version.split(".")
    assert urllib3_version != ["dev"]  # Verify urllib3 isn't installed from git.

    # Sometimes, urllib3 only reports its version as 16.1.
    if len(urllib3_version) == 2:
        urllib3_version.append("0")

    # Check urllib3 for compatibility.
    major, minor, patch = urllib3_version  # noqa: F811
    major, minor, patch = int(major), int(minor), int(patch)
    # urllib3 >= 1.21.1
    assert major >= 1
    if major == 1:
        assert minor >= 21

    # Check charset_normalizer for compatibility.
    if chardet_version:
        major, minor, patch = chardet_version.split(".")[:3]
        major, minor, patch = int(major), int(minor), int(patch)
        # chardet_version >= 3.0.2, < 6.0.0
        assert (3, 0, 2) <= (major, minor, patch) < (6, 0, 0)
    elif charset_normalizer_version:
        major, minor, patch = charset_normalizer_version.split(".")[:3]
        major, minor, patch = int(major), int(minor), int(patch)
        # charset_normalizer >= 2.0.0 < 4.0.0
        assert (2, 0, 0) <= (major, minor, patch) < (4, 0, 0)
    else:
        warnings.warn(
            "Unable to find acceptable character detection dependency "
            "(chardet or charset_normalizer).",
            RequestsDependencyWarning,
        )


def _check_cryptography(cryptography_version):
    # cryptography < 1.3.4
    try:
        cryptography_version = list(map(int, cryptography_version.split(".")))
    except ValueError:
        return

    if cryptography_version < [1, 3, 4]:
        warning = "Old version of cryptography ({}) may cause slowdown.".format(
            cryptography_version
        )
        warnings.warn(warning, RequestsDependencyWarning)


# Check imported dependencies for compatibility.
try:
    check_compatibility(
        urllib3.__version__, chardet_version, charset_normalizer_version
    )
except (AssertionError, ValueError):
    warnings.warn(
        "urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported "
        "version!".format(
            urllib3.__version__, chardet_version, charset_normalizer_version
        ),
        RequestsDependencyWarning,
    )

# Attempt to enable urllib3's fallback for SNI support
# if the standard library doesn't support SNI or the
# 'ssl' library isn't available.
try:
    try:
        import ssl
    except ImportError:
        ssl = None

    if not getattr(ssl, "HAS_SNI", False):
        from urllib3.contrib import pyopenssl

        pyopenssl.inject_into_urllib3()

        # Check cryptography version
        from cryptography import __version__ as cryptography_version

        _check_cryptography(cryptography_version)
except ImportError:
    pass

# urllib3's DependencyWarnings should be silenced.
from urllib3.exceptions import DependencyWarning

warnings.simplefilter("ignore", DependencyWarning)

# Set default logging handler to avoid "No handler found" warnings.
import logging
from logging import NullHandler

from . import packages, utils
from .__version__ import (
    __author__,
    __author_email__,
    __build__,
    __cake__,
    __copyright__,
    __description__,
    __license__,
    __title__,
    __url__,
    __version__,
)
from .api import delete, get, head, options, patch, post, put, request
from .exceptions import (
    ConnectionError,
    ConnectTimeout,
    FileModeWarning,
    HTTPError,
    JSONDecodeError,
    ReadTimeout,
    RequestException,
    Timeout,
    TooManyRedirects,
    URLRequired,
)
from .models import PreparedRequest, Request, Response
from .sessions import Session, session
from .status_codes import codes

logging.getLogger(__name__).addHandler(NullHandler())

# FileModeWarnings go off per the default.
warnings.simplefilter("default", FileModeWarning, append=True)

```

============================================================

ðŸŒ ã‚µãƒ¼ãƒãƒ¼å´ãƒ•ã‚¡ã‚¤ãƒ«
============================================================

ðŸ“„ FILE: app_v2/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 54 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:46:48
å­˜åœ¨: âœ…

CONTENT:
```
# é£Ÿäº‹åˆ†æž API v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ 
```

============================================================

ðŸ“„ FILE: app_v2/api/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 41 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:54:57
å­˜åœ¨: âœ…

CONTENT:
```
# API v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ 
```

============================================================

ðŸ“„ FILE: app_v2/api/v1/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 19 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:55:05
å­˜åœ¨: âœ…

CONTENT:
```
# API v1 endpoints 
```

============================================================

ðŸ“„ FILE: app_v2/api/v1/endpoints/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 57 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:55:13
å­˜åœ¨: âœ…

CONTENT:
```
from . import meal_analysis

__all__ = ["meal_analysis"] 
```

============================================================

ðŸ“„ FILE: app_v2/api/v1/endpoints/meal_analysis.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,696 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:11:02
å­˜åœ¨: âœ…

CONTENT:
```
from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from typing import Optional
import logging

from ....pipeline import MealAnalysisPipeline

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/complete")
async def complete_meal_analysis(
    image: UploadFile = File(...),
    save_results: bool = Form(True),
    save_detailed_logs: bool = Form(True)
):
    """
    å®Œå…¨ãªé£Ÿäº‹åˆ†æžã‚’å®Ÿè¡Œï¼ˆv2.0 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆï¼‰
    
    - Phase 1: Gemini AIã«ã‚ˆã‚‹ç”»åƒåˆ†æž
    - USDA Query: é£Ÿæã®USDAãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ
    - Phase 2: è¨ˆç®—æˆ¦ç•¥æ±ºå®šã¨æ „é¤Šä¾¡ç²¾ç·»åŒ– (TODO)
    - Nutrition Calculation: æœ€çµ‚æ „é¤Šä¾¡è¨ˆç®— (TODO)
    
    Args:
        image: åˆ†æžå¯¾è±¡ã®é£Ÿäº‹ç”»åƒ
        save_results: çµæžœã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
        save_detailed_logs: è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
    
    Returns:
        å®Œå…¨ãªåˆ†æžçµæžœã¨æ „é¤Šä¾¡è¨ˆç®—ã€è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    
    try:
        # ç”»åƒã®æ¤œè¨¼
        if not image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”»åƒã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        image_data = await image.read()
        logger.info(f"Starting complete meal analysis pipeline v2.0 (detailed_logs: {save_detailed_logs})")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
        pipeline = MealAnalysisPipeline()
        result = await pipeline.execute_complete_analysis(
            image_bytes=image_data,
            image_mime_type=image.content_type,
            save_results=save_results,
            save_detailed_logs=save_detailed_logs
        )
        
        logger.info(f"Complete analysis pipeline v2.0 finished successfully")
        
        return JSONResponse(
            status_code=200,
            content=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Complete analysis failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Complete analysis failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "healthy", "version": "v2.0", "message": "é£Ÿäº‹åˆ†æžAPI v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ"}


@router.get("/pipeline-info")
async def get_pipeline_info():
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã®å–å¾—"""
    pipeline = MealAnalysisPipeline()
    return pipeline.get_pipeline_info() 
```

============================================================

ðŸ“„ FILE: app_v2/components/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 576 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:08:59
å­˜åœ¨: âœ…

CONTENT:
```
from .base import BaseComponent
from .phase1_component import Phase1Component
from .usda_query_component import USDAQueryComponent
from .local_nutrition_search_component import LocalNutritionSearchComponent
# TODO: Phase2Componentã¨NutritionCalculationComponentã‚’å®Ÿè£…
# from .phase2_component import Phase2Component
# from .nutrition_calc_component import NutritionCalculationComponent

__all__ = [
    "BaseComponent",
    "Phase1Component", 
    "USDAQueryComponent",
    "LocalNutritionSearchComponent",
    # "Phase2Component",
    # "NutritionCalculationComponent"
] 
```

============================================================

ðŸ“„ FILE: app_v2/components/base.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 6,824 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:08:38
å­˜åœ¨: âœ…

CONTENT:
```
from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Any, Optional
import logging
from datetime import datetime

# åž‹å¤‰æ•°ã®å®šç¾©
InputType = TypeVar('InputType')
OutputType = TypeVar('OutputType')


class BaseComponent(ABC, Generic[InputType, OutputType]):
    """
    é£Ÿäº‹åˆ†æžãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæŠ½è±¡ã‚¯ãƒ©ã‚¹
    
    å…¨ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã“ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã€process ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    """
    
    def __init__(self, component_name: str, logger: Optional[logging.Logger] = None):
        """
        ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        
        Args:
            component_name: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå
            logger: ãƒ­ã‚¬ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        self.component_name = component_name
        self.logger = logger or logging.getLogger(f"{__name__}.{component_name}")
        self.created_at = datetime.now()
        self.execution_count = 0
        self.current_execution_log = None  # è©³ç´°ãƒ­ã‚°
        
    @abstractmethod
    async def process(self, input_data: InputType) -> OutputType:
        """
        ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            OutputType: å‡¦ç†çµæžœ
            
        Raises:
            ComponentError: å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        pass
    
    async def execute(self, input_data: InputType, execution_log: Optional['DetailedExecutionLog'] = None) -> OutputType:
        """
        ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ­ã‚°è¨˜éŒ²ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            execution_log: è©³ç´°å®Ÿè¡Œãƒ­ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            OutputType: å‡¦ç†çµæžœ
        """
        self.execution_count += 1
        execution_id = f"{self.component_name}_{self.execution_count}"
        
        # è©³ç´°ãƒ­ã‚°ã®è¨­å®š
        if execution_log:
            self.current_execution_log = execution_log
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            self.current_execution_log.set_input(self._safe_serialize_input(input_data))
        
        self.logger.info(f"[{execution_id}] Starting {self.component_name} processing")
        
        try:
            start_time = datetime.now()
            result = await self.process(input_data)
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            self.logger.info(f"[{execution_id}] {self.component_name} completed in {processing_time:.2f}s")
            
            # è©³ç´°ãƒ­ã‚°ã«å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.set_output(self._safe_serialize_output(result))
                self.current_execution_log.finalize()
            
            return result
            
        except Exception as e:
            self.logger.error(f"[{execution_id}] {self.component_name} failed: {str(e)}", exc_info=True)
            
            # è©³ç´°ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.add_error(str(e))
                self.current_execution_log.finalize()
            
            raise ComponentError(f"{self.component_name} processing failed: {str(e)}") from e
        finally:
            self.current_execution_log = None
    
    def log_prompt(self, prompt_name: str, prompt_content: str, variables: dict = None):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_prompt(prompt_name, prompt_content, variables)
    
    def log_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æŽ¨è«–ç†ç”±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_reasoning(decision_point, reason, confidence)
    
    def log_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_processing_detail(detail_key, detail_value)
    
    def log_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_confidence_score(metric_name, score)
    
    def log_warning(self, warning: str):
        """è­¦å‘Šã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_warning(warning)
    
    def _safe_serialize_input(self, input_data: InputType) -> dict:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(input_data, 'model_dump'):
                return input_data.model_dump()
            elif hasattr(input_data, '__dict__'):
                return input_data.__dict__
            else:
                return {"data": str(input_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def _safe_serialize_output(self, output_data: OutputType) -> dict:
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(output_data, 'model_dump'):
                return output_data.model_dump()
            elif hasattr(output_data, '__dict__'):
                return output_data.__dict__
            else:
                return {"data": str(output_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def get_component_info(self) -> dict:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
        return {
            "component_name": self.component_name,
            "created_at": self.created_at.isoformat(),
            "execution_count": self.execution_count,
            "component_type": self.__class__.__name__
        }


class ComponentError(Exception):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, component_name: str = None, original_error: Exception = None):
        super().__init__(message)
        self.component_name = component_name
        self.original_error = original_error
        self.timestamp = datetime.now()
    
    def to_dict(self) -> dict:
        """ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¾žæ›¸å½¢å¼ã§å–å¾—"""
        return {
            "error_message": str(self),
            "component_name": self.component_name,
            "timestamp": self.timestamp.isoformat(),
            "original_error": str(self.original_error) if self.original_error else None
        } 
```

============================================================

ðŸ“„ FILE: app_v2/components/local_nutrition_search_component.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 25,544 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 13:03:15
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Local Nutrition Search Component

USDA database queryã‚’ nutrition_db_experiment ã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã«ç½®ãæ›ãˆã‚‹
"""

import os
import sys
import json
import asyncio
from typing import Optional, List, Dict, Any
from pathlib import Path

from .base import BaseComponent
from ..models.usda_models import USDAQueryInput, USDAQueryOutput
from ..models.nutrition_search_models import (
    NutritionQueryInput, NutritionQueryOutput, NutritionMatch, NutritionNutrient,
    convert_usda_query_input_to_nutrition, convert_nutrition_to_usda_query_output
)
from ..config import get_settings

# nutrition_db_experimentã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
NUTRITION_DB_EXPERIMENT_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
    "nutrition_db_experiment"
)
sys.path.append(NUTRITION_DB_EXPERIMENT_PATH)

class LocalNutritionSearchComponent(BaseComponent[USDAQueryInput, USDAQueryOutput]):
    """
    ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    
    nutrition_db_experimentã§å®Ÿè£…ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦é£Ÿæåã‚’æ¤œç´¢ã—ã€
    USDAQueryComponentã¨äº’æ›æ€§ã®ã‚ã‚‹çµæžœã‚’è¿”ã—ã¾ã™ã€‚
    
    å†…éƒ¨çš„ã«ã¯æ±Žç”¨çš„ãªNutritionQueryãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã€
    å¤–éƒ¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã¯USDAQueryãƒ¢ãƒ‡ãƒ«ã¨ã®äº’æ›æ€§ã‚’ä¿æŒã—ã¾ã™ã€‚
    """
    
    def __init__(self):
        super().__init__("LocalNutritionSearchComponent")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
        self._initialize_local_search_system()
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆæ­£ã—ã„ãƒ‘ã‚¹ã«ä¿®æ­£ï¼‰
        self.local_db_paths = {
            "dish_db": os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "dish_db.json"),
            "ingredient_db": os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "ingredient_db.json"),
            "branded_db": os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "branded_db.json"),
            "unified_db": os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "nutrition_db", "unified_nutrition_db.json")
        }
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
        self.local_databases = self._load_local_databases()
    
    def _initialize_local_search_system(self):
        """ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        try:
            # nutrition_db_experimentã®æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            search_service_path = os.path.join(NUTRITION_DB_EXPERIMENT_PATH, "search_service")
            sys.path.append(search_service_path)
            
            from nlp.query_preprocessor import FoodQueryPreprocessor
            from api.search_handler import NutritionSearchHandler, SearchRequest
            
            self.query_preprocessor = FoodQueryPreprocessor()
            self.search_handler = NutritionSearchHandler()
            
            self.logger.info("Local nutrition search system initialized successfully")
            
        except ImportError as e:
            self.logger.error(f"Failed to import local search components: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒžãƒƒãƒãƒ³ã‚°
            self.query_preprocessor = None
            self.search_handler = None
            self.logger.warning("Using fallback simple string matching for local search")
        except Exception as e:
            self.logger.error(f"Error initializing local search system: {e}")
            self.query_preprocessor = None
            self.search_handler = None
    
    def _load_local_databases(self) -> Dict[str, List[Dict[str, Any]]]:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        databases = {}
        
        for db_name, db_path in self.local_db_paths.items():
            try:
                if os.path.exists(db_path):
                    with open(db_path, 'r', encoding='utf-8') as f:
                        databases[db_name] = json.load(f)
                    self.logger.info(f"Loaded {db_name}: {len(databases[db_name])} items")
                else:
                    self.logger.warning(f"Local database file not found: {db_path}")
                    databases[db_name] = []
            except Exception as e:
                self.logger.error(f"Error loading {db_name}: {e}")
                databases[db_name] = []
        
        total_items = sum(len(db) for db in databases.values())
        self.logger.info(f"Total local database items loaded: {total_items}")
        
        return databases
    
    async def process(self, input_data: USDAQueryInput) -> USDAQueryOutput:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®ä¸»å‡¦ç†ï¼ˆUSDAäº’æ›ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰
        
        Args:
            input_data: USDAQueryInput
            
        Returns:
            USDAQueryOutput: USDAäº’æ›ã®ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢çµæžœ
        """
        # USDAQueryInputã‚’æ±Žç”¨NutritionQueryInputã«å¤‰æ›
        nutrition_input = convert_usda_query_input_to_nutrition(input_data)
        nutrition_input.preferred_source = "local_database"
        
        # å†…éƒ¨çš„ãªæ±Žç”¨æ¤œç´¢å‡¦ç†ã‚’å®Ÿè¡Œ
        nutrition_result = await self._process_nutrition_search(nutrition_input)
        
        # çµæžœã‚’USDAQueryOutputå½¢å¼ã«å¤‰æ›ã—ã¦è¿”ã™
        return convert_nutrition_to_usda_query_output(nutrition_result)
    
    async def _process_nutrition_search(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        æ±Žç”¨æ „é¤Šæ¤œç´¢ã®ä¸»å‡¦ç†
        
        Args:
            input_data: NutritionQueryInput
            
        Returns:
            NutritionQueryOutput: æ±Žç”¨æ¤œç´¢çµæžœ
        """
        self.logger.info(f"Starting local nutrition search for {len(input_data.get_all_search_terms())} terms")
        
        search_terms = input_data.get_all_search_terms()
        
        # æ¤œç´¢å¯¾è±¡ã®è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("ingredient_names", input_data.ingredient_names)
        self.log_processing_detail("dish_names", input_data.dish_names)
        self.log_processing_detail("total_search_terms", len(search_terms))
        self.log_processing_detail("search_method", "local_nutrition_database")
        self.log_processing_detail("preferred_source", input_data.preferred_source)
        
        matches = {}
        warnings = []
        errors = []
        
        successful_matches = 0
        total_searches = len(search_terms)
        
        # å„æ¤œç´¢èªžå½™ã«ã¤ã„ã¦ç…§åˆã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Searching local database for: {search_term}")
            
            # æ¤œç´¢é–‹å§‹ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_term", search_term)
            self.log_processing_detail(f"search_{search_index}_start", f"Starting local search for '{search_term}'")
            
            try:
                # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ã®å®Ÿè¡Œ
                if self.search_handler and self.query_preprocessor:
                    # é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
                    match_result = await self._advanced_local_search(search_term, search_index, input_data)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒžãƒƒãƒãƒ³ã‚°
                    match_result = await self._simple_local_search(search_term, search_index, input_data)
                
                if match_result:
                    matches[search_term] = match_result
                    successful_matches += 1
                    self.logger.debug(f"Found local match for '{search_term}': ID {match_result.id}")
                else:
                    self.log_reasoning(
                        f"no_match_{search_index}",
                        f"No local database match found for '{search_term}' - may not exist in local nutrition database"
                    )
                    self.logger.warning(f"No local match found for: {search_term}")
                    warnings.append(f"No local match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"Local search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"search_error_{search_index}",
                    f"Local database search error for '{search_term}': {str(e)}"
                )
        
        # æ¤œç´¢ã‚µãƒžãƒªãƒ¼ã‚’ä½œæˆï¼ˆæ±Žç”¨å½¢å¼ï¼‰
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0,
            "search_method": "local_nutrition_database",
            "database_source": "nutrition_db_experiment",
            "preferred_source": input_data.preferred_source,
            "total_database_items": sum(len(db) for db in self.local_databases.values())
        }
        
        # å…¨ä½“çš„ãªæ¤œç´¢æˆåŠŸçŽ‡ã‚’ãƒ­ã‚°
        overall_success_rate = successful_matches / total_searches if total_searches > 0 else 0
        self.log_processing_detail("search_summary", search_summary)
        
        # æ¤œç´¢å“è³ªã®è©•ä¾¡ã‚’ãƒ­ã‚°
        if overall_success_rate >= 0.8:
            self.log_reasoning("search_quality", "Excellent local search results with high match rate")
        elif overall_success_rate >= 0.6:
            self.log_reasoning("search_quality", "Good local search results with acceptable match rate")
        elif overall_success_rate >= 0.4:
            self.log_reasoning("search_quality", "Moderate local search results, some items may need manual review")
        else:
            self.log_reasoning("search_quality", "Poor local search results, many items not found in local database")
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"Local nutrition search completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%})")
        
        return result
    
    async def _advanced_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        nutrition_db_experimentã®é«˜åº¦ãªæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªžå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        try:
            from api.search_handler import SearchRequest
            
            # æ¤œç´¢ã‚¿ã‚¤ãƒ—ã®æ±ºå®šï¼ˆæ–™ç†ã‹é£Ÿæã‹ã®æŽ¨å®šï¼‰
            db_type_filter = None  # å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢
            
            # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ã¨ã—ã¦å„ªå…ˆæ¤œç´¢
            if search_term in input_data.dish_names:
                db_type_filter = "dish"
                self.log_processing_detail(f"search_{search_index}_type", "dish")
            elif search_term in input_data.ingredient_names:
                db_type_filter = "ingredient"
                self.log_processing_detail(f"search_{search_index}_type", "ingredient")
            
            # æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
            request = SearchRequest(
                query=search_term,
                db_type_filter=db_type_filter,
                size=5  # ä¸Šä½5ä»¶ã‚’å–å¾—
            )
            
            # æ¤œç´¢å®Ÿè¡Œ
            response = self.search_handler.search(request)
            
            # æ¤œç´¢çµæžœã®è©³ç´°ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_results_count", response.total_hits)
            self.log_processing_detail(f"search_{search_index}_processing_time_ms", response.took_ms)
            self.log_processing_detail(f"search_{search_index}_processed_query", response.query_info.get('processed_query'))
            
            if response.results:
                # nutrition_db_experimentã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ãŸå ´åˆã¯ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                best_result = response.results[0]
                
                # æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆIDãŒ123456ã®å ´åˆã¯æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼‰
                if best_result.get('id') == 123456:
                    self.logger.warning(f"nutrition_db_experiment returned mock data for '{search_term}', falling back to direct database search")
                    return await self._direct_database_search(search_term, search_index, input_data)
                
                # ãƒžãƒƒãƒé¸æŠžã®æŽ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_result['search_name']}' (ID: {best_result['id']}) for search term '{search_term}' based on local search algorithm (score: {best_result.get('_score', 'N/A')})"
                )
                
                # è©³ç´°ãªãƒžãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_selected_id", best_result['id'])
                self.log_processing_detail(f"search_{search_index}_selected_name", best_result['search_name'])
                self.log_processing_detail(f"search_{search_index}_db_type", best_result['db_type'])
                self.log_processing_detail(f"search_{search_index}_score", best_result.get('_score'))
                
                # NutritionMatchå½¢å¼ã«å¤‰æ›
                return self._convert_to_nutrition_match(best_result, search_term)
            
            # çµæžœãŒãªã„å ´åˆã¯ç›´æŽ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._direct_database_search(search_term, search_index, input_data)
            
        except Exception as e:
            self.logger.error(f"Advanced local search failed for '{search_term}': {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚ç›´æŽ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._direct_database_search(search_term, search_index, input_data)
    
    async def _direct_database_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æŽ¥æ¤œç´¢
        
        Args:
            search_term: æ¤œç´¢èªžå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        try:
            self.log_processing_detail(f"search_{search_index}_method", "direct_database_search")
            
            search_term_lower = search_term.lower()
            best_match = None
            best_score = 0
            best_db_source = None
            
            # æ¤œç´¢å„ªå…ˆé †ä½ã®æ±ºå®š
            search_order = []
            
            # dish_namesã«å«ã¾ã‚Œã‚‹å ´åˆã¯æ–™ç†ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’å„ªå…ˆ
            if search_term in input_data.dish_names:
                search_order = ["dish_db", "unified_db", "ingredient_db", "branded_db"]
            elif search_term in input_data.ingredient_names:
                search_order = ["ingredient_db", "unified_db", "dish_db", "branded_db"]
            else:
                search_order = ["unified_db", "dish_db", "ingredient_db", "branded_db"]
            
            # å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ï¼ˆå„ªå…ˆé †ä½é †ï¼‰
            for db_name in search_order:
                if db_name not in self.local_databases:
                    continue
                    
                database = self.local_databases[db_name]
                
                for item in database:
                    # search_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§æ¤œç´¢
                    if 'search_name' not in item:
                        continue
                        
                    item_name = item['search_name'].lower()
                    score = 0
                    
                    # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                    if search_term_lower == item_name:
                        score = 1.0  # å®Œå…¨ä¸€è‡´
                    elif search_term_lower in item_name:
                        # éƒ¨åˆ†ä¸€è‡´ï¼ˆèªžé †è€ƒæ…®ï¼‰
                        if item_name.startswith(search_term_lower):
                            score = 0.9  # å‰æ–¹ä¸€è‡´
                        elif item_name.endswith(search_term_lower):
                            score = 0.8  # å¾Œæ–¹ä¸€è‡´
                        else:
                            score = 0.7  # ä¸­é–“ä¸€è‡´
                    elif item_name in search_term_lower:
                        score = 0.6  # é€†éƒ¨åˆ†ä¸€è‡´
                    else:
                        # å˜èªžãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                        search_words = search_term_lower.split()
                        item_words = item_name.split()
                        
                        common_words = set(search_words) & set(item_words)
                        if common_words:
                            score = len(common_words) / max(len(search_words), len(item_words)) * 0.5
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å„ªå…ˆåº¦ã«ã‚ˆã‚‹ãƒœãƒ¼ãƒŠã‚¹
                    db_bonus = 1.0
                    if db_name == search_order[0]:
                        db_bonus = 1.2  # ç¬¬ä¸€å„ªå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                    elif db_name == search_order[1]:
                        db_bonus = 1.1  # ç¬¬äºŒå„ªå…ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
                    
                    final_score = score * db_bonus
                    
                    if final_score > best_score:
                        best_score = final_score
                        best_match = item.copy()
                        best_db_source = db_name
            
            if best_match and best_score > 0.1:  # æœ€ä½Žé–¾å€¤
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
                best_match['_db_source'] = best_db_source
                best_match['_match_score'] = best_score
                
                self.log_reasoning(
                    f"match_selection_{search_index}",
                    f"Selected local item '{best_match['search_name']}' (ID: {best_match.get('id', 'N/A')}) for search term '{search_term}' from {best_db_source} using direct database search (score: {best_score:.3f})"
                )
                
                # è©³ç´°ãªãƒžãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_selected_id", best_match.get('id', 'N/A'))
                self.log_processing_detail(f"search_{search_index}_selected_name", best_match['search_name'])
                self.log_processing_detail(f"search_{search_index}_db_source", best_db_source)
                self.log_processing_detail(f"search_{search_index}_match_score", best_score)
                
                return self._convert_to_nutrition_match(best_match, search_term)
            
            return None
            
        except Exception as e:
            self.logger.error(f"Direct database search failed for '{search_term}': {e}")
            return None
    
    async def _simple_local_search(self, search_term: str, search_index: int, input_data: NutritionQueryInput) -> Optional[NutritionMatch]:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ãƒžãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½¿ç”¨ï¼‰
        
        Args:
            search_term: æ¤œç´¢èªžå½™
            search_index: æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ­ã‚°ç”¨ï¼‰
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—åˆ¤å®šç”¨ï¼‰
            
        Returns:
            NutritionMatch ã¾ãŸã¯ None
        """
        # é«˜åº¦æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã€ç›´æŽ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨
        return await self._direct_database_search(search_term, search_index, input_data)
    
    def _convert_to_nutrition_match(self, local_item: Dict[str, Any], search_term: str) -> NutritionMatch:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã‚’NutritionMatchå½¢å¼ã«å¤‰æ›
        
        Args:
            local_item: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¢ã‚¤ãƒ†ãƒ 
            search_term: å…ƒã®æ¤œç´¢èªžå½™
            
        Returns:
            NutritionMatch: å¤‰æ›ã•ã‚ŒãŸãƒžãƒƒãƒçµæžœ
        """
        # æ „é¤Šç´ æƒ…å ±ã®å¤‰æ›
        nutrients = []
        if 'nutrition' in local_item and local_item['nutrition']:
            nutrition_data = local_item['nutrition']
            
            # ä¸»è¦æ „é¤Šç´ ã®ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«DBã®å½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            nutrient_mapping = {
                'calories_kcal': ('Energy', '208', 'kcal'),
                'calories': ('Energy', '208', 'kcal'),  # åˆ¥åå¯¾å¿œ
                'protein_g': ('Protein', '203', 'g'),
                'protein': ('Protein', '203', 'g'),  # åˆ¥åå¯¾å¿œ
                'fat_g': ('Total lipid (fat)', '204', 'g'),
                'fat': ('Total lipid (fat)', '204', 'g'),  # åˆ¥åå¯¾å¿œ
                'carbohydrates_g': ('Carbohydrate, by difference', '205', 'g'),
                'carbs': ('Carbohydrate, by difference', '205', 'g'),  # åˆ¥åå¯¾å¿œ
                'carbohydrates': ('Carbohydrate, by difference', '205', 'g'),  # åˆ¥åå¯¾å¿œ
                'fiber_g': ('Fiber, total dietary', '291', 'g'),
                'fiber': ('Fiber, total dietary', '291', 'g'),  # åˆ¥åå¯¾å¿œ
                'sugars_g': ('Sugars, total', '269', 'g'),
                'sugars': ('Sugars, total', '269', 'g'),  # åˆ¥åå¯¾å¿œ
                'sodium_mg': ('Sodium, Na', '307', 'mg'),
                'sodium': ('Sodium, Na', '307', 'mg')  # åˆ¥åå¯¾å¿œ
            }
            
            for local_key, (usda_name, nutrient_number, unit) in nutrient_mapping.items():
                if local_key in nutrition_data and nutrition_data[local_key] is not None:
                    try:
                        amount = float(nutrition_data[local_key])
                        nutrients.append(NutritionNutrient(
                            name=usda_name,
                            amount=amount,
                            unit_name=unit,
                            nutrient_id=None,  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã«ã¯IDãŒãªã„
                            nutrient_number=nutrient_number
                        ))
                    except (ValueError, TypeError):
                        # æ•°å€¤ã«å¤‰æ›ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        continue
        
        # IDã®å–å¾—ï¼ˆæ§˜ã€…ãªå½¢å¼ã«å¯¾å¿œï¼‰
        item_id = local_item.get('id') or local_item.get('fdc_id') or local_item.get('_id') or 0
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æ±ºå®š
        data_type = "Local_Unknown"
        if 'db_type' in local_item:
            data_type = f"Local_{local_item['db_type'].title()}"
        elif '_db_source' in local_item:
            db_source = local_item['_db_source'].replace('_db', '')
            data_type = f"Local_{db_source.title()}"
        
        # èª¬æ˜Žã®å–å¾—ï¼ˆæ§˜ã€…ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã«å¯¾å¿œï¼‰
        description = (
            local_item.get('search_name') or 
            local_item.get('description') or 
            local_item.get('name') or 
            search_term
        )
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ï¼ˆbranded_dbã®å ´åˆï¼‰
        brand_owner = local_item.get('brand_owner') or local_item.get('brand_name')
        
        # é£Ÿæãƒªã‚¹ãƒˆï¼ˆdish_dbã®å ´åˆï¼‰
        ingredients_text = None
        if 'ingredients' in local_item:
            if isinstance(local_item['ingredients'], list):
                ingredients_text = ', '.join(local_item['ingredients'])
            elif isinstance(local_item['ingredients'], str):
                ingredients_text = local_item['ingredients']
        
        # ãƒžãƒƒãƒã‚¹ã‚³ã‚¢
        score = local_item.get('_match_score') or local_item.get('_score') or 1.0
        
        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        original_data = {
            "source": "local_nutrition_database",
            "original_data": local_item,
            "search_term": search_term,
            "db_source": local_item.get('_db_source', 'unknown'),
            "match_score": score
        }
        
        # NutritionMatchã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
        return NutritionMatch(
            id=item_id,
            description=description,
            data_type=data_type,
            source="local_database",
            brand_owner=brand_owner,
            ingredients_text=ingredients_text,
            nutrients=nutrients,
            score=score,
            original_data=original_data
        ) 
```

============================================================

ðŸ“„ FILE: app_v2/components/phase1_component.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 5,285 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 15:28:30
å­˜åœ¨: âœ…

CONTENT:
```
import json
from typing import Optional

from .base import BaseComponent
from ..models.phase1_models import Phase1Input, Phase1Output, Dish, Ingredient
from ..services.gemini_service import GeminiService
from ..config import get_settings
from ..config.prompts import Phase1Prompts


class Phase1Component(BaseComponent[Phase1Input, Phase1Output]):
    """
    Phase1: ç”»åƒåˆ†æžã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
    
    Gemini AIã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æžã—ã€USDAæ¤œç´¢ã«é©ã—ãŸæ–™ç†ã¨é£Ÿæåã‚’è­˜åˆ¥ã—ã¾ã™ã€‚
    """
    
    def __init__(self, gemini_service: Optional[GeminiService] = None):
        super().__init__("Phase1Component")
        
        # GeminiServiceã®åˆæœŸåŒ–
        if gemini_service is None:
            settings = get_settings()
            self.gemini_service = GeminiService(
                project_id=settings.GEMINI_PROJECT_ID,
                location=settings.GEMINI_LOCATION,
                model_name=settings.GEMINI_MODEL_NAME
            )
        else:
            self.gemini_service = gemini_service
    
    async def process(self, input_data: Phase1Input) -> Phase1Output:
        """
        Phase1ã®ä¸»å‡¦ç†: ç”»åƒåˆ†æžï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
        
        Args:
            input_data: Phase1Input (image_bytes, image_mime_type, optional_text)
            
        Returns:
            Phase1Output: åˆ†æžçµæžœï¼ˆæ–™ç†åãƒ»é£Ÿæåã®ã¿ï¼‰
        """
        self.logger.info(f"Starting Phase1 image analysis for USDA query generation")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã¨è¨˜éŒ²
        system_prompt = Phase1Prompts.get_system_prompt()
        user_prompt = Phase1Prompts.get_user_prompt(input_data.optional_text)
        
        self.log_prompt("system_prompt", system_prompt)
        self.log_prompt("user_prompt", user_prompt, {
            "optional_text": input_data.optional_text,
            "image_mime_type": input_data.image_mime_type
        })
        
        # ç”»åƒæƒ…å ±ã®ãƒ­ã‚°è¨˜éŒ²
        self.log_processing_detail("image_size_bytes", len(input_data.image_bytes))
        self.log_processing_detail("image_mime_type", input_data.image_mime_type)
        
        try:
            # Gemini AIã«ã‚ˆã‚‹ç”»åƒåˆ†æž
            self.log_processing_detail("gemini_api_call_start", "Calling Gemini API for image analysis")
            
            gemini_result = await self.gemini_service.analyze_phase1(
                image_bytes=input_data.image_bytes,
                image_mime_type=input_data.image_mime_type,
                optional_text=input_data.optional_text
            )
            
            self.log_processing_detail("gemini_raw_response", gemini_result)
            
            # çµæžœã‚’Pydanticãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›
            dishes = []
            for dish_index, dish_data in enumerate(gemini_result.get("dishes", [])):
                ingredients = []
                for ingredient_index, ingredient_data in enumerate(dish_data.get("ingredients", [])):
                    ingredient = Ingredient(
                        ingredient_name=ingredient_data["ingredient_name"]
                    )
                    ingredients.append(ingredient)
                    
                    # é£Ÿæè­˜åˆ¥ã®æŽ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                    self.log_reasoning(
                        f"ingredient_identification_dish{dish_index}_ingredient{ingredient_index}",
                        f"Identified ingredient '{ingredient_data['ingredient_name']}' for USDA search based on visual analysis"
                    )
                
                dish = Dish(
                    dish_name=dish_data["dish_name"],
                    ingredients=ingredients
                )
                dishes.append(dish)
                
                # æ–™ç†è­˜åˆ¥ã®æŽ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"dish_identification_{dish_index}",
                    f"Identified dish as '{dish_data['dish_name']}' for USDA search based on visual characteristics"
                )
            
            # åˆ†æžçµ±è¨ˆã®è¨˜éŒ²
            self.log_processing_detail("detected_dishes_count", len(dishes))
            self.log_processing_detail("total_ingredients_count", sum(len(dish.ingredients) for dish in dishes))
            
            # USDAæ¤œç´¢é©åˆæ€§ãƒã‚§ãƒƒã‚¯
            search_terms = []
            for dish in dishes:
                search_terms.append(dish.dish_name)
                for ingredient in dish.ingredients:
                    search_terms.append(ingredient.ingredient_name)
            
            self.log_processing_detail("usda_search_terms", search_terms)
            self.log_reasoning(
                "usda_search_preparation",
                f"Generated {len(search_terms)} search terms for USDA database queries"
            )
            
            result = Phase1Output(
                dishes=dishes,
                warnings=[]
            )
            
            self.logger.info(f"Phase1 completed: identified {len(dishes)} dishes with {len(search_terms)} total search terms")
            return result
            
        except Exception as e:
            self.logger.error(f"Phase1 processing failed: {str(e)}")
            raise 
```

============================================================

ðŸ“„ FILE: app_v2/components/usda_query_component.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 7,769 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 15:29:33
å­˜åœ¨: âœ…

CONTENT:
```
from typing import Optional, List, Dict

from .base import BaseComponent
from ..models.usda_models import USDAQueryInput, USDAQueryOutput, USDAMatch, USDANutrient
from ..services.usda_service import USDAService
from ..config import get_settings


class USDAQueryComponent(BaseComponent[USDAQueryInput, USDAQueryOutput]):
    """
    USDAç…§åˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    
    é£Ÿæåã‚’USDAãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§æ¤œç´¢ã—ã€æœ€é©ãªãƒžãƒƒãƒã‚’è¦‹ã¤ã‘ã¾ã™ã€‚
    """
    
    def __init__(self, usda_service: Optional[USDAService] = None):
        super().__init__("USDAQueryComponent")
        
        # USDAServiceã®åˆæœŸåŒ–
        if usda_service is None:
            settings = get_settings()
            self.usda_service = USDAService()
        else:
            self.usda_service = usda_service
    
    async def process(self, input_data: USDAQueryInput) -> USDAQueryOutput:
        """
        USDAç…§åˆã®ä¸»å‡¦ç†
        
        Args:
            input_data: USDAQueryInput (ingredient_names, dish_names)
            
        Returns:
            USDAQueryOutput: ç…§åˆçµæžœ
        """
        self.logger.info(f"Starting USDA query for {len(input_data.get_all_search_terms())} terms")
        
        search_terms = input_data.get_all_search_terms()
        
        # æ¤œç´¢å¯¾è±¡ã®è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        self.log_processing_detail("search_terms", search_terms)
        self.log_processing_detail("ingredient_names", input_data.ingredient_names)
        self.log_processing_detail("dish_names", input_data.dish_names)
        self.log_processing_detail("total_search_terms", len(search_terms))
        
        matches = {}
        warnings = []
        errors = []
        
        successful_matches = 0
        total_searches = len(search_terms)
        
        # å„æ¤œç´¢èªžå½™ã«ã¤ã„ã¦ç…§åˆã‚’å®Ÿè¡Œ
        for search_index, search_term in enumerate(search_terms):
            self.logger.debug(f"Searching USDA for: {search_term}")
            
            # æ¤œç´¢é–‹å§‹ã‚’ãƒ­ã‚°
            self.log_processing_detail(f"search_{search_index}_term", search_term)
            self.log_processing_detail(f"search_{search_index}_start", f"Starting USDA search for '{search_term}'")
            
            try:
                # USDAã‚µãƒ¼ãƒ“ã‚¹ã§æ¤œç´¢
                usda_results = await self.usda_service.search_foods(
                    query=search_term,
                    data_types=["Foundation", "SR Legacy", "FNDDS", "Branded"],
                    page_size=5
                )
                
                # æ¤œç´¢çµæžœã®è©³ç´°ã‚’ãƒ­ã‚°
                self.log_processing_detail(f"search_{search_index}_results_count", len(usda_results))
                
                if usda_results:
                    # æœ€ã‚‚é©åˆ‡ãªãƒžãƒƒãƒã‚’é¸æŠž
                    best_match = usda_results[0]
                    
                    # ãƒžãƒƒãƒé¸æŠžã®æŽ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                    self.log_reasoning(
                        f"match_selection_{search_index}",
                        f"Selected USDA item '{best_match.description}' (FDC: {best_match.fdc_id}) for search term '{search_term}' based on description similarity and data type '{best_match.data_type}'"
                    )
                    
                    # è©³ç´°ãªãƒžãƒƒãƒæƒ…å ±ã‚’ãƒ­ã‚°
                    self.log_processing_detail(f"search_{search_index}_selected_fdc_id", best_match.fdc_id)
                    self.log_processing_detail(f"search_{search_index}_selected_description", best_match.description)
                    self.log_processing_detail(f"search_{search_index}_data_type", best_match.data_type)
                    self.log_processing_detail(f"search_{search_index}_nutrients_count", len(best_match.food_nutrients))
                    
                    # USDANutrientã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                    nutrients = []
                    for nutrient in best_match.food_nutrients:
                        nutrients.append(USDANutrient(
                            name=nutrient.name,
                            amount=nutrient.amount,
                            unit_name=nutrient.unit_name,
                            nutrient_id=nutrient.nutrient_id,
                            nutrient_number=nutrient.nutrient_number
                        ))
                    
                    # USDAMatchã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                    match = USDAMatch(
                        fdc_id=best_match.fdc_id,
                        description=best_match.description,
                        data_type=best_match.data_type,
                        brand_owner=best_match.brand_owner,
                        ingredients_text=best_match.ingredients_text,
                        food_nutrients=nutrients,
                        score=best_match.score,
                        original_usda_data=best_match.original_data
                    )
                    
                    matches[search_term] = match
                    successful_matches += 1
                    
                    self.logger.debug(f"Found match for '{search_term}': FDC ID {best_match.fdc_id}")
                    
                else:
                    # ãƒžãƒƒãƒã—ãªã‹ã£ãŸç†ç”±ã‚’ãƒ­ã‚°
                    self.log_reasoning(
                        f"no_match_{search_index}",
                        f"No USDA match found for '{search_term}' - may be too specific, contain typos, or be a regional/non-standard food name"
                    )
                    self.logger.warning(f"No USDA match found for: {search_term}")
                    warnings.append(f"No USDA match found for: {search_term}")
                    
            except Exception as e:
                error_msg = f"USDA search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                
                # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ãƒ­ã‚°
                self.log_reasoning(
                    f"search_error_{search_index}",
                    f"USDA API error for '{search_term}': {str(e)}"
                )
        
        # æ¤œç´¢ã‚µãƒžãƒªãƒ¼ã‚’ä½œæˆ
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round((successful_matches / total_searches) * 100, 1) if total_searches > 0 else 0
        }
        
        # å…¨ä½“çš„ãªæ¤œç´¢æˆåŠŸçŽ‡ã‚’ãƒ­ã‚°
        overall_success_rate = successful_matches / total_searches if total_searches > 0 else 0
        self.log_processing_detail("search_summary", search_summary)
        
        # æ¤œç´¢å“è³ªã®è©•ä¾¡ã‚’ãƒ­ã‚°
        if overall_success_rate >= 0.8:
            self.log_reasoning("search_quality", "Excellent search results with high match rate")
        elif overall_success_rate >= 0.6:
            self.log_reasoning("search_quality", "Good search results with acceptable match rate")
        elif overall_success_rate >= 0.4:
            self.log_reasoning("search_quality", "Moderate search results, some items may need manual review")
        else:
            self.log_reasoning("search_quality", "Poor search results, many items not found in USDA database")
        
        result = USDAQueryOutput(
            matches=matches,
            search_summary=search_summary,
            warnings=warnings if warnings else None,
            errors=errors if errors else None
        )
        
        self.logger.info(f"USDA query completed: {successful_matches}/{total_searches} matches ({result.get_match_rate():.1%})")
        
        return result 
```

============================================================

ðŸ“„ FILE: app_v2/config/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 85 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:46:54
å­˜åœ¨: âœ…

CONTENT:
```
from .settings import Settings, get_settings

__all__ = ["Settings", "get_settings"] 
```

============================================================

ðŸ“„ FILE: app_v2/config/prompts/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 130 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:47:34
å­˜åœ¨: âœ…

CONTENT:
```
from .phase1_prompts import Phase1Prompts
from .phase2_prompts import Phase2Prompts

__all__ = ["Phase1Prompts", "Phase2Prompts"] 
```

============================================================

ðŸ“„ FILE: app_v2/config/prompts/phase1_prompts.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 1,854 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:44:16
å­˜åœ¨: âœ…

CONTENT:
```
class Phase1Prompts:
    """Phase1ï¼ˆç”»åƒåˆ†æžï¼‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰"""
    
    SYSTEM_PROMPT = """You are an experienced culinary analyst specialized in identifying dishes and ingredients for USDA database searches. Your task is to analyze meal images and provide clear, searchable names for dishes and ingredients in JSON format.

IMPORTANT: You MUST provide ALL responses in English only. This includes dish names, ingredient names, and any other text fields.

Please note the following:
1. Focus on accurate identification of dishes and ingredients, not quantities or weights.
2. Use clear, searchable names that would likely be found in the USDA food database.
3. Identify all dishes present in the image and their key ingredients.
4. There may be multiple dishes in a single image, so provide information about each dish and its ingredients separately.
5. Your output will be used for USDA database searches, so use standard, common food names.
6. Strictly follow the provided JSON schema in your response.
7. ALL text must be in English (dish names, ingredient names, etc.).
8. Do NOT include quantities, weights, portion sizes, or dish types - focus only on identification."""

    USER_PROMPT_TEMPLATE = "Please analyze this meal image and identify the dishes and their ingredients. Focus on providing clear, searchable names for USDA database queries."

    @classmethod
    def get_system_prompt(cls) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        return cls.SYSTEM_PROMPT
    
    @classmethod
    def get_user_prompt(cls, optional_text: str = None) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        base_prompt = cls.USER_PROMPT_TEMPLATE
        if optional_text:
            base_prompt += f"\n\nAdditional context: {optional_text}"
        return base_prompt 
```

============================================================

ðŸ“„ FILE: app_v2/config/prompts/phase2_prompts.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 5,272 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:48:26
å­˜åœ¨: âœ…

CONTENT:
```
class Phase2Prompts:
    """Phase2ï¼ˆè¨ˆç®—æˆ¦ç•¥æ±ºå®šï¼‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
    
    SYSTEM_PROMPT = """You are an expert food item identifier, data matcher, and nutritional analysis strategist. Your task is to refine an initial meal analysis by:
1.  Determining the best `calculation_strategy` ("dish_level" or "ingredient_level") for each identified dish/food item.
2.  Matching the dish/food item (if "dish_level") OR its constituent ingredients (if "ingredient_level") to the most appropriate USDA FoodData Central (FDC) entries based on provided candidate information.
3.  Providing the official USDA `usda_source_description` for all matched FDC IDs.

IMPORTANT:
1.  You MUST provide ALL responses in English only.
2.  Your primary goal is to output the `calculation_strategy` for each dish, and then the relevant `fdc_id`(s) and `usda_source_description`(s) according to that strategy.
3.  You DO NOT need to calculate or return any nutritional values (calories, protein, etc.). This will be handled by a separate system.
4.  The `weight_g` for each ingredient is already determined in a previous phase and should NOT be modified or output by you.
5.  Strictly follow the provided JSON schema for your response (see REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA).

Your tasks for EACH dish/food item identified in the initial analysis:

TASK 1: Determine `calculation_strategy`.
   - If the dish/food item is a single, simple item (e.g., "Apple", "Banana", "Chicken Breast Fillet") AND a good, specific FDC ID candidate exists for it:
     Choose `calculation_strategy: "dish_level"`.
   - If the dish is a complex, mixed dish (e.g., "Homemade Vegetable Stir-fry", "Mixed Salad with various toppings", "Beef Stew"):
     Choose `calculation_strategy: "ingredient_level"`. You will then need to identify FDC IDs for its constituent ingredients.
   - If the dish is a somewhat standardized prepared dish (e.g., "Pepperoni Pizza", "Cheeseburger") AND a representative FDC ID candidate exists for the *entire dish*:
     Choose `calculation_strategy: "dish_level"`.
   - If a standardized dish does NOT have a good representative FDC ID for the entire dish, OR if breaking it down into its main ingredients would be more accurate:
     Choose `calculation_strategy: "ingredient_level"`.
   - Provide a brief rationale for your choice of strategy if it's not obvious (though this rationale is not part of the JSON output).

TASK 2: Output FDC ID(s) and Description(s) based on the chosen `calculation_strategy`.

   IF `calculation_strategy` is "dish_level":
     a. From the USDA candidates for the *dish/food item itself*, select the single most appropriate FDC ID.
     b. Set this as the `fdc_id` for the dish in your JSON output.
     c. Set the corresponding `usda_source_description` for the dish.
     d. The `ingredients` array for this dish in your JSON output should still list the ingredients identified in Phase 1 (or refined by you if necessary for clarity), but these ingredients will NOT have their own `fdc_id` or `usda_source_description` set by you in this "dish_level" scenario (set them to `null` or omit). Their primary purpose here is descriptive.

   IF `calculation_strategy` is "ingredient_level":
     a. Set the `fdc_id` and `usda_source_description` for the *dish itself* to `null` in your JSON output.
     b. For EACH `ingredient` within that dish (from the initial analysis, possibly refined by you):
        i. From the USDA candidates provided for *that specific ingredient*, select the single most appropriate FDC ID.
        ii. Set this as the `fdc_id` for that ingredient in your JSON output.
        iii. Set the corresponding `usda_source_description` for that ingredient.
        iv. If no suitable FDC ID is found for an ingredient, set its `fdc_id` and `usda_source_description` to `null`.

General Guidelines for FDC ID Selection (for dish or ingredient):
- Consider typical uses of ingredients and the most plausible match to the image context (if discernible) and initial `ingredient_name`.
- Prioritize FDC ID candidates in this order if relevant and good matches exist: 'Foundation Foods', 'SR Legacy', 'FNDDS' (Survey), then 'Branded Foods'.
- You may slightly refine `dish_name` or `ingredient_name` if the USDA description offers a more precise or common English term for the same food item, ensuring it still accurately represents the food.

Output the final analysis in the specified JSON format."""

    USER_PROMPT_TEMPLATE = """Please refine the initial meal analysis using the provided USDA candidate information.

USDA Candidates:
{usda_candidates_text}

Initial Analysis:
{initial_analysis_data}

Based on this information, determine the optimal calculation strategy for each dish and match the appropriate USDA FDC IDs."""

    @classmethod
    def get_system_prompt(cls) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        return cls.SYSTEM_PROMPT
    
    @classmethod
    def get_user_prompt(cls, usda_candidates_text: str, initial_analysis_data: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        return cls.USER_PROMPT_TEMPLATE.format(
            usda_candidates_text=usda_candidates_text,
            initial_analysis_data=initial_analysis_data
        ) 
```

============================================================

ðŸ“„ FILE: app_v2/config/settings.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,417 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:11:08
å­˜åœ¨: âœ…

CONTENT:
```
from typing import Optional, List
from pydantic_settings import BaseSettings
from functools import lru_cache


class Settings(BaseSettings):
    """
    APIè¨­å®šã‚¯ãƒ©ã‚¹
    ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šå€¤ã‚’èª­ã¿è¾¼ã‚€
    """
    # Vertex AIè¨­å®š
    GEMINI_PROJECT_ID: str  # GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDï¼ˆå¿…é ˆï¼‰
    GEMINI_LOCATION: str = "us-central1"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
    GEMINI_MODEL_NAME: str = "gemini-2.5-flash-preview-05-20"
    
    # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢è¨­å®š
    USE_LOCAL_NUTRITION_SEARCH: bool = True  # ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
    NUTRITION_DB_EXPERIMENT_PATH: Optional[str] = None  # nutrition_db_experimentã¸ã®ãƒ‘ã‚¹ï¼ˆè‡ªå‹•æ¤œå‡ºã™ã‚‹å ´åˆã¯Noneï¼‰
    
    # USDA APIè¨­å®šï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
    USDA_API_KEY: str  # USDA FoodData Central APIã‚­ãƒ¼ï¼ˆå¿…é ˆï¼‰
    USDA_API_BASE_URL: str = "https://api.nal.usda.gov/fdc/v1"
    USDA_API_TIMEOUT: float = 10.0  # APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°
    USDA_SEARCH_CANDIDATES_LIMIT: int = 5  # 1å›žã®æ¤œç´¢ã§å–å¾—ã™ã‚‹æœ€å¤§å€™è£œæ•°
    # ä¸»è¦æ „é¤Šç´ ç•ªå·ï¼ˆã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã¨ã—ã¦ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼‰
    USDA_KEY_NUTRIENT_NUMBERS_STR: str = "208,203,204,205,291,269,307"
    # 208: Energy (kcal), 203: Protein, 204: Total lipid (fat), 
    # 205: Carbohydrate, 291: Fiber, 269: Total sugars, 307: Sodium
    
    @property
    def USDA_KEY_NUTRIENT_NUMBERS(self) -> List[str]:
        """ä¸»è¦æ „é¤Šç´ ç•ªå·ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        return self.USDA_KEY_NUTRIENT_NUMBERS_STR.split(",")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    CACHE_TYPE: str = "simple"  # "simple", "redis", "memcached"
    CACHE_REDIS_URL: Optional[str] = None  # Redisã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®URL
    USDA_CACHE_TTL_SECONDS: int = 3600  # USDAãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé–“ï¼ˆ1æ™‚é–“ï¼‰
    
    # APIè¨­å®š
    API_LOG_LEVEL: str = "INFO"
    FASTAPI_ENV: str = "development"
    
    # ã‚µãƒ¼ãƒãƒ¼è¨­å®š
    HOST: str = "0.0.0.0"
    PORT: int = 8000
    
    # APIãƒãƒ¼ã‚¸ãƒ§ãƒ³
    API_VERSION: str = "v1"
    
    # çµæžœä¿å­˜è¨­å®š
    RESULTS_DIR: str = "analysis_results"
    
    class Config:
        env_file = ".env"
        case_sensitive = True


@lru_cache()
def get_settings() -> Settings:
    """
    è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ï¼‰
    """
    return Settings() 
```

============================================================

ðŸ“„ FILE: app_v2/main/app.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 2,030 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:55:53
å­˜åœ¨: âœ…

CONTENT:
```
import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ..api.v1.endpoints import meal_analysis
from ..config import get_settings

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆæ—¢å­˜ã®appã¨åŒã˜ï¼‰
os.environ.setdefault("USDA_API_KEY", "vSWtKJ3jYD0Cn9LRyVJUFkuyCt9p8rEtVXz74PZg")
os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", "/Users/odasoya/meal_analysis_api /service-account-key.json")
os.environ.setdefault("GEMINI_PROJECT_ID", "recording-diet-ai-3e7cf")
os.environ.setdefault("GEMINI_LOCATION", "us-central1")
os.environ.setdefault("GEMINI_MODEL_NAME", "gemini-2.5-flash-preview-05-20")

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# FastAPIã‚¢ãƒ—ãƒªã®ä½œæˆ
app = FastAPI(
    title="é£Ÿäº‹åˆ†æž API v2.0",
    description="ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ã•ã‚ŒãŸé£Ÿäº‹åˆ†æžã‚·ã‚¹ãƒ†ãƒ ",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ç™»éŒ²
app.include_router(
    meal_analysis.router,
    prefix="/api/v1/meal-analyses",
    tags=["Complete Meal Analysis v2.0"]
)

# ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "é£Ÿäº‹åˆ†æž API v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ",
        "version": "2.0.0",
        "architecture": "Component-based Pipeline",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "version": "v2.0",
        "components": ["Phase1Component", "USDAQueryComponent"]
    }

if __name__ == "__main__":
    import uvicorn
    settings = get_settings()
    uvicorn.run(
        "app_v2.main.app:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=True
    ) 
```

============================================================

ðŸ“„ FILE: app_v2/models/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 767 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:41:11
å­˜åœ¨: âœ…

CONTENT:
```
from .phase1_models import *
from .usda_models import *
from .phase2_models import *
from .nutrition_models import *
from .nutrition_search_models import *

__all__ = [
    # Phase1 models
    "Phase1Input", "Phase1Output", "Ingredient", "Dish",
    
    # USDA models
    "USDAQueryInput", "USDAQueryOutput", "USDAMatch", "USDANutrient",
    
    # Phase2 models
    "Phase2Input", "Phase2Output", "RefinedDish", "RefinedIngredient",
    
    # Nutrition models
    "NutritionInput", "NutritionOutput", "CalculatedNutrients", "TotalNutrients",
    
    # Nutrition Search models (æ±Žç”¨)
    "NutritionQueryInput", "NutritionQueryOutput", "NutritionMatch", "NutritionNutrient",
    "convert_usda_query_input_to_nutrition", "convert_nutrition_to_usda_query_output"
] 
```

============================================================

ðŸ“„ FILE: app_v2/models/nutrition_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 4,046 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:57:44
å­˜åœ¨: âœ…

CONTENT:
```
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

from .phase2_models import RefinedDish


class CalculatedNutrients(BaseModel):
    """è¨ˆç®—æ¸ˆã¿æ „é¤Šç´ ãƒ¢ãƒ‡ãƒ«"""
    calories_kcal: float = Field(0.0, description="è¨ˆç®—ã•ã‚ŒãŸç·ã‚«ãƒ­ãƒªãƒ¼ (kcal)")
    protein_g: float = Field(0.0, description="è¨ˆç®—ã•ã‚ŒãŸç·ã‚¿ãƒ³ãƒ‘ã‚¯è³ª (g)")
    carbohydrates_g: float = Field(0.0, description="è¨ˆç®—ã•ã‚ŒãŸç·ç‚­æ°´åŒ–ç‰© (g)")
    fat_g: float = Field(0.0, description="è¨ˆç®—ã•ã‚ŒãŸç·è„‚è³ª (g)")
    fiber_g: Optional[float] = Field(None, description="è¨ˆç®—ã•ã‚ŒãŸç·é£Ÿç‰©ç¹Šç¶­ (g)")
    sodium_mg: Optional[float] = Field(None, description="è¨ˆç®—ã•ã‚ŒãŸç·ãƒŠãƒˆãƒªã‚¦ãƒ  (mg)")

    def to_dict(self) -> Dict[str, float]:
        """è¾žæ›¸å½¢å¼ã§æ „é¤Šç´ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        return {
            "calories_kcal": self.calories_kcal,
            "protein_g": self.protein_g,
            "carbohydrates_g": self.carbohydrates_g,
            "fat_g": self.fat_g,
            "fiber_g": self.fiber_g or 0.0,
            "sodium_mg": self.sodium_mg or 0.0
        }

    def add(self, other: 'CalculatedNutrients') -> 'CalculatedNutrients':
        """ä»–ã®æ „é¤Šç´ ã¨åˆè¨ˆã™ã‚‹"""
        return CalculatedNutrients(
            calories_kcal=self.calories_kcal + other.calories_kcal,
            protein_g=self.protein_g + other.protein_g,
            carbohydrates_g=self.carbohydrates_g + other.carbohydrates_g,
            fat_g=self.fat_g + other.fat_g,
            fiber_g=(self.fiber_g or 0.0) + (other.fiber_g or 0.0),
            sodium_mg=(self.sodium_mg or 0.0) + (other.sodium_mg or 0.0)
        )


class EnrichedRefinedDish(BaseModel):
    """æ „é¤Šä¾¡ãŒè¨ˆç®—ã•ã‚ŒãŸæ–™ç†ãƒ¢ãƒ‡ãƒ«"""
    # Phase2ã®æƒ…å ±ã‚’ç¶™æ‰¿
    dish_name: str = Field(..., description="æ–™ç†å")
    type: str = Field(..., description="æ–™ç†ã®ç¨®é¡ž")
    quantity_on_plate: str = Field(..., description="çš¿ã®ä¸Šã®é‡")
    calculation_strategy: str = Field(..., description="è¨ˆç®—æˆ¦ç•¥")
    
    # è¨ˆç®—ã•ã‚ŒãŸæ „é¤Šä¾¡æƒ…å ±
    dish_total_actual_nutrients: CalculatedNutrients = Field(..., description="ã“ã®æ–™ç†ã®åˆè¨ˆæ „é¤Šç´ ")
    ingredient_nutrients: Optional[List[Dict]] = Field(None, description="å„é£Ÿæã®æ „é¤Šä¾¡è©³ç´°")
    calculation_details: Optional[Dict] = Field(None, description="è¨ˆç®—ã®è©³ç´°æƒ…å ±")


class TotalNutrients(CalculatedNutrients):
    """é£Ÿäº‹å…¨ä½“ã®æ „é¤Šç´ ãƒ¢ãƒ‡ãƒ«ï¼ˆCalculatedNutrientsã‚’æ‹¡å¼µï¼‰"""
    dish_count: int = Field(0, description="æ–™ç†ã®æ•°")
    ingredient_count: int = Field(0, description="ç·é£Ÿææ•°")
    calculation_summary: Optional[Dict[str, int]] = Field(None, description="è¨ˆç®—ã‚µãƒžãƒªãƒ¼")


class NutritionInput(BaseModel):
    """æ „é¤Šè¨ˆç®—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    refined_dishes: List[RefinedDish] = Field(..., description="Phase2ã§ç²¾ç·»åŒ–ã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆ")
    calculation_options: Optional[Dict] = Field(None, description="è¨ˆç®—ã‚ªãƒ—ã‚·ãƒ§ãƒ³")


class NutritionOutput(BaseModel):
    """æ „é¤Šè¨ˆç®—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«"""
    enriched_dishes: List[EnrichedRefinedDish] = Field(..., description="æ „é¤Šä¾¡ãŒè¨ˆç®—ã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆ")
    total_meal_nutrients: TotalNutrients = Field(..., description="é£Ÿäº‹å…¨ä½“ã®æ „é¤Šä¾¡")
    calculation_summary: Dict[str, Any] = Field(default_factory=dict, description="è¨ˆç®—ã‚µãƒžãƒªãƒ¼")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    errors: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_total_calories(self) -> float:
        """ç·ã‚«ãƒ­ãƒªãƒ¼ã‚’å–å¾—"""
        return self.total_meal_nutrients.calories_kcal

    def get_dishes_by_strategy(self, strategy: str) -> List[EnrichedRefinedDish]:
        """æŒ‡å®šã•ã‚ŒãŸè¨ˆç®—æˆ¦ç•¥ã®æ–™ç†ã‚’å–å¾—"""
        return [dish for dish in self.enriched_dishes if dish.calculation_strategy == strategy] 
```

============================================================

ðŸ“„ FILE: app_v2/models/nutrition_search_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 6,916 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:40:52
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Nutrition Search Models

USDA database queryã¨ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®ä¸¡æ–¹ã§ä½¿ç”¨ã§ãã‚‹æ±Žç”¨çš„ãªãƒ¢ãƒ‡ãƒ«
"""

from typing import List, Dict, Optional, Any, Union
from pydantic import BaseModel, Field


class NutritionNutrient(BaseModel):
    """æ „é¤Šç´ æƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆæ±Žç”¨ï¼‰"""
    name: str = Field(..., description="æ „é¤Šç´ å")
    amount: float = Field(..., description="100gã¾ãŸã¯100mlã‚ãŸã‚Šã®é‡")
    unit_name: str = Field(..., description="å˜ä½å (ä¾‹: g, mg, kcal)")
    nutrient_id: Optional[Union[int, str]] = Field(None, description="æ „é¤Šç´ IDï¼ˆUSDA IDã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«IDï¼‰")
    nutrient_number: Optional[str] = Field(None, description="æ „é¤Šç´ ç•ªå·")


class NutritionMatch(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆçµæžœãƒ¢ãƒ‡ãƒ«ï¼ˆæ±Žç”¨ï¼‰"""
    id: Union[int, str] = Field(..., description="é£Ÿå“IDï¼ˆUSDA FDC IDã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«IDï¼‰")
    description: str = Field(..., description="é£Ÿå“ã®åç§°")
    data_type: Optional[str] = Field(None, description="ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (ä¾‹: SR Legacy, Branded, Local_Dish)")
    source: str = Field(..., description="ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ'usda_api' ã¾ãŸã¯ 'local_database'ï¼‰")
    brand_owner: Optional[str] = Field(None, description="ãƒ–ãƒ©ãƒ³ãƒ‰æ‰€æœ‰è€… (Branded Foodsã®å ´åˆ)")
    ingredients_text: Optional[str] = Field(None, description="åŽŸææ–™ãƒªã‚¹ãƒˆæ–‡å­—åˆ—")
    nutrients: List[NutritionNutrient] = Field(default_factory=list, description="æ „é¤Šç´ æƒ…å ±ã®ãƒªã‚¹ãƒˆ")
    score: Optional[float] = Field(None, description="æ¤œç´¢çµæžœã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢")
    original_data: Optional[Dict[str, Any]] = Field(None, description="å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿")
    
    # USDAäº’æ›æ€§ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
    @property
    def fdc_id(self) -> Union[int, str]:
        """USDAäº’æ›æ€§ã®ãŸã‚ã®fdc_idãƒ—ãƒ­ãƒ‘ãƒ†ã‚£"""
        return self.id
    
    @property 
    def food_nutrients(self) -> List[NutritionNutrient]:
        """USDAäº’æ›æ€§ã®ãŸã‚ã®food_nutrientsãƒ—ãƒ­ãƒ‘ãƒ†ã‚£"""
        return self.nutrients
    
    @property
    def original_usda_data(self) -> Optional[Dict[str, Any]]:
        """USDAäº’æ›æ€§ã®ãŸã‚ã®original_usda_dataãƒ—ãƒ­ãƒ‘ãƒ†ã‚£"""
        return self.original_data


class NutritionQueryInput(BaseModel):
    """æ „é¤Šæ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆæ±Žç”¨ï¼‰"""
    ingredient_names: List[str] = Field(..., description="æ¤œç´¢ã™ã‚‹é£Ÿæåã®ãƒªã‚¹ãƒˆ")
    dish_names: List[str] = Field(default_factory=list, description="æ¤œç´¢ã™ã‚‹æ–™ç†åã®ãƒªã‚¹ãƒˆ")
    search_options: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    preferred_source: Optional[str] = Field(None, description="å„ªå…ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ'usda_api' ã¾ãŸã¯ 'local_database'ï¼‰")

    def get_all_search_terms(self) -> List[str]:
        """å…¨ã¦ã®æ¤œç´¢ç”¨èªžã‚’å–å¾—"""
        return list(set(self.ingredient_names + self.dish_names))


class NutritionQueryOutput(BaseModel):
    """æ „é¤Šæ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆæ±Žç”¨ï¼‰"""
    matches: Dict[str, NutritionMatch] = Field(default_factory=dict, description="æ¤œç´¢èªžå½™ã¨ãƒžãƒƒãƒã—ãŸçµæžœã®ãƒžãƒƒãƒ—")
    search_summary: Dict[str, Any] = Field(default_factory=dict, description="æ¤œç´¢ã‚µãƒžãƒªãƒ¼ï¼ˆæˆåŠŸæ•°ã€å¤±æ•—æ•°ã€æ¤œç´¢æ–¹æ³•ãªã©ï¼‰")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    errors: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_match_rate(self) -> float:
        """ç…§åˆæˆåŠŸçŽ‡ã‚’è¨ˆç®—"""
        total_searches = self.search_summary.get("total_searches", 0)
        successful_matches = self.search_summary.get("successful_matches", 0)
        if total_searches == 0:
            return 0.0
        return successful_matches / total_searches

    def has_match(self, search_term: str) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸæ¤œç´¢èªžå½™ã«ãƒžãƒƒãƒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return search_term in self.matches and self.matches[search_term] is not None
    
    def get_source_summary(self) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã®ãƒžãƒƒãƒæ•°ã‚µãƒžãƒªãƒ¼ã‚’å–å¾—"""
        source_counts = {}
        for match in self.matches.values():
            source = match.source
            source_counts[source] = source_counts.get(source, 0) + 1
        return source_counts


# USDAäº’æ›æ€§ã®ãŸã‚ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é–¢æ•°
def convert_usda_query_input_to_nutrition(usda_input) -> NutritionQueryInput:
    """USDAQueryInputã‚’NutritionQueryInputã«å¤‰æ›"""
    return NutritionQueryInput(
        ingredient_names=usda_input.ingredient_names,
        dish_names=usda_input.dish_names,
        search_options=usda_input.search_options,
        preferred_source="usda_api"
    )

def convert_nutrition_to_usda_query_output(nutrition_output: NutritionQueryOutput):
    """NutritionQueryOutputã‚’USDAQueryOutputäº’æ›å½¢å¼ã«å¤‰æ›"""
    from .usda_models import USDAQueryOutput, USDAMatch, USDANutrient
    
    # USDAMatchã«å¤‰æ›
    usda_matches = {}
    for term, nutrition_match in nutrition_output.matches.items():
        usda_nutrients = []
        for nutrient in nutrition_match.nutrients:
            usda_nutrients.append(USDANutrient(
                name=nutrient.name,
                amount=nutrient.amount,
                unit_name=nutrient.unit_name,
                nutrient_id=nutrient.nutrient_id if isinstance(nutrient.nutrient_id, int) else None,
                nutrient_number=nutrient.nutrient_number
            ))
        
        usda_matches[term] = USDAMatch(
            fdc_id=nutrition_match.id if isinstance(nutrition_match.id, int) else 0,
            description=nutrition_match.description,
            data_type=nutrition_match.data_type,
            brand_owner=nutrition_match.brand_owner,
            ingredients_text=nutrition_match.ingredients_text,
            food_nutrients=usda_nutrients,
            score=nutrition_match.score,
            original_usda_data=nutrition_match.original_data
        )
    
    # æ•°å€¤ã®ã¿ã®search_summaryã‚’ä½œæˆï¼ˆUSDAäº’æ›æ€§ã®ãŸã‚ï¼‰
    numeric_summary = {}
    for key, value in nutrition_output.search_summary.items():
        if isinstance(value, (int, float)):
            numeric_summary[key] = int(value)
        elif key in ["total_searches", "successful_matches", "failed_searches", "match_rate_percent"]:
            try:
                numeric_summary[key] = int(value) if value is not None else 0
            except (ValueError, TypeError):
                numeric_summary[key] = 0
    
    return USDAQueryOutput(
        matches=usda_matches,
        search_summary=numeric_summary,
        warnings=nutrition_output.warnings,
        errors=nutrition_output.errors
    ) 
```

============================================================

ðŸ“„ FILE: app_v2/models/phase1_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 1,764 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:44:12
å­˜åœ¨: âœ…

CONTENT:
```
from typing import List, Optional
from pydantic import BaseModel, Field


class Ingredient(BaseModel):
    """é£Ÿææƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
    ingredient_name: str = Field(..., description="é£Ÿæã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ï¼‰")


class Dish(BaseModel):
    """æ–™ç†æƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
    dish_name: str = Field(..., description="ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ï¼‰")
    ingredients: List[Ingredient] = Field(..., description="ãã®æ–™ç†ã«å«ã¾ã‚Œã‚‹é£Ÿæã®ãƒªã‚¹ãƒˆ")


class Phase1Input(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    image_bytes: bytes = Field(..., description="ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆå½¢å¼ï¼‰")
    image_mime_type: str = Field(..., description="ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—")
    optional_text: Optional[str] = Field(None, description="ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±")

    class Config:
        arbitrary_types_allowed = True


class Phase1Output(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰"""
    dishes: List[Dish] = Field(..., description="ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆ")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_all_ingredient_names(self) -> List[str]:
        """å…¨ã¦ã®é£Ÿæåã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
        ingredient_names = []
        for dish in self.dishes:
            for ingredient in dish.ingredients:
                ingredient_names.append(ingredient.ingredient_name)
        return ingredient_names

    def get_all_dish_names(self) -> List[str]:
        """å…¨ã¦ã®æ–™ç†åã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰"""
        return [dish.dish_name for dish in self.dishes] 
```

============================================================

ðŸ“„ FILE: app_v2/models/phase2_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 4,056 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:50:28
å­˜åœ¨: âœ…

CONTENT:
```
from typing import List, Optional, Literal, Dict
from pydantic import BaseModel, Field, field_validator

from .phase1_models import Ingredient
from .usda_models import USDAQueryOutput


class RefinedIngredient(BaseModel):
    """USDAæƒ…å ±ã§ç²¾ç·»åŒ–ã•ã‚ŒãŸé£Ÿæãƒ¢ãƒ‡ãƒ«"""
    ingredient_name: str = Field(..., description="é£Ÿæã®åç§°ï¼ˆç²¾ç·»åŒ–å¾Œï¼‰")
    weight_g: float = Field(..., description="é£Ÿæã®æŽ¨å®šé‡é‡ï¼ˆã‚°ãƒ©ãƒ å˜ä½ã€Phase1ç”±æ¥ï¼‰", gt=0)
    fdc_id: Optional[int] = Field(None, description="å¯¾å¿œã™ã‚‹USDAé£Ÿå“ã®FDC ID (é£Ÿæãƒ¬ãƒ™ãƒ«ã®å ´åˆ)")
    usda_source_description: Optional[str] = Field(None, description="é¸æŠžã•ã‚ŒãŸUSDAé£Ÿå“ã®å…¬å¼åç§° (é£Ÿæãƒ¬ãƒ™ãƒ«ã®å ´åˆ)")
    key_nutrients_per_100g: Optional[Dict[str, float]] = Field(
        None,
        description="é¸æŠžã•ã‚ŒãŸUSDAé£Ÿå“ã®ä¸»è¦æ „é¤Šç´ ï¼ˆ100gã‚ãŸã‚Šï¼‰ã€‚ã‚­ãƒ¼ã¯'calories_kcal', 'protein_g', 'carbohydrates_g', 'fat_g'ã€‚",
    )

    @field_validator('key_nutrients_per_100g')
    def check_ingredient_nutrients_values(cls, v):
        if v is not None:
            for key, value in v.items():
                if value is None:
                    v[key] = 0.0
        return v


class RefinedDish(BaseModel):
    """USDAæƒ…å ±ã§ç²¾ç·»åŒ–ã•ã‚ŒãŸæ–™ç†ãƒ¢ãƒ‡ãƒ«"""
    dish_name: str = Field(..., description="ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆç²¾ç·»åŒ–å¾Œï¼‰")
    type: str = Field(..., description="æ–™ç†ã®ç¨®é¡žï¼ˆä¾‹: ä¸»èœ, å‰¯èœ, ã‚¹ãƒ¼ãƒ—ï¼‰")
    quantity_on_plate: str = Field(..., description="çš¿ã®ä¸Šã«è¼‰ã£ã¦ã„ã‚‹æ–™ç†ã®ãŠãŠã‚ˆãã®é‡ã‚„å€‹æ•°")
    
    calculation_strategy: Optional[Literal["dish_level", "ingredient_level"]] = Field(
        None, 
        description="ã“ã®æ–™ç†ã®æ „é¤Šè¨ˆç®—æ–¹é‡ (GeminiãŒæ±ºå®š)"
    )
    
    # dish_levelè¨ˆç®—æ™‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    fdc_id: Optional[int] = Field(None, description="æ–™ç†å…¨ä½“ã®FDC ID (dish_levelè¨ˆç®—æ™‚)")
    usda_source_description: Optional[str] = Field(None, description="æ–™ç†å…¨ä½“ã®USDAå…¬å¼åç§° (dish_levelè¨ˆç®—æ™‚)")
    key_nutrients_per_100g: Optional[Dict[str, float]] = Field(
        None,
        description="æ–™ç†å…¨ä½“ã®100gã‚ãŸã‚Šä¸»è¦æ „é¤Šç´  (dish_levelè¨ˆç®—æ™‚)ã€‚ã‚­ãƒ¼ã¯'calories_kcal', 'protein_g', 'carbohydrates_g', 'fat_g'ã€‚",
    )

    ingredients: List[RefinedIngredient] = Field(default_factory=list, description="ã“ã®æ–™ç†ã«å«ã¾ã‚Œã‚‹é£Ÿæã®ãƒªã‚¹ãƒˆ")

    @field_validator('key_nutrients_per_100g')
    def check_dish_nutrients_values(cls, v):
        if v is not None:
            for key, value in v.items():
                if value is None:
                    v[key] = 0.0
        return v


class Phase2Input(BaseModel):
    """Phase2ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    image_bytes: bytes = Field(..., description="ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆå½¢å¼ï¼‰")
    image_mime_type: str = Field(..., description="ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—")
    phase1_result: 'Phase1Output' = Field(..., description="Phase1ã®å‡ºåŠ›çµæžœ")
    usda_matches: USDAQueryOutput = Field(..., description="USDAç…§åˆçµæžœ")

    class Config:
        arbitrary_types_allowed = True


class Phase2Output(BaseModel):
    """Phase2ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«"""
    refined_dishes: List[RefinedDish] = Field(..., description="ç²¾ç·»åŒ–ã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆ")
    strategy_summary: Dict[str, int] = Field(default_factory=dict, description="è¨ˆç®—æˆ¦ç•¥ã®ã‚µãƒžãƒªãƒ¼")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    errors: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_dishes_by_strategy(self, strategy: str) -> List[RefinedDish]:
        """æŒ‡å®šã•ã‚ŒãŸæˆ¦ç•¥ã®æ–™ç†ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return [dish for dish in self.refined_dishes if dish.calculation_strategy == strategy]

# å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é¿ã‘ã‚‹ãŸã‚ã€ã“ã“ã§åž‹ã‚’æ›´æ–°
from .phase1_models import Phase1Output
Phase2Input.model_rebuild() 
```

============================================================

ðŸ“„ FILE: app_v2/models/usda_models.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 3,030 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 15:28:52
å­˜åœ¨: âœ…

CONTENT:
```
from typing import List, Dict, Optional
from pydantic import BaseModel, Field


class USDANutrient(BaseModel):
    """USDAæ „é¤Šç´ æƒ…å ±ãƒ¢ãƒ‡ãƒ«"""
    name: str = Field(..., description="æ „é¤Šç´ å")
    amount: float = Field(..., description="100gã¾ãŸã¯100mlã‚ãŸã‚Šã®é‡")
    unit_name: str = Field(..., description="å˜ä½å (ä¾‹: g, mg, kcal)")
    nutrient_id: Optional[int] = Field(None, description="USDAæ „é¤Šç´ ID")
    nutrient_number: Optional[str] = Field(None, description="USDAæ „é¤Šç´ ç•ªå·")


class USDAMatch(BaseModel):
    """USDAç…§åˆçµæžœãƒ¢ãƒ‡ãƒ«"""
    fdc_id: int = Field(..., description="USDA FoodData Central ID")
    description: str = Field(..., description="é£Ÿå“ã®å…¬å¼åç§°")
    data_type: Optional[str] = Field(None, description="USDAãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (ä¾‹: SR Legacy, Branded)")
    brand_owner: Optional[str] = Field(None, description="ãƒ–ãƒ©ãƒ³ãƒ‰æ‰€æœ‰è€… (Branded Foodsã®å ´åˆ)")
    ingredients_text: Optional[str] = Field(None, description="åŽŸææ–™ãƒªã‚¹ãƒˆæ–‡å­—åˆ— (Branded Foodsã®å ´åˆ)")
    food_nutrients: List[USDANutrient] = Field(default_factory=list, description="ä¸»è¦ãªæ „é¤Šç´ æƒ…å ±ã®ãƒªã‚¹ãƒˆ")
    score: Optional[float] = Field(None, description="æ¤œç´¢çµæžœã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢")
    original_usda_data: Optional[Dict] = Field(None, description="USDA APIã‹ã‚‰ã®ã‚ªãƒªã‚¸ãƒŠãƒ«JSONãƒ‡ãƒ¼ã‚¿")


class USDAQueryInput(BaseModel):
    """USDAã‚¯ã‚¨ãƒªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    ingredient_names: List[str] = Field(..., description="æ¤œç´¢ã™ã‚‹é£Ÿæåã®ãƒªã‚¹ãƒˆ")
    dish_names: List[str] = Field(default_factory=list, description="æ¤œç´¢ã™ã‚‹æ–™ç†åã®ãƒªã‚¹ãƒˆ")
    search_options: Optional[Dict] = Field(None, description="æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")

    def get_all_search_terms(self) -> List[str]:
        """å…¨ã¦ã®æ¤œç´¢ç”¨èªžã‚’å–å¾—"""
        return list(set(self.ingredient_names + self.dish_names))


class USDAQueryOutput(BaseModel):
    """USDAã‚¯ã‚¨ãƒªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«"""
    matches: Dict[str, USDAMatch] = Field(default_factory=dict, description="æ¤œç´¢èªžå½™ã¨ãƒžãƒƒãƒã—ãŸçµæžœã®ãƒžãƒƒãƒ—")
    search_summary: Dict[str, int] = Field(default_factory=dict, description="æ¤œç´¢ã‚µãƒžãƒªãƒ¼ï¼ˆæˆåŠŸæ•°ã€å¤±æ•—æ•°ãªã©ï¼‰")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    errors: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_match_rate(self) -> float:
        """ç…§åˆæˆåŠŸçŽ‡ã‚’è¨ˆç®—"""
        total_searches = self.search_summary.get("total_searches", 0)
        successful_matches = self.search_summary.get("successful_matches", 0)
        if total_searches == 0:
            return 0.0
        return successful_matches / total_searches

    def has_match(self, search_term: str) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸæ¤œç´¢èªžå½™ã«ãƒžãƒƒãƒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return search_term in self.matches and self.matches[search_term] is not None 
```

============================================================

ðŸ“„ FILE: app_v2/pipeline/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 142 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:08:07
å­˜åœ¨: âœ…

CONTENT:
```
from .orchestrator import MealAnalysisPipeline
from .result_manager import ResultManager

__all__ = ["MealAnalysisPipeline", "ResultManager"] 
```

============================================================

ðŸ“„ FILE: app_v2/pipeline/orchestrator.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 11,800 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:43:18
å­˜åœ¨: âœ…

CONTENT:
```
import uuid
import json
from datetime import datetime
from typing import Optional, Dict, Any
import logging

from ..components import Phase1Component, USDAQueryComponent, LocalNutritionSearchComponent
from ..models import (
    Phase1Input, Phase1Output,
    USDAQueryInput, USDAQueryOutput,
    NutritionQueryInput
)
from ..config import get_settings
from .result_manager import ResultManager

logger = logging.getLogger(__name__)


class MealAnalysisPipeline:
    """
    é£Ÿäº‹åˆ†æžãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
    
    4ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã‚’çµ±åˆã—ã¦å®Œå…¨ãªåˆ†æžã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    
    def __init__(self, use_local_nutrition_search: Optional[bool] = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            use_local_nutrition_search: ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
                                      None: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾—
                                      True: LocalNutritionSearchComponentä½¿ç”¨
                                      False: å¾“æ¥ã®USDAQueryComponentä½¿ç”¨
        """
        self.pipeline_id = str(uuid.uuid4())[:8]
        self.settings = get_settings()
        
        # è¨­å®šã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’æ±ºå®š
        if use_local_nutrition_search is None:
            self.use_local_nutrition_search = self.settings.USE_LOCAL_NUTRITION_SEARCH
        else:
            self.use_local_nutrition_search = use_local_nutrition_search
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.phase1_component = Phase1Component()
        
        # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é¸æŠž
        if self.use_local_nutrition_search:
            self.nutrition_search_component = LocalNutritionSearchComponent()
            self.search_component_name = "LocalNutritionSearchComponent"
            logger.info("Using local nutrition database search (nutrition_db_experiment)")
        else:
            self.nutrition_search_component = USDAQueryComponent()
            self.search_component_name = "USDAQueryComponent"
            logger.info("Using traditional USDA API search")
            
        # TODO: Phase2Componentã¨NutritionCalculationComponentã‚’è¿½åŠ 
        
        self.logger = logging.getLogger(f"{__name__}.{self.pipeline_id}")
        
    async def execute_complete_analysis(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        optional_text: Optional[str] = None,
        save_results: bool = True,
        save_detailed_logs: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œå…¨ãªé£Ÿäº‹åˆ†æžã‚’å®Ÿè¡Œ
        
        Args:
            image_bytes: ç”»åƒãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆ
            save_results: çµæžœã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            save_detailed_logs: è©³ç´°ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            å®Œå…¨ãªåˆ†æžçµæžœ
        """
        analysis_id = str(uuid.uuid4())[:8]
        start_time = datetime.now()
        
        # ResultManagerã®åˆæœŸåŒ–
        result_manager = ResultManager(analysis_id) if save_detailed_logs else None
        
        self.logger.info(f"[{analysis_id}] Starting complete meal analysis pipeline")
        self.logger.info(f"[{analysis_id}] Nutrition search method: {'Local Database' if self.use_local_nutrition_search else 'USDA API'}")
        
        try:
            # === Phase 1: ç”»åƒåˆ†æž ===
            self.logger.info(f"[{analysis_id}] Phase 1: Image analysis")
            
            phase1_input = Phase1Input(
                image_bytes=image_bytes,
                image_mime_type=image_mime_type,
                optional_text=optional_text
            )
            
            # Phase1ã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            phase1_log = result_manager.create_execution_log("Phase1Component", f"{analysis_id}_phase1") if result_manager else None
            
            phase1_result = await self.phase1_component.execute(phase1_input, phase1_log)
            
            self.logger.info(f"[{analysis_id}] Phase 1 completed - Detected {len(phase1_result.dishes)} dishes")
            
            # === Nutrition Search Phase: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ ===
            search_phase_name = "Local Nutrition Search" if self.use_local_nutrition_search else "USDA Query"
            self.logger.info(f"[{analysis_id}] {search_phase_name} Phase: Database matching")
            
            # çµ±ä¸€ã•ã‚ŒãŸæ „é¤Šæ¤œç´¢å…¥åŠ›ã‚’ä½œæˆï¼ˆUSDAäº’æ›æ€§ã‚’ä¿æŒï¼‰
            nutrition_search_input = USDAQueryInput(
                ingredient_names=phase1_result.get_all_ingredient_names(),
                dish_names=phase1_result.get_all_dish_names()
            )
            
            # Nutrition Searchã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            search_log = result_manager.create_execution_log(self.search_component_name, f"{analysis_id}_nutrition_search") if result_manager else None
            
            nutrition_search_result = await self.nutrition_search_component.execute(nutrition_search_input, search_log)
            
            self.logger.info(f"[{analysis_id}] {search_phase_name} completed - {nutrition_search_result.get_match_rate():.1%} match rate")
            
            # === æš«å®šçš„ãªçµæžœã®æ§‹ç¯‰ (Phase2ã¨Nutritionã¯å¾Œã§è¿½åŠ ) ===
            
            # Phase1ã®çµæžœã‚’è¾žæ›¸å½¢å¼ã«å¤‰æ›ï¼ˆæ¤œç´¢ç‰¹åŒ–ï¼‰
            phase1_dict = {
                "dishes": [
                    {
                        "dish_name": dish.dish_name,
                        "ingredients": [
                            {
                                "ingredient_name": ing.ingredient_name
                            }
                            for ing in dish.ingredients
                        ]
                    }
                    for dish in phase1_result.dishes
                ]
            }
            
            # ç°¡å˜ãªæ „é¤Šè¨ˆç®—ï¼ˆæš«å®šï¼‰
            total_calories = sum(
                len(dish.ingredients) * 50  # ä»®ã®è¨ˆç®—
                for dish in phase1_result.dishes
            )
            
            # å®Œå…¨åˆ†æžçµæžœã®æ§‹ç¯‰
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            complete_result = {
                "analysis_id": analysis_id,
                "phase1_result": phase1_dict,
                "nutrition_search_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary,
                    "search_method": "local_nutrition_database" if self.use_local_nutrition_search else "usda_api"
                },
                # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§ã®ãŸã‚ã€usdaã‚­ãƒ¼ã‚‚æ®‹ã™
                "usda_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary
                },
                "processing_summary": {
                    "total_dishes": len(phase1_result.dishes),
                    "total_ingredients": len(phase1_result.get_all_ingredient_names()),
                    "nutrition_search_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",
                    "usda_match_rate": f"{len(nutrition_search_result.matches)}/{len(nutrition_search_input.get_all_search_terms())} ({nutrition_search_result.get_match_rate():.1%})",  # ãƒ¬ã‚¬ã‚·ãƒ¼äº’æ›æ€§
                    "total_calories": total_calories,
                    "pipeline_status": "completed",
                    "processing_time_seconds": processing_time,
                    "search_method": "local_nutrition_database" if self.use_local_nutrition_search else "usda_api"
                },
                # æš«å®šçš„ãªæœ€çµ‚çµæžœ
                "final_nutrition_result": {
                    "dishes": phase1_dict["dishes"],
                    "total_meal_nutrients": {
                        "calories_kcal": total_calories,
                        "protein_g": total_calories * 0.15,  # ä»®ã®å€¤
                        "carbohydrates_g": total_calories * 0.55,  # ä»®ã®å€¤
                        "fat_g": total_calories * 0.30,  # ä»®ã®å€¤
                    }
                },
                "metadata": {
                    "pipeline_version": "v2.0",
                    "timestamp": datetime.now().isoformat(),
                    "components_used": ["Phase1Component", self.search_component_name],
                    "nutrition_search_method": "local_database" if self.use_local_nutrition_search else "usda_api"
                }
            }
            
            # ResultManagerã«æœ€çµ‚çµæžœã‚’è¨­å®š
            if result_manager:
                result_manager.set_final_result(complete_result)
                result_manager.finalize_pipeline()
            
            # çµæžœã®ä¿å­˜
            saved_files = {}
            if save_detailed_logs and result_manager:
                # æ–°ã—ã„ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ä¿å­˜æ–¹å¼
                saved_files = result_manager.save_phase_results()
                complete_result["analysis_folder"] = result_manager.get_analysis_folder_path()
                complete_result["saved_files"] = saved_files
                
                logger.info(f"[{analysis_id}] Detailed logs saved to folder: {result_manager.get_analysis_folder_path()}")
                logger.info(f"[{analysis_id}] Saved {len(saved_files)} files across all phases")
            
            if save_results:
                # é€šå¸¸ã®çµæžœä¿å­˜ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
                saved_file = f"analysis_results/meal_analysis_{analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                complete_result["legacy_saved_to"] = saved_file
            
            self.logger.info(f"[{analysis_id}] Complete analysis pipeline finished successfully in {processing_time:.2f}s")
            
            return complete_result
            
        except Exception as e:
            self.logger.error(f"[{analysis_id}] Complete analysis failed: {str(e)}", exc_info=True)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ResultManagerã‚’ä¿å­˜
            if result_manager:
                result_manager.set_final_result({"error": str(e), "timestamp": datetime.now().isoformat()})
                result_manager.finalize_pipeline()
                error_saved_files = result_manager.save_phase_results()
                self.logger.info(f"[{analysis_id}] Error analysis logs saved to folder: {result_manager.get_analysis_folder_path()}")
            
            raise
    
    def get_pipeline_info(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’å–å¾—"""
        return {
            "pipeline_id": self.pipeline_id,
            "version": "v2.0",
            "nutrition_search_method": "local_database" if self.use_local_nutrition_search else "usda_api",
            "components": [
                {
                    "component_name": "Phase1Component",
                    "component_type": "analysis",
                    "execution_count": 0
                },
                {
                    "component_name": self.search_component_name,
                    "component_type": "nutrition_search",
                    "execution_count": 0
                }
            ]
        } 
```

============================================================

ðŸ“„ FILE: app_v2/pipeline/result_manager.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 30,369 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 12:55:37
å­˜åœ¨: âœ…

CONTENT:
```
import json
import os
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class DetailedExecutionLog:
    """å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è©³ç´°å®Ÿè¡Œãƒ­ã‚°"""
    
    def __init__(self, component_name: str, execution_id: str):
        self.component_name = component_name
        self.execution_id = execution_id
        self.execution_start_time = datetime.now()
        self.execution_end_time = None
        self.input_data = {}
        self.output_data = {}
        self.processing_details = {}
        self.prompts_used = {}
        self.reasoning = {}
        self.confidence_scores = {}
        self.warnings = []
        self.errors = []
        
    def set_input(self, input_data: Dict[str, Any]):
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ï¼ˆæ©Ÿå¯†æƒ…å ±ã¯é™¤å¤–ï¼‰"""
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯å¤§ãã™ãŽã‚‹ã®ã§ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜
        safe_input = {}
        for key, value in input_data.items():
            if key == 'image_bytes':
                safe_input[key] = {
                    "size_bytes": len(value) if value else 0,
                    "type": "binary_image_data"
                }
            else:
                safe_input[key] = value
        self.input_data = safe_input
    
    def set_output(self, output_data: Dict[str, Any]):
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        self.output_data = output_data
        
    def add_prompt(self, prompt_name: str, prompt_content: str, variables: Dict[str, Any] = None):
        """ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨˜éŒ²"""
        self.prompts_used[prompt_name] = {
            "content": prompt_content,
            "variables": variables or {},
            "timestamp": datetime.now().isoformat()
        }
    
    def add_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æŽ¨è«–ç†ç”±ã‚’è¨˜éŒ²"""
        self.reasoning[decision_point] = {
            "reason": reason,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat()
        }
    
    def add_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’è¨˜éŒ²"""
        self.processing_details[detail_key] = detail_value
    
    def add_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²"""
        self.confidence_scores[metric_name] = score
    
    def add_warning(self, warning: str):
        """è­¦å‘Šã‚’è¨˜éŒ²"""
        self.warnings.append({
            "message": warning,
            "timestamp": datetime.now().isoformat()
        })
    
    def add_error(self, error: str):
        """ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²"""
        self.errors.append({
            "message": error,
            "timestamp": datetime.now().isoformat()
        })
    
    def finalize(self):
        """å®Ÿè¡Œå®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.execution_end_time = datetime.now()
    
    def get_execution_time(self) -> float:
        """å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆç§’ï¼‰"""
        if self.execution_end_time:
            return (self.execution_end_time - self.execution_start_time).total_seconds()
        return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾žæ›¸å½¢å¼ã§å–å¾—"""
        return {
            "component_name": self.component_name,
            "execution_id": self.execution_id,
            "execution_start_time": self.execution_start_time.isoformat(),
            "execution_end_time": self.execution_end_time.isoformat() if self.execution_end_time else None,
            "execution_time_seconds": self.get_execution_time(),
            "input_data": self.input_data,
            "output_data": self.output_data,
            "processing_details": self.processing_details,
            "prompts_used": self.prompts_used,
            "reasoning": self.reasoning,
            "confidence_scores": self.confidence_scores,
            "warnings": self.warnings,
            "errors": self.errors
        }


class ResultManager:
    """è§£æžçµæžœã¨è©³ç´°ãƒ­ã‚°ã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆãƒ•ã‚§ãƒ¼ã‚ºåˆ¥æ•´ç†ç‰ˆï¼‰"""
    
    def __init__(self, analysis_id: str, save_directory: str = "analysis_results"):
        self.analysis_id = analysis_id
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Ÿè¡Œã”ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.analysis_folder_name = f"analysis_{self.timestamp}_{self.analysis_id}"
        self.analysis_dir = Path(save_directory) / self.analysis_folder_name
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.phase1_dir = self.analysis_dir / "phase1"
        self.nutrition_search_dir = self.analysis_dir / "nutrition_search_query"
        self.phase2_dir = self.analysis_dir / "phase2"
        self.nutrition_dir = self.analysis_dir / "nutrition_calculation"
        
        for phase_dir in [self.phase1_dir, self.nutrition_search_dir, self.phase2_dir, self.nutrition_dir]:
            phase_dir.mkdir(exist_ok=True)
        
        self.pipeline_start_time = datetime.now()
        self.pipeline_end_time = None
        self.execution_logs: List[DetailedExecutionLog] = []
        self.final_result = {}
        self.pipeline_metadata = {
            "analysis_id": analysis_id,
            "version": "v2.0",
            "analysis_folder": self.analysis_folder_name,
            "pipeline_start_time": self.pipeline_start_time.isoformat()
        }
        
    def create_execution_log(self, component_name: str, execution_id: str) -> DetailedExecutionLog:
        """æ–°ã—ã„å®Ÿè¡Œãƒ­ã‚°ã‚’ä½œæˆ"""
        log = DetailedExecutionLog(component_name, execution_id)
        self.execution_logs.append(log)
        return log
    
    def set_final_result(self, result: Dict[str, Any]):
        """æœ€çµ‚çµæžœã‚’è¨­å®š"""
        self.final_result = result
        
    def finalize_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.pipeline_end_time = datetime.now()
        self.pipeline_metadata["pipeline_end_time"] = self.pipeline_end_time.isoformat()
        self.pipeline_metadata["total_execution_time_seconds"] = (
            self.pipeline_end_time - self.pipeline_start_time
        ).total_seconds()
    
    def save_phase_results(self) -> Dict[str, str]:
        """ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã«çµæžœã‚’ä¿å­˜"""
        saved_files = {}
        
        # å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ã‚°ã‚’å‡¦ç†
        executed_components = set()
        for log in self.execution_logs:
            if log.component_name == "Phase1Component":
                files = self._save_phase1_results(log)
                saved_files.update(files)
                executed_components.add("Phase1Component")
            elif log.component_name in ["USDAQueryComponent", "LocalNutritionSearchComponent"]:
                files = self._save_nutrition_search_results(log)
                saved_files.update(files)
                executed_components.add(log.component_name)
            elif log.component_name == "Phase2Component":
                files = self._save_phase2_results(log)
                saved_files.update(files)
                executed_components.add("Phase2Component")
            elif log.component_name == "NutritionCalculationComponent":
                files = self._save_nutrition_results(log)
                saved_files.update(files)
                executed_components.add("NutritionCalculationComponent")
        
        # æœªå®Ÿè£…/æœªå®Ÿè¡Œã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        if "Phase2Component" not in executed_components:
            placeholder_log = DetailedExecutionLog("Phase2Component", f"{self.analysis_id}_phase2_placeholder")
            placeholder_log.input_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.output_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.finalize()
            files = self._save_phase2_results(placeholder_log)
            saved_files.update(files)
        
        if "NutritionCalculationComponent" not in executed_components:
            placeholder_log = DetailedExecutionLog("NutritionCalculationComponent", f"{self.analysis_id}_nutrition_placeholder")
            placeholder_log.input_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.output_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.finalize()
            files = self._save_nutrition_results(placeholder_log)
            saved_files.update(files)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒžãƒªãƒ¼ã‚’ä¿å­˜
        summary_files = self._save_pipeline_summary()
        saved_files.update(summary_files)
        
        return saved_files
    
    def _save_phase1_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase1ã®çµæžœã‚’ä¿å­˜"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase1_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time()
            }, f, indent=2, ensure_ascii=False)
        files["phase1_input_output"] = str(input_output_file)
        
        # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æŽ¨è«–ç†ç”±ã®ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        prompts_md_file = self.phase1_dir / "prompts_and_reasoning.md"
        prompts_content = self._generate_phase1_prompts_md(log)
        with open(prompts_md_file, 'w', encoding='utf-8') as f:
            f.write(prompts_content)
        files["phase1_prompts_md"] = str(prompts_md_file)
        
        # 3. æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆ
        detected_items_file = self.phase1_dir / "detected_items.txt"
        detected_content = self._generate_phase1_detected_items_txt(log)
        with open(detected_items_file, 'w', encoding='utf-8') as f:
            f.write(detected_content)
        files["phase1_detected_txt"] = str(detected_items_file)
        
        return files
    
    def _save_nutrition_search_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµæžœã‚’ä¿å­˜ï¼ˆUSDAQueryComponentã€LocalNutritionSearchComponentä¸¡å¯¾å¿œï¼‰"""
        files = {}
        
        # æ¤œç´¢æ–¹æ³•ã®åˆ¤å®š
        search_method = "unknown"
        db_source = "unknown"
        
        if log.component_name == "USDAQueryComponent":
            search_method = "usda_api"
            db_source = "usda_database"
        elif log.component_name == "LocalNutritionSearchComponent":
            search_method = "local_search"
            db_source = "local_nutrition_database"
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢æ–¹æ³•æƒ…å ±ã‚’å«ã‚€ï¼‰
        input_output_file = self.nutrition_search_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "search_metadata": {
                    "component_name": log.component_name,
                    "search_method": search_method,
                    "database_source": db_source,
                    "timestamp": log.execution_end_time.isoformat() if log.execution_end_time else None
                }
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_search_input_output"] = str(input_output_file)
        
        # 2. æ¤œç´¢çµæžœã®è©³ç´°ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        search_results_md_file = self.nutrition_search_dir / "search_results.md"
        search_content = self._generate_nutrition_search_results_md(log, search_method, db_source)
        with open(search_results_md_file, 'w', encoding='utf-8') as f:
            f.write(search_content)
        files["nutrition_search_results_md"] = str(search_results_md_file)
        
        # 3. ãƒžãƒƒãƒè©³ç´°ã®ãƒ†ã‚­ã‚¹ãƒˆ
        match_details_file = self.nutrition_search_dir / "match_details.txt"
        match_content = self._generate_nutrition_match_details_txt(log, search_method, db_source)
        with open(match_details_file, 'w', encoding='utf-8') as f:
            f.write(match_content)
        files["nutrition_search_match_details"] = str(match_details_file)
        
        return files
    
    def _save_phase2_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase2ã®çµæžœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase2_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "Phase2Component is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["phase2_input_output"] = str(input_output_file)
        
        # 2. æˆ¦ç•¥æ±ºå®šã®ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        strategy_md_file = self.phase2_dir / "strategy_decisions.md"
        with open(strategy_md_file, 'w', encoding='utf-8') as f:
            f.write("# Phase2 Strategy Decisions\n\n*Phase2Component is not yet implemented*\n")
        files["phase2_strategy_md"] = str(strategy_md_file)
        
        # 3. é¸æŠžé …ç›®ã®ãƒ†ã‚­ã‚¹ãƒˆ
        selected_items_file = self.phase2_dir / "selected_items.txt"
        with open(selected_items_file, 'w', encoding='utf-8') as f:
            f.write("Phase2Component is not yet implemented\n")
        files["phase2_items_txt"] = str(selected_items_file)
        
        return files
    
    def _save_nutrition_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šè¨ˆç®—ã®çµæžœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.nutrition_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "NutritionCalculationComponent is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_input_output"] = str(input_output_file)
        
        # 2. è¨ˆç®—å¼ã®ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        formulas_md_file = self.nutrition_dir / "calculation_formulas.md"
        with open(formulas_md_file, 'w', encoding='utf-8') as f:
            f.write("# Nutrition Calculation Formulas\n\n*NutritionCalculationComponent is not yet implemented*\n")
        files["nutrition_formulas_md"] = str(formulas_md_file)
        
        # 3. æ „é¤Šã‚µãƒžãƒªãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆ
        summary_txt_file = self.nutrition_dir / "nutrition_summary.txt"
        with open(summary_txt_file, 'w', encoding='utf-8') as f:
            f.write("NutritionCalculationComponent is not yet implemented\n")
        files["nutrition_summary_txt"] = str(summary_txt_file)
        
        return files
    
    def _save_pipeline_summary(self) -> Dict[str, str]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒžãƒªãƒ¼ã‚’ä¿å­˜"""
        files = {}
        
        # 1. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚µãƒžãƒªãƒ¼JSON
        summary_file = self.analysis_dir / "pipeline_summary.json"
        summary_data = {
            "analysis_id": self.analysis_id,
            "timestamp": self.timestamp,
            "pipeline_metadata": self.pipeline_metadata,
            "execution_summary": {
                log.component_name: {
                    "execution_time": log.get_execution_time(),
                    "success": len(log.errors) == 0,
                    "warnings_count": len(log.warnings),
                    "errors_count": len(log.errors)
                }
                for log in self.execution_logs
            },
            "final_result": self.final_result
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        files["pipeline_summary"] = str(summary_file)
        
        # 2. å®Œå…¨ãªè©³ç´°ãƒ­ã‚°JSON
        complete_log_file = self.analysis_dir / "complete_analysis_log.json"
        complete_data = {
            "pipeline_metadata": self.pipeline_metadata,
            "execution_logs": [log.to_dict() for log in self.execution_logs],
            "final_result": self.final_result,
            "summary": {
                "total_components": len(self.execution_logs),
                "total_warnings": sum(len(log.warnings) for log in self.execution_logs),
                "total_errors": sum(len(log.errors) for log in self.execution_logs)
            }
        }
        
        with open(complete_log_file, 'w', encoding='utf-8') as f:
            json.dump(complete_data, f, indent=2, ensure_ascii=False)
        files["complete_log"] = str(complete_log_file)
        
        return files
    
    def _generate_phase1_prompts_md(self, log: DetailedExecutionLog) -> str:
        """Phase1ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æŽ¨è«–ç†ç”±ã®ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆ"""
        content = f"""# Phase1: ç”»åƒåˆ†æž - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æŽ¨è«–

## å®Ÿè¡Œæƒ…å ±
- å®Ÿè¡ŒID: {log.execution_id}
- é–‹å§‹æ™‚åˆ»: {log.execution_start_time.isoformat()}
- çµ‚äº†æ™‚åˆ»: {log.execution_end_time.isoformat() if log.execution_end_time else 'N/A'}
- å®Ÿè¡Œæ™‚é–“: {log.get_execution_time():.2f}ç§’

## ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
        for prompt_name, prompt_data in log.prompts_used.items():
            content += f"### {prompt_name.replace('_', ' ').title()}\n\n"
            content += f"**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—**: {prompt_data['timestamp']}\n\n"
            content += f"```\n{prompt_data['content']}\n```\n\n"
            
            if prompt_data.get('variables'):
                content += f"**å¤‰æ•°**:\n"
                for var_name, var_value in prompt_data['variables'].items():
                    content += f"- {var_name}: {var_value}\n"
                content += "\n"
        
        # æŽ¨è«–ç†ç”±
        content += "## AIæŽ¨è«–ã®è©³ç´°\n\n"
        
        # æ–™ç†è­˜åˆ¥ã®æŽ¨è«–
        dish_reasoning = [r for r in log.reasoning.items() if r[0].startswith('dish_identification_')]
        if dish_reasoning:
            content += "### æ–™ç†è­˜åˆ¥ã®æŽ¨è«–\n\n"
            for decision_point, reasoning_data in dish_reasoning:
                dish_num = decision_point.split('_')[-1]
                content += f"**æ–™ç† {dish_num}**:\n"
                content += f"- æŽ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # é£Ÿæé¸æŠžã®æŽ¨è«–
        ingredient_reasoning = [r for r in log.reasoning.items() if r[0].startswith('ingredient_selection_')]
        if ingredient_reasoning:
            content += "### é£Ÿæé¸æŠžã®æŽ¨è«–\n\n"
            for decision_point, reasoning_data in ingredient_reasoning:
                content += f"**{decision_point.replace('_', ' ').title()}**:\n"
                content += f"- æŽ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼
        if log.warnings:
            content += "## è­¦å‘Š\n\n"
            for warning in log.warnings:
                content += f"- {warning['message']} (at {warning['timestamp']})\n"
            content += "\n"
        
        if log.errors:
            content += "## ã‚¨ãƒ©ãƒ¼\n\n"
            for error in log.errors:
                content += f"- {error['message']} (at {error['timestamp']})\n"
            content += "\n"
        
        return content
    
    def _generate_phase1_detected_items_txt(self, log: DetailedExecutionLog) -> str:
        """Phase1ã§æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰"""
        content = f"Phase1 æ¤œå‡ºçµæžœ - {log.execution_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += "=" * 60 + "\n\n"
        
        if 'output_data' in log.output_data and 'dishes' in log.output_data['output_data']:
            dishes = log.output_data['output_data']['dishes']
            content += f"æ¤œå‡ºã•ã‚ŒãŸæ–™ç†æ•°: {len(dishes)}\n\n"
            
            for i, dish in enumerate(dishes, 1):
                content += f"æ–™ç† {i}: {dish['dish_name']}\n"
                content += f"  é£Ÿææ•°: {len(dish['ingredients'])}\n"
                content += "  é£Ÿæè©³ç´°:\n"
                
                for j, ingredient in enumerate(dish['ingredients'], 1):
                    content += f"    {j}. {ingredient['ingredient_name']}\n"
                content += "\n"
        
        # USDAæ¤œç´¢æº–å‚™æƒ…å ±
        if 'usda_search_terms' in log.processing_details:
            search_terms = log.processing_details['usda_search_terms']
            content += f"USDAæ¤œç´¢èªžå½™ ({len(search_terms)}å€‹):\n"
            for i, term in enumerate(search_terms, 1):
                content += f"  {i}. {term}\n"
            content += "\n"
        
        # å‡¦ç†è©³ç´°
        if log.processing_details:
            content += "å‡¦ç†è©³ç´°:\n"
            for detail_key, detail_value in log.processing_details.items():
                if detail_key == 'usda_search_terms':
                    continue  # æ—¢ã«ä¸Šã§è¡¨ç¤ºæ¸ˆã¿
                if isinstance(detail_value, (dict, list)):
                    content += f"  {detail_key}: {json.dumps(detail_value, ensure_ascii=False)}\n"
                else:
                    content += f"  {detail_key}: {detail_value}\n"
        
        return content
    
    def _generate_nutrition_search_results_md(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæžœã®ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆï¼ˆUSDA/ãƒ­ãƒ¼ã‚«ãƒ«å¯¾å¿œï¼‰"""
        content = []
        
        content.append(f"# Nutrition Database Search Results")
        content.append(f"")
        content.append(f"**Search Method:** {search_method}")
        content.append(f"**Database Source:** {db_source}")
        content.append(f"**Component:** {log.component_name}")
        content.append(f"**Execution Time:** {log.get_execution_time():.3f} seconds")
        content.append(f"**Timestamp:** {log.execution_start_time.isoformat()}")
        content.append(f"")
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.input_data:
            content.append(f"## Input Data")
            if 'ingredient_names' in log.input_data:
                ingredients = log.input_data['ingredient_names']
                content.append(f"**Ingredients ({len(ingredients)}):** {', '.join(ingredients)}")
            
            if 'dish_names' in log.input_data:
                dishes = log.input_data['dish_names']
                content.append(f"**Dishes ({len(dishes)}):** {', '.join(dishes)}")
            content.append(f"")
        
        # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            content.append(f"## Search Results")
            content.append(f"**Total Matches:** {len(matches)}")
            content.append(f"")
            
            for i, (search_term, match_data) in enumerate(matches.items(), 1):
                content.append(f"### {i}. {search_term}")
                if isinstance(match_data, dict):
                    content.append(f"**ID:** {match_data.get('id', 'N/A')}")
                    content.append(f"**Description:** {match_data.get('description', 'N/A')}")
                    content.append(f"**Data Type:** {match_data.get('data_type', 'N/A')}")
                    content.append(f"**Source:** {match_data.get('source', 'N/A')}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        content.append(f"**Nutrients ({len(match_data['nutrients'])}):**")
                        for nutrient in match_data['nutrients'][:5]:  # æœ€åˆã®5ã¤ã ã‘è¡¨ç¤º
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                content.append(f"  - {name}: {amount} {unit}")
                        if len(match_data['nutrients']) > 5:
                            content.append(f"  - ... and {len(match_data['nutrients']) - 5} more nutrients")
                content.append(f"")
        
        # æ¤œç´¢ã‚µãƒžãƒªãƒ¼
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            content.append(f"## Search Summary")
            content.append(f"**Total Searches:** {summary.get('total_searches', 0)}")
            content.append(f"**Successful Matches:** {summary.get('successful_matches', 0)}")
            content.append(f"**Failed Searches:** {summary.get('failed_searches', 0)}")
            content.append(f"**Match Rate:** {summary.get('match_rate_percent', 0)}%")
            content.append(f"**Search Method:** {summary.get('search_method', 'unknown')}")
            content.append(f"")
        
        # æŽ¨è«–ç†ç”±ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.reasoning:
            content.append(f"## Search Reasoning")
            for decision_point, reason_data in log.reasoning.items():
                reason = reason_data.get('reason', '') if isinstance(reason_data, dict) else str(reason_data)
                content.append(f"**{decision_point}:** {reason}")
            content.append(f"")
        
        # è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.warnings:
            content.append(f"## Warnings")
            for warning in log.warnings:
                content.append(f"- {warning}")
            content.append(f"")
        
        if log.errors:
            content.append(f"## Errors")
            for error in log.errors:
                content.append(f"- {error}")
            content.append(f"")
        
        return "\n".join(content)
    
    def _generate_nutrition_match_details_txt(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®ãƒžãƒƒãƒè©³ç´°ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆUSDA/ãƒ­ãƒ¼ã‚«ãƒ«å¯¾å¿œï¼‰"""
        lines = []
        
        lines.append(f"Nutrition Database Search Match Details")
        lines.append(f"=" * 50)
        lines.append(f"Search Method: {search_method}")
        lines.append(f"Database Source: {db_source}")
        lines.append(f"Component: {log.component_name}")
        lines.append(f"Execution Time: {log.get_execution_time():.3f} seconds")
        lines.append(f"Timestamp: {log.execution_start_time.isoformat()}")
        lines.append(f"")
        
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            lines.append(f"Total Matches: {len(matches)}")
            lines.append(f"")
            
            for search_term, match_data in matches.items():
                lines.append(f"Search Term: {search_term}")
                lines.append(f"-" * 30)
                
                if isinstance(match_data, dict):
                    lines.append(f"  ID: {match_data.get('id', 'N/A')}")
                    lines.append(f"  Description: {match_data.get('description', 'N/A')}")
                    lines.append(f"  Data Type: {match_data.get('data_type', 'N/A')}")
                    lines.append(f"  Source: {match_data.get('source', 'N/A')}")
                    lines.append(f"  Score: {match_data.get('score', 'N/A')}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        lines.append(f"  Nutrients ({len(match_data['nutrients'])}):")
                        for nutrient in match_data['nutrients']:
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                lines.append(f"    - {name}: {amount} {unit}")
                    
                    if 'original_data' in match_data:
                        original_data = match_data['original_data']
                        if isinstance(original_data, dict):
                            lines.append(f"  Original Data Source: {original_data.get('source', 'Unknown')}")
                            if search_method == "local_search":
                                lines.append(f"  Local DB Source: {original_data.get('db_source', 'Unknown')}")
                
                lines.append(f"")
        
        # æ¤œç´¢çµ±è¨ˆ
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            lines.append(f"Search Statistics:")
            lines.append(f"  Total Searches: {summary.get('total_searches', 0)}")
            lines.append(f"  Successful Matches: {summary.get('successful_matches', 0)}")
            lines.append(f"  Failed Searches: {summary.get('failed_searches', 0)}")
            lines.append(f"  Match Rate: {summary.get('match_rate_percent', 0)}%")
            
            if search_method == "local_search":
                lines.append(f"  Total Database Items: {summary.get('total_database_items', 0)}")
        
        return "\n".join(lines)
    
    def get_analysis_folder_path(self) -> str:
        """è§£æžãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—"""
        return str(self.analysis_dir) 
```

============================================================

ðŸ“„ FILE: app_v2/services/__init__.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 228 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:00:07
å­˜åœ¨: âœ…

CONTENT:
```
from .gemini_service import GeminiService
from .usda_service import USDAService  
from .nutrition_calculation_service import NutritionCalculationService

__all__ = ["GeminiService", "USDAService", "NutritionCalculationService"]

```

============================================================

ðŸ“„ FILE: app_v2/services/gemini_service.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 11,545 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 13:41:30
å­˜åœ¨: âœ…

CONTENT:
```
import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig, HarmCategory, HarmBlockThreshold
from typing import Dict, Optional
import json
import logging
from PIL import Image
import io

from ..config.prompts import Phase1Prompts, Phase2Prompts

logger = logging.getLogger(__name__)

# Geminiã®æ§‹é€ åŒ–å‡ºåŠ›ã®ãŸã‚ã®JSONã‚¹ã‚­ãƒ¼ãƒžã‚’å®šç¾©ï¼ˆUSDAæ¤œç´¢ç‰¹åŒ–ï¼‰
MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
            "description": "ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰ã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰ã€‚"},
                    "ingredients": {
                        "type": "array",
                        "description": "ã“ã®æ–™ç†ã«å«ã¾ã‚Œã‚‹ã¨æŽ¨å®šã•ã‚Œã‚‹ææ–™ã®ãƒªã‚¹ãƒˆï¼ˆUSDAæ¤œç´¢ç”¨ï¼‰ã€‚",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "ææ–™ã®åç§°ï¼ˆUSDAæ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰ã€‚"}
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}

REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
            "description": "ç”»åƒã‹ã‚‰ç‰¹å®šãƒ»ç²¾ç·»åŒ–ã•ã‚ŒãŸæ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "ç‰¹å®šã•ã‚ŒãŸæ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®åç§°ã€‚"},
                    "type": {"type": "string", "description": "æ–™ç†ã®ç¨®é¡žï¼ˆä¾‹: ä¸»èœ, å‰¯èœ, å˜å“é£Ÿå“ï¼‰ã€‚"},
                    "quantity_on_plate": {"type": "string", "description": "çš¿ã®ä¸Šã®é‡ã€‚"},
                    "calculation_strategy": {
                        "type": "string",
                        "enum": ["dish_level", "ingredient_level"],
                        "description": "ã“ã®ã‚¢ã‚¤ãƒ†ãƒ ã®æ „é¤Šè¨ˆç®—æ–¹é‡ã€‚"
                    },
                    "fdc_id": {
                        "type": "integer",
                        "description": "calculation_strategyãŒ'dish_level'ã®å ´åˆã€ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“ã®FDC IDã€‚ãã‚Œä»¥å¤–ã¯nullã€‚"
                    },
                    "usda_source_description": {
                        "type": "string",
                        "description": "calculation_strategyãŒ'dish_level'ã®å ´åˆã€ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ å…¨ä½“ã®USDAå…¬å¼åç§°ã€‚ãã‚Œä»¥å¤–ã¯nullã€‚"
                    },
                    "ingredients": {
                        "type": "array",
                        "description": "ã“ã®æ–™ç†/é£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã«å«ã¾ã‚Œã‚‹ã¨æŽ¨å®šã•ã‚Œã‚‹ææ–™ã®ãƒªã‚¹ãƒˆã€‚",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "ææ–™ã®åç§°ã€‚"},
                                "fdc_id": {
                                    "type": "integer",
                                    "description": "calculation_strategyãŒ'ingredient_level'ã®å ´åˆã€ã“ã®ææ–™ã®FDC IDã€‚ãã‚Œä»¥å¤–ã¯nullã¾ãŸã¯çœç•¥å¯ã€‚"
                                },
                                "usda_source_description": {
                                    "type": "string",
                                    "description": "calculation_strategyãŒ'ingredient_level'ã®å ´åˆã€ã“ã®ææ–™ã®USDAå…¬å¼åç§°ã€‚ãã‚Œä»¥å¤–ã¯nullã¾ãŸã¯çœç•¥å¯ã€‚"
                                }
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "type", "quantity_on_plate", "calculation_strategy", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}


class GeminiService:
    """Vertex AIçµŒç”±ã§Geminiã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æžã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_id: str, location: str, model_name: str = "gemini-2.5-flash-preview-05-20"):
        """
        åˆæœŸåŒ–
        
        Args:
            project_id: GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
            location: Vertex AIã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: us-central1ï¼‰
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        # Vertex AIã®åˆæœŸåŒ–
        vertexai.init(project=project_id, location=location)
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.model = GenerativeModel(model_name=model_name)
        
        # generation_configã‚’ä½œæˆ (Phase1ç”¨ - å‡ºåŠ›å®‰å®šåŒ–)
        self.generation_config = GenerationConfig(
            temperature=0.0,  # å®Œå…¨ã«deterministicã«
            top_p=1.0,       # nucleus samplingã‚’ç„¡åŠ¹åŒ–
            top_k=1,         # æœ€ã‚‚ç¢ºçŽ‡ã®é«˜ã„é¸æŠžè‚¢ã®ã¿
            max_output_tokens=8192,
            candidate_count=1,  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å€™è£œã‚’1ã¤ã«åˆ¶é™
            response_mime_type="application/json",
            response_schema=MEAL_ANALYSIS_GEMINI_SCHEMA
        )
        
        # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }
    
    async def analyze_phase1(
        self, 
        image_bytes: bytes, 
        image_mime_type: str, 
        optional_text: Optional[str] = None
    ) -> Dict:
        """
        Phase1: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æžã—ã¦é£Ÿäº‹æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜Ž
            
        Returns:
            åˆ†æžçµæžœã®è¾žæ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt = Phase1Prompts.get_system_prompt()
            user_prompt = Phase1Prompts.get_user_prompt(optional_text)
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase1 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e
    
    async def analyze_phase2(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        usda_candidates_text: str,
        initial_analysis_data: str
    ) -> Dict:
        """
        Phase2: USDAã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’å†åˆ†æž
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            usda_candidates_text: USDAå€™è£œæƒ…å ±ã®ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
            initial_analysis_data: Phase1ã®AIå‡ºåŠ›ï¼ˆJSONæ–‡å­—åˆ—ï¼‰
            
        Returns:
            ç²¾ç·»åŒ–ã•ã‚ŒãŸåˆ†æžçµæžœã®è¾žæ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt = Phase2Prompts.get_system_prompt()
            user_prompt = Phase2Prompts.get_user_prompt(
                usda_candidates_text=usda_candidates_text,
                initial_analysis_data=initial_analysis_data
            )
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Phase2ç”¨ã®Generation Config (å‡ºåŠ›å®‰å®šåŒ–)
            phase2_generation_config = GenerationConfig(
                temperature=0.0,  # å®Œå…¨ã«deterministicã«
                top_p=1.0,       # nucleus samplingã‚’ç„¡åŠ¹åŒ–
                top_k=1,         # æœ€ã‚‚ç¢ºçŽ‡ã®é«˜ã„é¸æŠžè‚¢ã®ã¿
                max_output_tokens=8192,
                candidate_count=1,  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å€™è£œã‚’1ã¤ã«åˆ¶é™
                response_mime_type="application/json",
                response_schema=REFINED_MEAL_ANALYSIS_GEMINI_SCHEMA
            )
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=phase2_generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase2 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e 
```

============================================================

ðŸ“„ FILE: app_v2/services/nutrition_calculation_service.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 7,017 bytes
æœ€çµ‚æ›´æ–°: 2025-06-05 12:59:59
å­˜åœ¨: âœ…

CONTENT:
```
"""
æ „é¤Šç´ è¨ˆç®—ã‚µãƒ¼ãƒ“ã‚¹

ã“ã®ã‚µãƒ¼ãƒ“ã‚¹ã¯ç´”ç²‹ãªè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚’æä¾›ã—ã¾ã™ï¼š
1. 100gã‚ãŸã‚Šã®æ „é¤Šç´ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã®æ „é¤Šç´ ã‚’è¨ˆç®—
2. é£Ÿæãƒªã‚¹ãƒˆã‹ã‚‰æ–™ç†å…¨ä½“ã®æ „é¤Šç´ ã‚’é›†è¨ˆ
3. æ–™ç†ãƒªã‚¹ãƒˆã‹ã‚‰é£Ÿäº‹å…¨ä½“ã®æ „é¤Šç´ ã‚’é›†è¨ˆ
"""

import logging
from typing import List, Optional, Dict
from ..models.nutrition_models import CalculatedNutrients
from ..models.phase2_models import RefinedIngredient, RefinedDish

logger = logging.getLogger(__name__)


class NutritionCalculationService:
    """æ „é¤Šç´ è¨ˆç®—ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def calculate_actual_nutrients(
        key_nutrients_per_100g: Dict[str, float], 
        estimated_weight_g: float
    ) -> CalculatedNutrients:
        """
        100gã‚ãŸã‚Šã®ä¸»è¦æ „é¤Šç´ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã®æ „é¤Šç´ é‡ã‚’è¨ˆç®—
        
        Args:
            key_nutrients_per_100g: 100gã‚ãŸã‚Šã®ä¸»è¦æ „é¤Šç´ ãƒ‡ãƒ¼ã‚¿
            estimated_weight_g: æŽ¨å®šã‚°ãƒ©ãƒ æ•°
            
        Returns:
            CalculatedNutrients: è¨ˆç®—æ¸ˆã¿æ „é¤Šç´ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if not key_nutrients_per_100g or estimated_weight_g <= 0:
            logger.warning(f"Invalid input: key_nutrients_per_100g={key_nutrients_per_100g}, estimated_weight_g={estimated_weight_g}")
            return CalculatedNutrients()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå…¨ã¦0.0ï¼‰ã‚’è¿”ã™
        
        try:
            # è¨ˆç®—å¼: (Nutrient_Value_per_100g / 100) Ã— estimated_weight_g
            multiplier = estimated_weight_g / 100.0
            
            # å„æ „é¤Šç´ ã‚’è¨ˆç®—ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„/Noneã®å ´åˆã¯0.0ã¨ã—ã¦æ‰±ã†ï¼‰
            calories_kcal = round((key_nutrients_per_100g.get('calories_kcal', 0.0) or 0.0) * multiplier, 2)
            protein_g = round((key_nutrients_per_100g.get('protein_g', 0.0) or 0.0) * multiplier, 2)
            carbohydrates_g = round((key_nutrients_per_100g.get('carbohydrates_g', 0.0) or 0.0) * multiplier, 2)
            fat_g = round((key_nutrients_per_100g.get('fat_g', 0.0) or 0.0) * multiplier, 2)
            
            result = CalculatedNutrients(
                calories_kcal=calories_kcal,
                protein_g=protein_g,
                carbohydrates_g=carbohydrates_g,
                fat_g=fat_g
            )
            
            logger.debug(f"Calculated nutrients for {estimated_weight_g}g: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Error calculating actual nutrients: {e}")
            return CalculatedNutrients()  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
    
    @staticmethod
    def aggregate_nutrients_for_dish_from_ingredients(
        ingredients: List[RefinedIngredient]
    ) -> CalculatedNutrients:
        """
        ææ–™ãƒªã‚¹ãƒˆã‹ã‚‰æ–™ç†å…¨ä½“ã®æ „é¤Šç´ ã‚’é›†è¨ˆ
        
        Args:
            ingredients: RefinedIngredientã®ãƒªã‚¹ãƒˆï¼ˆå„è¦ç´ ã¯è¨ˆç®—æ¸ˆã¿ã®actual_nutrientsã‚’æŒã¤ï¼‰
            
        Returns:
            CalculatedNutrients: æ–™ç†ã®é›†è¨ˆæ „é¤Šç´ 
        """
        if not ingredients:
            logger.warning("No ingredients provided for aggregation")
            return CalculatedNutrients()
        
        try:
            total_calories = 0.0
            total_protein = 0.0
            total_carbohydrates = 0.0
            total_fat = 0.0
            
            calculated_count = 0
            
            for ingredient in ingredients:
                if ingredient.actual_nutrients:
                    total_calories += ingredient.actual_nutrients.calories_kcal
                    total_protein += ingredient.actual_nutrients.protein_g
                    total_carbohydrates += ingredient.actual_nutrients.carbohydrates_g
                    total_fat += ingredient.actual_nutrients.fat_g
                    calculated_count += 1
                else:
                    logger.warning(f"Ingredient '{ingredient.ingredient_name}' has no actual_nutrients")
            
            # å°æ•°ç‚¹ä»¥ä¸‹2æ¡ã«ä¸¸ã‚ã‚‹
            result = CalculatedNutrients(
                calories_kcal=round(total_calories, 2),
                protein_g=round(total_protein, 2),
                carbohydrates_g=round(total_carbohydrates, 2),
                fat_g=round(total_fat, 2)
            )
            
            logger.info(f"Aggregated nutrients from {calculated_count}/{len(ingredients)} ingredients: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Error aggregating nutrients for dish: {e}")
            return CalculatedNutrients()
    
    @staticmethod
    def aggregate_nutrients_for_meal(
        dishes: List[RefinedDish]
    ) -> CalculatedNutrients:
        """
        æ–™ç†ãƒªã‚¹ãƒˆã‹ã‚‰é£Ÿäº‹å…¨ä½“ã®æ „é¤Šç´ ã‚’é›†è¨ˆ
        
        Args:
            dishes: RefinedDishã®ãƒªã‚¹ãƒˆï¼ˆå„è¦ç´ ã¯è¨ˆç®—æ¸ˆã¿ã®dish_total_actual_nutrientsã‚’æŒã¤ï¼‰
            
        Returns:
            CalculatedNutrients: é£Ÿäº‹å…¨ä½“ã®ç·æ „é¤Šç´ 
        """
        if not dishes:
            logger.warning("No dishes provided for meal aggregation")
            return CalculatedNutrients()
        
        try:
            total_calories = 0.0
            total_protein = 0.0
            total_carbohydrates = 0.0
            total_fat = 0.0
            
            calculated_count = 0
            
            for dish in dishes:
                if dish.dish_total_actual_nutrients:
                    total_calories += dish.dish_total_actual_nutrients.calories_kcal
                    total_protein += dish.dish_total_actual_nutrients.protein_g
                    total_carbohydrates += dish.dish_total_actual_nutrients.carbohydrates_g
                    total_fat += dish.dish_total_actual_nutrients.fat_g
                    calculated_count += 1
                else:
                    logger.warning(f"Dish '{dish.dish_name}' has no dish_total_actual_nutrients")
            
            # å°æ•°ç‚¹ä»¥ä¸‹2æ¡ã«ä¸¸ã‚ã‚‹
            result = CalculatedNutrients(
                calories_kcal=round(total_calories, 2),
                protein_g=round(total_protein, 2),
                carbohydrates_g=round(total_carbohydrates, 2),
                fat_g=round(total_fat, 2)
            )
            
            logger.info(f"Aggregated meal nutrients from {calculated_count}/{len(dishes)} dishes: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Error aggregating nutrients for meal: {e}")
            return CalculatedNutrients()


# ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°
def get_nutrition_calculation_service() -> NutritionCalculationService:
    """
    æ „é¤Šè¨ˆç®—ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    
    Returns:
        NutritionCalculationService: æ „é¤Šè¨ˆç®—ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    return NutritionCalculationService() 
```

============================================================

ðŸ“„ FILE: app_v2/services/usda_service.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 16,395 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 13:55:24
å­˜åœ¨: âœ…

CONTENT:
```
# app/services/usda_service.py
import httpx
import json
import logging
from typing import List, Optional, Dict, Any
from functools import lru_cache

from ..config import get_settings

logger = logging.getLogger(__name__)


class USDANutrient:
    """USDAæ „é¤Šç´ æƒ…å ±ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""
    def __init__(self, name: str, amount: float, unit_name: str, 
                 nutrient_id: Optional[int] = None, 
                 nutrient_number: Optional[str] = None):
        self.name = name
        self.amount = amount
        self.unit_name = unit_name
        self.nutrient_id = nutrient_id
        self.nutrient_number = nutrient_number


class USDASearchResultItem:
    """USDAæ¤œç´¢çµæžœã‚¢ã‚¤ãƒ†ãƒ ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""
    def __init__(self, fdc_id: int, description: str, 
                 data_type: Optional[str] = None,
                 brand_owner: Optional[str] = None,
                 ingredients_text: Optional[str] = None,
                 food_nutrients: List[USDANutrient] = None,
                 score: Optional[float] = None,
                 original_data: Optional[Dict[str, Any]] = None):
        self.fdc_id = fdc_id
        self.description = description
        self.data_type = data_type
        self.brand_owner = brand_owner
        self.ingredients_text = ingredients_text
        self.food_nutrients = food_nutrients or []
        self.score = score
        self.original_data = original_data or {}


class USDAService:
    """USDA FoodData Central APIã¨ã®é€šä¿¡ã‚’ç®¡ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        settings = get_settings()
        self.api_key = settings.USDA_API_KEY
        self.base_url = settings.USDA_API_BASE_URL
        self.timeout = settings.USDA_API_TIMEOUT
        self.key_nutrient_numbers = settings.USDA_KEY_NUTRIENT_NUMBERS
        
        if not self.api_key:
            logger.error("USDA_API_KEY is not configured.")
            raise ValueError("USDA API key not configured.")
        
        # httpx.AsyncClientã®è¨­å®š
        self.client = httpx.AsyncClient(
            timeout=self.timeout,
            headers={"X-Api-Key": self.api_key}
        )
    
    async def search_foods(
        self,
        query: str,
        data_types: Optional[List[str]] = None,
        page_size: int = 5,
        page_number: int = 1,
        sort_by: str = "score",
        sort_order: str = "desc"
    ) -> List[USDASearchResultItem]:
        """
        USDA FoodData Central APIã§é£Ÿå“ã‚’æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
            data_types: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: ["Foundation", "SR Legacy", "Branded"]ï¼‰
            page_size: 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®çµæžœæ•°
            page_number: å–å¾—ã™ã‚‹ãƒšãƒ¼ã‚¸ç•ªå·
            sort_by: ã‚½ãƒ¼ãƒˆã‚­ãƒ¼
            sort_order: ã‚½ãƒ¼ãƒˆé †ï¼ˆ"asc" ã¾ãŸã¯ "desc"ï¼‰
            
        Returns:
            USDASearchResultItemã®ãƒªã‚¹ãƒˆ
        """
        params = {
            "query": query,
            "api_key": self.api_key,
            "pageSize": page_size,
            "pageNumber": page_number,
            "sortBy": sort_by,
            "sortOrder": sort_order
        }
        
        if data_types:
            # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’ã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã¨ã—ã¦æ¸¡ã™
            params["dataType"] = ",".join(data_types)
        
        try:
            logger.info(f"USDA API search: query='{query}', page_size={page_size}")
            response = await self.client.get(f"{self.base_url}/foods/search", params=params)
            
            # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæƒ…å ±ã®ãƒ­ã‚°
            if "X-RateLimit-Remaining" in response.headers:
                logger.info(f"USDA API Rate Limit Remaining: {response.headers.get('X-RateLimit-Remaining')}")
            
            response.raise_for_status()
            data = response.json()
            
            results = []
            for food_data in data.get("foods", [])[:page_size]:
                nutrients_extracted = self._extract_nutrients(food_data.get("foodNutrients", []))
                
                results.append(USDASearchResultItem(
                    fdc_id=food_data.get("fdcId"),
                    description=food_data.get("description"),
                    data_type=food_data.get("dataType"),
                    brand_owner=food_data.get("brandOwner"),
                    ingredients_text=food_data.get("ingredients"),
                    food_nutrients=nutrients_extracted,
                    score=food_data.get("score"),
                    original_data=food_data
                ))
            
            logger.info(f"USDA API search returned {len(results)} results for query '{query}'")
            return results
            
        except httpx.HTTPStatusError as e:
            logger.error(f"USDA API HTTP error: {e.response.status_code} - {e.response.text}")
            if e.response.status_code == 429:
                raise RuntimeError(f"USDA API rate limit exceeded. Detail: {e.response.text}") from e
            raise RuntimeError(f"USDA API error: {e.response.status_code} - {e.response.text}") from e
        except httpx.RequestError as e:
            logger.error(f"USDA API request failed: {str(e)}")
            raise RuntimeError(f"USDA API request failed: {str(e)}") from e
        except (json.JSONDecodeError, TypeError, KeyError) as e:
            logger.error(f"USDA API response parsing error: {str(e)}")
            raise RuntimeError(f"USDA API response parsing error: {str(e)}") from e
        except Exception as e:
            logger.error(f"Unexpected error in USDAService.search_foods: {str(e)}")
            raise RuntimeError(f"Unexpected error in USDA service: {str(e)}") from e
    
    def _extract_nutrients(self, food_nutrients: List[Dict[str, Any]]) -> List[USDANutrient]:
        """
        foodNutrientsãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸»è¦æ „é¤Šç´ ã‚’æŠ½å‡º
        
        Args:
            food_nutrients: USDA APIã‹ã‚‰è¿”ã•ã‚Œã‚‹æ „é¤Šç´ ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            USDANutrientã®ãƒªã‚¹ãƒˆ
        """
        nutrients_extracted = []
        
        for nutrient_entry in food_nutrients:
            # æ „é¤Šç´ æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰
            nutrient_detail = nutrient_entry.get("nutrient", {})
            amount = nutrient_entry.get("amount")
            
            # Branded Foodsã®abridgedãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã¸ã®å¯¾å¿œ
            if not nutrient_detail and "nutrientId" in nutrient_entry:
                nutrient_id = nutrient_entry.get("nutrientId")
                name = nutrient_entry.get("nutrientName")
                number = nutrient_entry.get("nutrientNumber")
                unit_name = nutrient_entry.get("unitName")
                amount = nutrient_entry.get("value")  # Branded abridgedã§ã¯"value"
            else:
                # SR Legacy, Foundation, ã¾ãŸã¯ full Branded
                nutrient_id = nutrient_detail.get("id")
                name = nutrient_detail.get("name")
                number = nutrient_detail.get("number")
                unit_name = nutrient_detail.get("unitName")
            
            # ä¸»è¦æ „é¤Šç´ ã®ã¿ã‚’æŠ½å‡º
            if number and str(number) in self.key_nutrient_numbers:
                if name and amount is not None and unit_name:
                    nutrients_extracted.append(USDANutrient(
                        name=name,
                        amount=float(amount),
                        unit_name=unit_name,
                        nutrient_id=int(nutrient_id) if nutrient_id else None,
                        nutrient_number=str(number) if number else None
                    ))
        
        return nutrients_extracted
    
    async def get_food_details(
        self, 
        fdc_id: int, 
        format: str = "full",
        target_nutrient_numbers: Optional[List[str]] = None
    ) -> Optional[USDASearchResultItem]:
        """
        ç‰¹å®šã®FDC IDã®é£Ÿå“è©³ç´°æƒ…å ±ã‚’å–å¾—
        
        Args:
            fdc_id: é£Ÿå“ã®FDC ID
            format: ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ï¼ˆ"abridged" ã¾ãŸã¯ "full"ï¼‰
            target_nutrient_numbers: å–å¾—ã™ã‚‹æ „é¤Šç´ ç•ªå·ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            USDASearchResultItem ã¾ãŸã¯ None
        """
        params = {
            "api_key": self.api_key,
            "format": format
        }
        
        if target_nutrient_numbers:
            params["nutrients"] = ",".join(target_nutrient_numbers)
        
        try:
            logger.info(f"USDA API get food details: fdc_id={fdc_id}")
            response = await self.client.get(f"{self.base_url}/food/{fdc_id}", params=params)
            response.raise_for_status()
            
            food_data = response.json()
            nutrients_extracted = self._extract_nutrients(food_data.get("foodNutrients", []))
            
            return USDASearchResultItem(
                fdc_id=food_data.get("fdcId"),
                description=food_data.get("description"),
                data_type=food_data.get("dataType"),
                brand_owner=food_data.get("brandOwner"),
                ingredients_text=food_data.get("ingredients"),
                food_nutrients=nutrients_extracted,
                score=food_data.get("score"),
                original_data=food_data
            )
            
        except Exception as e:
            logger.error(f"Error fetching food details for FDC ID {fdc_id}: {str(e)}")
            return None

    async def get_food_details_for_nutrition(self, fdc_id: int) -> Optional[Dict[str, float]]:
        """
        æ „é¤Šè¨ˆç®—ç”¨ã®é£Ÿå“è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆä»•æ§˜æ›¸æº–æ‹ ï¼‰
        
        å…¥åŠ›: FDC ID
        å‡¦ç†: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªå¾Œã€å¿…è¦ãªã‚‰USDA APIã‹ã‚‰é£Ÿå“è©³ç´°ã‚’å–å¾—ã—ã€ä¸»è¦æ „é¤Šç´ ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã•ã‚ŒãŸIDï¼‰ã‚’100gã‚ãŸã‚Šã§æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹ã€‚çµæžœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã€‚
        å‡ºåŠ›: 100gã‚ãŸã‚Šã®ä¸»è¦æ „é¤Šç´ è¾žæ›¸ã€ã¾ãŸã¯ Noneã€‚
        
        Args:
            fdc_id: é£Ÿå“ã®FDC ID
            
        Returns:
            Optional[Dict[str, float]]: 100gã‚ãŸã‚Šã®ä¸»è¦æ „é¤Šç´ è¾žæ›¸ã€ã¾ãŸã¯ None
        """
        if not fdc_id:
            logger.warning("Invalid FDC ID provided")
            return None
        
        try:
            # TODO: å°†æ¥çš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã‚’å®Ÿè£…ï¼ˆRedisç­‰ï¼‰
            # ç¾çŠ¶ã¯ç›´æŽ¥APIã‹ã‚‰å–å¾—
            
            logger.info(f"USDA API get food details for nutrition: fdc_id={fdc_id}")
            
            params = {
                "api_key": self.api_key,
                "format": "full",  # è©³ç´°ãªæ „é¤Šæƒ…å ±ãŒå¿…è¦
                "nutrients": ",".join(self.key_nutrient_numbers)  # ä¸»è¦æ „é¤Šç´ ã®ã¿ã‚’å–å¾—
            }
            
            response = await self.client.get(f"{self.base_url}/food/{fdc_id}", params=params)
            
            # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæƒ…å ±ã®ãƒ­ã‚°
            if "X-RateLimit-Remaining" in response.headers:
                logger.info(f"USDA API Rate Limit Remaining: {response.headers.get('X-RateLimit-Remaining')}")
            
            response.raise_for_status()
            food_data_raw = response.json()
            
            # ä¸»è¦æ „é¤Šç´ ã‚’æŠ½å‡ºãƒ»ãƒ‘ãƒ¼ã‚¹
            key_nutrients = self._parse_nutrients_for_calculation(food_data_raw)
            
            if key_nutrients:
                logger.info(f"Successfully extracted {len(key_nutrients)} key nutrients for FDC ID {fdc_id}")
                # TODO: å°†æ¥çš„ã«ã“ã“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                return key_nutrients
            else:
                logger.warning(f"No key nutrients found for FDC ID {fdc_id}")
                return None
                
        except httpx.HTTPStatusError as e:
            logger.error(f"USDA API HTTP error for FDC ID {fdc_id}: {e.response.status_code} - {e.response.text}")
            if e.response.status_code == 404:
                logger.warning(f"Food with FDC ID {fdc_id} not found")
                return None
            elif e.response.status_code == 429:
                raise RuntimeError(f"USDA API rate limit exceeded for FDC ID {fdc_id}") from e
            raise RuntimeError(f"USDA API error for FDC ID {fdc_id}: {e.response.status_code}") from e
        except httpx.RequestError as e:
            logger.error(f"USDA API request failed for FDC ID {fdc_id}: {str(e)}")
            raise RuntimeError(f"USDA API request failed for FDC ID {fdc_id}: {str(e)}") from e
        except Exception as e:
            logger.error(f"Unexpected error getting food details for nutrition (FDC ID {fdc_id}): {str(e)}")
            return None

    def _parse_nutrients_for_calculation(self, food_data_raw: dict) -> Dict[str, float]:
        """
        USDA APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æ „é¤Šè¨ˆç®—ç”¨ã®ä¸»è¦æ „é¤Šç´ ã‚’æŠ½å‡ºï¼ˆå†…éƒ¨ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        
        Args:
            food_data_raw: USDA APIã‹ã‚‰ã®ç”Ÿã®é£Ÿå“ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Dict[str, float]: ä¸»è¦æ „é¤Šç´ è¾žæ›¸ï¼ˆã‚­ãƒ¼ã¯æ¨™æº–åŒ–ã•ã‚ŒãŸåå‰ï¼‰
        """
        key_nutrients = {}
        
        try:
            food_nutrients = food_data_raw.get("foodNutrients", [])
            
            for nutrient_entry in food_nutrients:
                # æ „é¤Šç´ æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¯ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã«ã‚ˆã£ã¦ç•°ãªã‚‹ï¼‰
                nutrient_detail = nutrient_entry.get("nutrient", {})
                amount = nutrient_entry.get("amount")
                
                # Branded Foodsã®abridgedãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã¸ã®å¯¾å¿œ
                if not nutrient_detail and "nutrientId" in nutrient_entry:
                    number = nutrient_entry.get("nutrientNumber")
                    amount = nutrient_entry.get("value")  # Branded abridgedã§ã¯"value"
                else:
                    # SR Legacy, Foundation, ã¾ãŸã¯ full Branded
                    number = nutrient_detail.get("number")
                
                # ä¸»è¦æ „é¤Šç´ ã®ãƒžãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ „é¤Šç´ ç•ªå·ã‹ã‚‰æ¨™æº–åŒ–ã•ã‚ŒãŸã‚­ãƒ¼åã¸ï¼‰
                if number and str(number) in self.key_nutrient_numbers and amount is not None:
                    if str(number) == "208":  # Energy (calories)
                        key_nutrients["calories_kcal"] = float(amount)
                    elif str(number) == "203":  # Protein
                        key_nutrients["protein_g"] = float(amount)
                    elif str(number) == "204":  # Total lipid (fat)
                        key_nutrients["fat_g"] = float(amount)
                    elif str(number) == "205":  # Carbohydrate, by difference
                        key_nutrients["carbohydrates_g"] = float(amount)
                    elif str(number) == "291":  # Fiber, total dietary (optional)
                        key_nutrients["fiber_g"] = float(amount)
                    elif str(number) == "269":  # Sugars, total (optional)
                        key_nutrients["sugars_g"] = float(amount)
                    elif str(number) == "307":  # Sodium (optional)
                        key_nutrients["sodium_mg"] = float(amount)
            
            # å¿…é ˆæ „é¤Šç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯0.0ã¨ã—ã¦è¨­å®š
            essential_nutrients = ["calories_kcal", "protein_g", "fat_g", "carbohydrates_g"]
            for nutrient in essential_nutrients:
                if nutrient not in key_nutrients:
                    key_nutrients[nutrient] = 0.0
                    logger.debug(f"Missing essential nutrient {nutrient}, set to 0.0")
            
            logger.debug(f"Parsed key nutrients: {key_nutrients}")
            return key_nutrients
            
        except Exception as e:
            logger.error(f"Error parsing nutrients for calculation: {str(e)}")
            return {}
    
    async def close_client(self):
        """HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ã‚º"""
        await self.client.aclose()


# FastAPIã®ä¾å­˜æ€§æ³¨å…¥ç”¨é–¢æ•°
async def get_usda_service():
    """
    USDAServiceã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æä¾›ã™ã‚‹ä¾å­˜æ€§æ³¨å…¥é–¢æ•°
    """
    service = USDAService()
    try:
        yield service
    finally:
        await service.close_client() 
```

============================================================

ðŸ“„ FILE: nutrition_db_experiment/search_service/api/query_builder.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 11,619 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 11:40:30
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Query Builder - Elasticsearch JSONã‚¯ã‚¨ãƒªæ§‹ç¯‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

BM25F + function_scoreã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
ä»•æ§˜æ›¸ã«å¾“ã£ãŸãƒžãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’å®Ÿè£…
"""

from typing import Dict, List, Optional, Any
import json

class NutritionSearchQueryBuilder:
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ç”¨ã®Elasticsearchã‚¯ã‚¨ãƒªãƒ“ãƒ«ãƒ€ãƒ¼"""
    
    def __init__(self):
        """ã‚¯ã‚¨ãƒªãƒ“ãƒ«ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°é‡ã¿
        self.default_weights = {
            "exact_phrase_bonus": 100.0,
            "exact_word_bonus": 80.0,
            "phrase_proximity_bonus": 50.0,
            "prefix_match_bonus": 10.0,
            "base_field_boost": 1.0,
            "exact_field_boost": 3.0
        }
    
    def build_search_query(
        self,
        processed_query: str,
        original_query: str,
        db_type_filter: Optional[str] = None,
        size: int = 20,
        weights: Optional[Dict[str, float]] = None,
        enable_highlight: bool = True,
        enable_synonyms: bool = True
    ) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            processed_query: å‰å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
            original_query: å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
            db_type_filter: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ ("dish", "ingredient", "branded")
            size: è¿”å´ã™ã‚‹çµæžœæ•°
            weights: ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚³ã‚¢é‡ã¿
            enable_highlight: ãƒã‚¤ãƒ©ã‚¤ãƒˆæ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
            enable_synonyms: é¡žç¾©èªžå±•é–‹ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ï¼ˆå°†æ¥æ‹¡å¼µç”¨ï¼‰
            
        Returns:
            Elasticsearch JSONã‚¯ã‚¨ãƒª
        """
        # é‡ã¿ã®ãƒžãƒ¼ã‚¸
        final_weights = self.default_weights.copy()
        if weights:
            final_weights.update(weights)
        
        # ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªï¼ˆBM25Fã‚¹ã‚³ã‚¢ç”¨ï¼‰
        base_query = self._build_base_query(processed_query, final_weights)
        
        # function_scoreã‚¯ã‚¨ãƒªã§ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆãƒ•ã‚£ãƒ«ã‚¿é©ç”¨å‰ï¼‰
        function_score_query = self._build_function_score_query(
            base_query, 
            original_query, 
            processed_query,
            final_weights
        )
        
        # ãƒ•ã‚£ãƒ«ã‚¿è¿½åŠ ï¼ˆfunction_scoreã‚¯ã‚¨ãƒªå…¨ä½“ã«é©ç”¨ï¼‰
        if db_type_filter and db_type_filter != "all":
            function_score_query = {
                "bool": {
                    "must": [function_score_query],
                    "filter": [
                        {"term": {"db_type": db_type_filter}}
                    ]
                }
            }
        
        # å®Œå…¨ãªã‚¯ã‚¨ãƒªæ§‹ç¯‰
        search_query = {
            "query": function_score_query,
            "size": size,
            "_source": ["db_type", "id", "search_name", "nutrition", "weight"]
        }
        
        # ãƒã‚¤ãƒ©ã‚¤ãƒˆè¿½åŠ 
        if enable_highlight:
            search_query["highlight"] = self._build_highlight_config()
        
        return search_query
    
    def _build_base_query(self, processed_query: str, weights: Dict[str, float]) -> Dict[str, Any]:
        """
        BM25Fãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            processed_query: å‰å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒª
            weights: ã‚¹ã‚³ã‚¢é‡ã¿
            
        Returns:
            ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªè¾žæ›¸
        """
        return {
            "multi_match": {
                "query": processed_query,
                "fields": [
                    f"search_name^{weights['base_field_boost']}",
                    f"search_name.exact^{weights['exact_field_boost']}"
                ],
                "type": "best_fields",
                "operator": "OR",
                "fuzziness": "AUTO",
                "max_expansions": 50,
                "prefix_length": 2
            }
        }
    
    def _add_filters(self, base_query: Dict[str, Any], db_type: str) -> Dict[str, Any]:
        """
        ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¿½åŠ ã—ã¦boolã‚¯ã‚¨ãƒªã«å¤‰æ›
        
        Args:
            base_query: ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª
            db_type: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
            
        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãboolã‚¯ã‚¨ãƒª
        """
        return {
            "bool": {
                "must": [base_query],
                "filter": [
                    {"term": {"db_type": db_type}}
                ]
            }
        }
    
    def _build_function_score_query(
        self, 
        base_query: Dict[str, Any], 
        original_query: str,
        processed_query: str,
        weights: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        function_scoreã‚¯ã‚¨ãƒªã§ãƒžãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚’æ§‹ç¯‰
        
        Args:
            base_query: ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒª
            original_query: å…ƒã®ã‚¯ã‚¨ãƒª
            processed_query: å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒª
            weights: ã‚¹ã‚³ã‚¢é‡ã¿
            
        Returns:
            function_scoreã‚¯ã‚¨ãƒª
        """
        functions = []
        
        # 1. å®Œå…¨ä¸€è‡´ãƒ•ãƒ¬ãƒ¼ã‚ºãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€å„ªå…ˆï¼‰
        functions.append({
            "filter": {
                "match_phrase": {
                    "search_name.exact": {
                        "query": original_query,
                        "slop": 0
                    }
                }
            },
            "weight": weights["exact_phrase_bonus"]
        })
        
        # 2. è¿‘æŽ¥ãƒ•ãƒ¬ãƒ¼ã‚ºãƒœãƒ¼ãƒŠã‚¹ï¼ˆslop=1è¨±å®¹ï¼‰
        functions.append({
            "filter": {
                "match_phrase": {
                    "search_name": {
                        "query": original_query,
                        "slop": 1
                    }
                }
            },
            "weight": weights["phrase_proximity_bonus"]
        })
        
        # 3. å®Œå…¨ä¸€è‡´å˜èªžãƒœãƒ¼ãƒŠã‚¹ï¼ˆå€‹åˆ¥å˜èªžãƒ¬ãƒ™ãƒ«ï¼‰
        # å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒªã®å„å˜èªžã«å¯¾ã—ã¦
        for word in processed_query.split():
            if len(word) > 2:  # çŸ­ã™ãŽã‚‹å˜èªžã¯é™¤å¤–
                functions.append({
                    "filter": {
                        "term": {
                            "search_name.exact": word
                        }
                    },
                    "weight": weights["exact_word_bonus"] * 0.5  # å˜èªžãƒ¬ãƒ™ãƒ«ã¯å°‘ã—ä½Žã‚ã«
                })
        
        # 4. å‰æ–¹ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆä½Žå„ªå…ˆåº¦ï¼‰
        functions.append({
            "filter": {
                "match_phrase_prefix": {
                    "search_name": {
                        "query": original_query,
                        "max_expansions": 10
                    }
                }
            },
            "weight": weights["prefix_match_bonus"]
        })
        
        return {
            "function_score": {
                "query": base_query,
                "functions": functions,
                "score_mode": "sum",  # å„ãƒœãƒ¼ãƒŠã‚¹ã‚’ç´¯ç©
                "boost_mode": "sum",  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ã«ãƒœãƒ¼ãƒŠã‚¹ã‚’åŠ ç®—
                "max_boost": 1000.0  # ä¸Šé™è¨­å®š
            }
        }
    
    def _build_highlight_config(self) -> Dict[str, Any]:
        """
        ãƒã‚¤ãƒ©ã‚¤ãƒˆè¨­å®šã‚’æ§‹ç¯‰
        
        Returns:
            ãƒã‚¤ãƒ©ã‚¤ãƒˆè¨­å®šè¾žæ›¸
        """
        return {
            "fields": {
                "search_name": {
                    "pre_tags": ["<mark>"],
                    "post_tags": ["</mark>"],
                    "fragment_size": 150,
                    "number_of_fragments": 1
                },
                "search_name.exact": {
                    "pre_tags": ["<strong>"],
                    "post_tags": ["</strong>"],
                    "fragment_size": 150,
                    "number_of_fragments": 1
                }
            },
            "require_field_match": False
        }
    
    def build_simple_query(
        self, 
        query: str, 
        db_type_filter: Optional[str] = None,
        size: int = 20
    ) -> Dict[str, Any]:
        """
        ã‚·ãƒ³ãƒ—ãƒ«ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            db_type_filter: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
            size: è¿”å´ã™ã‚‹çµæžœæ•°
            
        Returns:
            ã‚·ãƒ³ãƒ—ãƒ«ãªElasticsearchã‚¯ã‚¨ãƒª
        """
        base_query = {
            "multi_match": {
                "query": query,
                "fields": ["search_name^2", "search_name.exact^3"],
                "type": "best_fields",
                "fuzziness": "AUTO"
            }
        }
        
        if db_type_filter and db_type_filter != "all":
            base_query = {
                "bool": {
                    "must": [base_query],
                    "filter": [{"term": {"db_type": db_type_filter}}]
                }
            }
        
        return {
            "query": base_query,
            "size": size,
            "_source": ["db_type", "id", "search_name", "nutrition", "weight"]
        }
    
    def build_analysis_query(self, text: str, analyzer: str = "custom_food_analyzer") -> Dict[str, Any]:
        """
        ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ã®ãƒ†ã‚¹ãƒˆç”¨ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
        
        Args:
            text: åˆ†æžã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            analyzer: ä½¿ç”¨ã™ã‚‹ã‚¢ãƒŠãƒ©ã‚¤ã‚¶å
            
        Returns:
            åˆ†æžç”¨ã‚¯ã‚¨ãƒª
        """
        return {
            "analyzer": analyzer,
            "text": text
        }
    
    def get_weight_explanation(self) -> Dict[str, Any]:
        """
        ç¾åœ¨ã®é‡ã¿è¨­å®šã®èª¬æ˜Žã‚’å–å¾—
        
        Returns:
            é‡ã¿è¨­å®šã®èª¬æ˜Žè¾žæ›¸
        """
        return {
            "weights": self.default_weights,
            "explanations": {
                "exact_phrase_bonus": "å®Œå…¨ãƒ•ãƒ¬ãƒ¼ã‚ºä¸€è‡´æ™‚ã®æœ€é«˜ãƒœãƒ¼ãƒŠã‚¹",
                "exact_word_bonus": "å®Œå…¨å˜èªžä¸€è‡´æ™‚ã®ãƒœãƒ¼ãƒŠã‚¹",
                "phrase_proximity_bonus": "è¿‘æŽ¥ãƒ•ãƒ¬ãƒ¼ã‚ºä¸€è‡´æ™‚ã®ãƒœãƒ¼ãƒŠã‚¹",
                "prefix_match_bonus": "å‰æ–¹ä¸€è‡´æ™‚ã®ä½Žãƒœãƒ¼ãƒŠã‚¹",
                "base_field_boost": "search_nameãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®åŸºæœ¬ãƒ–ãƒ¼ã‚¹ãƒˆ",
                "exact_field_boost": "search_name.exactãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ–ãƒ¼ã‚¹ãƒˆ"
            },
            "strategy": {
                "score_mode": "sum",
                "boost_mode": "sum",
                "description": "BM25Fãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ + ç´¯ç©ãƒœãƒ¼ãƒŠã‚¹ã‚¹ã‚³ã‚¢"
            }
        }

# ä¾¿åˆ©é–¢æ•°
def build_nutrition_search_query(
    processed_query: str,
    original_query: str,
    db_type_filter: Optional[str] = None,
    size: int = 20,
    custom_weights: Optional[Dict[str, float]] = None
) -> Dict[str, Any]:
    """
    æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰
    
    Args:
        processed_query: å‰å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒª
        original_query: å…ƒã®ã‚¯ã‚¨ãƒª
        db_type_filter: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
        size: çµæžœæ•°
        custom_weights: ã‚«ã‚¹ã‚¿ãƒ é‡ã¿
        
    Returns:
        Elasticsearchã‚¯ã‚¨ãƒªè¾žæ›¸
    """
    builder = NutritionSearchQueryBuilder()
    return builder.build_search_query(
        processed_query=processed_query,
        original_query=original_query,
        db_type_filter=db_type_filter,
        size=size,
        weights=custom_weights
    ) 
```

============================================================

ðŸ“„ FILE: nutrition_db_experiment/search_service/api/search_handler.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 9,344 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 11:36:26
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Search Handler - æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã—ã€ã‚¯ã‚¨ãƒªå‰å‡¦ç†ã€ã‚¯ã‚¨ãƒªæ§‹ç¯‰ã€Elasticsearchæ¤œç´¢ã‚’çµ±åˆ
"""

import os
import sys
import json
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from nlp.query_preprocessor import preprocess_query, analyze_query
from api.query_builder import build_nutrition_search_query

# Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯è¨­å®šã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    print("Warning: Elasticsearch client not available. Install with: pip install elasticsearch")

@dataclass
class SearchRequest:
    """æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    query: str
    db_type_filter: Optional[str] = None
    size: int = 20
    enable_highlight: bool = True
    enable_synonyms: bool = True
    custom_weights: Optional[Dict[str, float]] = None

@dataclass
class SearchResponse:
    """æ¤œç´¢ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    results: List[Dict[str, Any]]
    total_hits: int
    query_info: Dict[str, Any]
    took_ms: int
    max_score: float

class NutritionSearchHandler:
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self, elasticsearch_host: str = "localhost:9200", index_name: str = "nutrition_db"):
        """
        æ¤œç´¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–
        
        Args:
            elasticsearch_host: Elasticsearchãƒ›ã‚¹ãƒˆ
            index_name: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
        """
        self.elasticsearch_host = elasticsearch_host
        self.index_name = index_name
        self.es_client = None
        
        # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        if ELASTICSEARCH_AVAILABLE:
            try:
                self.es_client = Elasticsearch([elasticsearch_host])
                # æŽ¥ç¶šãƒ†ã‚¹ãƒˆ
                if self.es_client.ping():
                    self.logger.info(f"ElasticsearchæŽ¥ç¶šæˆåŠŸ: {elasticsearch_host}")
                else:
                    self.logger.warning(f"ElasticsearchæŽ¥ç¶šå¤±æ•—: {elasticsearch_host}")
            except Exception as e:
                self.logger.error(f"ElasticsearchåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            self.logger.warning("Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    def search(self, request: SearchRequest) -> SearchResponse:
        """
        æ¤œç´¢ã‚’å®Ÿè¡Œ
        
        Args:
            request: æ¤œç´¢ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            
        Returns:
            æ¤œç´¢ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        start_time = datetime.now()
        
        try:
            # 1. ã‚¯ã‚¨ãƒªå‰å‡¦ç†
            processed_query = preprocess_query(
                request.query, 
                expand_synonyms=request.enable_synonyms
            )
            
            # 2. ã‚¯ã‚¨ãƒªåˆ†æžï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼‰
            query_analysis = analyze_query(request.query)
            
            # 3. Elasticsearchã‚¯ã‚¨ãƒªæ§‹ç¯‰
            es_query = build_nutrition_search_query(
                processed_query=processed_query,
                original_query=request.query,
                db_type_filter=request.db_type_filter,
                size=request.size,
                custom_weights=request.custom_weights
            )
            
            # 4. Elasticsearchæ¤œç´¢å®Ÿè¡Œ
            if self.es_client:
                response = self.es_client.search(
                    index=self.index_name,
                    body=es_query
                )
                results = self._format_search_results(response)
                total_hits = response['hits']['total']['value']
                max_score = response['hits']['max_score'] or 0.0
            else:
                # ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆElasticsearchæœªæŽ¥ç¶šæ™‚ï¼‰
                results = self._mock_search_results(request.query)
                total_hits = len(results)
                max_score = 1.0
            
            # 5. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹ç¯‰
            end_time = datetime.now()
            took_ms = int((end_time - start_time).total_seconds() * 1000)
            
            return SearchResponse(
                results=results,
                total_hits=total_hits,
                query_info={
                    "original_query": request.query,
                    "processed_query": processed_query,
                    "analysis": query_analysis,
                    "elasticsearch_query": es_query,
                    "db_type_filter": request.db_type_filter
                },
                took_ms=took_ms,
                max_score=max_score
            )
            
        except Exception as e:
            self.logger.error(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return SearchResponse(
                results=[],
                total_hits=0,
                query_info={
                    "original_query": request.query,
                    "error": str(e)
                },
                took_ms=0,
                max_score=0.0
            )
    
    def _format_search_results(self, es_response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Elasticsearchãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ•´å½¢
        
        Args:
            es_response: Elasticsearchãƒ¬ã‚¹ãƒãƒ³ã‚¹
            
        Returns:
            æ•´å½¢æ¸ˆã¿çµæžœãƒªã‚¹ãƒˆ
        """
        results = []
        
        for hit in es_response['hits']['hits']:
            result = {
                **hit['_source'],
                '_score': hit['_score'],
                '_id': hit['_id']
            }
            
            # ãƒã‚¤ãƒ©ã‚¤ãƒˆæƒ…å ±ã‚’è¿½åŠ 
            if 'highlight' in hit:
                result['_highlight'] = hit['highlight']
            
            results.append(result)
        
        return results
    
    def _mock_search_results(self, query: str) -> List[Dict[str, Any]]:
        """
        ãƒ¢ãƒƒã‚¯æ¤œç´¢çµæžœã‚’ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            
        Returns:
            ãƒ¢ãƒƒã‚¯çµæžœãƒªã‚¹ãƒˆ
        """
        # ç°¡å˜ãªãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
        mock_results = [
            {
                "db_type": "dish",
                "id": 123456,
                "search_name": f"Cooked {query} with vegetables",
                "nutrition": {
                    "calories": 150.0,
                    "protein": 25.0,
                    "fat": 5.0,
                    "carbs": 15.0
                },
                "weight": 100.0,
                "_score": 2.5
            },
            {
                "db_type": "ingredient", 
                "id": 789012,
                "search_name": f"{query.capitalize()}, raw, fresh",
                "nutrition": {
                    "calories": 120.0,
                    "protein": 20.0,
                    "fat": 3.0,
                    "carbs": 10.0
                },
                "weight": 100.0,
                "_score": 2.0
            }
        ]
        
        return mock_results
    
    def health_check(self) -> Dict[str, Any]:
        """
        ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        
        Returns:
            ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹æƒ…å ±
        """
        status = {
            "service": "nutrition_search",
            "status": "healthy",
            "elasticsearch": {
                "available": ELASTICSEARCH_AVAILABLE,
                "connected": False,
                "host": self.elasticsearch_host,
                "index": self.index_name
            },
            "components": {
                "query_preprocessor": True,
                "query_builder": True
            }
        }
        
        if self.es_client:
            try:
                status["elasticsearch"]["connected"] = self.es_client.ping()
                if status["elasticsearch"]["connected"]:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å­˜åœ¨ç¢ºèª
                    index_exists = self.es_client.indices.exists(index=self.index_name)
                    status["elasticsearch"]["index_exists"] = index_exists
            except Exception as e:
                status["elasticsearch"]["error"] = str(e)
                status["status"] = "degraded"
        
        return status

# ä¾¿åˆ©é–¢æ•°
def create_search_handler(
    elasticsearch_host: str = "localhost:9200",
    index_name: str = "nutrition_db"
) -> NutritionSearchHandler:
    """æ¤œç´¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°"""
    return NutritionSearchHandler(elasticsearch_host, index_name)

def search_nutrition_db(
    query: str,
    db_type_filter: Optional[str] = None,
    size: int = 20,
    elasticsearch_host: str = "localhost:9200",
    index_name: str = "nutrition_db"
) -> SearchResponse:
    """ä¾¿åˆ©ãªæ¤œç´¢é–¢æ•°"""
    handler = create_search_handler(elasticsearch_host, index_name)
    request = SearchRequest(
        query=query,
        db_type_filter=db_type_filter,
        size=size
    )
    return handler.search(request) 
```

============================================================

ðŸ“„ FILE: nutrition_db_experiment/search_service/nlp/query_preprocessor.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 11,049 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 11:41:37
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Query Preprocessor - spaCyãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

é£Ÿå“æ¤œç´¢ã®ãŸã‚ã®ã‚¯ã‚¨ãƒªå‰å‡¦ç†ã‚’è¡Œã„ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ï¼š
- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
- å°æ–‡å­—åŒ–
- ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤åŽ»
- ä¿è­·ã‚¿ãƒ¼ãƒ å‡¦ç†
- ãƒ¬ãƒ³ãƒžåŒ–ï¼ˆä¸Šæ›¸ããƒ«ãƒ¼ãƒ«é©ç”¨ï¼‰
- é¡žç¾©èªžå±•é–‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
"""

import os
import spacy
from spacy.tokens import Token
from spacy.language import Language
from typing import List, Dict, Set, Optional
import re

class FoodQueryPreprocessor:
    def __init__(self):
        """ã‚¯ã‚¨ãƒªå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–"""
        self.nlp = None
        self.protected_terms: Set[str] = set()
        self.lemma_overrides: Dict[str, str] = {}
        self.custom_stopwords: Set[str] = set()
        self.food_synonyms: Dict[str, List[str]] = {}
        
        # ãƒ¬ã‚­ã‚·ã‚³ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
        self.lexicon_base_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), 
            "lexicon_data"
        )
        
        self._load_lexicon_data()
        self._setup_spacy_pipeline()
    
    def _load_lexicon_data(self):
        """ãƒ¬ã‚­ã‚·ã‚³ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # ä¿è­·ã‚¿ãƒ¼ãƒ ã®èª­ã¿è¾¼ã¿
        protected_file = os.path.join(self.lexicon_base_path, "protected_food_terms.txt")
        try:
            with open(protected_file, "r", encoding="utf-8") as f:
                for line in f:
                    term = line.strip().lower()
                    if term and not term.startswith("#"):
                        self.protected_terms.add(term)
        except FileNotFoundError:
            print(f"Warning: protected_food_terms.txt not found at {protected_file}")
        
        # ãƒ¬ãƒ³ãƒžä¸Šæ›¸ããƒ«ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿
        override_file = os.path.join(self.lexicon_base_path, "food_lemma_overrides.txt")
        try:
            with open(override_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        parts = line.split("=>")
                        if len(parts) == 2:
                            original = parts[0].strip().lower()
                            override = parts[1].strip().lower()
                            self.lemma_overrides[original] = override
        except FileNotFoundError:
            print(f"Warning: food_lemma_overrides.txt not found at {override_file}")
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿
        stopwords_file = os.path.join(self.lexicon_base_path, "custom_food_stopwords.txt")
        try:
            with open(stopwords_file, "r", encoding="utf-8") as f:
                for line in f:
                    word = line.strip().lower()
                    if word and not word.startswith("#"):
                        self.custom_stopwords.add(word)
        except FileNotFoundError:
            print(f"Warning: custom_food_stopwords.txt not found at {stopwords_file}")
        
        # é¡žç¾©èªžã®èª­ã¿è¾¼ã¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        synonyms_file = os.path.join(self.lexicon_base_path, "food_synonyms.txt")
        try:
            with open(synonyms_file, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#"):
                        # åŒæ–¹å‘é¡žç¾©èªž: word1, word2, word3
                        if "=>" not in line and "," in line:
                            words = [w.strip().lower() for w in line.split(",")]
                            for word in words:
                                if word not in self.food_synonyms:
                                    self.food_synonyms[word] = []
                                self.food_synonyms[word].extend([w for w in words if w != word])
                        
                        # ç‰‡æ–¹å‘é¡žç¾©èªž: source => target1, target2
                        elif "=>" in line:
                            parts = line.split("=>")
                            if len(parts) == 2:
                                source = parts[0].strip().lower()
                                targets = [t.strip().lower() for t in parts[1].split(",")]
                                self.food_synonyms[source] = targets
        except FileNotFoundError:
            print(f"Warning: food_synonyms.txt not found at {synonyms_file}")
    
    def _setup_spacy_pipeline(self):
        """spaCyãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            # è‹±èªžã®å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆåŠ¹çŽ‡æ€§é‡è¦–ï¼‰
            self.nlp = spacy.load("en_core_web_sm")
        except OSError:
            print("Warning: en_core_web_sm model not found. Installing...")
            try:
                import subprocess
                subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"], check=True)
                self.nlp = spacy.load("en_core_web_sm")
            except Exception as e:
                print(f"Error: Could not load spaCy model: {e}")
                return
        
        # ã‚«ã‚¹ã‚¿ãƒ æ‹¡å¼µå±žæ€§ã‚’è¿½åŠ 
        if not Token.has_extension("is_protected"):
            Token.set_extension("is_protected", default=False)
        if not Token.has_extension("custom_lemma"):
            Token.set_extension("custom_lemma", default=None)
    
    def food_lexicon_processor_component(self, doc):
        """é£Ÿå“ãƒ¬ã‚­ã‚·ã‚³ãƒ³å‡¦ç†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
        for token in doc:
            token_lower = token.lower_
            
            # ä¿è­·ã‚¿ãƒ¼ãƒ ã®ãƒã‚§ãƒƒã‚¯
            if token_lower in self.protected_terms:
                token._.is_protected = True
                token._.custom_lemma = token_lower
            
            # ãƒ¬ãƒ³ãƒžä¸Šæ›¸ãã®ãƒã‚§ãƒƒã‚¯
            elif token_lower in self.lemma_overrides:
                token._.is_protected = True  # ä¸Šæ›¸ãã‚¿ãƒ¼ã‚‚ãƒ¬ãƒ³ãƒžåŒ–ã‹ã‚‰ä¿è­·
                token._.custom_lemma = self.lemma_overrides[token_lower]
        
        return doc
    
    def preprocess_query(self, query_text: str, expand_synonyms: bool = False) -> str:
        """
        ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆã‚’å‰å‡¦ç†
        
        Args:
            query_text: ç”Ÿã®ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            expand_synonyms: é¡žç¾©èªžå±•é–‹ã‚’è¡Œã†ã‹ã©ã†ã‹
            
        Returns:
            å‡¦ç†æ¸ˆã¿ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
        """
        if not self.nlp:
            return query_text.lower()
        
        # spaCyã§å‡¦ç†
        doc = self.nlp(query_text)
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ‰‹å‹•ã§é©ç”¨
        doc = self.food_lexicon_processor_component(doc)
        
        processed_tokens = []
        
        for token in doc:
            # å¥èª­ç‚¹ã€ç©ºç™½ã€æ•°å­—ã®ã¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if token.is_punct or token.is_space or (token.is_digit and len(token.text) > 2):
                continue
            
            # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚§ãƒƒã‚¯
            if token.lower_ in self.custom_stopwords:
                continue
            
            # ä¿è­·ã•ã‚ŒãŸå˜èªžã®ãƒ¬ãƒ³ãƒžå‡¦ç†
            if token._.is_protected and token._.custom_lemma:
                processed_tokens.append(token._.custom_lemma)
            
            # æ¨™æº–ãƒ¬ãƒ³ãƒžåŒ–
            else:
                lemma = token.lemma_.lower()
                processed_tokens.append(lemma)
        
        # é¡žç¾©èªžå±•é–‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if expand_synonyms:
            expanded_tokens = []
            for token in processed_tokens:
                expanded_tokens.append(token)
                if token in self.food_synonyms:
                    expanded_tokens.extend(self.food_synonyms[token])
            processed_tokens = expanded_tokens
        
        # é‡è¤‡é™¤åŽ»ã¨çµåˆ
        processed_tokens = list(dict.fromkeys(processed_tokens))  # é †åºã‚’ä¿æŒã—ã¦é‡è¤‡é™¤åŽ»
        
        return " ".join(processed_tokens)
    
    def get_processed_tokens(self, query_text: str) -> List[str]:
        """
        å‡¦ç†æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Args:
            query_text: ç”Ÿã®ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            å‡¦ç†æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        processed_query = self.preprocess_query(query_text)
        return processed_query.split()
    
    def analyze_query(self, query_text: str) -> Dict:
        """
        ã‚¯ã‚¨ãƒªã®è©³ç´°åˆ†æžæƒ…å ±ã‚’å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        
        Args:
            query_text: ç”Ÿã®ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            åˆ†æžçµæžœã®è¾žæ›¸
        """
        if not self.nlp:
            return {"error": "spaCy model not loaded"}
        
        doc = self.nlp(query_text)
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ‰‹å‹•ã§é©ç”¨
        doc = self.food_lexicon_processor_component(doc)
        
        analysis = {
            "original": query_text,
            "tokens": [],
            "processed": self.preprocess_query(query_text),
            "statistics": {
                "original_tokens": len(doc),
                "processed_tokens": len(self.get_processed_tokens(query_text)),
                "protected_terms": 0,
                "overridden_terms": 0,
                "removed_stopwords": 0
            }
        }
        
        for token in doc:
            token_info = {
                "text": token.text,
                "lemma": token.lemma_,
                "is_protected": getattr(token._, 'is_protected', False),
                "custom_lemma": getattr(token._, 'custom_lemma', None),
                "is_stopword": token.lower_ in self.custom_stopwords,
                "is_punct": token.is_punct,
                "pos": token.pos_
            }
            
            if token_info["is_protected"]:
                analysis["statistics"]["protected_terms"] += 1
            if token_info["custom_lemma"]:
                analysis["statistics"]["overridden_terms"] += 1
            if token_info["is_stopword"]:
                analysis["statistics"]["removed_stopwords"] += 1
            
            analysis["tokens"].append(token_info)
        
        return analysis

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_preprocessor = None

def get_preprocessor() -> FoodQueryPreprocessor:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚µã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _preprocessor
    if _preprocessor is None:
        _preprocessor = FoodQueryPreprocessor()
    return _preprocessor

def preprocess_query(query_text: str, expand_synonyms: bool = False) -> str:
    """ä¾¿åˆ©é–¢æ•°ï¼šã‚¯ã‚¨ãƒªã‚’å‰å‡¦ç†"""
    return get_preprocessor().preprocess_query(query_text, expand_synonyms)

def analyze_query(query_text: str) -> Dict:
    """ä¾¿åˆ©é–¢æ•°ï¼šã‚¯ã‚¨ãƒªã‚’åˆ†æž"""
    return get_preprocessor().analyze_query(query_text) 
```

============================================================

ðŸ“„ FILE: nutrition_db_experiment/search_service/tests/test_search_algorithm.py
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 17,015 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 11:52:55
å­˜åœ¨: âœ…

CONTENT:
```
#!/usr/bin/env python3
"""
Search Algorithm Tests - æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å˜ä½“ãƒ»çµåˆãƒ†ã‚¹ãƒˆ

ç‰¹ã«å˜èªžå¢ƒç•Œå•é¡Œï¼ˆcook vs cookieï¼‰ã®æ¤œè¨¼ã‚’ä¸­å¿ƒã¨ã™ã‚‹
"""

import os
import sys
import json
import unittest
from typing import List, Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ 
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)
sys.path.append(os.path.join(project_root, 'search_service'))

class TestSearchAlgorithm(unittest.TestCase):
    """æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹"""
    
    @classmethod
    def setUpClass(cls):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            from nlp.query_preprocessor import FoodQueryPreprocessor
            from api.query_builder import NutritionSearchQueryBuilder
            from api.search_handler import NutritionSearchHandler
            
            cls.preprocessor = FoodQueryPreprocessor()
            cls.query_builder = NutritionSearchQueryBuilder()
            cls.search_handler = NutritionSearchHandler()
            
        except Exception as e:
            raise Exception(f"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def test_word_boundary_preprocessing(self):
        """å˜èªžå¢ƒç•Œå•é¡Œã®ã‚¯ã‚¨ãƒªå‰å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        print("\nðŸ”§ å˜èªžå¢ƒç•Œå•é¡Œã®ã‚¯ã‚¨ãƒªå‰å‡¦ç†ãƒ†ã‚¹ãƒˆ")
        
        test_cases = [
            # cooké–¢é€£ã®ãƒ†ã‚¹ãƒˆ
            {
                "input": "cook",
                "expected": "cook",
                "should_match": ["cook", "cooked", "cooking"],
                "should_not_match": ["cookie", "cookies"]
            },
            {
                "input": "cooking",
                "expected": "cook",  # cookã«ãƒ¬ãƒ³ãƒžåŒ–ã•ã‚Œã‚‹
                "should_match": ["cook", "cooked", "cooking"],
                "should_not_match": ["cookie", "cookies"]
            },
            {
                "input": "cooked",
                "expected": "cooked",  # ä¿è­·ã•ã‚Œã¦ãã®ã¾ã¾æ®‹ã‚‹
                "should_match": ["cook", "cooked", "cooking"],
                "should_not_match": ["cookie", "cookies"]
            },
            # cookieé–¢é€£ã®ãƒ†ã‚¹ãƒˆ
            {
                "input": "cookie",
                "expected": "cookie",  # ä¿è­·ã•ã‚Œã¦ãã®ã¾ã¾æ®‹ã‚‹
                "should_match": ["cookie", "cookies"],
                "should_not_match": ["cook", "cooking", "cooked"]
            },
            {
                "input": "cookies",
                "expected": "cookies",  # ä¿è­·ã•ã‚Œã¦ãã®ã¾ã¾æ®‹ã‚‹
                "should_match": ["cookie", "cookies"],
                "should_not_match": ["cook", "cooking", "cooked"]
            }
        ]
        
        for test_case in test_cases:
            with self.subTest(input=test_case["input"]):
                result = self.preprocessor.preprocess_query(test_case["input"])
                print(f"ðŸ“ '{test_case['input']}' â†’ '{result}'")
                
                # æœŸå¾…ã•ã‚Œã‚‹çµæžœã®ç¢ºèª
                self.assertEqual(result, test_case["expected"],
                               f"'{test_case['input']}'ã®å‰å‡¦ç†çµæžœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™")
    
    def test_phrase_preprocessing(self):
        """ãƒ•ãƒ¬ãƒ¼ã‚ºã‚¯ã‚¨ãƒªã®å‰å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        print("\nðŸ”§ ãƒ•ãƒ¬ãƒ¼ã‚ºã‚¯ã‚¨ãƒªã®å‰å‡¦ç†ãƒ†ã‚¹ãƒˆ")
        
        test_cases = [
            {
                "input": "chicken breast",
                "expected": "chicken breast",
                "description": "åŸºæœ¬çš„ãªãƒ•ãƒ¬ãƒ¼ã‚º"
            },
            {
                "input": "chocolate chip cookies",
                "expected": "chocolate chip cookies",
                "description": "cookieãŒä¿è­·ã•ã‚Œã‚‹ãƒ•ãƒ¬ãƒ¼ã‚º"
            },
            {
                "input": "baking a cake with flour",
                "expected": "baking cake flour",  # "a", "with"ãŒã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤åŽ»
                "description": "ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤åŽ»ã‚’å«ã‚€ãƒ•ãƒ¬ãƒ¼ã‚º"
            },
            {
                "input": "cooking ground beef",
                "expected": "cook ground beef",  # cookingãŒcookã«ãƒ¬ãƒ³ãƒžåŒ–
                "description": "ãƒ¬ãƒ³ãƒžåŒ–ã‚’å«ã‚€ãƒ•ãƒ¬ãƒ¼ã‚º"
            }
        ]
        
        for test_case in test_cases:
            with self.subTest(input=test_case["input"]):
                result = self.preprocessor.preprocess_query(test_case["input"])
                print(f"ðŸ“ '{test_case['input']}' â†’ '{result}' ({test_case['description']})")
                
                self.assertEqual(result, test_case["expected"],
                               f"ãƒ•ãƒ¬ãƒ¼ã‚ºå‰å‡¦ç†ãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™: {test_case['description']}")
    
    def test_query_builder_structure(self):
        """ã‚¯ã‚¨ãƒªãƒ“ãƒ«ãƒ€ãƒ¼æ§‹é€ ãƒ†ã‚¹ãƒˆ"""
        print("\nðŸ”§ ã‚¯ã‚¨ãƒªãƒ“ãƒ«ãƒ€ãƒ¼æ§‹é€ ãƒ†ã‚¹ãƒˆ")
        
        test_cases = [
            {
                "processed_query": "cook",
                "original_query": "cook",
                "db_type_filter": None,
                "description": "åŸºæœ¬çš„ãªå˜èªžã‚¯ã‚¨ãƒª"
            },
            {
                "processed_query": "cookie",
                "original_query": "cookie",
                "db_type_filter": "branded",
                "description": "ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãã‚¯ã‚¨ãƒª"
            },
            {
                "processed_query": "chicken breast",
                "original_query": "chicken breast",
                "db_type_filter": "ingredient",
                "description": "ãƒ•ãƒ¬ãƒ¼ã‚ºã‚¯ã‚¨ãƒª"
            }
        ]
        
        for test_case in test_cases:
            with self.subTest(description=test_case["description"]):
                query = self.query_builder.build_search_query(
                    processed_query=test_case["processed_query"],
                    original_query=test_case["original_query"],
                    db_type_filter=test_case["db_type_filter"]
                )
                
                print(f"ðŸ“ {test_case['description']}")
                
                # åŸºæœ¬æ§‹é€ ã®ç¢ºèª
                self.assertIn("query", query)
                self.assertIn("size", query)
                self.assertIn("_source", query)
                
                # ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã®ç¢ºèª
                query_structure = query["query"]
                if test_case["db_type_filter"]:
                    # ãƒ•ã‚£ãƒ«ã‚¿ä»˜ãã®å ´åˆã¯boolã‚¯ã‚¨ãƒª
                    self.assertIn("bool", query_structure)
                    self.assertIn("must", query_structure["bool"])
                    self.assertIn("filter", query_structure["bool"])
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ã®ç¢ºèª
                    filter_term = query_structure["bool"]["filter"][0]
                    self.assertEqual(filter_term["term"]["db_type"], test_case["db_type_filter"])
                    
                    # å†…éƒ¨ã®function_scoreã®ç¢ºèª
                    inner_query = query_structure["bool"]["must"][0]
                    self.assertIn("function_score", inner_query)
                else:
                    # ãƒ•ã‚£ãƒ«ã‚¿ãªã—ã®å ´åˆã¯ç›´æŽ¥function_score
                    self.assertIn("function_score", query_structure)
                
                print(f"   âœ… æ§‹é€ ç¢ºèªå®Œäº†")
    
    def test_function_score_functions(self):
        """function_scoreé–¢æ•°ã®è©³ç´°ãƒ†ã‚¹ãƒˆ"""
        print("\nðŸ”§ function_scoreé–¢æ•°ã®è©³ç´°ãƒ†ã‚¹ãƒˆ")
        
        query = self.query_builder.build_search_query(
            processed_query="cook chicken",
            original_query="cooking chicken",
            db_type_filter=None
        )
        
        # function_scoreã®å–å¾—
        function_score = query["query"]["function_score"]
        functions = function_score["functions"]
        
        print(f"ðŸ“ é–¢æ•°æ•°: {len(functions)}")
        
        # é–¢æ•°ã®ç¨®é¡žç¢ºèª
        function_types = []
        for func in functions:
            filter_query = func["filter"]
            if "match_phrase" in filter_query:
                if "search_name.exact" in filter_query["match_phrase"]:
                    function_types.append("exact_phrase")
                elif "search_name" in filter_query["match_phrase"]:
                    function_types.append("proximity_phrase")
            elif "term" in filter_query:
                function_types.append("exact_word")
            elif "match_phrase_prefix" in filter_query:
                function_types.append("prefix_match")
        
        print(f"ðŸ“ é–¢æ•°ã‚¿ã‚¤ãƒ—: {function_types}")
        
        # æœŸå¾…ã•ã‚Œã‚‹é–¢æ•°ã‚¿ã‚¤ãƒ—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        expected_types = ["exact_phrase", "proximity_phrase", "prefix_match"]
        for expected_type in expected_types:
            self.assertIn(expected_type, function_types,
                         f"æœŸå¾…ã•ã‚Œã‚‹é–¢æ•°ã‚¿ã‚¤ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {expected_type}")
        
        # ã‚¹ã‚³ã‚¢ãƒ¢ãƒ¼ãƒ‰ã®ç¢ºèª
        self.assertEqual(function_score["score_mode"], "sum")
        self.assertEqual(function_score["boost_mode"], "sum")
        
        print("   âœ… function_scoreè¨­å®šç¢ºèªå®Œäº†")
    
    def test_search_handler_integration(self):
        """æ¤œç´¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆãƒ¢ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ï¼‰"""
        print("\nðŸ”§ æ¤œç´¢ãƒãƒ³ãƒ‰ãƒ©ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ")
        
        from api.search_handler import SearchRequest
        
        test_cases = [
            {
                "query": "cook",
                "db_type_filter": None,
                "description": "cookæ¤œç´¢ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãªã—ï¼‰"
            },
            {
                "query": "cookie",
                "db_type_filter": "branded",
                "description": "cookieæ¤œç´¢ï¼ˆbrandedãƒ•ã‚£ãƒ«ã‚¿ï¼‰"
            },
            {
                "query": "chicken breast",
                "db_type_filter": "ingredient",
                "description": "ãƒ•ãƒ¬ãƒ¼ã‚ºæ¤œç´¢ï¼ˆingredientãƒ•ã‚£ãƒ«ã‚¿ï¼‰"
            }
        ]
        
        for test_case in test_cases:
            with self.subTest(description=test_case["description"]):
                request = SearchRequest(
                    query=test_case["query"],
                    db_type_filter=test_case["db_type_filter"],
                    size=10
                )
                
                response = self.search_handler.search(request)
                
                print(f"ðŸ“ {test_case['description']}")
                print(f"   å…ƒã‚¯ã‚¨ãƒª: '{response.query_info['original_query']}'")
                print(f"   å‡¦ç†æ¸ˆã¿: '{response.query_info['processed_query']}'")
                print(f"   çµæžœæ•°: {response.total_hits}")
                print(f"   å‡¦ç†æ™‚é–“: {response.took_ms}ms")
                
                # åŸºæœ¬çš„ãªå¿œç­”ç¢ºèª
                self.assertIsInstance(response.results, list)
                self.assertGreaterEqual(response.total_hits, 0)
                self.assertGreaterEqual(response.took_ms, 0)
                self.assertIn("original_query", response.query_info)
                self.assertIn("processed_query", response.query_info)
                
                print("   âœ… çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
    
    def test_word_boundary_test_cases_validation(self):
        """å˜èªžå¢ƒç•Œãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®å¦¥å½“æ€§ç¢ºèª"""
        print("\nðŸ”§ å˜èªžå¢ƒç•Œãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®å¦¥å½“æ€§ç¢ºèª")
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        test_cases_file = os.path.join(
            os.path.dirname(__file__), 
            "test_data", 
            "word_boundary_test_cases.json"
        )
        
        self.assertTrue(os.path.exists(test_cases_file), 
                       "å˜èªžå¢ƒç•Œãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        
        with open(test_cases_file, 'r', encoding='utf-8') as f:
            test_cases = json.load(f)
        
        print(f"ðŸ“ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {len(test_cases)}")
        
        for i, test_case in enumerate(test_cases):
            with self.subTest(case_index=i):
                # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
                required_fields = ["query", "db_type_filter", "expected_top_results_patterns"]
                for field in required_fields:
                    self.assertIn(field, test_case, 
                                f"ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹{i}ã«å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰'{field}'ãŒã‚ã‚Šã¾ã›ã‚“")
                
                # ã‚¯ã‚¨ãƒªã®å‰å‡¦ç†ãƒ†ã‚¹ãƒˆ
                processed = self.preprocessor.preprocess_query(test_case["query"])
                print(f"   ã‚±ãƒ¼ã‚¹{i+1}: '{test_case['query']}' â†’ '{processed}'")
                
                # æ¤œç´¢ã‚¯ã‚¨ãƒªæ§‹ç¯‰ãƒ†ã‚¹ãƒˆ
                query = self.query_builder.build_search_query(
                    processed_query=processed,
                    original_query=test_case["query"],
                    db_type_filter=test_case["db_type_filter"]
                )
                
                self.assertIsInstance(query, dict)
                self.assertIn("query", query)
        
        print("   âœ… ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å¦¥å½“æ€§ç¢ºèªå®Œäº†")

class TestProtectedTerms(unittest.TestCase):
    """ä¿è­·ã‚¿ãƒ¼ãƒ ã®è©³ç´°ãƒ†ã‚¹ãƒˆ"""
    
    @classmethod
    def setUpClass(cls):
        """ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        from nlp.query_preprocessor import FoodQueryPreprocessor
        cls.preprocessor = FoodQueryPreprocessor()
    
    def test_protected_terms_preservation(self):
        """ä¿è­·ã‚¿ãƒ¼ãƒ ãŒæ­£ã—ãä¿è­·ã•ã‚Œã‚‹ã‹ãƒ†ã‚¹ãƒˆ"""
        print("\nðŸ”§ ä¿è­·ã‚¿ãƒ¼ãƒ ã®ä¿è­·ãƒ†ã‚¹ãƒˆ")
        
        # ä¿è­·ã•ã‚Œã‚‹ã¹ãå˜èªž
        protected_words = ["cookie", "cookies", "orange", "baked"]
        
        for word in protected_words:
            with self.subTest(word=word):
                analysis = self.preprocessor.analyze_query(word)
                
                # ä¿è­·ã•ã‚ŒãŸã‚¿ãƒ¼ãƒ ã®å­˜åœ¨ç¢ºèª
                protected_tokens = [
                    token for token in analysis["tokens"] 
                    if token["is_protected"]
                ]
                
                self.assertGreater(len(protected_tokens), 0,
                                 f"'{word}'ãŒä¿è­·ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                
                print(f"ðŸ“ '{word}' â†’ ä¿è­·æ¸ˆã¿: {len(protected_tokens)}å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³")
    
    def test_lemma_overrides(self):
        """ãƒ¬ãƒ³ãƒžä¸Šæ›¸ãã®å‹•ä½œãƒ†ã‚¹ãƒˆ"""
        print("\nðŸ”§ ãƒ¬ãƒ³ãƒžä¸Šæ›¸ãå‹•ä½œãƒ†ã‚¹ãƒˆ")
        
        override_cases = [
            {"input": "cooking", "expected": "cook"},
            {"input": "cooked", "expected": "cooked"},  # ä¿è­·ã•ã‚Œã‚‹
            {"input": "baking", "expected": "baking"},  # ä¿è­·ã•ã‚Œã‚‹
        ]
        
        for case in override_cases:
            with self.subTest(input=case["input"]):
                result = self.preprocessor.preprocess_query(case["input"])
                print(f"ðŸ“ '{case['input']}' â†’ '{result}'")
                
                # æœŸå¾…ã•ã‚Œã‚‹çµæžœã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
                self.assertEqual(result, case["expected"],
                               f"ãƒ¬ãƒ³ãƒžä¸Šæ›¸ãçµæžœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™")

def run_search_algorithm_tests():
    """æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("ðŸ§ª æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å˜ä½“ãƒ»çµåˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®ä½œæˆ
    test_suite = unittest.TestSuite()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®è¿½åŠ 
    test_suite.addTest(unittest.makeSuite(TestSearchAlgorithm))
    test_suite.addTest(unittest.makeSuite(TestProtectedTerms))
    
    # ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ã®è¨­å®š
    runner = unittest.TextTestRunner(
        verbosity=2,
        stream=sys.stdout,
        buffer=False
    )
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    result = runner.run(test_suite)
    
    # çµæžœã‚µãƒžãƒªãƒ¼
    print("\nðŸ“Š ãƒ†ã‚¹ãƒˆçµæžœã‚µãƒžãƒªãƒ¼")
    print("=" * 60)
    print(f"å®Ÿè¡Œ: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±æ•—: {len(result.failures)}")
    print(f"ã‚¨ãƒ©ãƒ¼: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("ðŸŽ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
    else:
        print("âš ï¸  ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        
        if result.failures:
            print("\nå¤±æ•—:")
            for test, traceback in result.failures:
                print(f"  - {test}: {traceback}")
        
        if result.errors:
            print("\nã‚¨ãƒ©ãƒ¼:")
            for test, traceback in result.errors:
                print(f"  - {test}: {traceback}")
    
    return result.wasSuccessful()

if __name__ == "__main__":
    run_search_algorithm_tests() 
```

============================================================

âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
============================================================

ðŸ“„ FILE: nutrition_db_experiment/nutrition_database_specification.md
--------------------------------------------------
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 5,856 bytes
æœ€çµ‚æ›´æ–°: 2025-06-06 13:42:19
å­˜åœ¨: âœ…

CONTENT:
```
# æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä»•æ§˜æ›¸ (Nutrition Database Specification)

## æ¦‚è¦ (Overview)

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€USDA Food Data Central API ã‹ã‚‰åŽé›†ã—ãŸç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«æ§‹ç¯‰ã—ãŸçµ±ä¸€æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä»•æ§˜ã‚’èª¬æ˜Žã—ã¾ã™ã€‚

## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€  (Database Structure)

### çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ (Unified Format)

å…¨ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã¯ä»¥ä¸‹ã®çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã«å¾“ã„ã¾ã™ï¼š

```json
{
  "db_type": "string",        // "dish", "ingredient", "branded"ã®ã„ãšã‚Œã‹
  "id": number,               // USDA Food Data Centralã®ID
  "search_name": "string",    // æ¤œç´¢ç”¨ã®åå‰
  "nutrition": {
    "calories": number,       // ã‚«ãƒ­ãƒªãƒ¼ (kcal/100g)
    "protein": number,        // ã‚¿ãƒ³ãƒ‘ã‚¯è³ª (g/100g)
    "fat": number,           // è„‚è³ª (g/100g)
    "carbs": number          // ç‚­æ°´åŒ–ç‰© (g/100g)
  },
  "weight": number            // åŸºæº–é‡é‡ (g)
}
```

## ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´° (Category Details)

### 1. Dish (æ–™ç†ãƒ»ãƒ¬ã‚·ãƒ”)

**èª¬æ˜Ž**: å®Œæˆã•ã‚ŒãŸæ–™ç†ã‚„ãƒ¬ã‚·ãƒ”ã®ãƒ‡ãƒ¼ã‚¿
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: USDA Recipe ãƒ‡ãƒ¼ã‚¿
**ç‰¹å¾´**: è¤‡æ•°ã®é£Ÿæã‚’çµ„ã¿åˆã‚ã›ãŸå®Œæˆå“

#### JSON ã‚µãƒ³ãƒ—ãƒ«:

```json
{
  "db_type": "dish",
  "id": 123456,
  "search_name": "Chicken stir-fry with vegetables",
  "nutrition": {
    "calories": 145.5,
    "protein": 18.2,
    "fat": 6.8,
    "carbs": 4.3
  },
  "weight": 150.0
}
```

#### å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹:

- `title` â†’ `search_name`
- `nutrients.servingSize` â†’ `weight` (gram æŠ½å‡º)
- æ „é¤Šç´ ã¯ 100g ã‚ãŸã‚Šã«æ­£è¦åŒ–

### 2. Ingredient (é£Ÿæãƒ»åŸºæœ¬é£Ÿå“)

**èª¬æ˜Ž**: å€‹åˆ¥ã®é£Ÿæã‚„åŸºæœ¬çš„ãªé£Ÿå“ã®ãƒ‡ãƒ¼ã‚¿
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: USDA Food ãƒ‡ãƒ¼ã‚¿
**ç‰¹å¾´**: å˜ä¸€é£Ÿæã€èª¿ç†å‰ã®çŠ¶æ…‹

#### JSON ã‚µãƒ³ãƒ—ãƒ«:

```json
{
  "db_type": "ingredient",
  "id": 789012,
  "search_name": "Chicken, breast, boneless, skinless, raw",
  "nutrition": {
    "calories": 165.0,
    "protein": 31.0,
    "fat": 3.6,
    "carbs": 0.0
  },
  "weight": 100.0
}
```

#### å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹:

- `name` + `description` â†’ `search_name`
- `units`ã‹ã‚‰`description="grams"`ã® amount ã‚’`weight`ã¨ã—ã¦ä½¿ç”¨
- æ „é¤Šç´ ã¯ 100g ã‚ãŸã‚Šã«æ­£è¦åŒ–

### 3. Branded (ãƒ–ãƒ©ãƒ³ãƒ‰é£Ÿå“)

**èª¬æ˜Ž**: ç‰¹å®šãƒ–ãƒ©ãƒ³ãƒ‰ã®å•†å“ãƒ‡ãƒ¼ã‚¿
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: USDA Branded Food ãƒ‡ãƒ¼ã‚¿
**ç‰¹å¾´**: å•†ç”¨è£½å“ã€ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é£Ÿå“

#### JSON ã‚µãƒ³ãƒ—ãƒ«:

```json
{
  "db_type": "branded",
  "id": 345678,
  "search_name": "KRAFT, Macaroni & Cheese Dinner Original",
  "nutrition": {
    "calories": 370.0,
    "protein": 11.0,
    "fat": 3.0,
    "carbs": 71.0
  },
  "weight": 70.0
}
```

#### å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹:

- `food_name` + `description` â†’ `search_name`
- `unit_weights`ã‹ã‚‰`description="grams"`ã® amount ã‚’`weight`ã¨ã—ã¦ä½¿ç”¨
- æ „é¤Šç´ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯`calories`/`serving_calories`, `proteins`/`serving_proteins`ç­‰ã‹ã‚‰å–å¾—
- æ „é¤Šç´ ã¯ 100g ã‚ãŸã‚Šã«æ­£è¦åŒ–

## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ (Database File Structure)

```
nutrition_db/
â”œâ”€â”€ dish_db.json              # æ–™ç†ãƒ‡ãƒ¼ã‚¿ã®ã¿
â”œâ”€â”€ ingredient_db.json        # é£Ÿæãƒ‡ãƒ¼ã‚¿ã®ã¿
â”œâ”€â”€ branded_db.json           # ãƒ–ãƒ©ãƒ³ãƒ‰é£Ÿå“ãƒ‡ãƒ¼ã‚¿ã®ã¿
â”œâ”€â”€ unified_nutrition_db.json # å…¨ã‚«ãƒ†ã‚´ãƒªçµ±åˆ
â””â”€â”€ build_stats.json          # æ§‹ç¯‰çµ±è¨ˆæƒ…å ±
```

## æ¤œç´¢ãƒ»åˆ©ç”¨æ–¹æ³• (Search & Usage)

### 1. ã‚«ãƒ†ã‚´ãƒªåˆ¥æ¤œç´¢

ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æ¤œç´¢ã—ãŸã„å ´åˆã¯ã€å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã€‚

### 2. çµ±åˆæ¤œç´¢

å…¨ã‚«ãƒ†ã‚´ãƒªã‚’æ¨ªæ–­ã—ã¦æ¤œç´¢ã—ãŸã„å ´åˆã¯ã€`unified_nutrition_db.json`ã‚’ä½¿ç”¨ã€‚

### 3. æ¤œç´¢ã‚­ãƒ¼

- `search_name`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢
- `db_type`ã§ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- `id`ã§ç‰¹å®šã‚¢ã‚¤ãƒ†ãƒ ã®ç›´æŽ¥ã‚¢ã‚¯ã‚»ã‚¹

## æ „é¤Šå€¤ã®æ­£è¦åŒ– (Nutrition Value Normalization)

- **åŸºæº–**: å…¨ã¦ã®æ „é¤Šå€¤ã¯ 100g ã‚ãŸã‚Šã§æ­£è¦åŒ–
- **è¨ˆç®—æ–¹æ³•**: `(å…ƒã®æ „é¤Šå€¤ / å…ƒã®é‡é‡) Ã— 100`
- **åˆ©ç‚¹**: ç•°ãªã‚‹é£Ÿå“é–“ã§ã®æ „é¤Šä¾¡æ¯”è¼ƒãŒå®¹æ˜“

## æ¤œç´¢ä»•æ§˜ (Search Specification)

### æ¤œç´¢å¯¾è±¡ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è©³ç´°

#### search_name ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰

- **ãƒ‡ãƒ¼ã‚¿åž‹**: string
- **æ§‹æˆ**: å…ƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã® `name` ã¨ `description` ã‚’çµåˆ
- **ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ**: ãƒ¡ã‚¤ãƒ³åç§° + ä¿®é£¾èªžï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰
  - ä¾‹: `"Onions, Cooked, boiled, drained, with salt"`
  - ä¾‹: `"Roasted Cherry Tomatoes with Mint"`
- **é•·ã•**:
  - **é€šå¸¸**: 5 å˜èªžä»¥å†…
  - **æœ€å¤§**: 10 å˜èªžç¨‹åº¦
- **ç‰¹å¾´**: è‹±èªžãƒ™ãƒ¼ã‚¹ã€é£Ÿæãƒ»æ–™ç†ã®è©³ç´°ãªèª¬æ˜Žã‚’å«ã‚€

### æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¦ä»¶

#### å˜èªžå¢ƒç•Œå•é¡Œã¸ã®å¯¾å‡¦

æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®å˜èªžå¢ƒç•Œå•é¡Œã«å¯¾å‡¦ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š

**å•é¡Œä¾‹**: `"cook"` ã§ã‚¯ã‚¨ãƒªã—ãŸå ´åˆã®æœŸå¾…ã•ã‚Œã‚‹çµæžœ

```
âœ… é«˜ã‚¹ã‚³ã‚¢ï¼ˆæ„å‘³çš„ã«é–¢é€£ï¼‰:
- "cook" (å®Œå…¨ä¸€è‡´)
- "cooking" (åŒã˜èªžå¹¹)
- "cooked" (åŒã˜èªžå¹¹)

âŒ ä½Žã‚¹ã‚³ã‚¢ï¼ˆæ„å‘³çš„ã«ç„¡é–¢ä¿‚ï¼‰:
- "cookie" (æ–‡å­—åˆ—çš„ã«ã¯ä¼¼ã¦ã„ã‚‹ãŒæ„å‘³ãŒç•°ãªã‚‹)
- "cookies" (æ–‡å­—åˆ—çš„ã«ã¯ä¼¼ã¦ã„ã‚‹ãŒæ„å‘³ãŒç•°ãªã‚‹)
```

#### å®Ÿè£…æŽ¨å¥¨äº‹é …

1. **èªžå¹¹å‡¦ç† (Stemming)**: èªžå°¾å¤‰åŒ–ã‚’æ­£è¦åŒ–
2. **BM25F æ¤œç´¢**: ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åˆ¥é‡ã¿ä»˜ãæ¤œç´¢
3. **ãƒžãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°**: è¤‡æ•°ã®é–¢é€£åº¦æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›
4. **åŒç¾©èªžè¾žæ›¸**: é£Ÿæãƒ»æ–™ç†å›ºæœ‰ã®åŒç¾©èªžå¯¾å¿œ
5. **éƒ¨åˆ†ä¸€è‡´ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°**: æ–‡å­—åˆ—é¡žä¼¼åº¦ vs æ„å‘³çš„é¡žä¼¼åº¦ã®ãƒãƒ©ãƒ³ã‚¹

### æ¤œç´¢æ€§èƒ½æŒ‡æ¨™

- **ç›®æ¨™ç²¾åº¦**: 90%ä»¥ä¸Šã®ãƒžãƒƒãƒçŽ‡
- **å¿œç­”æ™‚é–“**: 1 ç§’ä»¥å†…ï¼ˆ8,878 é …ç›®å¯¾è±¡ï¼‰
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: ElasticSearch åˆ©ç”¨ä¸å¯æ™‚ã®ç›´æŽ¥æ¤œç´¢å¯¾å¿œ

```

============================================================

ðŸ“Š ANALYSIS STATISTICS
----------------------------------------
ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 37
å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: 37
åˆ†æžå®Œäº†æ™‚åˆ»: 2025-06-06 13:56:43

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€test_local_nutrition_search_v2.pyå®Ÿè¡Œæ™‚ã«
å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
