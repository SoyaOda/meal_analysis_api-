====================================================================================================
MEAL ANALYSIS API v2.1 - Multi-Image Analysis System Architecture
====================================================================================================
Generated: 2025-06-15 15:34:52
Analysis Target: All files related to test_multi_image_analysis.py execution
====================================================================================================

ğŸ¯ LATEST MULTI-IMAGE ANALYSIS SUMMARY
------------------------------------------------------------
ğŸ“‚ Result Directory: multi_image_analysis_20250615_145032
ğŸ“„ Summary Preview:
# ğŸ½ï¸ Multi-Image Nutrition Analysis - åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼

**åˆ†ææ—¥æ™‚:** 2025å¹´06æœˆ15æ—¥ 14:52:12  
**åˆ†æç”»åƒæ•°:** 5æš  
**æˆåŠŸåˆ†æ:** 5æš  
**å¤±æ•—åˆ†æ:** 0æš  

## ğŸ“Š å…¨ç”»åƒæ „é¤Šã‚µãƒãƒªãƒ¼

| ç”»åƒ | æ–™ç†æ•° | é£Ÿææ•° | ç·ã‚«ãƒ­ãƒªãƒ¼ | ã‚¿ãƒ³ãƒ‘ã‚¯è³ª | è„‚è³ª | ç‚­æ°´åŒ–ç‰© | å‡¦ç†æ™‚é–“ |
|------|--------|--------|------------|------------|------|----------|----------|
| food1 | 3 | 9 | 823.1 kcal | 26.5g | 24.9g | 125.1g | 19.3s |
| food2 | 5 | 13 | 1255.1 kcal | 60.4g | 55.7g | 129.2g | 25.2s |
| food3 | 3 | 13 | 810.3 kcal | 66.5g | 31.8g | 63.4g | 17.2s |
| food4 | 3 | 17 | 647.6 kcal | 43.3g | 30.4g | 57.3g | 12.6s |
| food5 | 2 | 14 | 785.3 kcal | 36.7g | 49.0g | 49.4g | 25.2s |
| **åˆè¨ˆ** | - | - | **4321.5 kcal** | **233.3g** | **191.8g** | **424.4g** | **99.4s** |
| **å¹³å‡** | - | - | **864.3 kcal** | **46.7g** | **38.4g** | **84.9g** | **19.9s** |


## ğŸ¯ åˆ†æçµ±è¨ˆ

### ğŸ“ˆ æ „é¤Šçµ±è¨ˆ
- **ç·ã‚«ãƒ­ãƒªãƒ¼æ‘‚å–é‡:** 4321.5 kcal
- **ç·ã‚¿ãƒ³ãƒ‘ã‚¯è³ª:** 233.3 g

====================================================================================================

ğŸš€ MYNETDIARY-CONSTRAINED SEARCH & NUTRITION CALCULATION ARCHITECTURE
--------------------------------------------------------------------------------

ğŸ”„ EXECUTION FLOW:
1.  test_multi_image_analysis.py: Initiates batch processing of multiple JPG images.
2.  FastAPI Endpoint (/api/v1/meal-analyses/complete): Receives each image for full analysis.
    - `use_mynetdiary_specialized=True` flag is set.
3.  Orchestrator: Controls the pipeline execution.
4.  Phase 1 (Gemini Vision Analysis):
    - `phase1_prompts.py`: Uses a prompt now including the 1,142 MyNetDiary ingredients list and a new `weight_g` estimation requirement.
    - `gemini_service.py`: Calls the model and structures the output.
    - `phase1_models.py`: Pydantic model `Ingredient` now requires `weight_g`.
5.  Phase 2 (MyNetDiary-Constrained Search):
    - `mynetdiary_nutrition_search_component.py` is selected by the orchestrator.
    - `_strict_ingredient_search()`: Searches for ingredients with an "exactly one" match rule.
    - `_flexible_dish_search()`: Performs a standard flexible search for dishes.
6.  Phase 3 (Nutrition Calculation):
    - `nutrition_calculation_component.py`:
        - Takes `weight_g` from Phase 1 and nutrition data (per 100g) from Phase 2.
        - Calculates nutrition for the actual ingredient weight.
        - Aggregates nutrition totals for each dish and the entire meal.
7.  Result Manager: Saves all intermediate and final results to the filesystem.

ğŸ—ï¸ KEY ARCHITECTURE COMPONENTS:
â”œâ”€â”€ Test Layer
â”‚   â””â”€â”€ test_multi_image_analysis.py
â”œâ”€â”€ FastAPI Application Layer (app_v2)
â”‚   â””â”€â”€ pipeline/orchestrator.py (Conditionally selects search component)
â”œâ”€â”€ Component Layer
â”‚   â”œâ”€â”€ phase1_component.py
â”‚   â”œâ”€â”€ mynetdiary_nutrition_search_component.py (New: Strict & flexible search)
â”‚   â””â”€â”€ nutrition_calculation_component.py (New: Weight-based calculation)
â”œâ”€â”€ AI Service & Prompts
â”‚   â”œâ”€â”€ services/gemini_service.py
â”‚   â””â”€â”€ config/prompts/phase1_prompts.py (Updated with ingredient list & weight estimation)
â”œâ”€â”€ Data Models
â”‚   â”œâ”€â”€ models/phase1_models.py (Ingredient includes `weight_g`)
â”‚   â””â”€â”€ models/nutrition_calculation_models.py (New: For structured nutrition output)
â””â”€â”€ Utilities & Data
    â”œâ”€â”€ utils/mynetdiary_utils.py (New: Handles ingredient list)
    â””â”€â”€ data/mynetdiary_search_names.txt (New: 1,142 ingredient constraints)

====================================================================================================


################################################################################
## CATEGORY: Multi-Image Analysis ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«
################################################################################

----------------------------------------------------------------------
### FILE: test_multi_image_analysis.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/test_multi_image_analysis.py
----------------------------------------------------------------------

#!/usr/bin/env python3
"""
Multi-Image Nutrition Analysis Test - food1-5 Batch Processing
food1.jpg ã‹ã‚‰ food5.jpg ã¾ã§5ã¤ã®ç”»åƒã‚’ä¸€æ‹¬åˆ†æã—ã€
è¦‹ã‚„ã™ã„å½¢ã§çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import asyncio
import os
import sys
import logging
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app_v2.pipeline import MealAnalysisPipeline

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


def setup_environment():
    """ç’°å¢ƒå¤‰æ•°ã®è¨­å®š"""
    os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", "/Users/odasoya/meal_analysis_api_2/service-account-key.json")
    os.environ.setdefault("GEMINI_PROJECT_ID", "recording-diet-ai-3e7cf")
    os.environ.setdefault("GEMINI_LOCATION", "us-central1")
    os.environ.setdefault("GEMINI_MODEL_NAME", "gemini-2.5-flash-preview-05-20")


def get_image_mime_type(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰MIMEã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
    ext = Path(file_path).suffix.lower()
    mime_types = {
        '.jpg': 'image/jpeg',
        '.jpeg': 'image/jpeg',
        '.png': 'image/png',
        '.gif': 'image/gif',
        '.bmp': 'image/bmp',
        '.webp': 'image/webp'
    }
    return mime_types.get(ext, 'image/jpeg')


async def analyze_single_image(image_path: str, results_dir: str, image_index: int) -> Dict[str, Any]:
    """å˜ä¸€ç”»åƒã®åˆ†æã‚’å®Ÿè¡Œ"""
    
    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(image_path):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
        return None
    
    # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    with open(image_path, 'rb') as f:
        image_bytes = f.read()
    
    image_mime_type = get_image_mime_type(image_path)
    image_name = Path(image_path).stem
    
    print(f"\n{'='*80}")
    print(f"ğŸ½ï¸  ç”»åƒ {image_index}/5: {image_name}")
    print(f"ğŸ“ åˆ†æå¯¾è±¡: {image_path}")
    print(f"ğŸ“Š ç”»åƒã‚µã‚¤ã‚º: {len(image_bytes):,} bytes")
    print(f"ğŸ” MIMEã‚¿ã‚¤ãƒ—: {image_mime_type}")
    print(f"{'='*80}")
    
    # ç”»åƒå°‚ç”¨ã®çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    image_results_dir = f"{results_dir}/{image_name}"
    os.makedirs(image_results_dir, exist_ok=True)
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–ï¼ˆãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°æ¤œç´¢ã‚’ä½¿ç”¨ï¼‰
    pipeline = MealAnalysisPipeline(
        use_elasticsearch_search=True,
        use_fuzzy_matching=True  # æ–°ã—ã„ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
    )
    
    try:
        print(f"ğŸ”„ {image_name} åˆ†æé–‹å§‹...")
        analysis_start_time = time.time()
        
        result = await pipeline.execute_complete_analysis(
            image_bytes=image_bytes,
            image_mime_type=image_mime_type,
            save_detailed_logs=True,
            test_execution=True,
            test_results_dir=image_results_dir
        )
        
        analysis_end_time = time.time()
        analysis_time = analysis_end_time - analysis_start_time
        
        print(f"âœ… {image_name} åˆ†æå®Œäº†ï¼ ({analysis_time:.2f}ç§’)")
        
        # åŸºæœ¬çµæœã®è¡¨ç¤º
        print_image_analysis_summary(result, image_name)
        
        # çµæœã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        result["image_metadata"] = {
            "image_name": image_name,
            "image_path": image_path,
            "image_size_bytes": len(image_bytes),
            "analysis_time_seconds": analysis_time,
            "analysis_timestamp": datetime.now().isoformat()
        }
        
        # ç”»åƒå°‚ç”¨ã®è©³ç´°çµæœã‚’ä¿å­˜
        await save_image_detailed_results(result, image_results_dir, image_name)
        
        return result
        
    except Exception as e:
        print(f"âŒ {image_name} åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error(f"Analysis failed for {image_name}: {str(e)}", exc_info=True)
        return {
            "error": str(e),
            "image_name": image_name,
            "image_path": image_path,
            "analysis_timestamp": datetime.now().isoformat()
        }


def print_image_analysis_summary(result: dict, image_name: str):
    """ç”»åƒåˆ†æçµæœã®åŸºæœ¬ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    
    print(f"\nğŸ“‹ {image_name} åˆ†æçµæœã‚µãƒãƒªãƒ¼")
    print(f"{'â”€'*60}")
    
    # Phase1çµæœ
    phase1_result = result.get("phase1_result", {})
    dishes = phase1_result.get("dishes", [])
    
    print(f"ğŸ½ï¸  æ¤œå‡ºã•ã‚ŒãŸæ–™ç†: {len(dishes)}å€‹")
    for i, dish in enumerate(dishes, 1):
        dish_name = dish.get("dish_name", "ä¸æ˜")
        confidence = dish.get("confidence", 0.0)
        ingredients = dish.get("ingredients", [])
        print(f"   {i}. {dish_name} (ä¿¡é ¼åº¦: {confidence:.2f}, é£Ÿæ: {len(ingredients)}å€‹)")
    
    # æ „é¤Šè¨ˆç®—çµæœ
    final_nutrition = result.get("final_nutrition_result", {})
    total_nutrition = final_nutrition.get("total_nutrition", {})
    
    if total_nutrition:
        calories = total_nutrition.get("calories", 0)
        protein = total_nutrition.get("protein", 0)
        fat = total_nutrition.get("fat", 0)
        carbs = total_nutrition.get("carbs", 0)
        
        print(f"\nğŸ”¢ æ „é¤Šè¨ˆç®—çµæœ:")
        print(f"   ğŸ“Š ç·ã‚«ãƒ­ãƒªãƒ¼: {calories:.1f} kcal")
        print(f"   ğŸ¥© ã‚¿ãƒ³ãƒ‘ã‚¯è³ª: {protein:.1f} g")
        print(f"   ğŸ§ˆ è„‚è³ª: {fat:.1f} g")
        print(f"   ğŸ ç‚­æ°´åŒ–ç‰©: {carbs:.1f} g")
    
    # å‡¦ç†ã‚µãƒãƒªãƒ¼
    processing_summary = result.get("processing_summary", {})
    total_ingredients = processing_summary.get("total_ingredients", 0)
    match_rate = processing_summary.get("nutrition_search_match_rate", "ä¸æ˜")
    processing_time = processing_summary.get("processing_time_seconds", 0)
    
    print(f"\nâš¡ å‡¦ç†ã‚µãƒãƒªãƒ¼:")
    print(f"   ğŸ¥• ç·é£Ÿææ•°: {total_ingredients}å€‹")
    print(f"   ğŸ¯ ãƒãƒƒãƒç‡: {match_rate}")
    print(f"   â±ï¸  å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")


async def save_image_detailed_results(result: dict, image_results_dir: str, image_name: str):
    """ç”»åƒã®è©³ç´°çµæœã‚’ä¿å­˜"""
    
    # 1. å®Œå…¨ãªçµæœã‚’JSONã§ä¿å­˜
    complete_result_path = f"{image_results_dir}/complete_analysis_result.json"
    with open(complete_result_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    # 2. æ „é¤Šè¨ˆç®—çµæœã®ã‚µãƒãƒªãƒ¼ã‚’Markdownã§ä¿å­˜
    nutrition_summary_path = f"{image_results_dir}/nutrition_summary.md"
    await create_nutrition_summary_markdown(result, nutrition_summary_path, image_name)
    
    # 3. æ–™ç†åˆ¥è©³ç´°ã‚’Markdownã§ä¿å­˜
    dish_details_path = f"{image_results_dir}/dish_details.md"
    await create_dish_details_markdown(result, dish_details_path, image_name)
    
    print(f"ğŸ’¾ {image_name} è©³ç´°çµæœä¿å­˜å®Œäº†:")
    print(f"   ğŸ“„ å®Œå…¨çµæœ: {complete_result_path}")
    print(f"   ğŸ“Š æ „é¤Šã‚µãƒãƒªãƒ¼: {nutrition_summary_path}")
    print(f"   ğŸ½ï¸  æ–™ç†è©³ç´°: {dish_details_path}")


async def create_nutrition_summary_markdown(result: dict, file_path: str, image_name: str):
    """æ „é¤Šè¨ˆç®—çµæœã®ã‚µãƒãƒªãƒ¼Markdownã‚’ä½œæˆ"""
    
    final_nutrition = result.get("final_nutrition_result", {})
    total_nutrition = final_nutrition.get("total_nutrition", {})
    dishes = final_nutrition.get("dishes", [])
    
    content = f"""# {image_name} æ „é¤Šåˆ†æã‚µãƒãƒªãƒ¼

## ğŸ“Š é£Ÿäº‹å…¨ä½“ã®æ „é¤Šæƒ…å ±

| æ „é¤Šç´  | å€¤ |
|--------|-----|
| ğŸ”¥ ã‚«ãƒ­ãƒªãƒ¼ | {total_nutrition.get('calories', 0):.1f} kcal |
| ğŸ¥© ã‚¿ãƒ³ãƒ‘ã‚¯è³ª | {total_nutrition.get('protein', 0):.1f} g |
| ğŸ§ˆ è„‚è³ª | {total_nutrition.get('fat', 0):.1f} g |
| ğŸ ç‚­æ°´åŒ–ç‰© | {total_nutrition.get('carbs', 0):.1f} g |
| ğŸŒ¾ é£Ÿç‰©ç¹Šç¶­ | {total_nutrition.get('fiber') or 'ä¸æ˜'} g |
| ğŸ¯ ç³–è³ª | {total_nutrition.get('sugar') or 'ä¸æ˜'} g |
| ğŸ§‚ ãƒŠãƒˆãƒªã‚¦ãƒ  | {total_nutrition.get('sodium') or 'ä¸æ˜'} mg |

## ğŸ½ï¸ æ–™ç†åˆ¥æ „é¤Šæƒ…å ±

"""
    
    for i, dish in enumerate(dishes, 1):
        dish_name = dish.get("dish_name", "ä¸æ˜")
        dish_nutrition = dish.get("total_nutrition", {})
        ingredients = dish.get("ingredients", [])
        
        content += f"""### {i}. {dish_name}

**æ „é¤Šæƒ…å ±:**
- ğŸ”¥ ã‚«ãƒ­ãƒªãƒ¼: {dish_nutrition.get('calories', 0):.1f} kcal
- ğŸ¥© ã‚¿ãƒ³ãƒ‘ã‚¯è³ª: {dish_nutrition.get('protein', 0):.1f} g
- ğŸ§ˆ è„‚è³ª: {dish_nutrition.get('fat', 0):.1f} g
- ğŸ ç‚­æ°´åŒ–ç‰©: {dish_nutrition.get('carbs', 0):.1f} g

**å«ã¾ã‚Œã‚‹é£Ÿæ:** {len(ingredients)}å€‹

"""
        
        for ingredient in ingredients:
            ing_name = ingredient.get("ingredient_name", "ä¸æ˜")
            weight = ingredient.get("weight_g", 0)
            ing_nutrition = ingredient.get("calculated_nutrition", {})
            
            content += f"- **{ing_name}** ({weight}g): {ing_nutrition.get('calories', 0):.1f} kcal\n"
        
        content += "\n"
    
    # åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    image_metadata = result.get("image_metadata", {})
    processing_summary = result.get("processing_summary", {})
    
    content += f"""## ğŸ“ˆ åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

- **åˆ†ææ—¥æ™‚:** {image_metadata.get('analysis_timestamp', 'ä¸æ˜')}
- **å‡¦ç†æ™‚é–“:** {image_metadata.get('analysis_time_seconds', 0):.2f}ç§’
- **ç·é£Ÿææ•°:** {processing_summary.get('total_ingredients', 0)}å€‹
- **ãƒãƒƒãƒç‡:** {processing_summary.get('nutrition_search_match_rate', 'ä¸æ˜')}
- **ç”»åƒã‚µã‚¤ã‚º:** {image_metadata.get('image_size_bytes', 0):,} bytes

---
*Generated by Multi-Image Nutrition Analysis System*
"""
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)


async def create_dish_details_markdown(result: dict, file_path: str, image_name: str):
    """æ–™ç†åˆ¥è©³ç´°æƒ…å ±ã®Markdownã‚’ä½œæˆ"""
    
    phase1_result = result.get("phase1_result", {})
    dishes = phase1_result.get("dishes", [])
    final_nutrition = result.get("final_nutrition_result", {})
    nutrition_dishes = final_nutrition.get("dishes", [])
    
    content = f"""# {image_name} æ–™ç†è©³ç´°åˆ†æ

## ğŸ” æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ä¸€è¦§

"""
    
    for i, (phase1_dish, nutrition_dish) in enumerate(zip(dishes, nutrition_dishes), 1):
        dish_name = phase1_dish.get("dish_name", "ä¸æ˜")
        confidence = phase1_dish.get("confidence", 0.0)
        ingredients = phase1_dish.get("ingredients", [])
        nutrition_ingredients = nutrition_dish.get("ingredients", [])
        
        content += f"""## {i}. {dish_name}

**åŸºæœ¬æƒ…å ±:**
- ğŸ¯ ä¿¡é ¼åº¦: {confidence:.2f}
- ğŸ¥• é£Ÿææ•°: {len(ingredients)}å€‹
- âš–ï¸ ç·é‡é‡: {sum(ing.get('weight_g', 0) for ing in ingredients)}g

### ğŸ“‹ é£Ÿæè©³ç´°

| é£Ÿæå | é‡é‡ | ã‚«ãƒ­ãƒªãƒ¼ | ã‚¿ãƒ³ãƒ‘ã‚¯è³ª | è„‚è³ª | ç‚­æ°´åŒ–ç‰© | ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ |
|--------|------|----------|------------|------|----------|--------------|
"""
        
        for phase1_ing, nutrition_ing in zip(ingredients, nutrition_ingredients):
            ing_name = phase1_ing.get("ingredient_name", "ä¸æ˜")
            weight = phase1_ing.get("weight_g", 0)
            nutrition = nutrition_ing.get("calculated_nutrition", {})
            source_db = nutrition_ing.get("source_db", "ä¸æ˜")
            
            content += f"| {ing_name} | {weight}g | {nutrition.get('calories', 0):.1f} kcal | {nutrition.get('protein', 0):.1f}g | {nutrition.get('fat', 0):.1f}g | {nutrition.get('carbs', 0):.1f}g | {source_db} |\n"
        
        # æ–™ç†ã®æ „é¤Šåˆè¨ˆ
        dish_nutrition = nutrition_dish.get("total_nutrition", {})
        content += f"""
### ğŸ”¢ æ–™ç†åˆè¨ˆæ „é¤Š

- **ğŸ”¥ ç·ã‚«ãƒ­ãƒªãƒ¼:** {dish_nutrition.get('calories', 0):.1f} kcal
- **ğŸ¥© ç·ã‚¿ãƒ³ãƒ‘ã‚¯è³ª:** {dish_nutrition.get('protein', 0):.1f} g
- **ğŸ§ˆ ç·è„‚è³ª:** {dish_nutrition.get('fat', 0):.1f} g
- **ğŸ ç·ç‚­æ°´åŒ–ç‰©:** {dish_nutrition.get('carbs', 0):.1f} g

---

"""
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)


async def create_comprehensive_summary(all_results: List[Dict[str, Any]], results_dir: str):
    """å…¨ç”»åƒã®åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_path = f"{results_dir}/comprehensive_analysis_summary.md"
    
    # æˆåŠŸã—ãŸåˆ†æã®ã¿ã‚’æŠ½å‡º
    successful_results = [r for r in all_results if r and "error" not in r]
    failed_results = [r for r in all_results if r and "error" in r]
    
    content = f"""# ğŸ½ï¸ Multi-Image Nutrition Analysis - åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼

**åˆ†ææ—¥æ™‚:** {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}  
**åˆ†æç”»åƒæ•°:** {len(all_results)}æš  
**æˆåŠŸåˆ†æ:** {len(successful_results)}æš  
**å¤±æ•—åˆ†æ:** {len(failed_results)}æš  

## ğŸ“Š å…¨ç”»åƒæ „é¤Šã‚µãƒãƒªãƒ¼

| ç”»åƒ | æ–™ç†æ•° | é£Ÿææ•° | ç·ã‚«ãƒ­ãƒªãƒ¼ | ã‚¿ãƒ³ãƒ‘ã‚¯è³ª | è„‚è³ª | ç‚­æ°´åŒ–ç‰© | å‡¦ç†æ™‚é–“ |
|------|--------|--------|------------|------------|------|----------|----------|
"""
    
    total_calories = 0
    total_protein = 0
    total_fat = 0
    total_carbs = 0
    total_processing_time = 0
    
    for result in successful_results:
        image_name = result.get("image_metadata", {}).get("image_name", "ä¸æ˜")
        processing_summary = result.get("processing_summary", {})
        final_nutrition = result.get("final_nutrition_result", {})
        total_nutrition = final_nutrition.get("total_nutrition", {})
        
        dishes_count = processing_summary.get("total_dishes", 0)
        ingredients_count = processing_summary.get("total_ingredients", 0)
        calories = total_nutrition.get("calories", 0)
        protein = total_nutrition.get("protein", 0)
        fat = total_nutrition.get("fat", 0)
        carbs = total_nutrition.get("carbs", 0)
        processing_time = result.get("image_metadata", {}).get("analysis_time_seconds", 0)
        
        content += f"| {image_name} | {dishes_count} | {ingredients_count} | {calories:.1f} kcal | {protein:.1f}g | {fat:.1f}g | {carbs:.1f}g | {processing_time:.1f}s |\n"
        
        total_calories += calories
        total_protein += protein
        total_fat += fat
        total_carbs += carbs
        total_processing_time += processing_time
    
    # åˆè¨ˆè¡Œ
    content += f"| **åˆè¨ˆ** | - | - | **{total_calories:.1f} kcal** | **{total_protein:.1f}g** | **{total_fat:.1f}g** | **{total_carbs:.1f}g** | **{total_processing_time:.1f}s** |\n"
    
    # å¹³å‡å€¤
    if successful_results:
        avg_calories = total_calories / len(successful_results)
        avg_protein = total_protein / len(successful_results)
        avg_fat = total_fat / len(successful_results)
        avg_carbs = total_carbs / len(successful_results)
        avg_processing_time = total_processing_time / len(successful_results)
        
        content += f"| **å¹³å‡** | - | - | **{avg_calories:.1f} kcal** | **{avg_protein:.1f}g** | **{avg_fat:.1f}g** | **{avg_carbs:.1f}g** | **{avg_processing_time:.1f}s** |\n"
    
    content += f"""

## ğŸ¯ åˆ†æçµ±è¨ˆ

### ğŸ“ˆ æ „é¤Šçµ±è¨ˆ
- **ç·ã‚«ãƒ­ãƒªãƒ¼æ‘‚å–é‡:** {total_calories:.1f} kcal
- **ç·ã‚¿ãƒ³ãƒ‘ã‚¯è³ª:** {total_protein:.1f} g
- **ç·è„‚è³ª:** {total_fat:.1f} g
- **ç·ç‚­æ°´åŒ–ç‰©:** {total_carbs:.1f} g

### âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
- **ç·å‡¦ç†æ™‚é–“:** {total_processing_time:.1f}ç§’
- **å¹³å‡å‡¦ç†æ™‚é–“:** {avg_processing_time:.1f}ç§’/ç”»åƒ
- **æˆåŠŸç‡:** {len(successful_results)/len(all_results)*100:.1f}%

## ğŸ½ï¸ ç”»åƒåˆ¥è©³ç´°

"""
    
    for i, result in enumerate(successful_results, 1):
        image_name = result.get("image_metadata", {}).get("image_name", "ä¸æ˜")
        phase1_result = result.get("phase1_result", {})
        dishes = phase1_result.get("dishes", [])
        
        content += f"""### {i}. {image_name}

**æ¤œå‡ºã•ã‚ŒãŸæ–™ç†:**
"""
        
        for j, dish in enumerate(dishes, 1):
            dish_name = dish.get("dish_name", "ä¸æ˜")
            confidence = dish.get("confidence", 0.0)
            ingredients = dish.get("ingredients", [])
            content += f"- {j}. {dish_name} (ä¿¡é ¼åº¦: {confidence:.2f}, é£Ÿæ: {len(ingredients)}å€‹)\n"
        
        content += "\n"
    
    # ã‚¨ãƒ©ãƒ¼æƒ…å ±
    if failed_results:
        content += f"""## âŒ åˆ†æã‚¨ãƒ©ãƒ¼

"""
        for result in failed_results:
            image_name = result.get("image_name", "ä¸æ˜")
            error = result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")
            content += f"- **{image_name}:** {error}\n"
    
    content += f"""

---
*Generated by Multi-Image Nutrition Analysis System - {timestamp}*
"""
    
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"\nğŸ“‹ åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ä¿å­˜å®Œäº†: {summary_path}")
    return summary_path


async def analyze_all_food_images():
    """food1-5ã®å…¨ç”»åƒã‚’åˆ†æ"""
    
    # ç’°å¢ƒè¨­å®š
    setup_environment()
    
    # çµæœä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"analysis_results/multi_image_analysis_{timestamp}"
    os.makedirs(results_dir, exist_ok=True)
    
    print(f"ğŸš€ Multi-Image Nutrition Analysis é–‹å§‹")
    print(f"ğŸ“ çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {results_dir}")
    print(f"ğŸ”§ æ¤œç´¢æ–¹æ³•: MyNetDiaryå°‚ç”¨æ¤œç´¢ + æ „é¤Šè¨ˆç®—")
    print(f"ğŸ“Š å¯¾è±¡ç”»åƒ: food1.jpg - food5.jpg (5æš)")
    
    # ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    image_paths = [
        "test_images/food1.jpg",
        "test_images/food2.jpg", 
        "test_images/food3.jpg",
        "test_images/food4.jpg",
        "test_images/food5.jpg"
    ]
    
    all_results = []
    total_start_time = time.time()
    
    # å„ç”»åƒã‚’é †æ¬¡åˆ†æ
    for i, image_path in enumerate(image_paths, 1):
        try:
            result = await analyze_single_image(image_path, results_dir, i)
            all_results.append(result)
        except Exception as e:
            print(f"âŒ ç”»åƒ {i} ã®åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            all_results.append({
                "error": str(e),
                "image_name": Path(image_path).stem,
                "image_path": image_path
            })
    
    total_end_time = time.time()
    total_processing_time = total_end_time - total_start_time
    
    # åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
    summary_path = await create_comprehensive_summary(all_results, results_dir)
    
    # æœ€çµ‚çµæœã®è¡¨ç¤º
    successful_results = [r for r in all_results if r and "error" not in r]
    failed_results = [r for r in all_results if r and "error" in r]
    
    print(f"\n{'='*80}")
    print(f"ğŸ¯ Multi-Image Nutrition Analysis å®Œäº†!")
    print(f"{'='*80}")
    print(f"ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼:")
    print(f"   âœ… æˆåŠŸ: {len(successful_results)}/{len(all_results)} ç”»åƒ")
    print(f"   âŒ å¤±æ•—: {len(failed_results)}/{len(all_results)} ç”»åƒ")
    print(f"   â±ï¸  ç·å‡¦ç†æ™‚é–“: {total_processing_time:.2f}ç§’")
    print(f"   ğŸ“ çµæœä¿å­˜å…ˆ: {results_dir}")
    print(f"   ğŸ“‹ åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼: {summary_path}")
    
    if successful_results:
        total_calories = sum(r.get("final_nutrition_result", {}).get("total_nutrition", {}).get("calories", 0) for r in successful_results)
        print(f"   ğŸ”¥ ç·ã‚«ãƒ­ãƒªãƒ¼: {total_calories:.1f} kcal")
    
    print(f"{'='*80}")
    
    return all_results, results_dir


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ½ï¸ Multi-Image Nutrition Analysis Test")
    print("food1.jpg ã‹ã‚‰ food5.jpg ã¾ã§5ã¤ã®ç”»åƒã‚’ä¸€æ‹¬åˆ†æã—ã¾ã™")
    print()
    
    # éåŒæœŸå®Ÿè¡Œ
    asyncio.run(analyze_all_food_images())


if __name__ == "__main__":
    main() 


################################################################################
## CATEGORY: FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤ (app_v2)
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/main/app.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/main/app.py
----------------------------------------------------------------------

import os
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from ..api.v1.endpoints import meal_analysis
from ..config import get_settings

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
os.environ.setdefault("GOOGLE_APPLICATION_CREDENTIALS", "/Users/odasoya/meal_analysis_api /service-account-key.json")
os.environ.setdefault("GEMINI_PROJECT_ID", "recording-diet-ai-3e7cf")
os.environ.setdefault("GEMINI_LOCATION", "us-central1")
os.environ.setdefault("GEMINI_MODEL_NAME", "gemini-2.5-flash-preview-05-20")

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# FastAPIã‚¢ãƒ—ãƒªã®ä½œæˆ
app = FastAPI(
    title="é£Ÿäº‹åˆ†æ API v2.0",
    description="ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ã•ã‚ŒãŸé£Ÿäº‹åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ç™»éŒ²
app.include_router(
    meal_analysis.router,
    prefix="/api/v1/meal-analyses",
    tags=["Complete Meal Analysis v2.0"]
)

# ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "é£Ÿäº‹åˆ†æ API v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ",
        "version": "2.0.0",
        "architecture": "Component-based Pipeline",
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "version": "v2.0",
        "components": ["Phase1Component", "ElasticsearchNutritionSearchComponent"]
    }

if __name__ == "__main__":
    import uvicorn
    settings = get_settings()
    uvicorn.run(
        "app_v2.main.app:app",
        host=settings.HOST,
        port=settings.PORT,
        reload=True
    ) 

----------------------------------------------------------------------
### FILE: app_v2/api/v1/endpoints/meal_analysis.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/api/v1/endpoints/meal_analysis.py
----------------------------------------------------------------------

from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import JSONResponse
from typing import Optional
import logging

from ....pipeline import MealAnalysisPipeline

logger = logging.getLogger(__name__)

router = APIRouter()


@router.post("/complete")
async def complete_meal_analysis(
    image: UploadFile = File(...),
    save_detailed_logs: bool = Form(True),
    test_execution: bool = Form(False),
    test_results_dir: Optional[str] = Form(None)
):
    """
    å®Œå…¨ãªé£Ÿäº‹åˆ†æã‚’å®Ÿè¡Œï¼ˆv2.0 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆï¼‰
    
    - Phase 1: Gemini AIã«ã‚ˆã‚‹ç”»åƒåˆ†æ
    - Nutrition Search: é£Ÿæã®æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ
    - Phase 2: è¨ˆç®—æˆ¦ç•¥æ±ºå®šã¨æ „é¤Šä¾¡ç²¾ç·»åŒ– (TODO)
    - Nutrition Calculation: æœ€çµ‚æ „é¤Šä¾¡è¨ˆç®— (TODO)
    
    Args:
        image: åˆ†æå¯¾è±¡ã®é£Ÿäº‹ç”»åƒ
        save_detailed_logs: åˆ†æãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True)
    
    Returns:
        å®Œå…¨ãªåˆ†æçµæœã¨æ „é¤Šä¾¡è¨ˆç®—ã€åˆ†æãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    
    try:
        # ç”»åƒã®æ¤œè¨¼
        if not image.content_type.startswith('image/'):
            raise HTTPException(status_code=400, detail="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¯ç”»åƒã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        image_data = await image.read()
        logger.info(f"Starting complete meal analysis pipeline v2.0 (detailed_logs: {save_detailed_logs})")
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ
        pipeline = MealAnalysisPipeline()
        result = await pipeline.execute_complete_analysis(
            image_bytes=image_data,
            image_mime_type=image.content_type,
            save_detailed_logs=save_detailed_logs,
            test_execution=test_execution,
            test_results_dir=test_results_dir
        )
        
        logger.info(f"Complete analysis pipeline v2.0 finished successfully")
        
        return JSONResponse(
            status_code=200,
            content=result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Complete analysis failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Complete analysis failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "healthy", "version": "v2.0", "message": "é£Ÿäº‹åˆ†æAPI v2.0 - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŒ–ç‰ˆ"}


@router.get("/pipeline-info")
async def get_pipeline_info():
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã®å–å¾—"""
    pipeline = MealAnalysisPipeline()
    return pipeline.get_pipeline_info() 


################################################################################
## CATEGORY: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ¶å±¤
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/pipeline/orchestrator.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/pipeline/orchestrator.py
----------------------------------------------------------------------

import uuid
import json
import os
from datetime import datetime
from typing import Optional, Dict, Any
import logging

from ..components import Phase1Component, ElasticsearchNutritionSearchComponent, MyNetDiaryNutritionSearchComponent, FuzzyIngredientSearchComponent, NutritionCalculationComponent
from ..models import (
    Phase1Input, Phase1Output,
    NutritionQueryInput
)
from ..models.nutrition_calculation_models import NutritionCalculationInput
from ..config import get_settings
from .result_manager import ResultManager

logger = logging.getLogger(__name__)


class MealAnalysisPipeline:
    """
    é£Ÿäº‹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
    
    4ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã‚’çµ±åˆã—ã¦å®Œå…¨ãªåˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    
    def __init__(self, use_local_nutrition_search: Optional[bool] = None, use_elasticsearch_search: Optional[bool] = None, use_mynetdiary_specialized: Optional[bool] = False, use_fuzzy_matching: Optional[bool] = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            use_local_nutrition_search: å»ƒæ­¢äºˆå®šï¼ˆäº’æ›æ€§ã®ãŸã‚æ®‹å­˜ï¼‰
            use_elasticsearch_search: Elasticsearchæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
                                    None: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
                                    True: ElasticsearchNutritionSearchComponentä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
                                    False: ElasticsearchNutritionSearchComponentä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰
            use_mynetdiary_specialized: MyNetDiaryå°‚ç”¨æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
                                      True: MyNetDiaryNutritionSearchComponentä½¿ç”¨ï¼ˆingredientå³å¯†æ¤œç´¢ï¼‰
                                      False: å¾“æ¥ã®Elasticsearchæ¤œç´¢ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            use_fuzzy_matching: ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
                              True: FuzzyIngredientSearchComponentä½¿ç”¨ï¼ˆé«˜ç²¾åº¦ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ï¼‰
                              False: å¾“æ¥ã®æ¤œç´¢ä½¿ç”¨
                              None: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        """
        self.pipeline_id = str(uuid.uuid4())[:8]
        self.settings = get_settings()
        
        # Elasticsearchæ¤œç´¢å„ªå…ˆåº¦ã®æ±ºå®š
        if use_elasticsearch_search is not None:
            self.use_elasticsearch_search = use_elasticsearch_search
        elif hasattr(self.settings, 'USE_ELASTICSEARCH_SEARCH'):
            self.use_elasticsearch_search = self.settings.USE_ELASTICSEARCH_SEARCH
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Elasticsearchä½¿ç”¨
            self.use_elasticsearch_search = True
        
        # ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç„¡è¦–ï¼ˆå¸¸ã«Elasticsearchä½¿ç”¨ï¼‰
        self.use_local_nutrition_search = False
        
        # ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ä½¿ç”¨ã®æ±ºå®š
        if use_fuzzy_matching is not None:
            self.use_fuzzy_matching = use_fuzzy_matching
        elif hasattr(self.settings, 'fuzzy_search_enabled'):
            self.use_fuzzy_matching = self.settings.fuzzy_search_enabled
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ä½¿ç”¨
            self.use_fuzzy_matching = True
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.phase1_component = Phase1Component()
        
        # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®é¸æŠ
        if self.use_fuzzy_matching:
            # æ–°ã—ã„ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆæ¨å¥¨ï¼‰
            self.nutrition_search_component = FuzzyIngredientSearchComponent()
            self.search_component_name = "FuzzyIngredientSearchComponent"
            logger.info("Using Fuzzy Ingredient Search (5-tier cascade, high-precision matching)")
        elif use_mynetdiary_specialized:
            # MyNetDiaryå°‚ç”¨æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
            self.nutrition_search_component = MyNetDiaryNutritionSearchComponent(
                results_per_db=5
            )
            self.search_component_name = "MyNetDiaryNutritionSearchComponent"
            logger.info("Using MyNetDiary specialized nutrition search (ingredient strict matching)")
        else:
            # å¾“æ¥ã®Elasticsearchæ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
            self.nutrition_search_component = ElasticsearchNutritionSearchComponent(
                strategic_search_mode=True,
                results_per_db=5
            )
            self.search_component_name = "ElasticsearchNutritionSearchComponent"
            logger.info("Using Elasticsearch nutrition database search (high-performance, multi-DB mode)")
            
        # æ „é¤Šè¨ˆç®—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.nutrition_calculation_component = NutritionCalculationComponent()
        
        # TODO: Phase2Componentã‚’è¿½åŠ 
        
        self.logger = logging.getLogger(f"{__name__}.{self.pipeline_id}")
        
    async def execute_complete_analysis(
        self,
        image_bytes: bytes,
        image_mime_type: str,
        optional_text: Optional[str] = None,
        save_detailed_logs: bool = True,
        test_execution: bool = False,
        test_results_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å®Œå…¨ãªé£Ÿäº‹åˆ†æã‚’å®Ÿè¡Œ
        
        Args:
            image_bytes: ç”»åƒãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆ
            save_detailed_logs: åˆ†æãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            å®Œå…¨ãªåˆ†æçµæœ
        """
        analysis_id = str(uuid.uuid4())[:8]
        start_time = datetime.now()
        
        # ResultManagerã®åˆæœŸåŒ–
        if save_detailed_logs:
            if test_execution and test_results_dir:
                # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã¯ãƒ†ã‚¹ãƒˆçµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®api_calls/ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
                api_calls_dir = f"{test_results_dir}/api_calls"
                result_manager = ResultManager(analysis_id, save_directory=api_calls_dir)
            else:
                # é€šå¸¸ã®å®Ÿè¡Œæ™‚ã¯æ—¢å­˜ã®ä¿å­˜å…ˆ
                result_manager = ResultManager(analysis_id)
        else:
            result_manager = None
        
        self.logger.info(f"[{analysis_id}] Starting complete meal analysis pipeline")
        self.logger.info(f"[{analysis_id}] Nutrition search method: Elasticsearch (high-performance)")
        
        try:
            # === Phase 1: ç”»åƒåˆ†æ ===
            self.logger.info(f"[{analysis_id}] Phase 1: Image analysis")
            
            phase1_input = Phase1Input(
                image_bytes=image_bytes,
                image_mime_type=image_mime_type,
                optional_text=optional_text
            )
            
            # Phase1ã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            phase1_log = result_manager.create_execution_log("Phase1Component", f"{analysis_id}_phase1") if result_manager else None
            
            phase1_result = await self.phase1_component.execute(phase1_input, phase1_log)
            
            self.logger.info(f"[{analysis_id}] Phase 1 completed - Detected {len(phase1_result.dishes)} dishes")
            
            # === Nutrition Search Phase: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆ ===
            if self.use_fuzzy_matching:
                search_phase_name = "Fuzzy Ingredient Search"
            else:
                search_phase_name = "Elasticsearch Search"
            self.logger.info(f"[{analysis_id}] {search_phase_name} Phase: Database matching")
            
            # === çµ±ä¸€ã•ã‚ŒãŸæ „é¤Šæ¤œç´¢å…¥åŠ›ã‚’ä½œæˆ ===
            if self.use_fuzzy_matching:
                # ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°æ¤œç´¢ã®å ´åˆã¯ã€é£Ÿæåã®ãƒªã‚¹ãƒˆã‚’ç›´æ¥æ¸¡ã™
                ingredient_names = phase1_result.get_all_ingredient_names()
                nutrition_search_input = [{"name": name} for name in ingredient_names]
            else:
                # å¾“æ¥ã®Elasticsearchæ¤œç´¢ã‚’ä½¿ç”¨
                preferred_source = "elasticsearch"
                nutrition_search_input = NutritionQueryInput(
                    ingredient_names=phase1_result.get_all_ingredient_names(),
                    dish_names=phase1_result.get_all_dish_names(),
                    preferred_source=preferred_source
                )
            
            # Nutrition Searchã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            search_log = result_manager.create_execution_log(self.search_component_name, f"{analysis_id}_nutrition_search") if result_manager else None
            
            if self.use_fuzzy_matching:
                # ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å ´åˆã¯processãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
                nutrition_search_result = await self.nutrition_search_component.process(nutrition_search_input)
            else:
                # å¾“æ¥ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å ´åˆã¯executeãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
                nutrition_search_result = await self.nutrition_search_component.execute(nutrition_search_input, search_log)
            
            self.logger.info(f"[{analysis_id}] {search_phase_name} completed - {nutrition_search_result.get_match_rate():.1%} match rate")
            
            # === Nutrition Calculation Phase: æ „é¤Šè¨ˆç®— ===
            self.logger.info(f"[{analysis_id}] Nutrition Calculation Phase: Computing nutrition values")
            
            nutrition_calculation_input = NutritionCalculationInput(
                phase1_result=phase1_result,
                nutrition_search_result=nutrition_search_result
            )
            
            # Nutrition Calculationã®è©³ç´°ãƒ­ã‚°ã‚’ä½œæˆ
            calculation_log = result_manager.create_execution_log("NutritionCalculationComponent", f"{analysis_id}_nutrition_calculation") if result_manager else None
            
            nutrition_calculation_result = await self.nutrition_calculation_component.execute(nutrition_calculation_input, calculation_log)
            
            self.logger.info(f"[{analysis_id}] Nutrition Calculation completed - {nutrition_calculation_result.meal_nutrition.calculation_summary['total_ingredients']} ingredients, {nutrition_calculation_result.meal_nutrition.total_nutrition.calories:.1f} kcal total")
            
            # === çµæœã®æ§‹ç¯‰ ===
            
            # Phase1ã®çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰
            phase1_dict = {
                "detected_food_items": [
                    {
                        "item_name": item.item_name,
                        "confidence": item.confidence,
                        "attributes": [
                            {
                                "type": attr.type.value if hasattr(attr.type, 'value') else str(attr.type),
                                "value": attr.value,
                                "confidence": attr.confidence
                            }
                            for attr in item.attributes
                        ],
                        "brand": item.brand or "",
                        "category_hints": item.category_hints,
                        "negative_cues": item.negative_cues
                    }
                    for item in phase1_result.detected_food_items
                ],
                "dishes": [
                    {
                        "dish_name": dish.dish_name,
                        "confidence": dish.confidence,
                        "ingredients": [
                            {
                                "ingredient_name": ing.ingredient_name,
                                "confidence": ing.confidence,
                                "weight_g": ing.weight_g
                            }
                            for ing in dish.ingredients
                        ],
                        "attributes": [
                            {
                                "type": attr.type.value if hasattr(attr.type, 'value') else str(attr.type),
                                "value": attr.value,
                                "confidence": attr.confidence
                            }
                            for attr in dish.detected_attributes
                        ]
                    }
                    for dish in phase1_result.dishes
                ],
                "analysis_confidence": phase1_result.analysis_confidence,
                "processing_notes": phase1_result.processing_notes
            }
            
            # æ „é¤Šè¨ˆç®—çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            nutrition_calculation_dict = {
                "dishes": [
                    {
                        "dish_name": dish.dish_name,
                        "confidence": dish.confidence,
                        "ingredients": [
                            {
                                "ingredient_name": ing.ingredient_name,
                                "weight_g": ing.weight_g,
                                "nutrition_per_100g": ing.nutrition_per_100g,
                                "calculated_nutrition": {
                                    "calories": ing.calculated_nutrition.calories,
                                    "protein": ing.calculated_nutrition.protein,
                                    "fat": ing.calculated_nutrition.fat,
                                    "carbs": ing.calculated_nutrition.carbs,
                                    "fiber": ing.calculated_nutrition.fiber,
                                    "sugar": ing.calculated_nutrition.sugar,
                                    "sodium": ing.calculated_nutrition.sodium
                                },
                                "source_db": ing.source_db,
                                "calculation_notes": ing.calculation_notes
                            }
                            for ing in dish.ingredients
                        ],
                        "total_nutrition": {
                            "calories": dish.total_nutrition.calories,
                            "protein": dish.total_nutrition.protein,
                            "fat": dish.total_nutrition.fat,
                            "carbs": dish.total_nutrition.carbs,
                            "fiber": dish.total_nutrition.fiber,
                            "sugar": dish.total_nutrition.sugar,
                            "sodium": dish.total_nutrition.sodium
                        },
                        "calculation_metadata": dish.calculation_metadata
                    }
                    for dish in nutrition_calculation_result.meal_nutrition.dishes
                ],
                "total_nutrition": {
                    "calories": nutrition_calculation_result.meal_nutrition.total_nutrition.calories,
                    "protein": nutrition_calculation_result.meal_nutrition.total_nutrition.protein,
                    "fat": nutrition_calculation_result.meal_nutrition.total_nutrition.fat,
                    "carbs": nutrition_calculation_result.meal_nutrition.total_nutrition.carbs,
                    "fiber": nutrition_calculation_result.meal_nutrition.total_nutrition.fiber,
                    "sugar": nutrition_calculation_result.meal_nutrition.total_nutrition.sugar,
                    "sodium": nutrition_calculation_result.meal_nutrition.total_nutrition.sodium
                },
                "calculation_summary": nutrition_calculation_result.meal_nutrition.calculation_summary,
                "warnings": nutrition_calculation_result.meal_nutrition.warnings
            }
            
            # æ¤œç´¢æ–¹æ³•ã®ç‰¹å®šï¼ˆå¸¸ã«Elasticsearchï¼‰
            search_method = "elasticsearch"
            search_api_method = "elasticsearch"
            
            # å®Œå…¨åˆ†æçµæœã®æ§‹ç¯‰
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            complete_result = {
                "analysis_id": analysis_id,
                "phase1_result": phase1_dict,
                "nutrition_search_result": {
                    "matches_count": len(nutrition_search_result.matches),
                    "match_rate": nutrition_search_result.get_match_rate(),
                    "search_summary": nutrition_search_result.search_summary,
                    "search_method": search_method
                },

                "processing_summary": {
                    "total_dishes": len(phase1_result.dishes),
                    "total_ingredients": len(phase1_result.get_all_ingredient_names()),
                    "nutrition_search_match_rate": self._calculate_match_rate_display(nutrition_search_input, nutrition_search_result),
                    "nutrition_calculation_status": "completed",
                    "total_calories": nutrition_calculation_result.meal_nutrition.total_nutrition.calories,
                    "pipeline_status": "completed",
                    "processing_time_seconds": processing_time,
                    "search_method": search_method
                },
                # æœ€çµ‚æ „é¤Šçµæœ
                "final_nutrition_result": nutrition_calculation_dict,
                "metadata": {
                    "pipeline_version": "v2.0",
                    "timestamp": datetime.now().isoformat(),
                    "components_used": ["Phase1Component", self.search_component_name, "NutritionCalculationComponent"],
                    "nutrition_search_method": search_api_method
                }
            }
            
            # ResultManagerã«æœ€çµ‚çµæœã‚’è¨­å®š
            if result_manager:
                result_manager.set_final_result(complete_result)
                result_manager.finalize_pipeline()
            
            # çµæœã®ä¿å­˜
            saved_files = {}
            if save_detailed_logs and result_manager:
                # æ–°ã—ã„ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ä¿å­˜æ–¹å¼
                saved_files = result_manager.save_phase_results()
                complete_result["analysis_folder"] = result_manager.get_analysis_folder_path()
                complete_result["saved_files"] = saved_files
                
                logger.info(f"[{analysis_id}] Analysis logs saved to folder: {result_manager.get_analysis_folder_path()}")
                logger.info(f"[{analysis_id}] Saved {len(saved_files)} files across all phases")
            

            
            self.logger.info(f"[{analysis_id}] Complete analysis pipeline finished successfully in {processing_time:.2f}s")
            
            return complete_result
            
        except Exception as e:
            self.logger.error(f"[{analysis_id}] Complete analysis failed: {str(e)}", exc_info=True)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ResultManagerã‚’ä¿å­˜
            if result_manager:
                result_manager.set_final_result({"error": str(e), "timestamp": datetime.now().isoformat()})
                result_manager.finalize_pipeline()
                error_saved_files = result_manager.save_phase_results()
                self.logger.info(f"[{analysis_id}] Error analysis logs saved to folder: {result_manager.get_analysis_folder_path()}")
            
            raise
    
    def get_pipeline_info(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æƒ…å ±ã‚’å–å¾—"""
        search_method = "elasticsearch"
            
        return {
            "pipeline_id": self.pipeline_id,
            "version": "v2.0",
            "nutrition_search_method": search_method,
            "components": [
                {
                    "component_name": "Phase1Component",
                    "component_type": "analysis",
                    "execution_count": 0
                },
                {
                    "component_name": self.search_component_name,
                    "component_type": "nutrition_search",
                    "execution_count": 0
                },
                {
                    "component_name": "NutritionCalculationComponent",
                    "component_type": "nutrition_calculation",
                    "execution_count": 0
                }
            ]
        } 

    def _calculate_match_rate_display(self, nutrition_search_input, nutrition_search_result):
        """ãƒãƒƒãƒç‡ã®è¡¨ç¤ºæ–‡å­—åˆ—ã‚’è¨ˆç®—"""
        if self.use_fuzzy_matching:
            # ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã®å ´åˆã¯ãƒªã‚¹ãƒˆå½¢å¼
            total_searches = len(nutrition_search_input)
            successful_matches = len(nutrition_search_result.matches)
            match_rate = successful_matches / total_searches if total_searches > 0 else 0
            return f"{successful_matches}/{total_searches} ({match_rate:.1%})"
        else:
            # å¾“æ¥ã®æ¤œç´¢ã®å ´åˆã¯NutritionQueryInputã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            total_searches = len(nutrition_search_input.get_all_search_terms())
            successful_matches = len(nutrition_search_result.matches)
            match_rate = nutrition_search_result.get_match_rate()
            return f"{successful_matches}/{total_searches} ({match_rate:.1%})" 

----------------------------------------------------------------------
### FILE: app_v2/pipeline/result_manager.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/pipeline/result_manager.py
----------------------------------------------------------------------

import json
import os
from datetime import datetime
from typing import Dict, Any, Optional, List
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class DetailedExecutionLog:
    """å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è©³ç´°å®Ÿè¡Œãƒ­ã‚°"""
    
    def __init__(self, component_name: str, execution_id: str):
        self.component_name = component_name
        self.execution_id = execution_id
        self.execution_start_time = datetime.now()
        self.execution_end_time = None
        self.input_data = {}
        self.output_data = {}
        self.processing_details = {}
        self.prompts_used = {}
        self.reasoning = {}
        self.confidence_scores = {}
        self.warnings = []
        self.errors = []
        
    def set_input(self, input_data: Dict[str, Any]):
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²ï¼ˆæ©Ÿå¯†æƒ…å ±ã¯é™¤å¤–ï¼‰"""
        # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯å¤§ãã™ãã‚‹ã®ã§ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜
        safe_input = {}
        for key, value in input_data.items():
            if key == 'image_bytes':
                safe_input[key] = {
                    "size_bytes": len(value) if value else 0,
                    "type": "binary_image_data"
                }
            else:
                safe_input[key] = value
        self.input_data = safe_input
    
    def set_output(self, output_data: Dict[str, Any]):
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²"""
        self.output_data = output_data
        
    def add_prompt(self, prompt_name: str, prompt_content: str, variables: Dict[str, Any] = None):
        """ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨˜éŒ²"""
        self.prompts_used[prompt_name] = {
            "content": prompt_content,
            "variables": variables or {},
            "timestamp": datetime.now().isoformat()
        }
    
    def add_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æ¨è«–ç†ç”±ã‚’è¨˜éŒ²"""
        self.reasoning[decision_point] = {
            "reason": reason,
            "confidence": confidence,
            "timestamp": datetime.now().isoformat()
        }
    
    def add_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’è¨˜éŒ²"""
        self.processing_details[detail_key] = detail_value
    
    def add_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²"""
        self.confidence_scores[metric_name] = score
    
    def add_warning(self, warning: str):
        """è­¦å‘Šã‚’è¨˜éŒ²"""
        self.warnings.append({
            "message": warning,
            "timestamp": datetime.now().isoformat()
        })
    
    def add_error(self, error: str):
        """ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²"""
        self.errors.append({
            "message": error,
            "timestamp": datetime.now().isoformat()
        })
    
    def finalize(self):
        """å®Ÿè¡Œå®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.execution_end_time = datetime.now()
    
    def get_execution_time(self) -> float:
        """å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆç§’ï¼‰"""
        if self.execution_end_time:
            return (self.execution_end_time - self.execution_start_time).total_seconds()
        return 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "component_name": self.component_name,
            "execution_id": self.execution_id,
            "execution_start_time": self.execution_start_time.isoformat(),
            "execution_end_time": self.execution_end_time.isoformat() if self.execution_end_time else None,
            "execution_time_seconds": self.get_execution_time(),
            "input_data": self.input_data,
            "output_data": self.output_data,
            "processing_details": self.processing_details,
            "prompts_used": self.prompts_used,
            "reasoning": self.reasoning,
            "confidence_scores": self.confidence_scores,
            "warnings": self.warnings,
            "errors": self.errors
        }


class ResultManager:
    """è§£æçµæœã¨è©³ç´°ãƒ­ã‚°ã®ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆãƒ•ã‚§ãƒ¼ã‚ºåˆ¥æ•´ç†ç‰ˆï¼‰"""
    
    def __init__(self, analysis_id: str, save_directory: str = "analysis_results"):
        self.analysis_id = analysis_id
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å®Ÿè¡Œã”ã¨ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼ˆåˆ†ã‹ã‚Šã‚„ã™ã„éšå±¤æ§‹é€ ï¼‰
        timestamp_dir = Path(save_directory) / f"meal_analysis_{self.timestamp}"
        timestamp_dir.mkdir(parents=True, exist_ok=True)
        
        self.analysis_folder_name = f"analysis_{self.analysis_id}"
        self.analysis_dir = timestamp_dir / self.analysis_folder_name
        self.analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
        self.phase1_dir = self.analysis_dir / "phase1"
        self.nutrition_search_dir = self.analysis_dir / "nutrition_search_query"
        self.phase2_dir = self.analysis_dir / "phase2"
        self.nutrition_dir = self.analysis_dir / "nutrition_calculation"
        
        for phase_dir in [self.phase1_dir, self.nutrition_search_dir, self.phase2_dir, self.nutrition_dir]:
            phase_dir.mkdir(exist_ok=True)
        
        self.pipeline_start_time = datetime.now()
        self.pipeline_end_time = None
        self.execution_logs: List[DetailedExecutionLog] = []
        self.final_result = {}
        self.pipeline_metadata = {
            "analysis_id": analysis_id,
            "version": "v2.0",
            "analysis_folder": self.analysis_folder_name,
            "pipeline_start_time": self.pipeline_start_time.isoformat()
        }
        
    def create_execution_log(self, component_name: str, execution_id: str) -> DetailedExecutionLog:
        """æ–°ã—ã„å®Ÿè¡Œãƒ­ã‚°ã‚’ä½œæˆ"""
        log = DetailedExecutionLog(component_name, execution_id)
        self.execution_logs.append(log)
        return log
    
    def set_final_result(self, result: Dict[str, Any]):
        """æœ€çµ‚çµæœã‚’è¨­å®š"""
        self.final_result = result
        
    def finalize_pipeline(self):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†æ™‚ã®æœ€çµ‚å‡¦ç†"""
        self.pipeline_end_time = datetime.now()
        self.pipeline_metadata["pipeline_end_time"] = self.pipeline_end_time.isoformat()
        self.pipeline_metadata["total_execution_time_seconds"] = (
            self.pipeline_end_time - self.pipeline_start_time
        ).total_seconds()
    
    def save_phase_results(self) -> Dict[str, str]:
        """ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã«çµæœã‚’ä¿å­˜"""
        saved_files = {}
        
        # å®Ÿè¡Œã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ã‚°ã‚’å‡¦ç†
        executed_components = set()
        for log in self.execution_logs:
            if log.component_name == "Phase1Component":
                files = self._save_phase1_results(log)
                saved_files.update(files)
                executed_components.add("Phase1Component")
            elif log.component_name in ["ElasticsearchNutritionSearchComponent"]:
                files = self._save_nutrition_search_results(log)
                saved_files.update(files)
                executed_components.add(log.component_name)
            elif log.component_name == "Phase2Component":
                files = self._save_phase2_results(log)
                saved_files.update(files)
                executed_components.add("Phase2Component")
            elif log.component_name == "NutritionCalculationComponent":
                files = self._save_nutrition_results(log)
                saved_files.update(files)
                executed_components.add("NutritionCalculationComponent")
        
        # æœªå®Ÿè£…/æœªå®Ÿè¡Œã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        if "Phase2Component" not in executed_components:
            placeholder_log = DetailedExecutionLog("Phase2Component", f"{self.analysis_id}_phase2_placeholder")
            placeholder_log.input_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.output_data = {"note": "Phase2Component not yet implemented"}
            placeholder_log.finalize()
            files = self._save_phase2_results(placeholder_log)
            saved_files.update(files)
        
        if "NutritionCalculationComponent" not in executed_components:
            placeholder_log = DetailedExecutionLog("NutritionCalculationComponent", f"{self.analysis_id}_nutrition_placeholder")
            placeholder_log.input_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.output_data = {"note": "NutritionCalculationComponent not yet implemented"}
            placeholder_log.finalize()
            files = self._save_nutrition_results(placeholder_log)
            saved_files.update(files)
        
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
        summary_files = self._save_pipeline_summary()
        saved_files.update(summary_files)
        
        return saved_files
    
    def _save_phase1_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase1ã®çµæœã‚’ä¿å­˜"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase1_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time()
            }, f, indent=2, ensure_ascii=False)
        files["phase1_input_output"] = str(input_output_file)
        
        # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–ç†ç”±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        prompts_md_file = self.phase1_dir / "prompts_and_reasoning.md"
        prompts_content = self._generate_phase1_prompts_md(log)
        with open(prompts_md_file, 'w', encoding='utf-8') as f:
            f.write(prompts_content)
        files["phase1_prompts_md"] = str(prompts_md_file)
        
        # 3. æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆ
        detected_items_file = self.phase1_dir / "detected_items.txt"
        detected_content = self._generate_phase1_detected_items_txt(log)
        with open(detected_items_file, 'w', encoding='utf-8') as f:
            f.write(detected_content)
        files["phase1_detected_txt"] = str(detected_items_file)
        
        return files
    
    def _save_nutrition_search_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµæœã‚’ä¿å­˜ï¼ˆElasticsearchNutritionSearchComponentå¯¾å¿œï¼‰"""
        files = {}
        
        # æ¤œç´¢æ–¹æ³•ã®åˆ¤å®š
        search_method = "unknown"
        db_source = "unknown"
        
        if log.component_name == "ElasticsearchNutritionSearchComponent":
            search_method = "elasticsearch"
            db_source = "elasticsearch_nutrition_db"
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¤œç´¢æ–¹æ³•æƒ…å ±ã‚’å«ã‚€ï¼‰
        input_output_file = self.nutrition_search_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "search_metadata": {
                    "component_name": log.component_name,
                    "search_method": search_method,
                    "database_source": db_source,
                    "timestamp": log.execution_end_time.isoformat() if log.execution_end_time else None
                }
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_search_input_output"] = str(input_output_file)
        
        # 2. æ¤œç´¢çµæœã®è©³ç´°ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        search_results_md_file = self.nutrition_search_dir / "search_results.md"
        search_content = self._generate_nutrition_search_results_md(log, search_method, db_source)
        with open(search_results_md_file, 'w', encoding='utf-8') as f:
            f.write(search_content)
        files["nutrition_search_results_md"] = str(search_results_md_file)
        
        # 3. ãƒãƒƒãƒè©³ç´°ã®ãƒ†ã‚­ã‚¹ãƒˆ
        match_details_file = self.nutrition_search_dir / "match_details.txt"
        match_content = self._generate_nutrition_match_details_txt(log, search_method, db_source)
        with open(match_details_file, 'w', encoding='utf-8') as f:
            f.write(match_content)
        files["nutrition_search_match_details"] = str(match_details_file)
        
        return files
    
    def _save_phase2_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """Phase2ã®çµæœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.phase2_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "Phase2Component is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["phase2_input_output"] = str(input_output_file)
        
        # 2. æˆ¦ç•¥æ±ºå®šã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        strategy_md_file = self.phase2_dir / "strategy_decisions.md"
        with open(strategy_md_file, 'w', encoding='utf-8') as f:
            f.write("# Phase2 Strategy Decisions\n\n*Phase2Component is not yet implemented*\n")
        files["phase2_strategy_md"] = str(strategy_md_file)
        
        # 3. é¸æŠé …ç›®ã®ãƒ†ã‚­ã‚¹ãƒˆ
        selected_items_file = self.phase2_dir / "selected_items.txt"
        with open(selected_items_file, 'w', encoding='utf-8') as f:
            f.write("Phase2Component is not yet implemented\n")
        files["phase2_items_txt"] = str(selected_items_file)
        
        return files
    
    def _save_nutrition_results(self, log: DetailedExecutionLog) -> Dict[str, str]:
        """æ „é¤Šè¨ˆç®—ã®çµæœã‚’ä¿å­˜ï¼ˆå°†æ¥å®Ÿè£…ç”¨ï¼‰"""
        files = {}
        
        # 1. JSONå½¢å¼ã®å…¥å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿
        input_output_file = self.nutrition_dir / "input_output.json"
        with open(input_output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "input_data": log.input_data,
                "output_data": log.output_data,
                "execution_time_seconds": log.get_execution_time(),
                "note": "NutritionCalculationComponent is not yet implemented"
            }, f, indent=2, ensure_ascii=False)
        files["nutrition_input_output"] = str(input_output_file)
        
        # 2. è¨ˆç®—å¼ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
        formulas_md_file = self.nutrition_dir / "calculation_formulas.md"
        with open(formulas_md_file, 'w', encoding='utf-8') as f:
            f.write("# Nutrition Calculation Formulas\n\n*NutritionCalculationComponent is not yet implemented*\n")
        files["nutrition_formulas_md"] = str(formulas_md_file)
        
        # 3. æ „é¤Šã‚µãƒãƒªãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆ
        summary_txt_file = self.nutrition_dir / "nutrition_summary.txt"
        with open(summary_txt_file, 'w', encoding='utf-8') as f:
            f.write("NutritionCalculationComponent is not yet implemented\n")
        files["nutrition_summary_txt"] = str(summary_txt_file)
        
        return files
    
    def _save_pipeline_summary(self) -> Dict[str, str]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜"""
        files = {}
        
        # 1. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚µãƒãƒªãƒ¼JSON
        summary_file = self.analysis_dir / "pipeline_summary.json"
        summary_data = {
            "analysis_id": self.analysis_id,
            "timestamp": self.timestamp,
            "pipeline_metadata": self.pipeline_metadata,
            "execution_summary": {
                log.component_name: {
                    "execution_time": log.get_execution_time(),
                    "success": len(log.errors) == 0,
                    "warnings_count": len(log.warnings),
                    "errors_count": len(log.errors)
                }
                for log in self.execution_logs
            },
            "final_result": self.final_result
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        files["pipeline_summary"] = str(summary_file)
        
        # 2. å®Œå…¨ãªè©³ç´°ãƒ­ã‚°JSON
        complete_log_file = self.analysis_dir / "complete_analysis_log.json"
        complete_data = {
            "pipeline_metadata": self.pipeline_metadata,
            "execution_logs": [log.to_dict() for log in self.execution_logs],
            "final_result": self.final_result,
            "summary": {
                "total_components": len(self.execution_logs),
                "total_warnings": sum(len(log.warnings) for log in self.execution_logs),
                "total_errors": sum(len(log.errors) for log in self.execution_logs)
            }
        }
        
        with open(complete_log_file, 'w', encoding='utf-8') as f:
            json.dump(complete_data, f, indent=2, ensure_ascii=False)
        files["complete_log"] = str(complete_log_file)
        
        return files
    
    def _generate_phase1_prompts_md(self, log: DetailedExecutionLog) -> str:
        """Phase1ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–ç†ç”±ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆ"""
        content = f"""# Phase1: ç”»åƒåˆ†æ - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¨è«–

## å®Ÿè¡Œæƒ…å ±
- å®Ÿè¡ŒID: {log.execution_id}
- é–‹å§‹æ™‚åˆ»: {log.execution_start_time.isoformat()}
- çµ‚äº†æ™‚åˆ»: {log.execution_end_time.isoformat() if log.execution_end_time else 'N/A'}
- å®Ÿè¡Œæ™‚é–“: {log.get_execution_time():.2f}ç§’

## ä½¿ç”¨ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

"""
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±
        for prompt_name, prompt_data in log.prompts_used.items():
            content += f"### {prompt_name.replace('_', ' ').title()}\n\n"
            content += f"**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—**: {prompt_data['timestamp']}\n\n"
            content += f"```\n{prompt_data['content']}\n```\n\n"
            
            if prompt_data.get('variables'):
                content += f"**å¤‰æ•°**:\n"
                for var_name, var_value in prompt_data['variables'].items():
                    content += f"- {var_name}: {var_value}\n"
                content += "\n"
        
        # æ¨è«–ç†ç”±
        content += "## AIæ¨è«–ã®è©³ç´°\n\n"
        
        # æ–™ç†è­˜åˆ¥ã®æ¨è«–
        dish_reasoning = [r for r in log.reasoning.items() if r[0].startswith('dish_identification_')]
        if dish_reasoning:
            content += "### æ–™ç†è­˜åˆ¥ã®æ¨è«–\n\n"
            for decision_point, reasoning_data in dish_reasoning:
                dish_num = decision_point.split('_')[-1]
                content += f"**æ–™ç† {dish_num}**:\n"
                content += f"- æ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # é£Ÿæé¸æŠã®æ¨è«–
        ingredient_reasoning = [r for r in log.reasoning.items() if r[0].startswith('ingredient_selection_')]
        if ingredient_reasoning:
            content += "### é£Ÿæé¸æŠã®æ¨è«–\n\n"
            for decision_point, reasoning_data in ingredient_reasoning:
                content += f"**{decision_point.replace('_', ' ').title()}**:\n"
                content += f"- æ¨è«–: {reasoning_data['reason']}\n"
                content += f"- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {reasoning_data['timestamp']}\n\n"
        
        # è­¦å‘Šã¨ã‚¨ãƒ©ãƒ¼
        if log.warnings:
            content += "## è­¦å‘Š\n\n"
            for warning in log.warnings:
                content += f"- {warning['message']} (at {warning['timestamp']})\n"
            content += "\n"
        
        if log.errors:
            content += "## ã‚¨ãƒ©ãƒ¼\n\n"
            for error in log.errors:
                content += f"- {error['message']} (at {error['timestamp']})\n"
            content += "\n"
        
        return content
    
    def _generate_phase1_detected_items_txt(self, log: DetailedExecutionLog) -> str:
        """Phase1ã§æ¤œå‡ºã•ã‚ŒãŸæ–™ç†ãƒ»é£Ÿæã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        content = f"Phase1 æ¤œå‡ºçµæœ - {log.execution_start_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += "=" * 60 + "\n\n"
        
        if 'output_data' in log.output_data and 'dishes' in log.output_data['output_data']:
            dishes = log.output_data['output_data']['dishes']
            content += f"æ¤œå‡ºã•ã‚ŒãŸæ–™ç†æ•°: {len(dishes)}\n\n"
            
            for i, dish in enumerate(dishes, 1):
                content += f"æ–™ç† {i}: {dish['dish_name']}\n"
                content += f"  é£Ÿææ•°: {len(dish['ingredients'])}\n"
                content += "  é£Ÿæè©³ç´°:\n"
                
                for j, ingredient in enumerate(dish['ingredients'], 1):
                    content += f"    {j}. {ingredient['ingredient_name']}\n"
                content += "\n"
        

        
        # å‡¦ç†è©³ç´°
        if log.processing_details:
            content += "å‡¦ç†è©³ç´°:\n"
            for detail_key, detail_value in log.processing_details.items():
                if isinstance(detail_value, (dict, list)):
                    content += f"  {detail_key}: {json.dumps(detail_value, ensure_ascii=False)}\n"
                else:
                    content += f"  {detail_key}: {detail_value}\n"
        
        return content
    
    def _generate_nutrition_search_results_md(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«/Elasticsearchå¯¾å¿œï¼‰"""
        content = []
        
        content.append(f"# Nutrition Database Search Results")
        content.append(f"")
        content.append(f"**Search Method:** {search_method}")
        content.append(f"**Database Source:** {db_source}")
        content.append(f"**Component:** {log.component_name}")
        content.append(f"**Execution Time:** {log.get_execution_time():.3f} seconds")
        content.append(f"**Timestamp:** {log.execution_start_time.isoformat()}")
        content.append(f"")
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.input_data:
            content.append(f"## Input Data")
            if 'ingredient_names' in log.input_data:
                ingredients = log.input_data['ingredient_names']
                content.append(f"**Ingredients ({len(ingredients)}):** {', '.join(ingredients)}")
            
            if 'dish_names' in log.input_data:
                dishes = log.input_data['dish_names']
                content.append(f"**Dishes ({len(dishes)}):** {', '.join(dishes)}")
            content.append(f"")
        
        # å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            content.append(f"## Search Results")
            content.append(f"**Total Matches:** {len(matches)}")
            content.append(f"")
            
            for i, (search_term, match_data) in enumerate(matches.items(), 1):
                content.append(f"### {i}. {search_term}")
                if isinstance(match_data, dict):
                    content.append(f"**ID:** {match_data.get('id', 'N/A')}")
                    
                    # search_name ã¨ description ã‚’é©åˆ‡ã«è¡¨ç¤º
                    search_name = match_data.get('search_name', 'N/A')
                    description = match_data.get('description', None)
                    content.append(f"**Search Name:** {search_name}")
                    if description:
                        content.append(f"**Description:** {description}")
                    else:
                        content.append(f"**Description:** None")
                    
                    content.append(f"**Data Type:** {match_data.get('data_type', 'N/A')}")
                    content.append(f"**Source:** {match_data.get('source', 'N/A')}")
                    
                    # ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’æ”¹å–„
                    score = match_data.get('score', 'N/A')
                    if score != 'N/A' and 'search_metadata' in match_data:
                        metadata = match_data['search_metadata']
                        score_breakdown = metadata.get('score_breakdown', {})
                        calculation = metadata.get('calculation', '')
                        match_type = score_breakdown.get('match_type', 'unknown')
                        
                        if calculation:
                            content.append(f"**Score:** {score} *({match_type}: {calculation})*")
                        else:
                            content.append(f"**Score:** {score} *(text similarity + data type priority)*")
                    else:
                        content.append(f"**Score:** {score}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        content.append(f"**Nutrients ({len(match_data['nutrients'])}):**")
                        for nutrient in match_data['nutrients'][:5]:  # æœ€åˆã®5ã¤ã ã‘è¡¨ç¤º
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                content.append(f"  - {name}: {amount} {unit}")
                        if len(match_data['nutrients']) > 5:
                            content.append(f"  - ... and {len(match_data['nutrients']) - 5} more nutrients")
                content.append(f"")
        
        # æ¤œç´¢ã‚µãƒãƒªãƒ¼
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            content.append(f"## Search Summary")
            content.append(f"**Total Searches:** {summary.get('total_searches', 0)}")
            content.append(f"**Successful Matches:** {summary.get('successful_matches', 0)}")
            content.append(f"**Failed Searches:** {summary.get('failed_searches', 0)}")
            content.append(f"**Match Rate:** {summary.get('match_rate_percent', 0)}%")
            content.append(f"**Search Method:** {summary.get('search_method', 'unknown')}")
            content.append(f"")
        
        # æ¨è«–ç†ç”±ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.reasoning:
            content.append(f"## Search Reasoning")
            for decision_point, reason_data in log.reasoning.items():
                reason = reason_data.get('reason', '') if isinstance(reason_data, dict) else str(reason_data)
                content.append(f"**{decision_point}:** {reason}")
            content.append(f"")
        
        # è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°è¡¨ç¤º
        if log.warnings:
            content.append(f"## Warnings")
            for warning in log.warnings:
                content.append(f"- {warning}")
            content.append(f"")
        
        if log.errors:
            content.append(f"## Errors")
            for error in log.errors:
                content.append(f"- {error}")
            content.append(f"")
        
        return "\n".join(content)
    
    def _generate_nutrition_match_details_txt(self, log: DetailedExecutionLog, search_method: str, db_source: str) -> str:
        """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®ãƒãƒƒãƒè©³ç´°ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«/Elasticsearch/ãƒãƒ«ãƒDBå¯¾å¿œï¼‰"""
        lines = []
        
        lines.append(f"Nutrition Database Search Match Details")
        lines.append(f"=" * 50)
        lines.append(f"Search Method: {search_method}")
        lines.append(f"Database Source: {db_source}")
        lines.append(f"Component: {log.component_name}")
        lines.append(f"Execution Time: {log.get_execution_time():.3f} seconds")
        lines.append(f"Timestamp: {log.execution_start_time.isoformat()}")
        lines.append(f"")
        
        if log.output_data and 'matches' in log.output_data:
            matches = log.output_data['matches']
            
            # ç·ãƒãƒƒãƒæ•°ã‚’è¨ˆç®—ï¼ˆå˜ä¸€çµæœã¨ãƒªã‚¹ãƒˆçµæœä¸¡æ–¹ã«å¯¾å¿œï¼‰
            total_matches = 0
            for match_data in matches.values():
                if isinstance(match_data, list):
                    total_matches += len(match_data)
                elif isinstance(match_data, dict):
                    total_matches += 1
            
            lines.append(f"Total Matches: {total_matches}")
            lines.append(f"")
            
            for search_term, match_data in matches.items():
                lines.append(f"Query: {search_term}")
                lines.append(f"-" * 30)
                
                # ãƒãƒ«ãƒDBæ¤œç´¢çµæœï¼ˆãƒªã‚¹ãƒˆå½¢å¼ï¼‰ã¸ã®å¯¾å¿œ
                if isinstance(match_data, list):
                    lines.append(f"  Found {len(match_data)} results from multiple databases:")
                    lines.append(f"")
                    
                    for i, match_item in enumerate(match_data, 1):
                        lines.append(f"  Result {i}:")
                        lines.append(f"    ID: {match_item.get('id', 'N/A')}")
                        
                        search_name = match_item.get('search_name', 'N/A')
                        description = match_item.get('description', None)
                        lines.append(f"    Search Name: {search_name}")
                        if description:
                            lines.append(f"    Description: {description}")
                        else:
                            lines.append(f"    Description: None")
                        
                        lines.append(f"    Data Type: {match_item.get('data_type', 'N/A')}")
                        lines.append(f"    Source: {match_item.get('source', 'N/A')}")
                        
                        # ã‚¹ã‚³ã‚¢æƒ…å ±
                        score = match_item.get('score', 'N/A')
                        if score != 'N/A' and 'search_metadata' in match_item:
                            metadata = match_item['search_metadata']
                            source_db = metadata.get('source_database', 'unknown')
                            lines.append(f"    Score: {score:.3f} (from {source_db})")
                        else:
                            lines.append(f"    Score: {score}")
                        
                        # æ „é¤Šæƒ…å ±ï¼ˆç°¡ç•¥ç‰ˆï¼‰
                        if 'nutrition' in match_item and match_item['nutrition']:
                            nutrition = match_item['nutrition']
                            calories = nutrition.get('calories', 0)
                            protein = nutrition.get('protein', 0)
                            fat = nutrition.get('fat', 0)
                            carbs = nutrition.get('carbs', 0)
                            lines.append(f"    Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g")
                        
                        lines.append(f"")
                
                # å˜ä¸€çµæœï¼ˆè¾æ›¸å½¢å¼ï¼‰ã¸ã®å¯¾å¿œï¼ˆå¾“æ¥ã®æ–¹å¼ï¼‰
                elif isinstance(match_data, dict):
                    lines.append(f"  ID: {match_data.get('id', 'N/A')}")
                    
                    search_name = match_data.get('search_name', 'N/A')
                    description = match_data.get('description', None)
                    lines.append(f"  Search Name: {search_name}")
                    if description:
                        lines.append(f"  Description: {description}")
                    else:
                        lines.append(f"  Description: None")
                    
                    lines.append(f"  Data Type: {match_data.get('data_type', 'N/A')}")
                    lines.append(f"  Source: {match_data.get('source', 'N/A')}")
                    
                    # ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’æ”¹å–„
                    score = match_data.get('score', 'N/A')
                    if score != 'N/A' and 'search_metadata' in match_data:
                        metadata = match_data['search_metadata']
                        score_breakdown = metadata.get('score_breakdown', {})
                        calculation = metadata.get('calculation', '')
                        match_type = score_breakdown.get('match_type', 'unknown')
                        
                        if calculation:
                            lines.append(f"  Score: {score} ({match_type}: {calculation})")
                        else:
                            lines.append(f"  Score: {score} (text similarity + data type priority)")
                    else:
                        lines.append(f"  Score: {score}")
                    
                    if 'nutrients' in match_data and match_data['nutrients']:
                        lines.append(f"  Nutrients ({len(match_data['nutrients'])}):")
                        for nutrient in match_data['nutrients']:
                            if isinstance(nutrient, dict):
                                name = nutrient.get('name', 'Unknown')
                                amount = nutrient.get('amount', 0)
                                unit = nutrient.get('unit_name', '')
                                lines.append(f"    - {name}: {amount} {unit}")
                    
                    if 'original_data' in match_data:
                        original_data = match_data['original_data']
                        if isinstance(original_data, dict):
                            lines.append(f"  Original Data Source: {original_data.get('source', 'Unknown')}")
                            if search_method == "local_search":
                                lines.append(f"  Local DB Source: {original_data.get('db_source', 'Unknown')}")
                    
                    lines.append(f"")
        
        # æ¤œç´¢çµ±è¨ˆ
        if log.output_data and 'search_summary' in log.output_data:
            summary = log.output_data['search_summary']
            lines.append(f"Search Statistics:")
            lines.append(f"  Total Searches: {summary.get('total_searches', 0)}")
            lines.append(f"  Successful Matches: {summary.get('successful_matches', 0)}")
            lines.append(f"  Failed Searches: {summary.get('failed_searches', 0)}")
            lines.append(f"  Match Rate: {summary.get('match_rate_percent', 0)}%")
            
            # ãƒãƒ«ãƒDBæ¤œç´¢ã®å ´åˆã®è¿½åŠ æƒ…å ±
            if 'target_databases' in summary:
                lines.append(f"  Target Databases: {', '.join(summary['target_databases'])}")
                lines.append(f"  Results per Database: {summary.get('results_per_db', 'N/A')}")
                lines.append(f"  Total Results: {summary.get('total_results', 'N/A')}")
            
            if search_method == "local_search":
                lines.append(f"  Total Database Items: {summary.get('total_database_items', 0)}")
        
        return "\n".join(lines)
    
    def get_analysis_folder_path(self) -> str:
        """è§£æãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—"""
        return str(self.analysis_dir) 


################################################################################
## CATEGORY: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - Phase1 AIåˆ†æ
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/components/base.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/components/base.py
----------------------------------------------------------------------

from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Any, Optional
import logging
from datetime import datetime

# å‹å¤‰æ•°ã®å®šç¾©
InputType = TypeVar('InputType')
OutputType = TypeVar('OutputType')


class BaseComponent(ABC, Generic[InputType, OutputType]):
    """
    é£Ÿäº‹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæŠ½è±¡ã‚¯ãƒ©ã‚¹
    
    å…¨ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã“ã®ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã€process ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    """
    
    def __init__(self, component_name: str, logger: Optional[logging.Logger] = None):
        """
        ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        
        Args:
            component_name: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå
            logger: ãƒ­ã‚¬ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆï¼‰
        """
        self.component_name = component_name
        self.logger = logger or logging.getLogger(f"{__name__}.{component_name}")
        self.created_at = datetime.now()
        self.execution_count = 0
        self.current_execution_log = None  # è©³ç´°ãƒ­ã‚°
        
    @abstractmethod
    async def process(self, input_data: InputType) -> OutputType:
        """
        ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆæŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            OutputType: å‡¦ç†çµæœ
            
        Raises:
            ComponentError: å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
        """
        pass
    
    async def execute(self, input_data: InputType, execution_log: Optional['DetailedExecutionLog'] = None) -> OutputType:
        """
        ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸå®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆãƒ­ã‚°è¨˜éŒ²ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        
        Args:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            execution_log: è©³ç´°å®Ÿè¡Œãƒ­ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            OutputType: å‡¦ç†çµæœ
        """
        self.execution_count += 1
        execution_id = f"{self.component_name}_{self.execution_count}"
        
        # è©³ç´°ãƒ­ã‚°ã®è¨­å®š
        if execution_log:
            self.current_execution_log = execution_log
            # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            self.current_execution_log.set_input(self._safe_serialize_input(input_data))
        
        self.logger.info(f"[{execution_id}] Starting {self.component_name} processing")
        
        try:
            start_time = datetime.now()
            result = await self.process(input_data)
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            self.logger.info(f"[{execution_id}] {self.component_name} completed in {processing_time:.2f}s")
            
            # è©³ç´°ãƒ­ã‚°ã«å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.set_output(self._safe_serialize_output(result))
                self.current_execution_log.finalize()
            
            return result
            
        except Exception as e:
            self.logger.error(f"[{execution_id}] {self.component_name} failed: {str(e)}", exc_info=True)
            
            # è©³ç´°ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
            if self.current_execution_log:
                self.current_execution_log.add_error(str(e))
                self.current_execution_log.finalize()
            
            raise ComponentError(f"{self.component_name} processing failed: {str(e)}") from e
        finally:
            self.current_execution_log = None
    
    def log_prompt(self, prompt_name: str, prompt_content: str, variables: dict = None):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_prompt(prompt_name, prompt_content, variables)
    
    def log_reasoning(self, decision_point: str, reason: str, confidence: float = None):
        """æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_reasoning(decision_point, reason, confidence)
    
    def log_processing_detail(self, detail_key: str, detail_value: Any):
        """å‡¦ç†è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_processing_detail(detail_key, detail_value)
    
    def log_confidence_score(self, metric_name: str, score: float):
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_confidence_score(metric_name, score)
    
    def log_warning(self, warning: str):
        """è­¦å‘Šã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        if self.current_execution_log:
            self.current_execution_log.add_warning(warning)
    
    def _safe_serialize_input(self, input_data: InputType) -> dict:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(input_data, 'model_dump'):
                return input_data.model_dump()
            elif hasattr(input_data, '__dict__'):
                return input_data.__dict__
            else:
                return {"data": str(input_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def _safe_serialize_output(self, output_data: OutputType) -> dict:
        """å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        try:
            if hasattr(output_data, 'model_dump'):
                return output_data.model_dump()
            elif hasattr(output_data, '__dict__'):
                return output_data.__dict__
            else:
                return {"data": str(output_data)}
        except Exception as e:
            return {"serialization_error": str(e)}
    
    def get_component_info(self) -> dict:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæƒ…å ±ã‚’å–å¾—"""
        return {
            "component_name": self.component_name,
            "created_at": self.created_at.isoformat(),
            "execution_count": self.execution_count,
            "component_type": self.__class__.__name__
        }


class ComponentError(Exception):
    """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼"""
    
    def __init__(self, message: str, component_name: str = None, original_error: Exception = None):
        super().__init__(message)
        self.component_name = component_name
        self.original_error = original_error
        self.timestamp = datetime.now()
    
    def to_dict(self) -> dict:
        """ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "error_message": str(self),
            "component_name": self.component_name,
            "timestamp": self.timestamp.isoformat(),
            "original_error": str(self.original_error) if self.original_error else None
        } 

----------------------------------------------------------------------
### FILE: app_v2/components/phase1_component.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/components/phase1_component.py
----------------------------------------------------------------------

import json
from typing import Optional

from .base import BaseComponent
from ..models.phase1_models import (
    Phase1Input, Phase1Output, Dish, Ingredient, 
    DetectedFoodItem, FoodAttribute, AttributeType
)
from ..services.gemini_service import GeminiService
from ..config import get_settings
from ..config.prompts import Phase1Prompts


class Phase1Component(BaseComponent[Phase1Input, Phase1Output]):
    """
    Phase1: ç”»åƒåˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œãƒ»æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ç‰¹åŒ–ï¼‰
    
    Gemini AIã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸè©³ç´°æƒ…å ±
    ï¼ˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã€å±æ€§ã€ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ç­‰ï¼‰ã‚’å«ã‚€æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«é©ã—ãŸå‡ºåŠ›ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    """
    
    def __init__(self, gemini_service: Optional[GeminiService] = None):
        super().__init__("Phase1Component")
        
        # GeminiServiceã®åˆæœŸåŒ–
        if gemini_service is None:
            settings = get_settings()
            self.gemini_service = GeminiService(
                project_id=settings.GEMINI_PROJECT_ID,
                location=settings.GEMINI_LOCATION,
                model_name=settings.GEMINI_MODEL_NAME
            )
        else:
            self.gemini_service = gemini_service
    
    async def process(self, input_data: Phase1Input) -> Phase1Output:
        """
        Phase1ã®ä¸»å‡¦ç†: æ§‹é€ åŒ–ç”»åƒåˆ†æï¼ˆæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ç‰¹åŒ–ï¼‰
        
        Args:
            input_data: Phase1Input (image_bytes, image_mime_type, optional_text)
            
        Returns:
            Phase1Output: æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æçµæœï¼ˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã€å±æ€§ã€ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ç­‰ã‚’å«ã‚€ï¼‰
        """
        self.logger.info(f"Starting Phase1 structured image analysis for enhanced nutrition database query generation")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã¨è¨˜éŒ²ï¼ˆçµ±ä¸€ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚·ã‚¹ãƒ†ãƒ ä½¿ç”¨ï¼‰
        system_prompt = Phase1Prompts.get_system_prompt()
        user_prompt = Phase1Prompts.get_user_prompt(input_data.optional_text)
        
        self.log_prompt("structured_system_prompt", system_prompt)
        self.log_prompt("user_prompt", user_prompt, {
            "optional_text": input_data.optional_text,
            "image_mime_type": input_data.image_mime_type
        })
        
        # ç”»åƒæƒ…å ±ã®ãƒ­ã‚°è¨˜éŒ²
        self.log_processing_detail("image_size_bytes", len(input_data.image_bytes))
        self.log_processing_detail("image_mime_type", input_data.image_mime_type)
        
        try:
            # Gemini AIã«ã‚ˆã‚‹æ§‹é€ åŒ–ç”»åƒåˆ†æ
            self.log_processing_detail("gemini_structured_api_call_start", "Calling Gemini API for structured image analysis")
            
            gemini_result = await self.gemini_service.analyze_phase1_structured(
                image_bytes=input_data.image_bytes,
                image_mime_type=input_data.image_mime_type,
                optional_text=input_data.optional_text,
                system_prompt=system_prompt
            )
            
            self.log_processing_detail("gemini_structured_response", gemini_result)
            
            # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†
            detected_food_items = []
            if "detected_food_items" in gemini_result:
                for item_index, item_data in enumerate(gemini_result["detected_food_items"]):
                    # å±æ€§ã‚’å‡¦ç†
                    attributes = []
                    for attr_data in item_data.get("attributes", []):
                        # AttributeTypeã«å­˜åœ¨ã—ãªã„å ´åˆã¯PREPARATIONã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        attr_type_str = attr_data.get("type", "ingredient")
                        try:
                            attr_type = AttributeType(attr_type_str)
                        except ValueError:
                            # æœªçŸ¥ã®å±æ€§ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯æœ€ã‚‚è¿‘ã„æ—¢å­˜ã‚¿ã‚¤ãƒ—ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                            if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                attr_type = AttributeType.PREPARATION
                            elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                attr_type = AttributeType.COOKING_METHOD
                            elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                attr_type = AttributeType.TEXTURE
                            else:
                                attr_type = AttributeType.PREPARATION  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                        
                        attribute = FoodAttribute(
                            type=attr_type,
                            value=attr_data["value"],
                            confidence=attr_data.get("confidence", 0.5)
                        )
                        attributes.append(attribute)
                    
                    # DetectedFoodItemã‚’ä½œæˆ
                    detected_item = DetectedFoodItem(
                        item_name=item_data["item_name"],
                        confidence=item_data.get("confidence", 0.5),
                        attributes=attributes,
                        brand=item_data.get("brand"),
                        category_hints=item_data.get("category_hints", []),
                        negative_cues=item_data.get("negative_cues", [])
                    )
                    detected_food_items.append(detected_item)
                    
                    # æ§‹é€ åŒ–ã‚¢ã‚¤ãƒ†ãƒ è­˜åˆ¥ã®æ¨è«–ç†ç”±ã‚’ãƒ­ã‚°
                    self.log_reasoning(
                        f"structured_item_identification_{item_index}",
                        f"Structured identification: '{item_data['item_name']}' (confidence: {item_data.get('confidence', 0.5):.2f}, "
                        f"attributes: {len(attributes)}, brand: {item_data.get('brand', 'N/A')})"
                    )
            
            # å¾“æ¥äº’æ›æ€§ã®ãŸã‚ã®dishesã‚‚ç”Ÿæˆ
            dishes = []
            if "dishes" in gemini_result:
                for dish_index, dish_data in enumerate(gemini_result.get("dishes", [])):
                    ingredients = []
                    for ingredient_index, ingredient_data in enumerate(dish_data.get("ingredients", [])):
                        # æ§‹é€ åŒ–å±æ€§ã‚’å¾“æ¥å½¢å¼ã«å¤‰æ›
                        ingredient_attributes = []
                        if "attributes" in ingredient_data:
                            for attr_data in ingredient_data["attributes"]:
                                # AttributeTypeã«å­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
                                attr_type_str = attr_data.get("type", "ingredient")
                                try:
                                    attr_type = AttributeType(attr_type_str)
                                except ValueError:
                                    # æœªçŸ¥ã®å±æ€§ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯ãƒãƒƒãƒ”ãƒ³ã‚°
                                    if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                        attr_type = AttributeType.PREPARATION
                                    elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                        attr_type = AttributeType.COOKING_METHOD
                                    elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                        attr_type = AttributeType.TEXTURE
                                    else:
                                        attr_type = AttributeType.PREPARATION
                                
                                attr = FoodAttribute(
                                    type=attr_type,
                                    value=attr_data["value"],
                                    confidence=attr_data.get("confidence", 0.5)
                                )
                                ingredient_attributes.append(attr)
                        
                        # weight_gãŒå¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã®ã§ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                        if "weight_g" not in ingredient_data:
                            error_msg = f"Missing required field 'weight_g' for ingredient '{ingredient_data.get('ingredient_name', 'unknown')}'. Gemini must provide weight estimation for all ingredients."
                            self.logger.error(error_msg)
                            raise ValueError(error_msg)
                        
                        ingredient = Ingredient(
                            ingredient_name=ingredient_data["ingredient_name"],
                            weight_g=ingredient_data["weight_g"],
                            confidence=ingredient_data.get("confidence"),
                            detected_attributes=ingredient_attributes
                        )
                        ingredients.append(ingredient)
                    
                    # æ–™ç†ãƒ¬ãƒ™ãƒ«ã®å±æ€§
                    dish_attributes = []
                    if "attributes" in dish_data:
                        for attr_data in dish_data["attributes"]:
                            # AttributeTypeã«å­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
                            attr_type_str = attr_data.get("type", "preparation")
                            try:
                                attr_type = AttributeType(attr_type_str)
                            except ValueError:
                                # æœªçŸ¥ã®å±æ€§ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯ãƒãƒƒãƒ”ãƒ³ã‚°
                                if attr_type_str in ["cut", "chopped", "sliced", "diced"]:
                                    attr_type = AttributeType.PREPARATION
                                elif attr_type_str in ["fresh", "cooked", "raw", "fried", "grilled"]:
                                    attr_type = AttributeType.COOKING_METHOD
                                elif attr_type_str in ["sweet", "salty", "spicy", "sour"]:
                                    attr_type = AttributeType.TEXTURE
                                else:
                                    attr_type = AttributeType.PREPARATION
                            
                            attr = FoodAttribute(
                                type=attr_type,
                                value=attr_data["value"],
                                confidence=attr_data.get("confidence", 0.5)
                            )
                            dish_attributes.append(attr)
                    
                    dish = Dish(
                        dish_name=dish_data["dish_name"],
                        confidence=dish_data.get("confidence"),
                        ingredients=ingredients,
                        detected_attributes=dish_attributes
                    )
                    dishes.append(dish)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾“æ¥å½¢å¼ã‚’ç”Ÿæˆ
            if not dishes and detected_food_items:
                dishes = self._convert_structured_to_legacy(detected_food_items)
            
            # åˆ†æçµ±è¨ˆã®è¨˜éŒ²
            self.log_processing_detail("detected_structured_items_count", len(detected_food_items))
            self.log_processing_detail("detected_dishes_count", len(dishes))
            self.log_processing_detail("total_ingredients_count", sum(len(dish.ingredients) for dish in dishes))
            
            # å…¨ä½“çš„ãªåˆ†æä¿¡é ¼åº¦ã‚’è¨ˆç®—
            overall_confidence = self._calculate_overall_confidence(detected_food_items, dishes)
            
            # å‡¦ç†ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
            processing_notes = [
                f"Structured analysis generated {len(detected_food_items)} food items",
                f"Overall confidence: {overall_confidence:.2f}",
                f"Legacy compatibility: {len(dishes)} dishes generated"
            ]
            
            result = Phase1Output(
                detected_food_items=detected_food_items,
                dishes=dishes,
                analysis_confidence=overall_confidence,
                processing_notes=processing_notes,
                warnings=[]
            )
            
            self.log_processing_detail("structured_search_terms", result.get_structured_search_terms())
            self.log_reasoning(
                "structured_analysis_completion",
                f"Phase1 structured analysis completed: {len(detected_food_items)} structured items, "
                f"overall confidence {overall_confidence:.2f}"
            )
            
            self.logger.info(f"Phase1 structured analysis completed: {len(detected_food_items)} items, "
                           f"confidence {overall_confidence:.2f}")
            return result
            
        except Exception as e:
            self.logger.error(f"Phase1 structured processing failed: {str(e)}")
            raise
    

    
    def _convert_structured_to_legacy(self, detected_items: list) -> list:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’å¾“æ¥å½¢å¼ã«å¤‰æ›ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        # ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯é‡é‡æƒ…å ±ãŒä¸å®Œå…¨ãªå ´åˆã«ä½¿ç”¨ã•ã‚Œã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹
        error_msg = "Cannot convert structured data to legacy format without weight information. Gemini must provide weight_g for all ingredients in the standard dishes format."
        self.logger.error(error_msg)
        raise ValueError(error_msg)
    
    def _calculate_overall_confidence(self, structured_items: list, dishes: list) -> float:
        """å…¨ä½“çš„ãªåˆ†æä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        if not structured_items and not dishes:
            return 0.0
        
        total_confidence = 0.0
        count = 0
        
        # æ§‹é€ åŒ–ã‚¢ã‚¤ãƒ†ãƒ ã®ä¿¡é ¼åº¦
        for item in structured_items:
            total_confidence += item.confidence
            count += 1
        
        # æ–™ç†ã®ä¿¡é ¼åº¦
        for dish in dishes:
            if dish.confidence is not None:
                total_confidence += dish.confidence
                count += 1
        
        return total_confidence / count if count > 0 else 0.5 


################################################################################
## CATEGORY: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - MyNetDiaryåˆ¶ç´„ä»˜ãæ¤œç´¢
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/components/mynetdiary_nutrition_search_component.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/components/mynetdiary_nutrition_search_component.py
----------------------------------------------------------------------

"""
MyNetDiaryå°‚ç”¨æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
"""
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging

from .base import BaseComponent
from ..models.nutrition_search_models import NutritionQueryInput, NutritionQueryOutput, NutritionMatch
from ..utils.mynetdiary_utils import validate_ingredient_against_mynetdiary

# Elasticsearchã®å¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
try:
    from elasticsearch import Elasticsearch
    ELASTICSEARCH_AVAILABLE = True
except ImportError:
    ELASTICSEARCH_AVAILABLE = False
    Elasticsearch = None

class MyNetDiaryNutritionSearchComponent(BaseComponent[NutritionQueryInput, NutritionQueryOutput]):
    """
    MyNetDiaryå°‚ç”¨æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    
    ç‰¹å¾´:
    - ingredientã«å¯¾ã—ã¦ã¯å®Œå…¨ä¸€è‡´ã®1ã¤ã®é …ç›®ã®ã¿ã‚’å–å¾—
    - è¤‡æ•°é …ç›®ã‚„0ä»¶ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã§å…¨ä½“ã®è§£æã‚’åœæ­¢
    - dishã«å¯¾ã—ã¦ã¯å¾“æ¥é€šã‚Šã®æ¤œç´¢ã‚’å®Ÿè¡Œ
    """
    
    def __init__(
        self, 
        elasticsearch_url: str = "http://localhost:9200",
        results_per_db: int = 3
    ):
        """
        MyNetDiaryNutritionSearchComponentã®åˆæœŸåŒ–
        
        Args:
            elasticsearch_url: Elasticsearchã®URL
            results_per_db: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚ãŸã‚Šã®çµæœæ•°ï¼ˆdishã®å ´åˆï¼‰
        """
        super().__init__("MyNetDiaryNutritionSearchComponent")
        self.elasticsearch_url = elasticsearch_url
        self.results_per_db = results_per_db
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
        self.index_name = "nutrition_db"
        
        # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.es_client = None
        self._initialize_elasticsearch()

    def _initialize_elasticsearch(self):
        """Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        if not ELASTICSEARCH_AVAILABLE:
            self.logger.error("Elasticsearch library not available")
            return
        
        try:
            self.es_client = Elasticsearch([self.elasticsearch_url])
            
            # æ¥ç¶šãƒ†ã‚¹ãƒˆ
            if self.es_client.ping():
                self.logger.info(f"Successfully connected to Elasticsearch at {self.elasticsearch_url}")
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å­˜åœ¨ç¢ºèª
                if self.es_client.indices.exists(index=self.index_name):
                    self.logger.info(f"Index '{self.index_name}' exists and ready")
                else:
                    self.logger.error(f"Index '{self.index_name}' does not exist")
            else:
                self.logger.error(f"Failed to connect to Elasticsearch at {self.elasticsearch_url}")
                self.es_client = None
                
        except Exception as e:
            self.logger.error(f"Error initializing Elasticsearch: {e}")
            self.es_client = None

    async def process(self, input_data: NutritionQueryInput) -> NutritionQueryOutput:
        """
        MyNetDiaryå°‚ç”¨æ¤œç´¢ã®ä¸»å‡¦ç†
        
        Args:
            input_data: NutritionQueryInput
            
        Returns:
            NutritionQueryOutput: MyNetDiaryå°‚ç”¨æ¤œç´¢çµæœ
            
        Raises:
            RuntimeError: ElasticsearchãŒåˆ©ç”¨ã§ããªã„å ´åˆã€ã¾ãŸã¯ingredientæ¤œç´¢ã§è¤‡æ•°/0ä»¶ã®å ´åˆ
        """
        # Elasticsearchåˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        if not ELASTICSEARCH_AVAILABLE or not self.es_client:
            raise RuntimeError("Elasticsearch not available for MyNetDiary search")
        
        start_time = datetime.now()
        
        search_terms = input_data.get_all_search_terms()
        self.logger.info(f"Starting MyNetDiary specialized search for {len(search_terms)} terms")
        
        matches = {}
        successful_matches = 0
        total_searches = len(search_terms)
        errors = []
        
        # å„æ¤œç´¢èªå½™ã«ã¤ã„ã¦å‡¦ç†
        for search_index, search_term in enumerate(search_terms):
            try:
                # ingredientã‹dishã‹ã‚’åˆ¤å®š
                if search_term in input_data.ingredient_names:
                    # ingredientæ¤œç´¢: å³å¯†ãª1ä»¶ãƒãƒƒãƒãƒ³ã‚°
                    result = await self._strict_ingredient_search(search_term)
                    if result:
                        matches[search_term] = result
                        successful_matches += 1
                        self.logger.info(f"Strict ingredient match for '{search_term}': {result.name}")
                    else:
                        # ingredientæ¤œç´¢ã§çµæœãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã§åœæ­¢
                        error_msg = f"CRITICAL: No exact match found for ingredient '{search_term}' in MyNetDiary database"
                        self.logger.error(error_msg)
                        raise RuntimeError(error_msg)
                        
                elif search_term in input_data.dish_names:
                    # dishæ¤œç´¢: å¾“æ¥é€šã‚Šã®æ¤œç´¢
                    results = await self._flexible_dish_search(search_term)
                    if results:
                        matches[search_term] = results
                        successful_matches += 1
                        self.logger.info(f"Dish search for '{search_term}': {len(results)} results")
                    else:
                        self.logger.warning(f"No dish results found for '{search_term}'")
                        
            except Exception as e:
                error_msg = f"MyNetDiary search error for '{search_term}': {str(e)}"
                self.logger.error(error_msg)
                errors.append(error_msg)
                # ingredientæ¤œç´¢ã®ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…¨ä½“ã‚’åœæ­¢
                if search_term in input_data.ingredient_names:
                    raise RuntimeError(error_msg)
        
        # çµæœã®æ§‹ç¯‰
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        total_results = sum(len(result_list) if isinstance(result_list, list) else 1 for result_list in matches.values())
        match_rate = (successful_matches / total_searches * 100) if total_searches > 0 else 0
        
        search_summary = {
            "total_searches": total_searches,
            "successful_matches": successful_matches,
            "failed_searches": total_searches - successful_matches,
            "match_rate_percent": round(match_rate, 1),
            "search_method": "mynetdiary_specialized",
            "search_time_ms": search_time_ms,
            "total_results": total_results,
            "ingredient_strict_matching": True,
            "dish_flexible_matching": True
        }
        
        result = NutritionQueryOutput(
            matches=matches,
            search_summary=search_summary,
            errors=errors if errors else None
        )
        
        self.logger.info(f"MyNetDiary specialized search completed: {successful_matches}/{total_searches} matches in {search_time_ms}ms")
        
        return result

    async def _strict_ingredient_search(self, ingredient_name: str) -> Optional[NutritionMatch]:
        """
        ingredientç”¨ã®å³å¯†æ¤œç´¢ï¼ˆå®Œå…¨ä¸€è‡´ã®1ä»¶ã®ã¿ï¼‰
        
        Args:
            ingredient_name: æ¤œç´¢ã™ã‚‹é£Ÿæå
            
        Returns:
            NutritionMatch: å®Œå…¨ä¸€è‡´ã®1ä»¶ã€ã¾ãŸã¯ None
            
        Raises:
            RuntimeError: è¤‡æ•°ä»¶ãƒãƒƒãƒã—ãŸå ´åˆ
        """
        # ã¾ãšMyNetDiaryãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not validate_ingredient_against_mynetdiary(ingredient_name):
            self.logger.error(f"Ingredient '{ingredient_name}' not found in MyNetDiary list")
            return None
        
        # MyNetDiaryãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å®Œå…¨ä¸€è‡´æ¤œç´¢
        query = {
            "size": 10,  # è¤‡æ•°ä»¶ãƒã‚§ãƒƒã‚¯ç”¨
            "query": {
                "bool": {
                    "must": [
                        {"term": {"search_name.exact": ingredient_name}},
                        {"term": {"source_db": "mynetdiary"}}
                    ]
                }
            }
        }
        
        try:
            response = self.es_client.search(index=self.index_name, body=query)
            hits = response.get('hits', {}).get('hits', [])
            
            if len(hits) == 0:
                self.logger.error(f"No exact match found for ingredient '{ingredient_name}' in MyNetDiary database")
                return None
            elif len(hits) > 1:
                # è¤‡æ•°ä»¶ãƒãƒƒãƒã—ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼
                hit_names = [hit['_source'].get('search_name', 'N/A') for hit in hits]
                error_msg = f"Multiple matches found for ingredient '{ingredient_name}': {hit_names}"
                self.logger.error(error_msg)
                raise RuntimeError(error_msg)
            else:
                # æ­£ç¢ºã«1ä»¶ãƒãƒƒãƒã—ãŸå ´åˆ
                hit = hits[0]
                match = self._convert_es_hit_to_nutrition_match(hit, ingredient_name)
                match.search_metadata["search_type"] = "strict_ingredient"
                match.search_metadata["validation_passed"] = True
                return match
                
        except Exception as e:
            self.logger.error(f"Error in strict ingredient search for '{ingredient_name}': {e}")
            raise

    async def _flexible_dish_search(self, dish_name: str) -> List[NutritionMatch]:
        """
        dishç”¨ã®æŸ”è»Ÿæ¤œç´¢ï¼ˆå¾“æ¥é€šã‚Šï¼‰
        
        Args:
            dish_name: æ¤œç´¢ã™ã‚‹æ–™ç†å
            
        Returns:
            List[NutritionMatch]: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        # è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢
        query = {
            "size": self.results_per_db,
            "query": {
                "bool": {
                    "must": [
                        {
                            "multi_match": {
                                "query": dish_name,
                                "fields": [
                                    "search_name^3",
                                    "description^1"
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO"
                            }
                        }
                    ],
                    "should": [
                        {"term": {"data_type": "dish"}},
                        {"term": {"data_type": "branded"}}
                    ]
                }
            },
            "sort": [
                {"_score": {"order": "desc"}}
            ]
        }
        
        try:
            response = self.es_client.search(index=self.index_name, body=query)
            hits = response.get('hits', {}).get('hits', [])
            
            results = []
            for hit in hits:
                match = self._convert_es_hit_to_nutrition_match(hit, dish_name)
                match.search_metadata["search_type"] = "flexible_dish"
                results.append(match)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Error in flexible dish search for '{dish_name}': {e}")
            return []

    def _convert_es_hit_to_nutrition_match(self, hit: Dict[str, Any], search_term: str) -> NutritionMatch:
        """Elasticsearchã®hitã‚’NutritionMatchã«å¤‰æ›"""
        source = hit['_source']
        score = hit['_score']
        
        return NutritionMatch(
            id=source.get('id', hit.get('_id', 'unknown')),
            name=source.get('search_name', 'Unknown'),
            search_name=source.get('search_name', 'Unknown'),
            description=source.get('description'),
            data_type=source.get('data_type', 'unknown'),
            source_db=source.get('source_db', 'unknown'),
            nutrition=source.get('nutrition', {}),
            weight=source.get('weight', 100),
            score=score,
            search_metadata={
                "search_term": search_term,
                "elasticsearch_score": score,
                "search_method": "mynetdiary_specialized",
                "source_database": source.get('source_db', 'unknown'),
                "index_name": self.index_name,
                "data_type": source.get('data_type', 'unknown')
            }
        ) 


################################################################################
## CATEGORY: ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - æ „é¤Šè¨ˆç®—
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/components/nutrition_calculation_component.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/components/nutrition_calculation_component.py
----------------------------------------------------------------------

#!/usr/bin/env python3
"""
Nutrition Calculation Component

Phase1çµæœã¨æ „é¤Šæ¤œç´¢çµæœã‹ã‚‰å®Ÿéš›ã®æ „é¤Šç´ ã‚’è¨ˆç®—ã™ã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
"""

import logging
from datetime import datetime
from typing import Dict, List, Any, Optional

from .base import BaseComponent
from ..models.nutrition_calculation_models import (
    NutritionCalculationInput, 
    NutritionCalculationOutput,
    NutritionInfo,
    IngredientNutrition,
    DishNutrition,
    MealNutrition
)
from ..models.phase1_models import Phase1Output
from ..models.nutrition_search_models import NutritionQueryOutput, NutritionMatch


class NutritionCalculationComponent(BaseComponent[NutritionCalculationInput, NutritionCalculationOutput]):
    """æ „é¤Šè¨ˆç®—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ"""
    
    def __init__(self):
        super().__init__("NutritionCalculationComponent")
        
    async def process(self, input_data: NutritionCalculationInput) -> NutritionCalculationOutput:
        """
        æ „é¤Šè¨ˆç®—ã®ä¸»å‡¦ç†
        
        Args:
            input_data: Phase1çµæœã¨æ „é¤Šæ¤œç´¢çµæœ
            
        Returns:
            NutritionCalculationOutput: è¨ˆç®—ã•ã‚ŒãŸæ „é¤Šæƒ…å ±
        """
        start_time = datetime.now()
        
        phase1_result: Phase1Output = input_data.phase1_result
        nutrition_search_result: NutritionQueryOutput = input_data.nutrition_search_result
        
        self.logger.info(f"Starting nutrition calculation for {len(phase1_result.dishes)} dishes")
        
        # æ–™ç†åˆ¥ã®æ „é¤Šè¨ˆç®—
        dish_nutritions = []
        total_meal_nutrition = None
        warnings = []
        
        for dish in phase1_result.dishes:
            try:
                dish_nutrition = await self._calculate_dish_nutrition(
                    dish, nutrition_search_result.matches
                )
                dish_nutritions.append(dish_nutrition)
                
                # é£Ÿäº‹å…¨ä½“ã®æ „é¤Šæƒ…å ±ã‚’ç´¯ç©
                if total_meal_nutrition is None:
                    total_meal_nutrition = dish_nutrition.total_nutrition
                else:
                    total_meal_nutrition = total_meal_nutrition + dish_nutrition.total_nutrition
                    
            except Exception as e:
                warning_msg = f"Failed to calculate nutrition for dish '{dish.dish_name}': {str(e)}"
                self.logger.warning(warning_msg)
                warnings.append(warning_msg)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
        if total_meal_nutrition is None:
            total_meal_nutrition = NutritionInfo(
                calories=0.0, protein=0.0, fat=0.0, carbs=0.0
            )
        
        # è¨ˆç®—ã‚µãƒãƒªãƒ¼ã®ä½œæˆ
        end_time = datetime.now()
        processing_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        calculation_summary = {
            "total_dishes": len(phase1_result.dishes),
            "successful_calculations": len(dish_nutritions),
            "failed_calculations": len(phase1_result.dishes) - len(dish_nutritions),
            "total_ingredients": sum(len(dish.ingredients) for dish in dish_nutritions),
            "processing_time_ms": processing_time_ms
        }
        
        # é£Ÿäº‹å…¨ä½“ã®æ „é¤Šæƒ…å ±
        meal_nutrition = MealNutrition(
            dishes=dish_nutritions,
            total_nutrition=total_meal_nutrition,
            calculation_summary=calculation_summary,
            warnings=warnings
        )
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        calculation_metadata = {
            "calculation_timestamp": end_time.isoformat(),
            "nutrition_search_method": nutrition_search_result.get_search_method(),
            "total_nutrition_matches": nutrition_search_result.get_total_matches()
        }
        
        result = NutritionCalculationOutput(
            meal_nutrition=meal_nutrition,
            calculation_metadata=calculation_metadata,
            processing_time_ms=processing_time_ms
        )
        
        self.logger.info(f"Nutrition calculation completed: {len(dish_nutritions)} dishes, "
                        f"{calculation_summary['total_ingredients']} ingredients, "
                        f"{total_meal_nutrition.calories:.1f} kcal total")
        
        return result
    
    async def _calculate_dish_nutrition(self, dish, nutrition_matches: Dict[str, Any]) -> DishNutrition:
        """
        æ–™ç†ãƒ¬ãƒ™ãƒ«ã®æ „é¤Šè¨ˆç®—
        
        Args:
            dish: Phase1ã§æ¤œå‡ºã•ã‚ŒãŸæ–™ç†
            nutrition_matches: æ „é¤Šæ¤œç´¢çµæœã®ãƒãƒƒãƒãƒ³ã‚°
            
        Returns:
            DishNutrition: æ–™ç†ã®æ „é¤Šè¨ˆç®—çµæœ
        """
        ingredient_nutritions = []
        dish_total_nutrition = None
        
        for ingredient in dish.ingredients:
            try:
                ingredient_nutrition = await self._calculate_ingredient_nutrition(
                    ingredient, nutrition_matches
                )
                ingredient_nutritions.append(ingredient_nutrition)
                
                # æ–™ç†å…¨ä½“ã®æ „é¤Šæƒ…å ±ã‚’ç´¯ç©
                if dish_total_nutrition is None:
                    dish_total_nutrition = ingredient_nutrition.calculated_nutrition
                else:
                    dish_total_nutrition = dish_total_nutrition + ingredient_nutrition.calculated_nutrition
                    
            except Exception as e:
                self.logger.error(f"Failed to calculate nutrition for ingredient '{ingredient.ingredient_name}': {e}")
                raise
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
        if dish_total_nutrition is None:
            dish_total_nutrition = NutritionInfo(
                calories=0.0, protein=0.0, fat=0.0, carbs=0.0
            )
        
        # è¨ˆç®—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        calculation_metadata = {
            "ingredient_count": len(ingredient_nutritions),
            "total_weight_g": sum(ing.weight_g for ing in ingredient_nutritions),
            "calculation_method": "weight_based_scaling"
        }
        
        return DishNutrition(
            dish_name=dish.dish_name,
            confidence=dish.confidence or 0.0,
            ingredients=ingredient_nutritions,
            total_nutrition=dish_total_nutrition,
            calculation_metadata=calculation_metadata
        )
    
    async def _calculate_ingredient_nutrition(self, ingredient, nutrition_matches: Dict[str, Any]) -> IngredientNutrition:
        """
        é£Ÿæãƒ¬ãƒ™ãƒ«ã®æ „é¤Šè¨ˆç®—
        
        Args:
            ingredient: Phase1ã§æ¤œå‡ºã•ã‚ŒãŸé£Ÿæ
            nutrition_matches: æ „é¤Šæ¤œç´¢çµæœã®ãƒãƒƒãƒãƒ³ã‚°
            
        Returns:
            IngredientNutrition: é£Ÿæã®æ „é¤Šè¨ˆç®—çµæœ
        """
        ingredient_name = ingredient.ingredient_name
        weight_g = ingredient.weight_g
        
        # æ „é¤Šæ¤œç´¢çµæœã‹ã‚‰è©²å½“ã™ã‚‹é£Ÿæã‚’å–å¾—
        if ingredient_name not in nutrition_matches:
            raise ValueError(f"No nutrition data found for ingredient '{ingredient_name}'")
        
        nutrition_match = nutrition_matches[ingredient_name]
        
        # ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã¯æœ€åˆã®è¦ç´ ã‚’ä½¿ç”¨
        if isinstance(nutrition_match, list):
            if len(nutrition_match) == 0:
                raise ValueError(f"Empty nutrition match list for ingredient '{ingredient_name}'")
            nutrition_match = nutrition_match[0]
        
        # NutritionMatchã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æ „é¤Šæƒ…å ±ã‚’å–å¾—
        if not isinstance(nutrition_match, NutritionMatch):
            raise ValueError(f"Invalid nutrition match type for ingredient '{ingredient_name}': {type(nutrition_match)}")
        
        nutrition_per_100g = nutrition_match.nutrition
        source_db = nutrition_match.source_db
        
        # é‡é‡ã«åŸºã¥ãæ „é¤Šè¨ˆç®—ï¼ˆ100gã‚ãŸã‚Š â†’ å®Ÿéš›ã®é‡é‡ï¼‰
        scaling_factor = weight_g / 100.0
        
        calculated_nutrition = NutritionInfo(
            calories=nutrition_per_100g.get('calories', 0.0) * scaling_factor,
            protein=nutrition_per_100g.get('protein', 0.0) * scaling_factor,
            fat=nutrition_per_100g.get('fat', 0.0) * scaling_factor,
            carbs=nutrition_per_100g.get('carbs', 0.0) * scaling_factor,
            fiber=nutrition_per_100g.get('fiber') * scaling_factor if nutrition_per_100g.get('fiber') is not None else None,
            sugar=nutrition_per_100g.get('sugar') * scaling_factor if nutrition_per_100g.get('sugar') is not None else None,
            sodium=nutrition_per_100g.get('sodium') * scaling_factor if nutrition_per_100g.get('sodium') is not None else None
        )
        
        # è¨ˆç®—ãƒãƒ¼ãƒˆ
        calculation_notes = [
            f"Scaled from 100g base data using factor {scaling_factor:.3f}",
            f"Source: {source_db} database"
        ]
        
        return IngredientNutrition(
            ingredient_name=ingredient_name,
            weight_g=weight_g,
            nutrition_per_100g=nutrition_per_100g,
            calculated_nutrition=calculated_nutrition,
            source_db=source_db,
            calculation_notes=calculation_notes
        ) 


################################################################################
## CATEGORY: ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å±¤
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/models/phase1_models.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/models/phase1_models.py
----------------------------------------------------------------------

from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field
from enum import Enum


class AttributeType(str, Enum):
    """å±æ€§ã‚¿ã‚¤ãƒ—ã®åˆ—æŒ™"""
    INGREDIENT = "ingredient"
    PREPARATION = "preparation"
    COLOR = "color"
    TEXTURE = "texture"
    COOKING_METHOD = "cooking_method"
    SERVING_STYLE = "serving_style"
    ALLERGEN = "allergen"


class FoodAttribute(BaseModel):
    """é£Ÿå“å±æ€§ãƒ¢ãƒ‡ãƒ«ï¼ˆææ–™ã€èª¿ç†æ³•ãªã©ï¼‰"""
    type: AttributeType = Field(..., description="å±æ€§ã®ã‚¿ã‚¤ãƒ—")
    value: str = Field(..., description="å±æ€§ã®å€¤")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ã“ã®å±æ€§ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢")


class DetectedFoodItem(BaseModel):
    """æ¤œå‡ºã•ã‚ŒãŸé£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆæ§‹é€ åŒ–ï¼‰"""
    item_name: str = Field(..., description="é£Ÿå“åï¼ˆä¸»è¦ãªå€™è£œï¼‰")
    confidence: float = Field(..., ge=0.0, le=1.0, description="é£Ÿå“åã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢")
    attributes: List[FoodAttribute] = Field(default=[], description="é£Ÿå“ã®å±æ€§ãƒªã‚¹ãƒˆï¼ˆææ–™ã€èª¿ç†æ³•ãªã©ï¼‰")
    brand: Optional[str] = Field(None, description="èªè­˜ã•ã‚ŒãŸãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰")
    category_hints: List[str] = Field(default=[], description="æ¨å®šã•ã‚Œã‚‹é£Ÿå“ã‚«ãƒ†ã‚´ãƒª")
    negative_cues: List[str] = Field(default=[], description="ç”»åƒã‹ã‚‰åˆ¤æ–­ã§ãã‚‹ã€Œå«ã¾ã‚Œãªã„ã€è¦ç´ ")


class Ingredient(BaseModel):
    """é£Ÿææƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ç”¨ãƒ»å¾“æ¥äº’æ›æ€§ï¼‰"""
    ingredient_name: str = Field(..., description="é£Ÿæã®åç§°ï¼ˆæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã§ä½¿ç”¨ï¼‰")
    weight_g: float = Field(..., gt=0, description="å†™çœŸã‹ã‚‰æ¨å®šã•ã‚Œã‚‹é£Ÿæã®é‡é‡ï¼ˆã‚°ãƒ©ãƒ ï¼‰")
    confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="é£Ÿæç‰¹å®šã®ä¿¡é ¼åº¦")
    detected_attributes: List[FoodAttribute] = Field(default=[], description="ã“ã®é£Ÿæã«é–¢é€£ã™ã‚‹å±æ€§")


class Dish(BaseModel):
    """æ–™ç†æƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ç”¨ãƒ»å¾“æ¥äº’æ›æ€§ï¼‰"""
    dish_name: str = Field(..., description="ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ï¼ˆæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã§ä½¿ç”¨ï¼‰")
    confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="æ–™ç†ç‰¹å®šã®ä¿¡é ¼åº¦")
    ingredients: List[Ingredient] = Field(..., description="ãã®æ–™ç†ã«å«ã¾ã‚Œã‚‹é£Ÿæã®ãƒªã‚¹ãƒˆ")
    detected_attributes: List[FoodAttribute] = Field(default=[], description="ã“ã®æ–™ç†ã«é–¢é€£ã™ã‚‹å±æ€§")


class Phase1Input(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›ãƒ¢ãƒ‡ãƒ«"""
    image_bytes: bytes = Field(..., description="ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒˆå½¢å¼ï¼‰")
    image_mime_type: str = Field(..., description="ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—")
    optional_text: Optional[str] = Field(None, description="ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±")

    class Config:
        arbitrary_types_allowed = True


class Phase1Output(BaseModel):
    """Phase1ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ åŒ–ãƒ»æ‹¡å¼µç‰ˆï¼‰"""
    # æ–°ã—ã„æ§‹é€ åŒ–å‡ºåŠ›
    detected_food_items: List[DetectedFoodItem] = Field(default=[], description="èªè­˜ã•ã‚ŒãŸé£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆï¼ˆæ§‹é€ åŒ–ï¼‰")
    
    # å¾“æ¥äº’æ›æ€§ã®ãŸã‚ã®å‡ºåŠ›
    dishes: List[Dish] = Field(..., description="ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆ")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    analysis_confidence: float = Field(..., ge=0.0, le=1.0, description="å…¨ä½“çš„ãªåˆ†æã®ä¿¡é ¼åº¦")
    processing_notes: List[str] = Field(default=[], description="å‡¦ç†ã«é–¢ã™ã‚‹æ³¨è¨˜")
    warnings: Optional[List[str]] = Field(None, description="å‡¦ç†ä¸­ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")

    def get_all_ingredient_names(self) -> List[str]:
        """å…¨ã¦ã®é£Ÿæåã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ç”¨ãƒ»å¾“æ¥äº’æ›æ€§ï¼‰"""
        ingredient_names = []
        for dish in self.dishes:
            for ingredient in dish.ingredients:
                ingredient_names.append(ingredient.ingredient_name)
        return ingredient_names

    def get_all_dish_names(self) -> List[str]:
        """å…¨ã¦ã®æ–™ç†åã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ç”¨ãƒ»å¾“æ¥äº’æ›æ€§ï¼‰"""
        return [dish.dish_name for dish in self.dishes]
    
    def get_structured_search_terms(self) -> Dict[str, Any]:
        """æ§‹é€ åŒ–ã•ã‚ŒãŸæ¤œç´¢ç”¨èªã‚’å–å¾—ï¼ˆæ–°ã—ã„æ¤œç´¢æˆ¦ç•¥ç”¨ï¼‰"""
        return {
            "high_confidence_items": [
                {
                    "item_name": item.item_name,
                    "confidence": item.confidence,
                    "brand": item.brand
                }
                for item in self.detected_food_items 
                if item.confidence >= 0.8
            ],
            "medium_confidence_items": [
                {
                    "item_name": item.item_name,
                    "confidence": item.confidence,
                    "brand": item.brand
                }
                for item in self.detected_food_items 
                if 0.5 <= item.confidence < 0.8
            ],
            "brands": [
                item.brand for item in self.detected_food_items 
                if item.brand is not None and item.brand != ""
            ],
            "ingredients": [
                attr.value for item in self.detected_food_items 
                for attr in item.attributes 
                if attr.type == AttributeType.INGREDIENT
            ],
            "cooking_methods": [
                attr.value for item in self.detected_food_items 
                for attr in item.attributes 
                if attr.type == AttributeType.PREPARATION
            ],
            "negative_cues": [
                cue for item in self.detected_food_items 
                for cue in item.negative_cues
            ]
        }
    
    def get_primary_search_terms(self) -> List[str]:
        """ãƒ—ãƒ©ã‚¤ãƒãƒªæ¤œç´¢ç”¨èªã‚’å–å¾—ï¼ˆé«˜ä¿¡é ¼åº¦ã‚¢ã‚¤ãƒ†ãƒ ï¼‰"""
        primary_terms = []
        
        # é«˜ä¿¡é ¼åº¦ã®æ¤œå‡ºã‚¢ã‚¤ãƒ†ãƒ 
        for item in self.detected_food_items:
            if item.confidence >= 0.7:
                primary_terms.append(item.item_name)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–™ç†åã¨é£Ÿæå
        if not primary_terms:
            primary_terms.extend(self.get_all_dish_names())
            primary_terms.extend(self.get_all_ingredient_names())
        
        return primary_terms 

----------------------------------------------------------------------
### FILE: app_v2/models/nutrition_search_models.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/models/nutrition_search_models.py
----------------------------------------------------------------------

#!/usr/bin/env python3
"""
Nutrition Search Models

ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã§ä½¿ç”¨ã™ã‚‹ç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ åŒ–å…¥åŠ›å¯¾å¿œï¼‰
"""

from typing import List, Dict, Optional, Any, Union
from pydantic import BaseModel, Field


class NutritionMatch(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç…§åˆçµæœãƒ¢ãƒ‡ãƒ«ï¼ˆç´”ç²‹ãªãƒ­ãƒ¼ã‚«ãƒ«å½¢å¼ï¼‰"""
    id: Union[int, str] = Field(..., description="é£Ÿå“IDï¼ˆãƒ­ãƒ¼ã‚«ãƒ«IDï¼‰")
    name: str = Field(..., description="é£Ÿå“å")  # search_nameã‹ã‚‰å¤‰æ›´
    search_name: str = Field(..., description="æ¤œç´¢åï¼ˆç°¡æ½”ãªåç§°ï¼‰")
    description: Optional[str] = Field(None, description="è©³ç´°èª¬æ˜")
    data_type: str = Field(..., description="ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ— (dish, ingredient, branded)")
    source_db: str = Field(..., description="ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆyazio, mynetdiary, eatthismuchï¼‰")
    source: str = Field(default="local_database", description="ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆ'local_database'ï¼‰")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«DBã®ç”Ÿã®æ „é¤Šãƒ‡ãƒ¼ã‚¿ï¼ˆ100gã‚ãŸã‚Šæ­£è¦åŒ–æ¸ˆã¿ï¼‰
    nutrition: Dict[str, float] = Field(default_factory=dict, description="ãƒ­ãƒ¼ã‚«ãƒ«DBã®æ „é¤Šãƒ‡ãƒ¼ã‚¿ï¼ˆ100gã‚ãŸã‚Šï¼‰")
    weight: Optional[float] = Field(None, description="å…ƒãƒ‡ãƒ¼ã‚¿ã®é‡é‡ï¼ˆgï¼‰")
    
    # æ¤œç´¢ã‚¹ã‚³ã‚¢
    score: Optional[float] = Field(None, description="æ¤œç´¢çµæœã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢")
    
    # æ¤œç´¢ã«é–¢ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    search_metadata: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ã«é–¢ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")


class AdvancedSearchOptions(BaseModel):
    """é«˜åº¦ãªæ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³"""
    enable_fuzzy_matching: bool = Field(default=True, description="ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    enable_two_stage_search: bool = Field(default=True, description="äºŒæ®µéšæ¤œç´¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹")
    primary_term_boost: float = Field(default=3.0, description="ãƒ—ãƒ©ã‚¤ãƒãƒªç”¨èªã®ãƒ–ãƒ¼ã‚¹ãƒˆå€¤")
    brand_boost: float = Field(default=2.5, description="ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ã®ãƒ–ãƒ¼ã‚¹ãƒˆå€¤")
    ingredient_boost: float = Field(default=1.5, description="ææ–™æƒ…å ±ã®ãƒ–ãƒ¼ã‚¹ãƒˆå€¤")
    preparation_boost: float = Field(default=1.2, description="èª¿ç†æ³•æƒ…å ±ã®ãƒ–ãƒ¼ã‚¹ãƒˆå€¤")
    jaro_winkler_threshold: float = Field(default=0.8, description="Jaro-Winkleré¡ä¼¼åº¦ã®é–¾å€¤")
    levenshtein_threshold: float = Field(default=0.7, description="Levenshteiné¡ä¼¼åº¦ã®é–¾å€¤")
    first_stage_size: int = Field(default=50, description="ç¬¬ä¸€æ®µéšã§å–å¾—ã™ã‚‹å€™è£œæ•°")
    final_result_size: int = Field(default=10, description="æœ€çµ‚çµæœæ•°")


class NutritionQueryInput(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢å…¥åŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ åŒ–å…¥åŠ›å¯¾å¿œï¼‰"""
    ingredient_names: List[str] = Field(default_factory=list, description="é£Ÿæåã®ãƒªã‚¹ãƒˆ")
    dish_names: List[str] = Field(default_factory=list, description="æ–™ç†åã®ãƒªã‚¹ãƒˆ")
    search_options: Optional[Dict[str, Any]] = Field(None, description="æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    preferred_source: str = Field(default="local_database", description="å„ªå…ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
    
    # æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒ¼ãƒˆ
    structured_analysis: Optional[Dict[str, Any]] = Field(None, description="Phase1ã‹ã‚‰ã®æ§‹é€ åŒ–åˆ†æãƒ‡ãƒ¼ã‚¿")
    phase1_output: Optional[Any] = Field(None, description="Phase1Outputã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰")
    
    # é«˜åº¦ãªæ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    advanced_search_options: Optional[AdvancedSearchOptions] = Field(None, description="é«˜åº¦ãªæ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
    
    # æ¤œç´¢æˆ¦ç•¥
    search_strategy: str = Field(default="basic", description="æ¤œç´¢æˆ¦ç•¥ï¼ˆbasic, strategic, advanced_structuredï¼‰")

    def get_all_search_terms(self) -> List[str]:
        """å…¨ã¦ã®æ¤œç´¢èªå½™ã‚’å–å¾—"""
        return list(set(self.ingredient_names + self.dish_names))
    
    def get_structured_search_terms(self) -> Optional[Dict[str, Any]]:
        """æ§‹é€ åŒ–ã•ã‚ŒãŸæ¤œç´¢ç”¨èªã‚’å–å¾—"""
        if self.phase1_output and hasattr(self.phase1_output, 'get_structured_search_terms'):
            return self.phase1_output.get_structured_search_terms()
        elif self.structured_analysis:
            return self.structured_analysis
        else:
            return None
    
    def get_primary_search_terms(self) -> List[str]:
        """ãƒ—ãƒ©ã‚¤ãƒãƒªæ¤œç´¢ç”¨èªã‚’å–å¾—ï¼ˆé«˜ä¿¡é ¼åº¦ã‚¢ã‚¤ãƒ†ãƒ ï¼‰"""
        if self.phase1_output and hasattr(self.phase1_output, 'get_primary_search_terms'):
            return self.phase1_output.get_primary_search_terms()
        else:
            return self.get_all_search_terms()
    
    def has_structured_data(self) -> bool:
        """æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return (
            self.structured_analysis is not None or
            (self.phase1_output and hasattr(self.phase1_output, 'detected_food_items'))
        )
    
    def is_advanced_search_enabled(self) -> bool:
        """é«˜åº¦ãªæ¤œç´¢ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        return (
            self.search_strategy in ["advanced_structured", "strategic"] or
            self.has_structured_data()
        )


class NutritionQueryOutput(BaseModel):
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢çµæœãƒ¢ãƒ‡ãƒ«ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œï¼‰"""
    # ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢å¯¾å¿œï¼šå˜ä¸€çµæœã¾ãŸã¯ãƒªã‚¹ãƒˆçµæœã‚’å—ã‘å…¥ã‚Œã‚‹
    matches: Dict[str, Union[NutritionMatch, List[NutritionMatch]]] = Field(
        default_factory=dict, 
        description="æ¤œç´¢èªå½™ã¨å¯¾å¿œã™ã‚‹ç…§åˆçµæœã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå˜ä¸€çµæœã¾ãŸã¯ãƒãƒ«ãƒDBçµæœãƒªã‚¹ãƒˆï¼‰"
    )
    search_summary: Dict[str, Any] = Field(
        default_factory=dict, 
        description="æ¤œç´¢çµæœã®ã‚µãƒãƒªãƒ¼æƒ…å ±ï¼ˆæŸ”è»Ÿãªå‹å¯¾å¿œï¼‰"
    )
    warnings: Optional[List[str]] = Field(None, description="è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ")
    errors: Optional[List[str]] = Field(None, description="ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ")
    
    # é«˜åº¦ãªæ¤œç´¢çµæœã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    advanced_search_metadata: Optional[Dict[str, Any]] = Field(None, description="é«˜åº¦ãªæ¤œç´¢ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")

    def get_match_rate(self) -> float:
        """ç…§åˆæˆåŠŸç‡ã‚’è¨ˆç®—"""
        total_searches = self.search_summary.get("total_searches", 0)
        successful_matches = self.search_summary.get("successful_matches", 0)
        if total_searches == 0:
            return 0.0
        return successful_matches / total_searches

    def get_total_matches(self) -> int:
        """ç·ç…§åˆä»¶æ•°ã‚’å–å¾—ï¼ˆãƒãƒ«ãƒDBæ¤œç´¢å¯¾å¿œï¼‰"""
        total = 0
        for match_result in self.matches.values():
            if isinstance(match_result, list):
                total += len(match_result)
            else:
                total += 1
        return total
    
    def get_total_individual_results(self) -> int:
        """å€‹åˆ¥çµæœã®ç·æ•°ã‚’å–å¾—ï¼ˆãƒãƒ«ãƒDBæ¤œç´¢ç”¨ï¼‰"""
        return self.get_total_matches()
    
    def has_errors(self) -> bool:
        """ã‚¨ãƒ©ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.errors is not None and len(self.errors) > 0
    
    def has_warnings(self) -> bool:
        """è­¦å‘ŠãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.warnings is not None and len(self.warnings) > 0
    
    def get_search_method(self) -> str:
        """ä½¿ç”¨ã•ã‚ŒãŸæ¤œç´¢æ–¹æ³•ã‚’å–å¾—"""
        return self.search_summary.get("search_method", "unknown")
    
    def is_advanced_search_result(self) -> bool:
        """é«˜åº¦ãªæ¤œç´¢ã®çµæœã‹ãƒã‚§ãƒƒã‚¯"""
        search_method = self.get_search_method()
        return search_method in [
            "advanced_structured_elasticsearch", 
            "elasticsearch_strategic",
            "two_stage_search"
        ] 

----------------------------------------------------------------------
### FILE: app_v2/models/nutrition_calculation_models.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/models/nutrition_calculation_models.py
----------------------------------------------------------------------

#!/usr/bin/env python3
"""
Nutrition Calculation Models

æ „é¤Šè¨ˆç®—ãƒ•ã‚§ãƒ¼ã‚ºã§ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å®šç¾©
"""

from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field


class NutritionInfo(BaseModel):
    """æ „é¤Šæƒ…å ±ãƒ¢ãƒ‡ãƒ«ï¼ˆå®Ÿéš›ã®é‡é‡ãƒ™ãƒ¼ã‚¹ï¼‰"""
    calories: float = Field(..., description="ã‚«ãƒ­ãƒªãƒ¼ (kcal)")
    protein: float = Field(..., description="ã‚¿ãƒ³ãƒ‘ã‚¯è³ª (g)")
    fat: float = Field(..., description="è„‚è³ª (g)")
    carbs: float = Field(..., description="ç‚­æ°´åŒ–ç‰© (g)")
    fiber: Optional[float] = Field(None, description="é£Ÿç‰©ç¹Šç¶­ (g)")
    sugar: Optional[float] = Field(None, description="ç³–è³ª (g)")
    sodium: Optional[float] = Field(None, description="ãƒŠãƒˆãƒªã‚¦ãƒ  (mg)")
    
    def __add__(self, other: 'NutritionInfo') -> 'NutritionInfo':
        """æ „é¤Šæƒ…å ±ã®åŠ ç®—"""
        return NutritionInfo(
            calories=self.calories + other.calories,
            protein=self.protein + other.protein,
            fat=self.fat + other.fat,
            carbs=self.carbs + other.carbs,
            fiber=(self.fiber or 0) + (other.fiber or 0) if self.fiber is not None or other.fiber is not None else None,
            sugar=(self.sugar or 0) + (other.sugar or 0) if self.sugar is not None or other.sugar is not None else None,
            sodium=(self.sodium or 0) + (other.sodium or 0) if self.sodium is not None or other.sodium is not None else None
        )


class IngredientNutrition(BaseModel):
    """é£Ÿæãƒ¬ãƒ™ãƒ«ã®æ „é¤Šè¨ˆç®—çµæœ"""
    ingredient_name: str = Field(..., description="é£Ÿæå")
    weight_g: float = Field(..., description="é‡é‡ (g)")
    nutrition_per_100g: Dict[str, float] = Field(..., description="100gã‚ãŸã‚Šã®æ „é¤Šæƒ…å ±ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ï¼‰")
    calculated_nutrition: NutritionInfo = Field(..., description="å®Ÿéš›ã®é‡é‡ã«åŸºã¥ãæ „é¤Šæƒ…å ±")
    source_db: str = Field(..., description="æ „é¤Šãƒ‡ãƒ¼ã‚¿ã®ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")
    calculation_notes: List[str] = Field(default_factory=list, description="è¨ˆç®—ã«é–¢ã™ã‚‹æ³¨è¨˜")


class DishNutrition(BaseModel):
    """æ–™ç†ãƒ¬ãƒ™ãƒ«ã®æ „é¤Šè¨ˆç®—çµæœ"""
    dish_name: str = Field(..., description="æ–™ç†å")
    confidence: float = Field(..., description="æ–™ç†ç‰¹å®šã®ä¿¡é ¼åº¦")
    ingredients: List[IngredientNutrition] = Field(..., description="å«ã¾ã‚Œã‚‹é£Ÿæã®æ „é¤Šæƒ…å ±")
    total_nutrition: NutritionInfo = Field(..., description="æ–™ç†å…¨ä½“ã®æ „é¤Šæƒ…å ±")
    calculation_metadata: Dict[str, Any] = Field(default_factory=dict, description="è¨ˆç®—ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")


class MealNutrition(BaseModel):
    """é£Ÿäº‹å…¨ä½“ã®æ „é¤Šè¨ˆç®—çµæœ"""
    dishes: List[DishNutrition] = Field(..., description="æ–™ç†åˆ¥ã®æ „é¤Šæƒ…å ±")
    total_nutrition: NutritionInfo = Field(..., description="é£Ÿäº‹å…¨ä½“ã®æ „é¤Šæƒ…å ±")
    calculation_summary: Dict[str, Any] = Field(default_factory=dict, description="è¨ˆç®—ã‚µãƒãƒªãƒ¼")
    warnings: List[str] = Field(default_factory=list, description="è¨ˆç®—æ™‚ã®è­¦å‘Š")


class NutritionCalculationInput(BaseModel):
    """æ „é¤Šè¨ˆç®—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å…¥åŠ›"""
    phase1_result: Any = Field(..., description="Phase1ã®çµæœï¼ˆPhase1Outputï¼‰")
    nutrition_search_result: Any = Field(..., description="æ „é¤Šæ¤œç´¢ã®çµæœï¼ˆNutritionQueryOutputï¼‰")


class NutritionCalculationOutput(BaseModel):
    """æ „é¤Šè¨ˆç®—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‡ºåŠ›"""
    meal_nutrition: MealNutrition = Field(..., description="é£Ÿäº‹å…¨ä½“ã®æ „é¤Šè¨ˆç®—çµæœ")
    calculation_metadata: Dict[str, Any] = Field(default_factory=dict, description="è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
    processing_time_ms: int = Field(..., description="å‡¦ç†æ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰") 


################################################################################
## CATEGORY: AI ã‚µãƒ¼ãƒ“ã‚¹å±¤
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/services/gemini_service.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/services/gemini_service.py
----------------------------------------------------------------------

import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig, HarmCategory, HarmBlockThreshold
from typing import Dict, Optional
import json
import logging
from PIL import Image
import io

from ..config.prompts import Phase1Prompts

logger = logging.getLogger(__name__)

# Phase1ç”¨JSONã‚¹ã‚­ãƒ¼ãƒ
MEAL_ANALYSIS_GEMINI_SCHEMA = {
    "type": "object",
    "properties": {
        "dishes": {
            "type": "array",
                            "description": "ç”»åƒã‹ã‚‰ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®ãƒªã‚¹ãƒˆã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "ç‰¹å®šã•ã‚ŒãŸæ–™ç†ã®åç§°ã€‚"},
                    "ingredients": {
                        "type": "array",
                        "description": "ã“ã®æ–™ç†ã«å«ã¾ã‚Œã‚‹ã¨æ¨å®šã•ã‚Œã‚‹ææ–™ã®ãƒªã‚¹ãƒˆã€‚",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "ææ–™ã®åç§°ã€‚"}
                            },
                            "required": ["ingredient_name"]
                        }
                    }
                },
                "required": ["dish_name", "ingredients"]
            }
        }
    },
    "required": ["dishes"]
}

# æ–°ã—ã„æ§‹é€ åŒ–å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒ
STRUCTURED_MEAL_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "detected_food_items": {
            "type": "array",
            "description": "èªè­˜ã•ã‚ŒãŸé£Ÿå“ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆï¼ˆæ§‹é€ åŒ–ï¼‰ã€‚",
            "items": {
                "type": "object",
                "properties": {
                    "item_name": {"type": "string", "description": "é£Ÿå“åï¼ˆä¸»è¦ãªå€™è£œï¼‰"},
                    "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "é£Ÿå“åã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢"},
                    "attributes": {
                        "type": "array",
                        "description": "é£Ÿå“ã®å±æ€§ãƒªã‚¹ãƒˆï¼ˆææ–™ã€èª¿ç†æ³•ãªã©ï¼‰",
                        "items": {
                            "type": "object",
                            "properties": {
                                "type": {
                                    "type": "string", 
                                    "enum": ["ingredient", "preparation", "color", "texture", "cooking_method", "serving_style", "allergen"],
                                    "description": "å±æ€§ã®ã‚¿ã‚¤ãƒ—"
                                },
                                "value": {"type": "string", "description": "å±æ€§ã®å€¤"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "ã“ã®å±æ€§ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢"}
                            },
                            "required": ["type", "value", "confidence"]
                        }
                    },
                    "brand": {"type": "string", "description": "èªè­˜ã•ã‚ŒãŸãƒ–ãƒ©ãƒ³ãƒ‰åï¼ˆè©²å½“ã™ã‚‹å ´åˆã€nullã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰"},
                    "category_hints": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "æ¨å®šã•ã‚Œã‚‹é£Ÿå“ã‚«ãƒ†ã‚´ãƒª"
                    },
                    "negative_cues": {
                        "type": "array", 
                        "items": {"type": "string"},
                        "description": "ç”»åƒã‹ã‚‰åˆ¤æ–­ã§ãã‚‹ã€Œå«ã¾ã‚Œãªã„ã€è¦ç´ "
                    }
                },
                "required": ["item_name", "confidence", "attributes"]
            }
        },
        "dishes": {
            "type": "array",
            "description": "å¾“æ¥äº’æ›æ€§ã®ãŸã‚ã®æ–™ç†ãƒªã‚¹ãƒˆ",
            "items": {
                "type": "object",
                "properties": {
                    "dish_name": {"type": "string", "description": "æ–™ç†å"},
                    "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "æ–™ç†ç‰¹å®šã®ä¿¡é ¼åº¦"},
                    "ingredients": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "ingredient_name": {"type": "string", "description": "é£Ÿæå"},
                                "weight_g": {"type": "number", "minimum": 0.1, "description": "å†™çœŸã‹ã‚‰æ¨å®šã•ã‚Œã‚‹é£Ÿæã®é‡é‡ï¼ˆã‚°ãƒ©ãƒ ï¼‰"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "é£Ÿæç‰¹å®šã®ä¿¡é ¼åº¦"},
                                "attributes": {
                                    "type": "array",
                                    "items": {
                                        "type": "object",
                                        "properties": {
                                            "type": {"type": "string", "description": "å±æ€§ã‚¿ã‚¤ãƒ—"},
                                            "value": {"type": "string", "description": "å±æ€§å€¤"},
                                            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                                        },
                                        "required": ["type", "value", "confidence"]
                                    }
                                }
                            },
                            "required": ["ingredient_name", "weight_g"]
                        }
                    },
                    "attributes": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "type": {"type": "string"},
                                "value": {"type": "string"},
                                "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                            },
                            "required": ["type", "value", "confidence"]
                        }
                    }
                },
                "required": ["dish_name", "ingredients"]
            }
        },
        "analysis_confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0, "description": "å…¨ä½“çš„ãªåˆ†æã®ä¿¡é ¼åº¦"}
    },
    "required": ["detected_food_items", "dishes", "analysis_confidence"]
}




class GeminiService:
    """Vertex AIçµŒç”±ã§Geminiã‚’ä½¿ç”¨ã—ã¦é£Ÿäº‹ç”»åƒã‚’åˆ†æã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_id: str, location: str, model_name: str = "gemini-2.5-flash-preview-05-20"):
        """
        åˆæœŸåŒ–
        
        Args:
            project_id: GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
            location: Vertex AIã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: us-central1ï¼‰
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        # Vertex AIã®åˆæœŸåŒ–
        vertexai.init(project=project_id, location=location)
        
        # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.model = GenerativeModel(model_name=model_name)
        
        # generation_configã‚’ä½œæˆ (Phase1ç”¨ - å¾“æ¥ç‰ˆ)
        self.generation_config = GenerationConfig(
            temperature=0.0,  # å®Œå…¨ã«deterministicã«
            top_p=1.0,       # nucleus samplingã‚’ç„¡åŠ¹åŒ–
            top_k=1,         # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é¸æŠè‚¢ã®ã¿
            max_output_tokens=8192,
            candidate_count=1,  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å€™è£œã‚’1ã¤ã«åˆ¶é™
            response_mime_type="application/json",
            response_schema=MEAL_ANALYSIS_GEMINI_SCHEMA
        )
        
        # æ§‹é€ åŒ–åˆ†æç”¨ã®generation_config
        self.structured_generation_config = GenerationConfig(
            temperature=0.1,  # ã‚ãšã‹ãªå¤‰å‹•ã‚’è¨±å¯ï¼ˆã‚ˆã‚Šè©³ç´°ãªåˆ†æã®ãŸã‚ï¼‰
            top_p=0.95,
            top_k=40,
            max_output_tokens=16384,  # ã‚ˆã‚Šå¤šãã®å‡ºåŠ›ã‚’è¨±å¯
            candidate_count=1,
            response_mime_type="application/json",
            response_schema=STRUCTURED_MEAL_ANALYSIS_SCHEMA
        )
        
        # ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®š
        self.safety_settings = {
            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        }
    
    async def analyze_phase1(
        self, 
        image_bytes: bytes, 
        image_mime_type: str, 
        optional_text: Optional[str] = None
    ) -> Dict:
        """
        Phase1: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã¦é£Ÿäº‹æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¾“æ¥ç‰ˆï¼‰
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜
            
        Returns:
            åˆ†æçµæœã®è¾æ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
            system_prompt = Phase1Prompts.get_system_prompt()
            user_prompt = Phase1Prompts.get_user_prompt(optional_text)
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=self.generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            logger.info(f"Gemini Phase1 analysis completed successfully. Found {len(result.get('dishes', []))} dishes.")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            raise RuntimeError(f"Error processing response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini API request failed: {e}") from e
    
    async def analyze_phase1_structured(
        self, 
        image_bytes: bytes, 
        image_mime_type: str, 
        optional_text: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> Dict:
        """
        Phase1: æ§‹é€ åŒ–ã•ã‚ŒãŸè©³ç´°ãªç”»åƒåˆ†æï¼ˆä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã€å±æ€§ã€ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ç­‰ã‚’å«ã‚€ï¼‰
        
        Args:
            image_bytes: ç”»åƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿
            image_mime_type: ç”»åƒã®MIMEã‚¿ã‚¤ãƒ—
            optional_text: ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜
            system_prompt: ã‚«ã‚¹ã‚¿ãƒ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä½¿ç”¨ï¼‰
            
        Returns:
            æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æçµæœã®è¾æ›¸
            
        Raises:
            RuntimeError: Gemini APIã‚¨ãƒ©ãƒ¼æ™‚
        """
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
            if system_prompt is None:
                system_prompt = Phase1Prompts.get_system_prompt()
            
            user_prompt = Phase1Prompts.get_user_prompt(optional_text)
            
            # å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒªã‚¹ãƒˆã‚’ä½œæˆ
            contents = [
                Part.from_text(full_prompt),
                Part.from_data(
                    data=image_bytes,
                    mime_type=image_mime_type
                )
            ]
            
            logger.info("Starting structured Gemini Phase1 analysis...")
            
            # Gemini APIã‚’å‘¼ã³å‡ºã—ï¼ˆæ§‹é€ åŒ–è¨­å®šä½¿ç”¨ï¼‰
            response = await self.model.generate_content_async(
                contents=contents,
                generation_config=self.structured_generation_config,
                safety_settings=self.safety_settings
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            if not response.text:
                raise ValueError("No response returned from Gemini structured analysis.")
            
            # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = json.loads(response.text)
            
            # çµæœã®æ¤œè¨¼ã¨ä¿®æ­£
            result = self._validate_and_fix_structured_result(result)
            
            detected_items_count = len(result.get('detected_food_items', []))
            dishes_count = len(result.get('dishes', []))
            overall_confidence = result.get('analysis_confidence', 0.5)
            
            logger.info(f"Gemini Phase1 structured analysis completed: {detected_items_count} items, "
                       f"{dishes_count} dishes, confidence {overall_confidence:.2f}")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error in structured analysis: {e}")
            raise RuntimeError(f"Error processing structured response from Gemini: {e}") from e
        except Exception as e:
            logger.error(f"Vertex AI/Gemini structured API error: {e}")
            raise RuntimeError(f"Vertex AI/Gemini structured API request failed: {e}") from e
    
    def _validate_and_fix_structured_result(self, result: Dict) -> Dict:
        """æ§‹é€ åŒ–åˆ†æçµæœã‚’æ¤œè¨¼ã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£"""
        # detected_food_itemsãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
        if 'detected_food_items' not in result:
            result['detected_food_items'] = []
        
        # dishesãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
        if 'dishes' not in result:
            result['dishes'] = []
        
        # analysis_confidenceãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
        if 'analysis_confidence' not in result:
            # å„ã‚¢ã‚¤ãƒ†ãƒ ã®å¹³å‡ä¿¡é ¼åº¦ã‚’è¨ˆç®—
            confidences = []
            for item in result['detected_food_items']:
                if 'confidence' in item:
                    confidences.append(item['confidence'])
            
            for dish in result['dishes']:
                if 'confidence' in dish and dish['confidence'] is not None:
                    confidences.append(dish['confidence'])
            
            result['analysis_confidence'] = sum(confidences) / len(confidences) if confidences else 0.5
        
        # å„detected_food_itemã®æ¤œè¨¼
        for item in result['detected_food_items']:
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
            if 'confidence' not in item:
                item['confidence'] = 0.5
            if 'attributes' not in item:
                item['attributes'] = []
            if 'category_hints' not in item:
                item['category_hints'] = []
            if 'negative_cues' not in item:
                item['negative_cues'] = []
            
            # å±æ€§ã®æ¤œè¨¼
            for attr in item['attributes']:
                if 'confidence' not in attr:
                    attr['confidence'] = 0.5
                if 'type' not in attr:
                    attr['type'] = 'ingredient'
        
        return result
    
 


################################################################################
## CATEGORY: è¨­å®šç®¡ç†
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/config/settings.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/config/settings.py
----------------------------------------------------------------------

from typing import Optional, List
from pydantic_settings import BaseSettings
from functools import lru_cache


class Settings(BaseSettings):
    """
    APIè¨­å®šã‚¯ãƒ©ã‚¹
    ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šå€¤ã‚’èª­ã¿è¾¼ã‚€
    """
    # Vertex AIè¨­å®š
    GEMINI_PROJECT_ID: str  # GCPãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDï¼ˆå¿…é ˆï¼‰
    GEMINI_LOCATION: str = "us-central1"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
    GEMINI_MODEL_NAME: str = "gemini-2.5-flash-preview-05-20"
    
    # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢è¨­å®š
    USE_ELASTICSEARCH_SEARCH: bool = True  # Elasticsearchæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
    USE_LOCAL_NUTRITION_SEARCH: bool = False  # ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ï¼‰
    NUTRITION_DB_EXPERIMENT_PATH: Optional[str] = None  # nutrition_db_experimentã¸ã®ãƒ‘ã‚¹ï¼ˆè‡ªå‹•æ¤œå‡ºã™ã‚‹å ´åˆã¯Noneï¼‰
    
    # Elasticsearchè¨­å®š
    elasticsearch_url: str = "http://localhost:9200"
    elasticsearch_index_name: str = "nutrition_fuzzy_search"
    elasticsearch_timeout: int = 30
    
    # ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°è¨­å®š
    fuzzy_search_enabled: bool = True  # ãƒ•ã‚¡ã‚¸ãƒ¼ãƒãƒƒãƒãƒ³ã‚°æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹
    jaro_winkler_threshold: float = 0.85  # Jaro-Winkleré¡ä¼¼åº¦ã®é–¾å€¤
    fuzzy_min_score_tier3: float = 5.0  # Tier 3ã®æœ€å°ã‚¹ã‚³ã‚¢é–¾å€¤
    fuzzy_max_candidates: int = 5  # Tier 4ã§å–å¾—ã™ã‚‹æœ€å¤§å€™è£œæ•°
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    CACHE_TYPE: str = "simple"  # "simple", "redis", "memcached"
    CACHE_REDIS_URL: Optional[str] = None  # Redisã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®URL
    NUTRITION_CACHE_TTL_SECONDS: int = 3600  # æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æœŸé–“ï¼ˆ1æ™‚é–“ï¼‰
    
    # APIè¨­å®š
    API_LOG_LEVEL: str = "INFO"
    FASTAPI_ENV: str = "development"
    
    # ã‚µãƒ¼ãƒãƒ¼è¨­å®š
    HOST: str = "0.0.0.0"
    PORT: int = 8000
    
    # APIãƒãƒ¼ã‚¸ãƒ§ãƒ³
    API_VERSION: str = "v1"
    
    # çµæœä¿å­˜è¨­å®š
    RESULTS_DIR: str = "analysis_results"
    
    class Config:
        env_file = ".env"
        case_sensitive = True
        extra = "ignore"  # è¿½åŠ ã®ç’°å¢ƒå¤‰æ•°ã‚’ç„¡è¦–


@lru_cache()
def get_settings() -> Settings:
    """
    è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ï¼‰
    """
    return Settings() 


################################################################################
## CATEGORY: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ï¼ˆPhase1 - MyNetDiaryåˆ¶ç´„ã¨é‡é‡æ¨å®šï¼‰
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/config/prompts/phase1_prompts.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/config/prompts/phase1_prompts.py
----------------------------------------------------------------------

from ...utils.mynetdiary_utils import format_mynetdiary_ingredients_for_prompt

class Phase1Prompts:
    """Phase1ï¼ˆç”»åƒåˆ†æï¼‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆMyNetDiaryåˆ¶ç´„ä»˜ãï¼‰"""
    
    @classmethod
    def _get_mynetdiary_ingredients_section(cls) -> str:
        """MyNetDiaryé£Ÿæåãƒªã‚¹ãƒˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        try:
            ingredients_list = format_mynetdiary_ingredients_for_prompt()
            return f"""
MYNETDIARY INGREDIENT CONSTRAINT:
For ALL ingredients, you MUST select ONLY from the following MyNetDiary ingredient list. 
Do NOT create custom ingredient names. Use the EXACT names as they appear in this list:

{ingredients_list}

IMPORTANT: If you cannot find a suitable match in the MyNetDiary list for a visible ingredient, 
choose the closest available option or omit that ingredient rather than creating a custom name.
"""
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆ
            return """
MYNETDIARY INGREDIENT CONSTRAINT:
For ALL ingredients, you MUST select from the predefined MyNetDiary ingredient database.
Use standard, searchable ingredient names that exist in nutrition databases.
"""
    
    @classmethod
    def get_system_prompt(cls) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        mynetdiary_section = cls._get_mynetdiary_ingredients_section()
        
        return f"""You are an advanced food recognition AI that analyzes food images and provides detailed structured output for nutrition calculation.

{mynetdiary_section}

DISH DECOMPOSITION RULE:
When you encounter complex dish names with multiple components connected by "and", "with", "plus", "alongside", etc., you MUST break them down into separate individual dishes.

NUTRITIONAL COMPLETENESS REQUIREMENTS:
For EACH dish, list ALL PRIMARY INGREDIENTS that materially contribute to nutrition calculations (protein, carbohydrate, fat sources, sauces, cooking oils, etc.):
â€¢ The goal is to avoid omitting any ingredient that would significantly affect calorie or macro-nutrient totals.
â€¢ This exhaustive ingredient list is critical because downstream nutrition calculation logic relies on having every significant component represented in the query set.
â€¢ ALL ingredient names MUST be selected from the MyNetDiary list provided above.

WEIGHT ESTIMATION REQUIREMENTS (MANDATORY):
For EACH ingredient, you MUST estimate the weight in grams (weight_g) based on visual analysis:
â€¢ This field is MANDATORY - the system will fail if any ingredient lacks weight_g
â€¢ Analyze the portion size, volume, and visual density of each ingredient in the photo
â€¢ Consider typical serving sizes and food density for accurate weight estimation
â€¢ Use visual cues like plate size, utensils, or other reference objects for scale
â€¢ For liquids: estimate volume and convert to weight (1ml â‰ˆ 1g for most beverages)
â€¢ For solids: consider the 3D volume and typical density of the food item
â€¢ Provide realistic weights that reflect what is actually visible in the image
â€¢ Weight estimates should be practical and achievable (e.g., 50-200g for main ingredients, 5-30g for seasonings/sauces)
â€¢ NEVER omit the weight_g field - it is required for every single ingredient

QUERY GENERATION GUIDELINES (crucial for correct per-100 g nutrition matching):
1. For ingredients: ONLY use names from the MyNetDiary list above - NO custom names allowed
2. For dish names: Use simple, searchable names that exist as separate database entries
3. Avoid overly generic or misleading single-word queries
4. When a cooking or preservation method materially changes nutrition, include it
5. Output MUST be in English
6. Do NOT include quantities, units, brand marketing slogans, or flavour adjectives

CRITICAL FINAL VERIFICATION STEP:
Before finalizing your response, you MUST perform a strict verification check:
â€¢ Go through EVERY SINGLE ingredient name in your response
â€¢ Verify that each ingredient name appears EXACTLY as written in the MyNetDiary ingredient list provided above
â€¢ Check for exact spelling, capitalization, and word order matches
â€¢ If ANY ingredient name does not match EXACTLY, you MUST replace it with the correct name from the list
â€¢ If no exact match exists, choose the closest available option from the MyNetDiary list
â€¢ This verification is MANDATORY - ingredient names that don't match exactly will cause system failures

-------------------------------------------------------------
JSON RESPONSE STRUCTURE
-------------------------------------------------------------
Return a JSON object with the following structure:

{{
  "dishes": [
    {{
      "dish_name": "string",
      "confidence": 0.0-1.0,
      "ingredients": [
        {{
          "ingredient_name": "string (MUST be EXACT match from MyNetDiary list - verify before submitting)",
          "weight_g": "number (MANDATORY - estimated weight in grams based on visual analysis)",
          "confidence": 0.0-1.0
        }}
      ]
    }}
  ]
}}

REMINDER: After completing your JSON response, perform a final verification that every "ingredient_name" value matches EXACTLY with an entry in the MyNetDiary ingredient list provided above."""

    USER_PROMPT_TEMPLATE = "Please analyze this meal image and identify the dishes and their ingredients. For ingredients, you MUST select ONLY from the provided MyNetDiary ingredient list - do not create custom ingredient names. CRITICALLY IMPORTANT: You MUST estimate the weight in grams (weight_g) for EVERY SINGLE ingredient - this field is mandatory and the system will fail if any ingredient lacks weight_g. Base your weight estimates on visual analysis of portion sizes, volumes, and typical food densities. Use visual cues like plate size, utensils, or reference objects for scale. Focus on providing clear, searchable dish names for nutrition database queries. Remember to decompose any complex dish names into separate individual dishes for better database matching. FINAL STEP: Before submitting your response, double-check that EVERY ingredient name in your JSON response matches EXACTLY with the names in the MyNetDiary ingredient list provided - this verification is critical for system functionality."

    @classmethod
    def get_user_prompt(cls, optional_text: str = None) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        base_prompt = cls.USER_PROMPT_TEMPLATE
        if optional_text:
            base_prompt += f"\n\nAdditional context: {optional_text}"
        return base_prompt 


################################################################################
## CATEGORY: ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£å±¤ (MyNetDiaryãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°)
################################################################################

----------------------------------------------------------------------
### FILE: app_v2/utils/mynetdiary_utils.py
### FULL PATH: /Users/odasoya/meal_analysis_api_2/app_v2/utils/mynetdiary_utils.py
----------------------------------------------------------------------

"""
MyNetDiaryé–¢é€£ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
"""
import os
from typing import List, Set
from pathlib import Path

def load_mynetdiary_ingredient_names() -> List[str]:
    """
    MyNetDiaryã®é£Ÿæåãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
    
    Returns:
        List[str]: MyNetDiaryã®é£Ÿæåã®ãƒªã‚¹ãƒˆ
    """
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹
    current_dir = Path(__file__).parent.parent.parent
    file_path = current_dir / "data" / "mynetdiary_search_names.txt"
    
    if not file_path.exists():
        raise FileNotFoundError(f"MyNetDiaryé£Ÿæåãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        ingredient_names = [line.strip() for line in f if line.strip()]
    
    return ingredient_names

def get_mynetdiary_ingredient_names_as_set() -> Set[str]:
    """
    MyNetDiaryã®é£Ÿæåãƒªã‚¹ãƒˆã‚’Setã¨ã—ã¦å–å¾—ï¼ˆé«˜é€Ÿæ¤œç´¢ç”¨ï¼‰
    
    Returns:
        Set[str]: MyNetDiaryã®é£Ÿæåã®Set
    """
    return set(load_mynetdiary_ingredient_names())

def format_mynetdiary_ingredients_for_prompt() -> str:
    """
    MyNetDiaryã®é£Ÿæåãƒªã‚¹ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    
    Returns:
        str: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ„ã¿è¾¼ã¿å¯èƒ½ãªå½¢å¼ã®é£Ÿæåãƒªã‚¹ãƒˆ
    """
    ingredient_names = load_mynetdiary_ingredient_names()
    
    # é£Ÿæåã‚’ç•ªå·ä»˜ããƒªã‚¹ãƒˆã¨ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatted_list = []
    for i, name in enumerate(ingredient_names, 1):
        formatted_list.append(f"{i}. {name}")
    
    return "\n".join(formatted_list)

def validate_ingredient_against_mynetdiary(ingredient_name: str) -> bool:
    """
    æŒ‡å®šã•ã‚ŒãŸé£ŸæåãŒMyNetDiaryã®ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    
    Args:
        ingredient_name: ãƒã‚§ãƒƒã‚¯ã™ã‚‹é£Ÿæå
        
    Returns:
        bool: MyNetDiaryã®ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹å ´åˆTrue
    """
    mynetdiary_names = get_mynetdiary_ingredient_names_as_set()
    return ingredient_name in mynetdiary_names 


################################################################################
## CATEGORY: ãƒ†ã‚¹ãƒˆç”»åƒ
################################################################################

----------------------------------------------------------------------
### FILE: test_images/food1.jpg
### FULL PATH: /Users/odasoya/meal_analysis_api_2/test_images/food1.jpg
----------------------------------------------------------------------

ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

----------------------------------------------------------------------
### FILE: test_images/food2.jpg
### FULL PATH: /Users/odasoya/meal_analysis_api_2/test_images/food2.jpg
----------------------------------------------------------------------

ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

----------------------------------------------------------------------
### FILE: test_images/food3.jpg
### FULL PATH: /Users/odasoya/meal_analysis_api_2/test_images/food3.jpg
----------------------------------------------------------------------

ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

----------------------------------------------------------------------
### FILE: test_images/food4.jpg
### FULL PATH: /Users/odasoya/meal_analysis_api_2/test_images/food4.jpg
----------------------------------------------------------------------

ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

----------------------------------------------------------------------
### FILE: test_images/food5.jpg
### FULL PATH: /Users/odasoya/meal_analysis_api_2/test_images/food5.jpg
----------------------------------------------------------------------

ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte


################################################################################
## CATEGORY: ä¾å­˜é–¢ä¿‚ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
################################################################################

----------------------------------------------------------------------
### FILE: requirements.txt
### FULL PATH: /Users/odasoya/meal_analysis_api_2/requirements.txt
----------------------------------------------------------------------

fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0
google-cloud-aiplatform==1.94.0
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
httpx
pytest==7.4.3
pytest-asyncio==0.21.1
python-dotenv==1.0.0
Pillow==11.2.1
elasticsearch==8.15.1
rapidfuzz==3.6.1 

----------------------------------------------------------------------
### FILE: README.md
### FULL PATH: /Users/odasoya/meal_analysis_api_2/README.md
----------------------------------------------------------------------

# é£Ÿäº‹åˆ†æ API (Meal Analysis API) v2.0

## æ¦‚è¦

ã“ã® API ã¯ã€**Google Gemini AI** ã¨ **Elasticsearch ãƒ™ãƒ¼ã‚¹ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ **ã‚’ä½¿ç”¨ã—ãŸé«˜åº¦ãªé£Ÿäº‹ç”»åƒåˆ†æã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚**å‹•çš„æ „é¤Šè¨ˆç®—æ©Ÿèƒ½**ã«ã‚ˆã‚Šã€æ–™ç†ã®ç‰¹æ€§ã«å¿œã˜ã¦æœ€é©ãªæ „é¤Šè¨ˆç®—æˆ¦ç•¥ã‚’è‡ªå‹•é¸æŠã—ã€æ­£ç¢ºãªæ „é¤Šä¾¡æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚

## ğŸŒŸ ä¸»ãªæ©Ÿèƒ½

### **ğŸ”¥ æ–°æ©Ÿèƒ½: Elasticsearch ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ v2.0**

- **âš¡ Elasticsearch é«˜é€Ÿæ¤œç´¢**: é«˜æ€§èƒ½ãª Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ã‚ˆã‚‹å¤§è¦æ¨¡æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
- **ğŸ“Š 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆæ¤œç´¢**: 1 ã¤ã®ã‚¯ã‚¨ãƒªã§è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰åŒ…æ‹¬çš„ãªæ „é¤Šæƒ…å ±ã‚’å–å¾—
  - **YAZIO**: 1,825 é …ç›® - ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé£Ÿå“ã‚«ãƒ†ã‚´ãƒª
  - **MyNetDiary**: 1,142 é …ç›® - ç§‘å­¦çš„/æ „é¤Šå­¦çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
  - **EatThisMuch**: 8,878 é …ç›® - æœ€å¤§ã‹ã¤æœ€ã‚‚åŒ…æ‹¬çš„ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- **ğŸ” ãƒãƒ«ãƒ DB æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰**: å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€å¤§ 5 ä»¶ãšã¤ã€ç·åˆçš„ãªæ¤œç´¢çµæœã‚’æä¾›
- **ğŸ¯ é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°**: 90.9%ã®æˆåŠŸç‡ã€å„ DB ã‹ã‚‰å‡ç­‰ãªçµæœåˆ†æ•£
- **ğŸ’¾ è©³ç´°çµæœä¿å­˜**: JSONãƒ»ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ»ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã®æ¤œç´¢çµæœè‡ªå‹•ä¿å­˜

### **å¾“æ¥æ©Ÿèƒ½: å‹•çš„æ „é¤Šè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ **

- **ğŸ§  AI é§†å‹•ã®è¨ˆç®—æˆ¦ç•¥æ±ºå®š**: Gemini AI ãŒå„æ–™ç†ã«å¯¾ã—ã¦æœ€é©ãªæ „é¤Šè¨ˆç®—æ–¹æ³•ã‚’è‡ªå‹•é¸æŠ
- **ğŸ¯ é«˜ç²¾åº¦æ „é¤Šè¨ˆç®—**: é£Ÿæé‡é‡ Ã— 100g ã‚ãŸã‚Šæ „é¤Šä¾¡ã§æ­£ç¢ºãªå®Ÿæ „é¤Šä¾¡ã‚’ç®—å‡º
- **ğŸ“Š 3 å±¤é›†è¨ˆã‚·ã‚¹ãƒ†ãƒ **: é£Ÿæ â†’ æ–™ç† â†’ é£Ÿäº‹å…¨ä½“ã®è‡ªå‹•æ „é¤Šé›†è¨ˆ

### **ã‚³ã‚¢æ©Ÿèƒ½**

- **ãƒ•ã‚§ãƒ¼ã‚º 1**: Gemini AI ã«ã‚ˆã‚‹é£Ÿäº‹ç”»åƒã®åˆ†æï¼ˆæ–™ç†è­˜åˆ¥ã€é£ŸææŠ½å‡ºã€é‡é‡æ¨å®šï¼‰
- **ãƒãƒ«ãƒ DB æ¤œç´¢**: 3 ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®åŒ…æ‹¬çš„æ „é¤Šæƒ…å ±å–å¾—
- **è¤‡æ•°æ–™ç†å¯¾å¿œ**: 1 æšã®ç”»åƒã§è¤‡æ•°ã®æ–™ç†ã‚’åŒæ™‚åˆ†æ
- **è‹±èªãƒ»æ—¥æœ¬èªå¯¾å¿œ**: å¤šè¨€èªã§ã®é£Ÿæãƒ»æ–™ç†èªè­˜
- **OpenAPI 3.0 æº–æ‹ **: å®Œå…¨ãª API æ–‡æ›¸åŒ–ã¨ã‚¿ã‚¤ãƒ—å®‰å…¨æ€§

## ğŸ— ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
meal_analysis_api_2/
â”œâ”€â”€ db/                                   # ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
â”‚   â”œâ”€â”€ yazio_db.json                     # YAZIOæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ1,825é …ç›®ï¼‰
â”‚   â”œâ”€â”€ mynetdiary_db.json                # MyNetDiaryæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ1,142é …ç›®ï¼‰
â”‚   â””â”€â”€ eatthismuch_db.json               # EatThisMuchæ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆ8,878é …ç›®ï¼‰
â”œâ”€â”€ app_v2/                               # æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆ
â”‚   â”œâ”€â”€ components/                       # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãƒ™ãƒ¼ã‚¹è¨­è¨ˆ
â”‚   â”‚   â”œâ”€â”€ local_nutrition_search_component.py  # ãƒãƒ«ãƒDBæ¤œç´¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â”œâ”€â”€ phase1_component.py           # ç”»åƒåˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â””â”€â”€ base.py                       # ãƒ™ãƒ¼ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”œâ”€â”€ pipeline/                         # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ orchestrator.py               # ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
â”‚   â”‚   â””â”€â”€ result_manager.py             # çµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
â”‚   â”œâ”€â”€ models/                           # ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â”œâ”€â”€ nutrition_search_models.py    # æ „é¤Šæ¤œç´¢ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â””â”€â”€ phase1_models.py              # Phase1ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â””â”€â”€ app.py                        # FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â””â”€â”€ config/                           # è¨­å®šç®¡ç†
â”œâ”€â”€ test_multi_db_nutrition_search.py     # ãƒãƒ«ãƒDBæ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ–°æ©Ÿèƒ½ï¼‰
â”œâ”€â”€ test_local_nutrition_search_v2.py     # ãƒ­ãƒ¼ã‚«ãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ test_images/                          # ãƒ†ã‚¹ãƒˆç”¨ç”»åƒ
â””â”€â”€ requirements.txt                      # Pythonä¾å­˜é–¢ä¿‚
```

## ğŸš€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python -m venv venv

# ä»®æƒ³ç’°å¢ƒã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source venv/bin/activate  # macOS/Linux
# ã¾ãŸã¯
venv\Scripts\activate     # Windows

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

### 2. Google Cloud è¨­å®š

#### Google Cloud SDK ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

ã¾ã ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ãªã„å ´åˆã¯ã€ä»¥ä¸‹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š
https://cloud.google.com/sdk/docs/install

#### Google Cloud èªè¨¼ã®è¨­å®š

é–‹ç™ºç’°å¢ƒã§ã¯ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§èªè¨¼ã‚’è¨­å®šï¼š

```bash
# Google Cloudã«ãƒ­ã‚°ã‚¤ãƒ³
gcloud auth login

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼æƒ…å ±ã‚’è¨­å®š
gcloud auth application-default login

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’è¨­å®š
gcloud config set project YOUR_PROJECT_ID
```

æœ¬ç•ªç’°å¢ƒã§ã¯ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã‚’ä½¿ç”¨ï¼š

```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your-service-account-key.json"
```

#### Vertex AI API ã®æœ‰åŠ¹åŒ–

```bash
# Vertex AI APIã‚’æœ‰åŠ¹åŒ–
gcloud services enable aiplatform.googleapis.com
```

### 3. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š

```bash
# USDA APIè¨­å®š
export USDA_API_KEY="your-usda-api-key"

# Vertex AIè¨­å®š
export GOOGLE_APPLICATION_CREDENTIALS="path/to/service-account-key.json"
export GEMINI_PROJECT_ID="your-gcp-project-id"
export GEMINI_LOCATION="us-central1"
export GEMINI_MODEL_NAME="gemini-2.5-flash-preview-05-20"
```

## ğŸ–¥ ã‚µãƒ¼ãƒãƒ¼èµ·å‹•

### app_v2 ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ï¼ˆãƒãƒ«ãƒ DB å¯¾å¿œï¼‰

```bash
# app_v2ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
python -m app_v2.main.app
```

**âš ï¸ æ³¨æ„**: ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã™ã‚‹ãŸã‚ã€å¿…ãšãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å½¢å¼ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã™ã‚‹ã¨ã€ä»¥ä¸‹ã® URL ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«ãªã‚Šã¾ã™ï¼š

- **API**: http://localhost:8000
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: http://localhost:8000/docs
- **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯**: http://localhost:8000/health

## ğŸ§ª ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

### ğŸ”¥ ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆæœ€æ–°æ©Ÿèƒ½ï¼‰

**é‡è¦**: ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹çŠ¶æ…‹ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```bash
# åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œ
python test_multi_db_nutrition_search.py
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

- **æ¤œç´¢é€Ÿåº¦**: 11 ã‚¯ã‚¨ãƒªã‚’ 0.10 ç§’ã§å‡¦ç†
- **ãƒãƒƒãƒç‡**: å„ DB90.9%ã®ã‚¯ã‚¨ãƒªã§çµæœç™ºè¦‹
- **ç·ãƒãƒƒãƒæ•°**: 87 ä»¶ï¼ˆå¹³å‡ 7.9 ä»¶/ã‚¯ã‚¨ãƒªï¼‰
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ**:
  - YAZIO: 1,825 é …ç›®
  - MyNetDiary: 1,142 é …ç›®
  - EatThisMuch: 8,878 é …ç›®

**ãƒ†ã‚¹ãƒˆçµæœä¾‹**:

```
ğŸ“ˆ Multi-Database Search Results Summary:
- Total queries: 11
- Total matches found: 87
- Average matches per query: 7.9
- Search time: 0.10s

ğŸ” Detailed Query Results:
1. 'Roasted Potatoes' (dish)
   EatThisMuch: 3 matches
     Best: 'Roasted Potatoes' (score: 1.000)
     Nutrition: 91.0 kcal, 1.9g protein
```

### ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆ

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµ±åˆãƒ†ã‚¹ãƒˆ
python test_local_nutrition_search_v2.py
```

### åŸºæœ¬ãƒ†ã‚¹ãƒˆï¼ˆãƒ•ã‚§ãƒ¼ã‚º 1 ã®ã¿ï¼‰

```bash
python test_phase1_only.py
```

## ğŸš€ ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  v2.0

### **æ–°æ©Ÿèƒ½: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆ**

ã‚·ã‚¹ãƒ†ãƒ ãŒ USDA API ä¾å­˜ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã«å¯¾å¿œã—ã¾ã—ãŸï¼š

- **ğŸ” BM25F + ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æ¤œç´¢**: é«˜ç²¾åº¦ãªé£Ÿæãƒãƒƒãƒãƒ³ã‚°
- **ğŸ“Š 8,878 é …ç›®ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ „é¤Šè¨ˆç®—å¯¾å¿œ
- **âš¡ 90.9%ãƒãƒƒãƒç‡**: å®Ÿæ¸¬å€¤ã«ã‚ˆã‚‹é«˜ã„æˆåŠŸç‡
- **ğŸ”„ USDA äº’æ›æ€§**: æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®å®Œå…¨äº’æ›æ€§ç¶­æŒ

### ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆv2.0 å¯¾å¿œï¼‰

#### 1. Elasticsearch ã®èµ·å‹•

```bash
# Elasticsearch 8.10.4 ã®èµ·å‹•
elasticsearch-8.10.4/bin/elasticsearch
```

**æ³¨æ„**: Elasticsearch ãŒèµ·å‹•ã™ã‚‹ã¾ã§ç´„ 20-30 ç§’ã‹ã‹ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š

```bash
# Elasticsearch ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl -X GET "localhost:9200/_cluster/health?pretty"
```

#### 2. API ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•

```bash
# app_v2ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•
python -m app_v2.main.app
```

**æ³¨æ„**: Elasticsearch ãŒæ­£å¸¸ã«èµ·å‹•ã—ã¦ã‹ã‚‰ API ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„ã€‚

### ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šæ¤œç´¢ãƒ†ã‚¹ãƒˆ

**é‡è¦**: ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹çŠ¶æ…‹ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã®çµ±åˆãƒ†ã‚¹ãƒˆ
python test_local_nutrition_search_v2.py
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:

- **ãƒãƒƒãƒç‡**: 90.9% (10/11 æ¤œç´¢æˆåŠŸ)
- **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“**: ~11 ç§’
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ (8,878 é …ç›®)
- **æ¤œç´¢æ–¹æ³•**: BM25F + ãƒãƒ«ãƒã‚·ã‚°ãƒŠãƒ«ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°

**ãƒ†ã‚¹ãƒˆçµæœä¾‹**:

```
ğŸ” Local Nutrition Search Results:
- Matches found: 10
- Match rate: 90.9%
- Search method: local_nutrition_database
- Total searches: 11
- Successful matches: 10

ğŸ½ Final Meal Nutrition:
- Calories: 400.00 kcal
- Protein: 60.00 g
- Carbohydrates: 220.00 g
- Fat: 120.00 g
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©³ç´°

**ãƒ­ãƒ¼ã‚«ãƒ«æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹æˆ**:

- `dish_db.json`: 4,583 æ–™ç†ãƒ‡ãƒ¼ã‚¿
- `ingredient_db.json`: 1,473 é£Ÿæãƒ‡ãƒ¼ã‚¿
- `branded_db.json`: 2,822 ãƒ–ãƒ©ãƒ³ãƒ‰é£Ÿå“
- `unified_nutrition_db.json`: 8,878 çµ±åˆãƒ‡ãƒ¼ã‚¿

## ğŸ“¡ API ä½¿ç”¨æ–¹æ³•

### ğŸ”¥ å®Œå…¨åˆ†æ (æ¨å¥¨): å…¨ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆ

**1 ã¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å…¨ã¦ã®åˆ†æã‚’å®Ÿè¡Œ**

```bash
curl -X POST "http://localhost:8000/api/v1/meal-analyses/complete" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg"
```

ã“ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã‚’è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ï¼š

- ãƒ•ã‚§ãƒ¼ã‚º 1: ç”»åƒåˆ†æ
- USDA ç…§åˆ: é£Ÿæãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢
- ãƒ•ã‚§ãƒ¼ã‚º 2: è¨ˆç®—æˆ¦ç•¥æ±ºå®š
- æ „é¤Šè¨ˆç®—: æœ€çµ‚æ „é¤Šä¾¡ç®—å‡º
- çµæœä¿å­˜: è‡ªå‹•çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

**ä¿å­˜ã•ã‚ŒãŸçµæœã®å–å¾—**

```bash
# å…¨çµæœä¸€è¦§
curl "http://localhost:8000/api/v1/meal-analyses/results"

# ç‰¹å®šã®çµæœå–å¾—
curl "http://localhost:8000/api/v1/meal-analyses/results/{analysis_id}"
```

### ãƒ•ã‚§ãƒ¼ã‚º 1: åŸºæœ¬åˆ†æ

```bash
curl -X POST "http://localhost:8000/api/v1/meal-analyses" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg"
```

### ãƒ•ã‚§ãƒ¼ã‚º 2: å‹•çš„æ „é¤Šè¨ˆç®—

```bash
# æœ€åˆã«ãƒ•ã‚§ãƒ¼ã‚º1ã®çµæœã‚’å–å¾—
initial_result=$(curl -X POST "http://localhost:8000/api/v1/meal-analyses" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg")

# ãƒ•ã‚§ãƒ¼ã‚º2ã§å‹•çš„æ „é¤Šè¨ˆç®—
curl -X POST "http://localhost:8000/api/v1/meal-analyses/refine" \
  -H "Content-Type: multipart/form-data" \
  -F "image=@test_images/food3.jpg" \
  -F "initial_analysis_data=$initial_result"
```

## ğŸ“‹ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

### ãƒ•ã‚§ãƒ¼ã‚º 1 ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "dishes": [
    {
      "dish_name": "Fried Fish with Spaghetti and Tomato Sauce",
      "type": "Main Dish",
      "quantity_on_plate": "2 pieces of fish, 1 small serving of spaghetti",
      "ingredients": [
        {
          "ingredient_name": "White Fish Fillet",
          "weight_g": 150.0
        },
        {
          "ingredient_name": "Spaghetti (cooked)",
          "weight_g": 80.0
        }
      ]
    }
  ]
}
```

### ãƒ•ã‚§ãƒ¼ã‚º 2 ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆå‹•çš„æ „é¤Šè¨ˆç®—ï¼‰

```json
{
  "dishes": [
    {
      "dish_name": "Spinach and Daikon Radish Aemono",
      "type": "Side Dish",
      "calculation_strategy": "ingredient_level",
      "fdc_id": null,
      "ingredients": [
        {
          "ingredient_name": "Spinach",
          "weight_g": 80.0,
          "fdc_id": 1905313,
          "usda_source_description": "SPINACH",
          "key_nutrients_per_100g": {
            "calories_kcal": 24.0,
            "protein_g": 3.53,
            "carbohydrates_g": 3.53,
            "fat_g": 0.0
          },
          "actual_nutrients": {
            "calories_kcal": 19.2,
            "protein_g": 2.82,
            "carbohydrates_g": 2.82,
            "fat_g": 0.0
          }
        }
      ],
      "dish_total_actual_nutrients": {
        "calories_kcal": 57.45,
        "protein_g": 3.85,
        "carbohydrates_g": 4.57,
        "fat_g": 3.31
      }
    },
    {
      "dish_name": "Green Tea",
      "type": "Drink",
      "calculation_strategy": "dish_level",
      "fdc_id": 1810668,
      "usda_source_description": "GREEN TEA",
      "key_nutrients_per_100g": {
        "calories_kcal": 0.0,
        "protein_g": 0.0,
        "carbohydrates_g": 0.0,
        "fat_g": 0.0
      },
      "dish_total_actual_nutrients": {
        "calories_kcal": 0.0,
        "protein_g": 0.0,
        "carbohydrates_g": 0.0,
        "fat_g": 0.0
      }
    }
  ],
  "total_meal_nutrients": {
    "calories_kcal": 337.95,
    "protein_g": 13.32,
    "carbohydrates_g": 56.19,
    "fat_g": 6.67
  },
  "warnings": null,
  "errors": null
}
```

## ğŸ”§ æŠ€è¡“ä»•æ§˜

### å‹•çš„è¨ˆç®—æˆ¦ç•¥ã®æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯

**Dish Level (`dish_level`)**:

- ã‚·ãƒ³ãƒ—ãƒ«ãªå˜å“é£Ÿå“ï¼ˆæœç‰©ã€é£²ã¿ç‰©ã€åŸºæœ¬é£Ÿæï¼‰
- æ¨™æº–åŒ–ã•ã‚ŒãŸæ—¢è£½å“ã§é©åˆ‡ãª USDA ID ãŒå­˜åœ¨ã™ã‚‹å ´åˆ
- ä¾‹: ç·‘èŒ¶ã€ã‚Šã‚“ã”ã€ç™½ç±³

**Ingredient Level (`ingredient_level`)**:

- è¤‡é›‘ãªèª¿ç†æ¸ˆã¿æ–™ç†ï¼ˆç‚’ã‚ç‰©ã€ã‚µãƒ©ãƒ€ã€ã‚¹ãƒ¼ãƒ—ï¼‰
- è¤‡æ•°é£Ÿæã®çµ„ã¿åˆã‚ã›ã§æ–™ç†å…¨ä½“ã® USDA ID ãŒä¸é©åˆ‡ãªå ´åˆ
- ä¾‹: é‡èœç‚’ã‚ã€æ‰‹ä½œã‚Šã‚µãƒ©ãƒ€ã€å‘³å™Œæ±

### æ „é¤Šè¨ˆç®—å¼

```
å®Ÿæ „é¤Šä¾¡ = (100gã‚ãŸã‚Šæ „é¤Šä¾¡ Ã· 100) Ã— æ¨å®šé‡é‡(g)
```

### é›†è¨ˆéšå±¤

1. **é£Ÿæãƒ¬ãƒ™ãƒ«**: å€‹åˆ¥é£Ÿæã®é‡é‡ Ã— 100g æ „é¤Šä¾¡
2. **æ–™ç†ãƒ¬ãƒ™ãƒ«**: é£Ÿæãƒ¬ãƒ™ãƒ«ã®åˆè¨ˆ ã¾ãŸã¯ æ–™ç†å…¨ä½“è¨ˆç®—
3. **é£Ÿäº‹ãƒ¬ãƒ™ãƒ«**: å…¨æ–™ç†ã®æ „é¤Šä¾¡åˆè¨ˆ

## âš ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

API ã¯ä»¥ä¸‹ã® HTTP ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã—ã¾ã™ï¼š

- `200 OK`: æ­£å¸¸ãªåˆ†æå®Œäº†
- `400 Bad Request`: ä¸æ­£ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆç”»åƒå½¢å¼ã‚¨ãƒ©ãƒ¼ãªã©ï¼‰
- `422 Unprocessable Entity`: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼
- `503 Service Unavailable`: å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆUSDA/Geminiï¼‰ã‚¨ãƒ©ãƒ¼
- `500 Internal Server Error`: ã‚µãƒ¼ãƒãƒ¼å†…éƒ¨ã‚¨ãƒ©ãƒ¼

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### èªè¨¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆ

```bash
# ç¾åœ¨ã®èªè¨¼çŠ¶æ…‹ã‚’ç¢ºèª
gcloud auth list

# ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã‚’ç¢ºèª
gcloud config list

# å¿…è¦ã«å¿œã˜ã¦å†åº¦èªè¨¼
gcloud auth application-default login
```

### Vertex AI API ãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ãªã„å ´åˆ

```bash
# APIã®æœ‰åŠ¹çŠ¶æ³ã‚’ç¢ºèª
gcloud services list --enabled | grep aiplatform

# æœ‰åŠ¹ã§ãªã„å ´åˆã¯æœ‰åŠ¹åŒ–
gcloud services enable aiplatform.googleapis.com
```

### USDA API ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆ

- API ã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
- ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆï¼ˆ3,600 ä»¶/æ™‚ï¼‰ã«é”ã—ã¦ã„ãªã„ã‹ç¢ºèª
- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª

## ğŸ’» é–‹ç™ºæƒ…å ±

- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: FastAPI 0.104+
- **AI ã‚µãƒ¼ãƒ“ã‚¹**: Google Vertex AI (Gemini 2.5 Flash)
- **æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: USDA FoodData Central API
- **èªè¨¼**: Google Cloud ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
- **Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 3.9+
- **ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
  - `google-cloud-aiplatform` (Vertex AI)
  - `httpx` (éåŒæœŸ HTTP)
  - `pydantic` (ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³)
  - `pillow` (ç”»åƒå‡¦ç†)

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ MIT ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## æ³¨æ„äº‹é …

**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: API ã‚­ãƒ¼ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã¯çµ¶å¯¾ã«ãƒªãƒã‚¸ãƒˆãƒªã«ã‚³ãƒŸãƒƒãƒˆã—ãªã„ã§ãã ã•ã„ã€‚ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦å®‰å…¨ã«ç®¡ç†ã—ã¦ãã ã•ã„ã€‚


----------------------------------------------------------------------
### FILE: .gitignore
### FULL PATH: /Users/odasoya/meal_analysis_api_2/.gitignore
----------------------------------------------------------------------

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
ENV/
env/
.venv

# IDE
.idea/
.vscode/
*.swp
*.swo
*~
.cursor/

# Environment variables
# .env
.env.local
.env.*.local

# OS files
.DS_Store
Thumbs.db

# Test coverage
htmlcov/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Logs
*.log

# Database
*.db
*.sqlite3

# Jupyter Notebook
.ipynb_checkpoints

# pyenv
.python-version

# Test images (actual image files)
test_images/*.jpg
test_images/*.jpeg
test_images/*.png
test_images/*.webp
test_images/*.heic
test_images/*.heif

# Google Cloud Service Account Keys
service-account-key.json
*.json
!package.json
!tsconfig.json
!openapi.json 

# Sensitive analysis files that may contain credentials
test_advanced_elasticsearch_architecture_*.txt

/raw_nutrition_data/
elasticsearch-*/data/
elasticsearch-*/logs/
elasticsearch-8.10.4/



