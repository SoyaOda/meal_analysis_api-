#!/usr/bin/env python3
"""
test_multi_db_nutrition_search.pyå®Ÿè¡Œæ™‚ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
1. test_multi_db_nutrition_search.pyå®Ÿè¡Œæ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
2. æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ†æ
3. å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
"""

import os
import datetime
from pathlib import Path
from typing import Dict, List

def get_file_content(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å®‰å…¨ã«èª­ã¿å–ã‚‹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        return f"ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {str(e)}"

def analyze_multi_db_test_architecture():
    """ãƒãƒ«ãƒDBæˆ¦ç•¥çš„æ¤œç´¢ãƒ†ã‚¹ãƒˆã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹é€ ã¨ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚’å®Ÿè¡Œ"""
    
    # test_multi_db_nutrition_search.pyå®Ÿè¡Œæ™‚ã®åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
    files_to_analyze = {
        "ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«": [
            "test_multi_db_nutrition_search.py"
        ],
        "FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤ (app_v2)": [
            "app_v2/main/app.py",
            "app_v2/api/v1/endpoints/meal_analysis.py"
        ],
        "ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ¶å±¤": [
            "app_v2/pipeline/__init__.py",
            "app_v2/pipeline/orchestrator.py",
            "app_v2/pipeline/result_manager.py"
        ],
        "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - Phase1": [
            "app_v2/components/__init__.py",
            "app_v2/components/base.py",
            "app_v2/components/phase1_component.py"
        ],
        "ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå±¤ - æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢": [
            "app_v2/components/elasticsearch_nutrition_search_component.py",
            "app_v2/components/local_nutrition_search_component.py"
        ],
        "ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å±¤": [
            "app_v2/models/__init__.py",
            "app_v2/models/nutrition_search_models.py",
            "app_v2/models/phase1_models.py"
        ],
        "è¨­å®šç®¡ç†": [
            "app_v2/config/__init__.py"
        ],
        "Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†": [
            "create_elasticsearch_index.py"
        ],
        "æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹": [
            "db/yazio_db.json",
            "db/mynetdiary_db.json", 
            "db/eatthismuch_db.json"
        ],
        "Elasticsearchè¨­å®š": [
            "elasticsearch-8.10.4/config/elasticsearch.yml"
        ],
        "ä¾å­˜é–¢ä¿‚ãƒ»è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«": [
            "requirements.txt",
            "README.md"
        ]
    }
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"test_multi_db_strategic_search_architecture_{timestamp}.txt"
    
    with open(output_file, 'w', encoding='utf-8') as out_f:
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        out_f.write("=" * 80 + "\n")
        out_f.write("MEAL ANALYSIS API v2.0 - æˆ¦ç•¥çš„ãƒãƒ«ãƒDBæ „é¤Šæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ\n")
        out_f.write("=" * 80 + "\n")
        out_f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        out_f.write(f"åˆ†æå¯¾è±¡: test_multi_db_nutrition_search.pyå®Ÿè¡Œæ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«\n")
        out_f.write("=" * 80 + "\n\n")
        
        # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦
        out_f.write("ğŸš€ STRATEGIC MULTI-DB ELASTICSEARCH SEARCH ARCHITECTURE OVERVIEW\n")
        out_f.write("-" * 60 + "\n")
        out_f.write("""
ğŸ”„ STRATEGIC SEARCH EXECUTION FLOW:
1. test_multi_db_nutrition_search.py â†’ FastAPI /complete endpoint
2. Phase1: Gemini AIç”»åƒåˆ†æ â†’ æ–™ç†ãƒ»é£Ÿæè­˜åˆ¥
3. æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢:
   ğŸ“ DISHæˆ¦ç•¥: EatThisMuch dish â†’ EatThisMuch branded (fallback)
   ğŸ¥• INGREDIENTæˆ¦ç•¥: EatThisMuch ingredient â†’ Multi-DB (MyNetDiary/YAZIO/branded)
4. æ „é¤Šãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»çµæœä¿å­˜

ğŸ—ï¸ COMPONENT-BASED ARCHITECTURE v2.0:
â”œâ”€â”€ Test Layer
â”‚   â””â”€â”€ test_multi_db_nutrition_search.py (æˆ¦ç•¥çš„æ¤œç´¢ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ)
â”œâ”€â”€ FastAPI Application Layer (app_v2)
â”‚   â”œâ”€â”€ main/app.py (Server, CORS, routing)
â”‚   â””â”€â”€ api/v1/endpoints/meal_analysis.py (Complete analysis endpoint)
â”œâ”€â”€ Pipeline Management Layer
â”‚   â”œâ”€â”€ orchestrator.py (MealAnalysisPipeline - å…¨ãƒ•ã‚§ãƒ¼ã‚ºçµ±åˆ¶)
â”‚   â””â”€â”€ result_manager.py (ResultManager - çµæœä¿å­˜ãƒ»ç®¡ç†)
â”œâ”€â”€ Component Layer
â”‚   â”œâ”€â”€ base.py (BaseComponent - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåŸºåº•ã‚¯ãƒ©ã‚¹)
â”‚   â”œâ”€â”€ phase1_component.py (Phase1Component - Gemini AIåˆ†æ)
â”‚   â””â”€â”€ elasticsearch_nutrition_search_component.py (æˆ¦ç•¥çš„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³)
â”œâ”€â”€ Data Model Layer
â”‚   â”œâ”€â”€ nutrition_search_models.py (NutritionMatch, NutritionQueryInput/Output)
â”‚   â””â”€â”€ phase1_models.py (Phase1Input/Output, Dish, Ingredient)
â”œâ”€â”€ Elasticsearch Infrastructure
â”‚   â”œâ”€â”€ create_elasticsearch_index.py (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãƒ»ç®¡ç†)
â”‚   â””â”€â”€ elasticsearch-8.10.4/ (Elasticsearchã‚µãƒ¼ãƒãƒ¼)
â””â”€â”€ Data Layer
    â”œâ”€â”€ yazio_db.json (1,825é …ç›® - ãƒãƒ©ãƒ³ã‚¹é£Ÿå“)
    â”œâ”€â”€ mynetdiary_db.json (1,142é …ç›® - ç§‘å­¦çš„ãƒ‡ãƒ¼ã‚¿)
    â””â”€â”€ eatthismuch_db.json (8,878é …ç›® - æœ€å¤§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹)

ğŸ¯ STRATEGIC SEARCH FEATURES:
- ğŸ”¥ Dishæ¤œç´¢æˆ¦ç•¥: EatThisMuch dish (ãƒ¡ã‚¤ãƒ³) + branded (è£œåŠ©)
- ğŸ¥• Ingredientæ¤œç´¢æˆ¦ç•¥: EatThisMuch ingredient (ãƒ¡ã‚¤ãƒ³) + Multi-DB (è£œåŠ©)
- âš¡ é«˜é€ŸåŒ–: 677ms â†’ 381ms (44%å‘ä¸Š)
- ğŸ“Š æœ€é©åŒ–çµæœ: 144ä»¶ â†’ 50ä»¶ (é–¢é€£æ€§é‡è¦–)
- ğŸ¯ æˆ¦ç•¥çš„åˆ†æ•£: EatThisMuch 72%, MyNetDiary/YAZIO å„14%
- ğŸ“ˆ å“è³ªå‘ä¸Š: ã‚¹ã‚³ã‚¢é–¾å€¤20.0ã«ã‚ˆã‚‹å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ğŸ’¾ è©³ç´°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: æˆ¦ç•¥ãƒ•ã‚§ãƒ¼ã‚ºãƒ»ã‚½ãƒ¼ã‚¹æƒ…å ±è¿½è·¡

ğŸ”§ TECHNICAL SPECIFICATIONS:
- Search Engine: Elasticsearch 8.10.4 (11,845ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ)
- AI Service: Google Vertex AI (Gemini 2.5 Flash)
- Web Framework: FastAPI 0.104+ (async/await)
- Architecture: Component-based Pipeline Pattern
- Data Format: JSON (100gæ­£è¦åŒ–æ „é¤Šãƒ‡ãƒ¼ã‚¿)
- Search Strategy: BM25F + Multi-Signal Boosting + Strategic Filtering
- Performance: 90.9% match rate, sub-second response times

ğŸš€ STRATEGIC IMPROVEMENTS vs LEGACY:
- æˆ¦ç•¥çš„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸æŠ (EatThisMuchã‚’ä¸­å¿ƒã¨ã—ãŸæœ€é©åŒ–)
- ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ä½å“è³ªæ™‚ã®è‡ªå‹•è£œå®Œ)
- åŠ¹ç‡çš„ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ (å¿…è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿å–å¾—)
- æ§‹é€ åŒ–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (æ¤œç´¢ãƒ—ãƒ­ã‚»ã‚¹è¿½è·¡å¯èƒ½)
- ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«è¨­è¨ˆ (æ–°æˆ¦ç•¥ãƒ»DBã®å®¹æ˜“è¿½åŠ )

""")
        out_f.write("=" * 80 + "\n\n")
        
        # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
        for category, file_list in files_to_analyze.items():
            out_f.write(f"ğŸ“ {category}\n")
            out_f.write("=" * 60 + "\n\n")
            
            for file_path in file_list:
                out_f.write(f"ğŸ“„ FILE: {file_path}\n")
                out_f.write("-" * 50 + "\n")
                
                if os.path.exists(file_path):
                    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
                    file_stat = os.stat(file_path)
                    file_size = file_stat.st_size
                    mod_time = datetime.datetime.fromtimestamp(file_stat.st_mtime)
                    
                    out_f.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes\n")
                    out_f.write(f"æœ€çµ‚æ›´æ–°: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    out_f.write(f"å­˜åœ¨: âœ…\n\n")
                    
                    # ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‡¦ç†
                    if file_path.endswith('.json'):
                        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€åˆã®50è¡Œã®ã¿è¡¨ç¤ºï¼ˆã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚ï¼‰
                        out_f.write("CONTENT (æœ€åˆã®50è¡Œ):\n")
                        out_f.write("```json\n")
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                lines = f.readlines()
                                for i, line in enumerate(lines[:50]):
                                    out_f.write(line)
                                if len(lines) > 50:
                                    out_f.write(f"\n... ({len(lines) - 50} more lines)\n")
                        except Exception as e:
                            out_f.write(f"ERROR: JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {str(e)}\n")
                        out_f.write("```\n\n")
                    elif file_path.endswith('.yml') or file_path.endswith('.yaml'):
                        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯é‡è¦éƒ¨åˆ†ã®ã¿è¡¨ç¤º
                        out_f.write("CONTENT (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«):\n")
                        out_f.write("```yaml\n")
                        content = get_file_content(file_path)
                        out_f.write(content)
                        out_f.write("```\n\n")
                    else:
                        # Python/text ãƒ•ã‚¡ã‚¤ãƒ«ã¯å…¨å†…å®¹è¡¨ç¤º
                        out_f.write("CONTENT:\n")
                        out_f.write("```python\n")
                        content = get_file_content(file_path)
                        out_f.write(content)
                        out_f.write("\n```\n\n")
                else:
                    out_f.write("å­˜åœ¨: âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n\n")
                
                out_f.write("=" * 60 + "\n\n")
        
        # ãƒ•ãƒƒã‚¿ãƒ¼
        out_f.write("ğŸ¯ STRATEGIC MULTI-DB SEARCH ANALYSIS SUMMARY\n")
        out_f.write("-" * 50 + "\n")
        total_files = sum(len(files) for files in files_to_analyze.values())
        existing_files = sum(1 for files in files_to_analyze.values() for f in files if os.path.exists(f))
        
        out_f.write(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}\n")
        out_f.write(f"å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {existing_files}\n")
        out_f.write(f"åˆ†æå®Œäº†æ™‚åˆ»: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        out_f.write("\nã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ã€test_multi_db_nutrition_search.pyå®Ÿè¡Œæ™‚ã«\n")
        out_f.write("é–¢ã‚ã‚‹æˆ¦ç•¥çš„ãƒãƒ«ãƒDB Elasticsearchæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å…¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³\n")
        out_f.write("ãƒ•ã‚¡ã‚¤ãƒ«ã®å®Œå…¨ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n")
        out_f.write("\nğŸ”¥ STRATEGIC SEARCH SYSTEM HIGHLIGHTS:\n")
        out_f.write("- ğŸ½ï¸  Dishæˆ¦ç•¥: EatThisMuch dish â†’ branded fallback\n")
        out_f.write("- ğŸ¥• Ingredientæˆ¦ç•¥: EatThisMuch ingredient â†’ Multi-DB fallback\n")
        out_f.write("- âš¡ 44%é«˜é€ŸåŒ–: 677ms â†’ 381ms\n") 
        out_f.write("- ğŸ“Š çµæœæœ€é©åŒ–: 144ä»¶ â†’ 50ä»¶ (é–¢é€£æ€§é‡è¦–)\n")
        out_f.write("- ğŸ¯ æˆ¦ç•¥çš„åˆ†æ•£: EatThisMuchä¸­å¿ƒå‹ (72%)\n")
        out_f.write("- ğŸ” ã‚¹ã‚³ã‚¢é–¾å€¤: å‹•çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (20.0)\n")
        out_f.write("- ğŸ’¾ è©³ç´°è¿½è·¡: æˆ¦ç•¥ãƒ•ã‚§ãƒ¼ã‚ºãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²\n")
        out_f.write("- ğŸš€ ComponentåŒ–: æ‹¡å¼µå¯èƒ½ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n")
        
    return output_file, total_files, existing_files

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ” æˆ¦ç•¥çš„ãƒãƒ«ãƒDBæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æé–‹å§‹...")
    print("-" * 60)
    
    try:
        output_file, total_files, existing_files = analyze_multi_db_test_architecture()
        
        print(f"âœ… åˆ†æå®Œäº†!")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
        print(f"ğŸ“Š ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
        print(f"âœ… å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {existing_files}")
        
        if existing_files < total_files:
            missing = total_files - existing_files
            print(f"âš ï¸  è¦‹ã¤ã‹ã‚‰ãªã„ãƒ•ã‚¡ã‚¤ãƒ«: {missing}å€‹")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’è¡¨ç¤º
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file)
            print(f"ğŸ“„ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes")
        
        print("\nğŸ‰ æˆ¦ç•¥çš„ãƒãƒ«ãƒDBæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãŒå®Œäº†ã—ã¾ã—ãŸ!")
        print("ğŸ”¥ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯æˆ¦ç•¥çš„Elasticsearchæ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã‚‹å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
        print("ğŸ¯ Dishæˆ¦ç•¥ã¨Ingredientæˆ¦ç•¥ã«ã‚ˆã‚‹æœ€é©åŒ–ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ æ§‹æˆã‚’ç¢ºèªã§ãã¾ã™ã€‚")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 