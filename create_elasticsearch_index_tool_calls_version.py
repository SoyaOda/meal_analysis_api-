#!/usr/bin/env python3
"""
Tool callsç‰ˆã§å¤‰æ›ã•ã‚ŒãŸMyNetDiaryãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
æ¤œç´¢æ„å›³æœ€é©åŒ–ã«å¯¾å¿œã—ãŸè¨­å®š
"""

import json
import sqlite3
from elasticsearch import Elasticsearch
from datetime import datetime

# Elasticsearchè¨­å®š
ES_HOST = "localhost"
ES_PORT = 9200
INDEX_NAME = "mynetdiary_tool_calls_db"

def create_elasticsearch_index():
    """Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
    
    # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    es = Elasticsearch([f"http://{ES_HOST}:{ES_PORT}"])
    
    # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤
    if es.indices.exists(index=INDEX_NAME):
        print(f"ğŸ—‘ï¸ Deleting existing index: {INDEX_NAME}")
        es.indices.delete(index=INDEX_NAME)
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šï¼ˆæ¤œç´¢æ„å›³æœ€é©åŒ–å¯¾å¿œï¼‰
    index_settings = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "food_name_analyzer": {
                        "type": "custom",
                        "tokenizer": "standard",
                        "filter": [
                            "lowercase",
                            "food_synonym_filter",
                            "food_stemmer"
                        ]
                    },
                    "food_exact_analyzer": {
                        "type": "custom", 
                        "tokenizer": "keyword",
                        "filter": ["lowercase"]
                    }
                },
                "filter": {
                    "food_synonym_filter": {
                        "type": "synonym",
                        "synonyms": [
                            "chickpeas,garbanzo beans,garbanzos",
                            "scallions,green onions,spring onions",
                            "cilantro,coriander leaves",
                            "bell pepper,sweet pepper",
                            "zucchini,courgette"
                        ]
                    },
                    "food_stemmer": {
                        "type": "stemmer",
                        "language": "light_english"
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "id": {"type": "long"},
                "original_name": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "exact": {
                            "type": "text",
                            "analyzer": "food_exact_analyzer"
                        }
                    }
                },
                "search_name": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "exact": {
                            "type": "text", 
                            "analyzer": "food_exact_analyzer"
                        },
                        "lemmatized": {
                            "type": "text",
                            "analyzer": "food_name_analyzer"
                        }
                    }
                },
                "description": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "exact": {
                            "type": "text",
                            "analyzer": "food_exact_analyzer"
                        }
                    }
                },
                "nutrition": {
                    "properties": {
                        "calories": {"type": "float"},
                        "protein": {"type": "float"},
                        "fat": {"type": "float"},
                        "carbs": {"type": "float"}
                    }
                },
                "data_type": {"type": "keyword"},
                "source": {"type": "keyword"},
                "processing_method": {"type": "keyword"},
                "conversion_timestamp": {"type": "date"}
            }
        }
    }
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    print(f"ğŸ”§ Creating index: {INDEX_NAME}")
    es.indices.create(index=INDEX_NAME, body=index_settings)
    print("âœ… Index created successfully")
    
    return es

def load_and_index_data(es):
    """å¤‰æ›æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    print("ğŸ“‚ Loading converted data...")
    with open('db/mynetdiary_converted_tool_calls.json', 'r', encoding='utf-8') as f:
        converted_data = json.load(f)
    
    print(f"âœ… Loaded {len(converted_data)} items")
    
    # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰
    print("ğŸ’¾ Creating SQLite database...")
    conn = sqlite3.connect('db/mynetdiary_tool_calls.db')
    cursor = conn.cursor()
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS nutrition_data (
            id INTEGER PRIMARY KEY,
            original_name TEXT NOT NULL,
            search_name TEXT NOT NULL,
            description TEXT,
            calories REAL,
            protein REAL,
            fat REAL,
            carbs REAL,
            data_type TEXT,
            source_db TEXT,
            processing_method TEXT,
            conversion_timestamp REAL
        )
    ''')
    
    # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ã¨Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    print("ğŸ“¤ Indexing data to Elasticsearch and SQLite...")
    indexed_count = 0
    batch_size = 100
    batch = []
    
    for item in converted_data:
        # Elasticsearchç”¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™
        doc = {
            "id": item["id"],
            "original_name": item["original_name"],
            "search_name": item["search_name"],
            "description": item["description"],
            "nutrition": item["nutrition"],
            "data_type": item["data_type"],
            "source": item["source"],
            "processing_method": item["processing_method"],
            "conversion_timestamp": datetime.fromtimestamp(item["conversion_timestamp"])
        }
        
        batch.append({
            "_index": INDEX_NAME,
            "_id": item["id"],
            "_source": doc
        })
        
        # SQLiteã«æŒ¿å…¥
        nutrition = item["nutrition"]
        cursor.execute('''
            INSERT OR REPLACE INTO nutrition_data 
            (id, original_name, search_name, description, calories, protein, fat, carbs, 
             data_type, source_db, processing_method, conversion_timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            item["id"],
            item["original_name"],
            item["search_name"],
            item["description"],
            nutrition.get("calories", 0),
            nutrition.get("protein", 0),
            nutrition.get("fat", 0),
            nutrition.get("carbs", 0),
            item["data_type"],
            item["source"],
            item["processing_method"],
            item["conversion_timestamp"]
        ))
        
        # ãƒãƒƒãƒã§Elasticsearchã«é€ä¿¡
        if len(batch) >= batch_size:
            from elasticsearch.helpers import bulk
            try:
                success, failed = bulk(es, batch, stats_only=True)
                indexed_count += success
                if failed > 0:
                    print(f"   âš ï¸ {failed} documents failed in this batch")
                print(f"   ğŸ“Š Progress: {indexed_count}/{len(converted_data)} ({indexed_count/len(converted_data)*100:.1f}%)")
            except Exception as e:
                print(f"   âŒ Batch error: {e}")
                # å€‹åˆ¥ã«å‡¦ç†ã—ã¦ã¿ã‚‹
                for doc in batch:
                    try:
                        es.index(index=INDEX_NAME, id=doc["_id"], body=doc["_source"])
                        indexed_count += 1
                    except Exception as doc_error:
                        print(f"   ğŸ”´ Failed to index document {doc['_id']}: {doc_error}")
            batch = []
    
    # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†
    if batch:
        from elasticsearch.helpers import bulk
        try:
            success, failed = bulk(es, batch, stats_only=True)
            indexed_count += success
            if failed > 0:
                print(f"   âš ï¸ {failed} documents failed in final batch")
        except Exception as e:
            print(f"   âŒ Final batch error: {e}")
            # å€‹åˆ¥ã«å‡¦ç†ã—ã¦ã¿ã‚‹
            for doc in batch:
                try:
                    es.index(index=INDEX_NAME, id=doc["_id"], body=doc["_source"])
                    indexed_count += 1
                except Exception as doc_error:
                    print(f"   ğŸ”´ Failed to index document {doc['_id']}: {doc_error}")
    
    conn.commit()
    conn.close()
    
    print(f"âœ… Successfully indexed {indexed_count} documents")
    print(f"ğŸ’¾ SQLite database saved: db/mynetdiary_tool_calls.db")
    
    return indexed_count

def test_search_functionality(es):
    """æ¤œç´¢æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("\nğŸ” Testing search functionality...")
    
    test_queries = [
        "tomato",           # ä¸€èˆ¬æ¤œç´¢ï¼ˆé‡èœã®ãƒˆãƒãƒˆï¼‰
        "tomato powder",    # ç‰¹å®šè£½å“æ¤œç´¢
        "coffee",           # ä¸€èˆ¬æ¤œç´¢ï¼ˆåŸºæœ¬ã‚³ãƒ¼ãƒ’ãƒ¼ï¼‰
        "coffee powder",    # ç‰¹å®šè£½å“æ¤œç´¢
        "chicken breast",   # è‚‰é¡æ¤œç´¢
        "beans"            # è±†é¡æ¤œç´¢
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Testing query: '{query}'")
        
        search_body = {
            "query": {
                "multi_match": {
                    "query": query,
                    "fields": ["search_name^3", "description^1", "original_name^2"],
                    "type": "best_fields",
                    "fuzziness": "AUTO"
                }
            },
            "size": 5
        }
        
        result = es.search(index=INDEX_NAME, body=search_body)
        
        for i, hit in enumerate(result['hits']['hits'], 1):
            source = hit['_source']
            score = hit['_score']
            print(f"   {i}. [{score:.2f}] {source['search_name']} | {source['description']}")
            print(f"      Original: {source['original_name']}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    print("ğŸš€ MyNetDiary Tool Callsç‰ˆ Elasticsearch ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ")
    print("=" * 70)
    
    try:
        # Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        es = create_elasticsearch_index()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        indexed_count = load_and_index_data(es)
        
        # æ¤œç´¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
        test_search_functionality(es)
        
        print(f"\nğŸ‰ å®Œäº†! {indexed_count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã•ã‚Œã¾ã—ãŸ")
        print(f"ğŸ“Š ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å: {INDEX_NAME}")
        print("ğŸ” æ¤œç´¢æ„å›³æœ€é©åŒ–ã«ã‚ˆã‚Šã€ä¸€èˆ¬æ¤œç´¢ã¨ç‰¹å®šè£½å“æ¤œç´¢ãŒé©åˆ‡ã«åˆ†é›¢ã•ã‚Œã¾ã™")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 