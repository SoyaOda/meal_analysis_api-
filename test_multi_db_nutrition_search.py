#!/usr/bin/env python3
"""
Multi-Database Nutrition Search Test v1.0

dbãƒ•ã‚©ãƒ«ãƒ€å†…ã®3ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆyazioã€mynetdiaryã€eatthismuchï¼‰ã‚’å¯¾è±¡ã«ã€
1ã¤ã®queryã«ã¤ãå„dbã‹ã‚‰ä¸Šä½3ã¤ã®çµæœã‚’å–å¾—ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import requests
import json
import time
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
import math

# APIè¨­å®šï¼ˆæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰ˆï¼‰
BASE_URL = "http://localhost:8000/api/v1"

# ãƒ†ã‚¹ãƒˆç”»åƒã®ãƒ‘ã‚¹
image_path = "test_images/food3.jpg"

# dbãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
DB_CONFIGS = {
    "yazio": {
        "file_path": "db/yazio_db.json",
        "source": "YAZIO"
    },
    "mynetdiary": {
        "file_path": "db/mynetdiary_db.json", 
        "source": "MyNetDiary"
    },
    "eatthismuch": {
        "file_path": "db/eatthismuch_db.json",
        "source": "EatThisMuch"
    }
}

class MultiDbNutritionSearcher:
    """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ „é¤Šæƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.databases = {}
        self.load_databases()
    
    def load_databases(self):
        """3ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ”„ Loading databases...")
        
        for db_name, config in DB_CONFIGS.items():
            try:
                print(f"   Loading {db_name}...")
                with open(config["file_path"], 'r', encoding='utf-8') as f:
                    database = json.load(f)
                    self.databases[db_name] = {
                        "data": database,
                        "source": config["source"],
                        "count": len(database)
                    }
                    print(f"   âœ… {db_name}: {len(database)} items loaded")
            except Exception as e:
                print(f"   âŒ Failed to load {db_name}: {e}")
                self.databases[db_name] = {
                    "data": [],
                    "source": config["source"],
                    "count": 0
                }
        
        total_items = sum(db["count"] for db in self.databases.values())
        print(f"âœ… Total items loaded: {total_items}")
    
    def search_single_database(self, query: str, db_name: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """å˜ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢"""
        if db_name not in self.databases:
            return []
        
        database = self.databases[db_name]["data"]
        query_lower = query.lower()
        scored_results = []
        
        for item in database:
            if 'search_name' not in item:
                continue
            
            item_name = item['search_name'].lower()
            score = self.calculate_match_score(query_lower, item_name)
            
            if score > 0.1:  # æœ€ä½é–¾å€¤
                result = item.copy()
                result['_match_score'] = score
                result['_query'] = query
                result['_db_name'] = db_name
                scored_results.append(result)
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½kä»¶ã‚’è¿”ã™
        scored_results.sort(key=lambda x: x['_match_score'], reverse=True)
        return scored_results[:top_k]
    
    def calculate_match_score(self, query: str, item_name: str) -> float:
        """ãƒãƒƒãƒã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if query == item_name:
            return 1.0  # å®Œå…¨ä¸€è‡´
        elif query in item_name:
            if item_name.startswith(query):
                return 0.9  # å‰æ–¹ä¸€è‡´
            elif item_name.endswith(query):
                return 0.8  # å¾Œæ–¹ä¸€è‡´
            else:
                return 0.7  # ä¸­é–“ä¸€è‡´
        elif item_name in query:
            return 0.6  # é€†éƒ¨åˆ†ä¸€è‡´
        else:
            # å˜èªãƒ¬ãƒ™ãƒ«ã®ä¸€è‡´
            query_words = set(query.split())
            item_words = set(item_name.split())
            common_words = query_words & item_words
            if common_words:
                return len(common_words) / max(len(query_words), len(item_words)) * 0.5
        
        return 0.0
    
    def search_all_databases(self, query: str, top_k_per_db: int = 3) -> Dict[str, List[Dict[str, Any]]]:
        """å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢"""
        results = {}
        
        for db_name in self.databases.keys():
            db_results = self.search_single_database(query, db_name, top_k_per_db)
            results[db_name] = db_results
        
        return results
    
    def search_multiple_queries(self, queries: List[str], top_k_per_db: int = 3) -> Dict[str, Dict[str, List[Dict[str, Any]]]]:
        """è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã«ã¤ã„ã¦å…¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢"""
        all_results = {}
        
        for query in queries:
            all_results[query] = self.search_all_databases(query, top_k_per_db)
        
        return all_results

def test_multi_db_nutrition_search():
    """ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ „é¤Šæ¤œç´¢ã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("=== Multi-Database Nutrition Search Test v1.0 ===")
    print(f"Using image: {image_path}")
    print("ğŸ” Testing search across 3 databases: YAZIO, MyNetDiary, EatThisMuch")
    
    try:
        # å®Œå…¨åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—ã¦Phase1çµæœã‚’å–å¾—
        with open(image_path, "rb") as f:
            files = {"image": ("food3.jpg", f, "image/jpeg")}
            data = {"save_results": True}
            
            print("Starting complete analysis to get Phase1 results...")
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/meal-analyses/complete", files=files, data=data)
            end_time = time.time()
        
        print(f"Status Code: {response.status_code}")
        print(f"Response Time: {end_time - start_time:.2f}s")
        
        if response.status_code != 200:
            print("âŒ Failed to get Phase1 results!")
            print(f"Error: {response.text}")
            return False
        
        result = response.json()
        analysis_id = result.get("analysis_id")
        print(f"Analysis ID: {analysis_id}")
        
        # Phase1çµæœã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
        phase1_result = result.get("phase1_result", {})
        dishes = phase1_result.get("dishes", [])
        
        all_queries = []
        dish_names = []
        ingredient_names = []
        
        for dish in dishes:
            dish_name = dish.get("dish_name")
            if dish_name:
                dish_names.append(dish_name)
                all_queries.append(dish_name)
            
            ingredients = dish.get("ingredients", [])
            for ingredient in ingredients:
                ingredient_name = ingredient.get("ingredient_name")
                if ingredient_name:
                    ingredient_names.append(ingredient_name)
                    all_queries.append(ingredient_name)
        
        # é‡è¤‡ã‚’é™¤å»
        all_queries = list(set(all_queries))
        dish_names = list(set(dish_names))
        ingredient_names = list(set(ingredient_names))
        
        print(f"\nğŸ“Š Extracted Search Queries:")
        print(f"- Total dishes: {len(dish_names)}")
        print(f"- Total ingredients: {len(ingredient_names)}")
        print(f"- Total unique queries: {len(all_queries)}")
        
        if len(all_queries) == 0:
            print("âŒ No search queries extracted from Phase1 results!")
            return False
        
        # ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        searcher = MultiDbNutritionSearcher()
        
        # å…¨ã‚¯ã‚¨ãƒªã«ã¤ã„ã¦ãƒãƒ«ãƒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’å®Ÿè¡Œ
        print(f"\nğŸ” Starting multi-database search for {len(all_queries)} queries...")
        search_start_time = time.time()
        
        search_results = searcher.search_multiple_queries(all_queries, top_k_per_db=3)
        
        search_end_time = time.time()
        search_time = search_end_time - search_start_time
        
        print(f"âœ… Multi-database search completed in {search_time:.2f}s")
        
        # çµæœã®åˆ†æ
        total_matches = 0
        db_stats = {db_name: {"matches": 0, "queries_with_results": 0} for db_name in DB_CONFIGS.keys()}
        
        query_results_summary = []
        
        for query, db_results in search_results.items():
            query_summary = {
                "query": query,
                "type": "dish" if query in dish_names else "ingredient",
                "results_per_db": {}
            }
            
            for db_name, results in db_results.items():
                num_results = len(results)
                query_summary["results_per_db"][db_name] = num_results
                
                if num_results > 0:
                    db_stats[db_name]["queries_with_results"] += 1
                    db_stats[db_name]["matches"] += num_results
                    total_matches += num_results
            
            query_results_summary.append(query_summary)
        
        # çµæœã®è¡¨ç¤º
        print(f"\nğŸ“ˆ Multi-Database Search Results Summary:")
        print(f"- Total queries: {len(all_queries)}")
        print(f"- Total matches found: {total_matches}")
        print(f"- Average matches per query: {total_matches / len(all_queries):.1f}")
        print(f"- Search time: {search_time:.2f}s")
        print(f"- Average time per query: {search_time / len(all_queries):.3f}s")
        
        print(f"\nğŸ“Š Database Statistics:")
        for db_name, stats in db_stats.items():
            db_info = searcher.databases[db_name]
            coverage = (stats["queries_with_results"] / len(all_queries)) * 100
            print(f"- {db_name} ({db_info['source']}):")
            print(f"  Database size: {db_info['count']} items")
            print(f"  Queries with results: {stats['queries_with_results']}/{len(all_queries)} ({coverage:.1f}%)")
            print(f"  Total matches: {stats['matches']}")
            print(f"  Avg matches per successful query: {stats['matches'] / max(stats['queries_with_results'], 1):.1f}")
        
        print(f"\nğŸ” Detailed Query Results:")
        for i, query_summary in enumerate(query_results_summary[:10], 1):  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
            query = query_summary["query"]
            query_type = query_summary["type"]
            results_per_db = query_summary["results_per_db"]
            
            print(f"{i:2d}. '{query}' ({query_type})")
            
            for db_name, count in results_per_db.items():
                if count > 0:
                    # ä¸Šä½çµæœã®è©³ç´°ã‚’è¡¨ç¤º
                    top_result = search_results[query][db_name][0]
                    print(f"    {db_name}: {count} matches")
                    print(f"      Best: '{top_result['search_name']}' (score: {top_result['_match_score']:.3f})")
                    nutrition = top_result.get('nutrition', {})
                    if nutrition:
                        calories = nutrition.get('calories', 0)
                        protein = nutrition.get('protein', 0)
                        print(f"      Nutrition: {calories:.1f} kcal, {protein:.1f}g protein")
                else:
                    print(f"    {db_name}: No matches")
        
        if len(query_results_summary) > 10:
            print(f"    ... and {len(query_results_summary) - 10} more queries")
        
        # è©³ç´°çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        save_detailed_results(analysis_id, search_results, query_results_summary, db_stats, searcher.databases)
        
        return True
        
    except Exception as e:
        print(f"âŒ Error during multi-database nutrition search: {e}")
        import traceback
        traceback.print_exc()
        return False

def save_detailed_results(analysis_id: str, search_results: Dict, query_summary: List, db_stats: Dict, db_info: Dict):
    """è©³ç´°ãªæ¤œç´¢çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"analysis_results/multi_db_search_{analysis_id}_{timestamp}"
    os.makedirs(results_dir, exist_ok=True)
    
    # 1. å…¨æ¤œç´¢çµæœã‚’JSONã§ä¿å­˜
    results_file = os.path.join(results_dir, "complete_search_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "analysis_id": analysis_id,
            "timestamp": timestamp,
            "search_results": search_results,
            "summary": {
                "total_queries": len(search_results),
                "database_stats": db_stats,
                "database_info": {db_name: {"count": info["count"], "source": info["source"]} 
                                for db_name, info in db_info.items()}
            }
        }, f, indent=2, ensure_ascii=False)
    
    # 2. æ¤œç´¢ã‚µãƒãƒªãƒ¼ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§ä¿å­˜
    summary_file = os.path.join(results_dir, "search_summary.md")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(f"# Multi-Database Nutrition Search Results\n\n")
        f.write(f"**Analysis ID:** {analysis_id}\n")
        f.write(f"**Timestamp:** {timestamp}\n")
        f.write(f"**Total Queries:** {len(search_results)}\n\n")
        
        f.write(f"## Database Statistics\n\n")
        for db_name, stats in db_stats.items():
            db_inf = db_info[db_name]
            coverage = (stats["queries_with_results"] / len(search_results)) * 100
            f.write(f"### {db_name} ({db_inf['source']})\n")
            f.write(f"- Database size: {db_inf['count']} items\n")
            f.write(f"- Query coverage: {stats['queries_with_results']}/{len(search_results)} ({coverage:.1f}%)\n")
            f.write(f"- Total matches: {stats['matches']}\n")
            f.write(f"- Avg matches per successful query: {stats['matches'] / max(stats['queries_with_results'], 1):.1f}\n\n")
        
        f.write(f"## Query Results Detail\n\n")
        for i, query_summary in enumerate(query_summary, 1):
            query = query_summary["query"]
            query_type = query_summary["type"]
            results_per_db = query_summary["results_per_db"]
            
            f.write(f"### {i}. {query} ({query_type})\n")
            
            for db_name, count in results_per_db.items():
                if count > 0:
                    f.write(f"- **{db_name}**: {count} matches\n")
                    # ä¸Šä½3ä»¶ã®è©³ç´°
                    for j, result in enumerate(search_results[query][db_name][:3], 1):
                        f.write(f"  {j}. {result['search_name']} (score: {result['_match_score']:.3f})\n")
                        nutrition = result.get('nutrition', {})
                        if nutrition:
                            calories = nutrition.get('calories', 0)
                            protein = nutrition.get('protein', 0)
                            fat = nutrition.get('fat', 0)
                            carbs = nutrition.get('carbs', 0)
                            f.write(f"     Nutrition (100g): {calories:.1f} kcal, P:{protein:.1f}g, F:{fat:.1f}g, C:{carbs:.1f}g\n")
                else:
                    f.write(f"- **{db_name}**: No matches\n")
            f.write(f"\n")
    
    print(f"\nğŸ’¾ Detailed results saved to:")
    print(f"   ğŸ“ {results_dir}/")
    print(f"   ğŸ“„ complete_search_results.json")
    print(f"   ğŸ“„ search_summary.md")

if __name__ == "__main__":
    print("ğŸš€ Starting Multi-Database Nutrition Search Test")
    success = test_multi_db_nutrition_search()
    
    if success:
        print("\nâœ… Multi-database nutrition search test completed successfully!")
    else:
        print("\nâŒ Multi-database nutrition search test failed!") 