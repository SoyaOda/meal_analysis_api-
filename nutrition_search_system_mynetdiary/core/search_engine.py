"""
æ „é¤Šæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ - 7æ®µéšæ¤œç´¢æˆ¦ç•¥ã¨è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½
"""

import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Any, Optional

from .models import SearchQuery, SearchResponse, SearchResult, NutritionInfo, BatchSearchQuery, BatchSearchResponse
from utils.lemmatization import lemmatize_term, create_lemmatized_query_variations
from utils.elasticsearch_client import get_elasticsearch_client

logger = logging.getLogger(__name__)


class NutritionSearchEngine:
    """æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.es_client = get_elasticsearch_client()
        
        # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.lemmatized_exact_match_boost = 2.0
        self.compound_word_penalty = 0.8
        self.enable_lemmatization = True
        
        # çµ±è¨ˆ
        self.total_searches = 0
        self.total_response_time = 0
        
        logger.info("ğŸ” NutritionSearchEngine initialized")
    
    async def search(self, query: SearchQuery) -> SearchResponse:
        """å˜ä¸€æ¤œç´¢å®Ÿè¡Œ"""
        start_time = datetime.now()
        
        if not self.es_client.is_connected():
            return SearchResponse(
                query=query.query,
                results=[],
                total_found=0,
                search_time_ms=0,
                lemmatized_query=None
            )
        
        # è¦‹å‡ºã—èªåŒ–å‡¦ç†
        lemmatized_query = lemmatize_term(query.query) if self.enable_lemmatization else query.query
        
        # 7æ®µéšæ¤œç´¢ã‚¯ã‚¨ãƒªæ§‹ç¯‰
        es_query = self._build_advanced_search_query(query.query, lemmatized_query, query.max_results)
        
        # Elasticsearchæ¤œç´¢å®Ÿè¡Œ
        response = await self.es_client.search(es_query)
        
        # çµæœå¤‰æ›
        results = []
        if response and response.get('hits', {}).get('hits'):
            results = await self._convert_es_results(response['hits']['hits'], query.query, lemmatized_query)
            
            # ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            results = [r for r in results if r.score >= query.min_score]
            
            # ã‚½ãƒ¼ã‚¹DBãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if query.source_db_filter:
                results = [r for r in results if r.source_db in query.source_db_filter]
        
        # çµ±è¨ˆæ›´æ–°
        end_time = datetime.now()
        search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        self.total_searches += 1
        self.total_response_time += search_time_ms
        
        return SearchResponse(
            query=query.query,
            results=results,
            total_found=len(results),
            search_time_ms=search_time_ms,
            lemmatized_query=lemmatized_query if lemmatized_query != query.query else None
        )
    
    async def batch_search(self, batch_query: BatchSearchQuery) -> BatchSearchResponse:
        """ãƒãƒƒãƒæ¤œç´¢å®Ÿè¡Œ"""
        start_time = datetime.now()
        
        # ä¸¦åˆ—æ¤œç´¢å®Ÿè¡Œ
        search_tasks = []
        for query_text in batch_query.queries:
            search_query = SearchQuery(
                query=query_text,
                max_results=batch_query.max_results
            )
            search_tasks.append(self.search(search_query))
        
        responses = await asyncio.gather(*search_tasks)
        
        # çµ±è¨ˆæƒ…å ±ä½œæˆ
        end_time = datetime.now()
        total_search_time_ms = int((end_time - start_time).total_seconds() * 1000)
        
        total_results = sum(len(r.results) for r in responses)
        successful_searches = sum(1 for r in responses if len(r.results) > 0)
        
        summary = {
            "total_queries": len(batch_query.queries),
            "successful_searches": successful_searches,
            "total_results": total_results,
            "average_results_per_query": total_results / len(batch_query.queries) if batch_query.queries else 0,
            "match_rate_percent": (successful_searches / len(batch_query.queries) * 100) if batch_query.queries else 0
        }
        
        return BatchSearchResponse(
            queries=batch_query.queries,
            responses=responses,
            total_search_time_ms=total_search_time_ms,
            summary=summary
        )
    
    def _build_advanced_search_query(self, original_term: str, lemmatized_term: str, max_results: int) -> Dict[str, Any]:
        """7æ®µéšæ¤œç´¢æˆ¦ç•¥ã®Elasticsearchã‚¯ã‚¨ãƒªæ§‹ç¯‰"""
        
        bool_query = {
            "bool": {
                "should": [
                    # 1. è¦‹å‡ºã—èªåŒ–å®Œå…¨ä¸€è‡´ï¼ˆæœ€é«˜ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰
                    {
                        "match": {
                            "search_name_lemmatized.exact": {
                                "query": lemmatized_term,
                                "boost": self.lemmatized_exact_match_boost * 3.0
                            }
                        }
                    },
                    # 2. è¦‹å‡ºã—èªåŒ–ä¸€è‡´ï¼ˆé«˜ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰
                    {
                        "match": {
                            "search_name_lemmatized": {
                                "query": lemmatized_term,
                                "boost": self.lemmatized_exact_match_boost * 2.0
                            }
                        }
                    },
                    # 3. å…ƒã®èªã§ã®å®Œå…¨ä¸€è‡´
                    {
                        "match": {
                            "search_name.exact": {
                                "query": original_term,
                                "boost": 1.8
                            }
                        }
                    },
                    # 4. å…ƒã®èªã§ã®ä¸€è‡´
                    {
                        "match": {
                            "search_name": {
                                "query": original_term,
                                "boost": 1.5
                            }
                        }
                    },
                    # 5. è¦‹å‡ºã—èªåŒ–éƒ¨åˆ†ä¸€è‡´
                    {
                        "match_phrase": {
                            "search_name_lemmatized": {
                                "query": lemmatized_term,
                                "boost": self.lemmatized_exact_match_boost
                            }
                        }
                    },
                    # 6. å…ƒã®èªã§ã®éƒ¨åˆ†ä¸€è‡´
                    {
                        "match_phrase": {
                            "search_name": {
                                "query": original_term,
                                "boost": 1.0
                            }
                        }
                    },
                    # 7. ã‚ã„ã¾ã„æ¤œç´¢ï¼ˆtypoå¯¾å¿œï¼‰
                    {
                        "fuzzy": {
                            "search_name_lemmatized": {
                                "value": lemmatized_term,
                                "fuzziness": "AUTO",
                                "boost": 0.5
                            }
                        }
                    }
                ],
                "minimum_should_match": 1
            }
        }
        
        return {
            "query": bool_query,
            "size": max_results * 2,  # ã‚¹ã‚³ã‚¢èª¿æ•´å¾Œã«çµã‚Šè¾¼ã‚€ãŸã‚å¤šã‚ã«å–å¾—
            "_source": ["id", "search_name", "description", "original_name", "data_type", "nutrition", "source_db"]
        }
    
    async def _convert_es_results(self, es_hits: List[Dict[str, Any]], original_term: str, lemmatized_term: str) -> List[SearchResult]:
        """Elasticsearchçµæœã‚’SearchResultã«å¤‰æ›"""
        results = []
        
        for hit in es_hits:
            source = hit['_source']
            
            # æ „é¤Šæƒ…å ±å¤‰æ›
            nutrition_data = source.get('nutrition', {})
            nutrition = NutritionInfo(**nutrition_data)
            
            # ãƒãƒƒãƒã‚¿ã‚¤ãƒ—åˆ¤å®š
            match_type = self._determine_match_type(
                source.get('search_name', ''),
                source.get('search_name_lemmatized', ''),
                original_term,
                lemmatized_term
            )
            
            # ã‚¹ã‚³ã‚¢èª¿æ•´
            adjusted_score = self._calculate_adjusted_score(
                hit['_score'],
                original_term,
                lemmatized_term,
                source.get('search_name', ''),
                source.get('search_name_lemmatized', ''),
                match_type
            )
            
            result = SearchResult(
                id=str(source.get('id', '')),
                name=source.get('search_name', ''),
                description=source.get('description', ''),
                original_name=source.get('original_name', ''),
                nutrition=nutrition,
                source_db=source.get('source_db', ''),
                score=adjusted_score,
                match_type=match_type
            )
            
            results.append(result)
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.score, reverse=True)
        
        # åŒã‚¹ã‚³ã‚¢çµæœã®å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆsearch_name + description ã§ã®å†æ¤œç´¢ï¼‰
        results = await self._rerank_tied_scores(results, original_term, lemmatized_term)
        
        return results
    
    def _determine_match_type(self, db_name: str, db_name_lemmatized: str, original_term: str, lemmatized_term: str) -> str:
        """ãƒãƒƒãƒã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        db_name_lower = db_name.lower()
        db_lemmatized_lower = db_name_lemmatized.lower()
        original_lower = original_term.lower()
        lemmatized_lower = lemmatized_term.lower()
        
        if db_name_lower == original_lower or db_lemmatized_lower == lemmatized_lower:
            return "exact"
        elif db_lemmatized_lower == lemmatized_lower:
            return "lemmatized"
        elif original_lower in db_name_lower or lemmatized_lower in db_lemmatized_lower:
            return "partial"
        else:
            return "fuzzy"
    
    def _calculate_adjusted_score(
        self, 
        base_score: float, 
        original_term: str, 
        lemmatized_term: str, 
        db_name: str, 
        db_name_lemmatized: str, 
        match_type: str
    ) -> float:
        """ã‚¹ã‚³ã‚¢èª¿æ•´è¨ˆç®—"""
        
        # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢èª¿æ•´
        if match_type == "exact":
            adjustment = self.lemmatized_exact_match_boost
        elif match_type == "lemmatized":
            adjustment = self.lemmatized_exact_match_boost * 0.9
        elif match_type == "partial":
            adjustment = 0.8
        else:  # fuzzy
            adjustment = 0.5
        
        # è¤‡åˆèªãƒšãƒŠãƒ«ãƒ†ã‚£
        if len(original_term.split()) > 1:
            adjustment *= self.compound_word_penalty
        
        return base_score * adjustment
    
    async def _rerank_tied_scores(self, results: List[SearchResult], original_term: str, lemmatized_term: str) -> List[SearchResult]:
        """åŒã˜ã‚¹ã‚³ã‚¢ã®çµæœã‚’search_name + descriptionã§å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        if len(results) <= 1:
            return results
        
        # ã‚¹ã‚³ã‚¢ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
        score_groups = {}
        for result in results:
            rounded_score = round(result.score, 2)  # å°æ•°ç‚¹2æ¡ã§ä¸¸ã‚ã‚‹
            if rounded_score not in score_groups:
                score_groups[rounded_score] = []
            score_groups[rounded_score].append(result)
        
        # å„ã‚¹ã‚³ã‚¢ã‚°ãƒ«ãƒ¼ãƒ—ã§å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        reranked_results = []
        for score in sorted(score_groups.keys(), reverse=True):
            group = score_groups[score]
            
            if len(group) == 1:
                # å˜ä¸€çµæœã¯ãã®ã¾ã¾
                reranked_results.extend(group)
            else:
                # è¤‡æ•°çµæœã‚’å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
                reranked_group = await self._rerank_group_with_description(group, original_term, lemmatized_term)
                reranked_results.extend(reranked_group)
        
        return reranked_results
    
    async def _rerank_group_with_description(self, group: List[SearchResult], original_term: str, lemmatized_term: str) -> List[SearchResult]:
        """åŒã‚¹ã‚³ã‚¢ã‚°ãƒ«ãƒ¼ãƒ—ã‚’search_name + descriptionã§å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        try:
            # ã‚°ãƒ«ãƒ¼ãƒ—ã®IDã‚’å–å¾—
            group_ids = [result.id for result in group]
            
            # search_name + description ã§ã®å†æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰
            rerank_query = self._build_description_search_query(original_term, lemmatized_term, group_ids)
            
            # å†æ¤œç´¢å®Ÿè¡Œ
            response = await self.es_client.search(rerank_query)
            
            if not response or not response.get('hits', {}).get('hits'):
                return group  # å†æ¤œç´¢å¤±æ•—æ™‚ã¯å…ƒã®é †åºã‚’ç¶­æŒ
            
            # å†æ¤œç´¢çµæœã§ã‚¹ã‚³ã‚¢ãƒãƒƒãƒ—ã‚’ä½œæˆ
            rerank_scores = {}
            for hit in response['hits']['hits']:
                item_id = str(hit['_source'].get('id', ''))
                rerank_scores[item_id] = hit['_score']
            
            # å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Ÿè¡Œã¨ã‚¹ã‚³ã‚¢æ›´æ–°
            original_base_score = group[0].score  # åŒã‚¹ã‚³ã‚¢ã‚°ãƒ«ãƒ¼ãƒ—ãªã®ã§æœ€åˆã®ã‚¹ã‚³ã‚¢ã‚’åŸºæº–ã«
            reranked_group = []
            
            # descriptionã®é–¢é€£æ€§ã‚’è©•ä¾¡ã—ã¦å·®åˆ¥åŒ–
            for i, result in enumerate(sorted(group, key=lambda r: rerank_scores.get(r.id, 0), reverse=True)):
                # descriptionã«ã‚ˆã‚‹è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—
                desc_bonus = self._calculate_description_relevance_bonus(result.description, original_term, lemmatized_term)
                rerank_bonus = rerank_scores.get(result.id, 0) * 0.1
                position_penalty = i * 0.001  # åŒã˜rerankã‚¹ã‚³ã‚¢ã§ã‚‚é †åºã‚’ä¿ã¤ãŸã‚ã®å¾®èª¿æ•´
                
                new_score = original_base_score + rerank_bonus + desc_bonus - position_penalty
                
                # çµæœã®ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
                updated_result = SearchResult(
                    id=result.id,
                    name=result.name,
                    description=result.description,
                    original_name=result.original_name,
                    nutrition=result.nutrition,
                    source_db=result.source_db,
                    score=new_score,
                    match_type=result.match_type
                )
                reranked_group.append(updated_result)
            
            return reranked_group
            
        except Exception as e:
            logger.warning(f"Reranking failed: {e}")
            return group  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®é †åºã‚’ç¶­æŒ
    
    def _calculate_description_relevance_bonus(self, description: str, original_term: str, lemmatized_term: str) -> float:
        """descriptionã®é–¢é€£æ€§ã«åŸºã¥ãã‚¹ã‚³ã‚¢ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—"""
        if not description or description == "None":
            return 0.0
        
        desc_lower = description.lower()
        original_lower = original_term.lower()
        lemmatized_lower = lemmatized_term.lower()
        
        bonus = 0.0
        
        # é–¢é€£æ€§ã®é«˜ã„èª¿ç†æ³•ãƒ»å½¢æ…‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«ãƒœãƒ¼ãƒŠã‚¹
        cooking_methods = ["raw", "cooked", "boiled", "baked", "grilled", "fried", "steamed", "roasted"]
        forms = ["boneless", "skinless", "whole", "ground", "tenderloins", "meat only"]
        containers = ["canned", "fresh", "frozen", "dried"]
        
        # èª¿ç†æ³•ã®é–¢é€£æ€§
        for method in cooking_methods:
            if method in desc_lower:
                if "raw" in desc_lower and ("fresh" in original_lower or "raw" in original_lower):
                    bonus += 0.02  # ç”Ÿã«é–¢é€£ã™ã‚‹æ¤œç´¢ã«ã¯ç”Ÿé£Ÿå“ã‚’å„ªå…ˆ
                elif method in original_lower or method in lemmatized_lower:
                    bonus += 0.05  # æ¤œç´¢èªã«å«ã¾ã‚Œã‚‹èª¿ç†æ³•
                else:
                    bonus += 0.01  # ä¸€èˆ¬çš„ãªèª¿ç†æ³•
        
        # å½¢æ…‹ã®é–¢é€£æ€§
        for form in forms:
            if form in desc_lower:
                if form in original_lower or form in lemmatized_lower:
                    bonus += 0.03
                else:
                    bonus += 0.005
        
        # å®¹å™¨ãƒ»ä¿å­˜å½¢æ…‹
        for container in containers:
            if container in desc_lower:
                bonus += 0.002
        
        # å˜èªæ•°ã«ã‚ˆã‚‹è¤‡é›‘ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆç°¡æ½”ãªè¨˜è¿°ã‚’å„ªå…ˆï¼‰
        word_count = len(description.split(", "))
        if word_count > 3:
            bonus -= (word_count - 3) * 0.001
        
        return bonus
    
    def _build_description_search_query(self, original_term: str, lemmatized_term: str, target_ids: List[str]) -> Dict[str, Any]:
        """search_name + description ã§ã®å†æ¤œç´¢ã‚¯ã‚¨ãƒªæ§‹ç¯‰"""
        
        bool_query = {
            "bool": {
                "must": [
                    # å¯¾è±¡IDã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    {
                        "terms": {
                            "id": target_ids
                        }
                    }
                ],
                "should": [
                    # search_name + description ã®çµ„ã¿åˆã‚ã›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã®æ¤œç´¢
                    {
                        "multi_match": {
                            "query": lemmatized_term,
                            "fields": ["search_name_lemmatized^2.0", "description^1.0"],
                            "type": "best_fields"
                        }
                    },
                    {
                        "multi_match": {
                            "query": original_term,
                            "fields": ["search_name^2.0", "description^1.0"],
                            "type": "best_fields"
                        }
                    },
                    # ãƒ•ãƒ¬ãƒ¼ã‚ºãƒãƒƒãƒ
                    {
                        "multi_match": {
                            "query": lemmatized_term,
                            "fields": ["search_name_lemmatized", "description"],
                            "type": "phrase",
                            "boost": 1.5
                        }
                    },
                    # ãƒ•ã‚¡ã‚¸ãƒ¼æ¤œç´¢
                    {
                        "multi_match": {
                            "query": lemmatized_term,
                            "fields": ["search_name_lemmatized", "description"],
                            "fuzziness": "AUTO",
                            "boost": 0.5
                        }
                    }
                ],
                "minimum_should_match": 1
            }
        }
        
        return {
            "query": bool_query,
            "size": len(target_ids),
            "_source": ["id", "search_name", "description", "original_name"]
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """æ¤œç´¢çµ±è¨ˆå–å¾—"""
        avg_response_time = (self.total_response_time / self.total_searches) if self.total_searches > 0 else 0
        
        return {
            "total_searches": self.total_searches,
            "average_response_time_ms": avg_response_time,
            "total_documents": self.es_client.get_total_documents(),
            "elasticsearch_health": self.es_client.get_index_stats()
        } 