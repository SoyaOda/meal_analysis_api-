#!/usr/bin/env python3
"""
Elasticsearch Index Creation Script with Lemmatization Support

ç¾çŠ¶ã®JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã™ã‚‹ï¼ˆè¦‹å‡ºã—èªåŒ–å¯¾å¿œç‰ˆï¼‰
"""

import json
import os
from elasticsearch import Elasticsearch
from typing import Dict, List, Any
import time

# è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from app_v2.utils.lemmatization import lemmatize_term
    LEMMATIZATION_AVAILABLE = True
    print("âœ… è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âš ï¸ è¦‹å‡ºã—èªåŒ–æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“: {e}")
    print("   åŸºæœ¬æ©Ÿèƒ½ã®ã¿ã§å®Ÿè¡Œã—ã¾ã™")
    LEMMATIZATION_AVAILABLE = False
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
    def lemmatize_term(term: str) -> str:
        return term.lower() if term else ""


def create_index_mapping() -> Dict[str, Any]:
    """Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®šç¾©"""
    return {
        "mappings": {
            "properties": {
                "id": {
                    "type": "keyword"
                },
                "search_name": {
                    "type": "text",
                    "analyzer": "standard",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        },
                        "suggest": {
                            "type": "completion"
                        }
                    }
                },
                "search_name_lemmatized": {
                    "type": "text",
                    "analyzer": "standard",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        }
                    }
                },
                "description": {
                    "type": "text",
                    "analyzer": "standard"
                },
                "data_type": {
                    "type": "keyword"
                },
                "nutrition": {
                    "type": "object",
                    "properties": {
                        "calories": {"type": "float"},
                        "protein": {"type": "float"},
                        "fat": {"type": "float"},
                        "carbs": {"type": "float"},
                        "carbohydrates": {"type": "float"},
                        "fiber": {"type": "float"},
                        "sugar": {"type": "float"},
                        "sodium": {"type": "float"}
                    }
                },
                "weight": {
                    "type": "float"
                },
                "source_db": {
                    "type": "keyword"
                }
            }
        },
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "food_analyzer": {
                        "type": "standard",
                        "stopwords": "_none_"
                    }
                }
            }
        }
    }


def load_json_databases() -> Dict[str, List[Dict[str, Any]]]:
    """JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    databases = {}
    
    db_configs = {
        "yazio": "db/yazio_db.json",
        "mynetdiary": "db/mynetdiary_db.json", 
        "eatthismuch": "db/eatthismuch_db.json"
    }
    
    for db_name, file_path in db_configs.items():
        try:
            if os.path.exists(file_path):
                print(f"Loading {db_name} from {file_path}...")
                with open(file_path, 'r', encoding='utf-8') as f:
                    database = json.load(f)
                    databases[db_name] = database
                    print(f"âœ… Loaded {db_name}: {len(database)} items")
            else:
                print(f"âš ï¸  File not found: {file_path}")
                databases[db_name] = []
        except Exception as e:
            print(f"âŒ Error loading {db_name}: {e}")
            databases[db_name] = []
    
    return databases


def prepare_document(item: Dict[str, Any], source_db: str) -> Dict[str, Any]:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Elasticsearchç”¨ã«æº–å‚™ï¼ˆè¦‹å‡ºã—èªåŒ–å¯¾å¿œï¼‰"""
    search_name = item.get("search_name", "")
    
    # è¦‹å‡ºã—èªåŒ–ã•ã‚ŒãŸsearch_nameã‚’ç”Ÿæˆ
    search_name_lemmatized = ""
    if search_name and LEMMATIZATION_AVAILABLE:
        try:
            search_name_lemmatized = lemmatize_term(search_name)
        except Exception as e:
            print(f"âš ï¸ è¦‹å‡ºã—èªåŒ–ã‚¨ãƒ©ãƒ¼ for '{search_name}': {e}")
            search_name_lemmatized = search_name.lower()
    else:
        search_name_lemmatized = search_name.lower()
    
    doc = {
        "id": item.get("id", 0),
        "search_name": search_name,
        "search_name_lemmatized": search_name_lemmatized,
        "description": item.get("description"),
        "data_type": item.get("data_type", "unknown"),
        "nutrition": item.get("nutrition", {}),
        "weight": item.get("weight"),
        "source_db": source_db
    }
    
    # ç©ºã®å€¤ã‚’é™¤å»
    return {k: v for k, v in doc.items() if v is not None}


def bulk_index_documents(es_client: Elasticsearch, index_name: str, documents: List[Dict[str, Any]], batch_size: int = 1000):
    """ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ """
    total_docs = len(documents)
    indexed_count = 0
    
    print(f"ğŸ“¥ Indexing {total_docs} documents in batches of {batch_size}...")
    
    for i in range(0, total_docs, batch_size):
        batch = documents[i:i + batch_size]
        
        # ãƒãƒ«ã‚¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æ§‹ç¯‰
        bulk_body = []
        for doc in batch:
            bulk_body.append({
                "index": {
                    "_index": index_name,
                    "_id": f"{doc['source_db']}_{doc['id']}"
                }
            })
            bulk_body.append(doc)
        
        try:
            response = es_client.bulk(body=bulk_body)
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if response.get("errors"):
                error_count = sum(1 for item in response["items"] if "error" in item.get("index", {}))
                print(f"âš ï¸  Batch {i//batch_size + 1}: {error_count} errors in batch")
            
            indexed_count += len(batch)
            print(f"   Progress: {indexed_count}/{total_docs} ({indexed_count/total_docs*100:.1f}%)")
            
        except Exception as e:
            print(f"âŒ Error indexing batch {i//batch_size + 1}: {e}")
    
    print(f"âœ… Indexing completed: {indexed_count} documents")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== Elasticsearch Index Creation ===")
    
    # Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    print("\n1. Connecting to Elasticsearch...")
    es_client = Elasticsearch(["http://localhost:9200"])
    
    if not es_client.ping():
        print("âŒ Cannot connect to Elasticsearch. Make sure it's running on localhost:9200")
        return False
    
    print("âœ… Connected to Elasticsearch")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
    index_name = "nutrition_db"
    
    # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‰Šé™¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    print(f"\n2. Checking existing index '{index_name}'...")
    if es_client.indices.exists(index=index_name):
        print(f"   Index '{index_name}' already exists. Deleting...")
        es_client.indices.delete(index=index_name)
        print("   âœ… Deleted existing index")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
    print(f"\n3. Creating index '{index_name}'...")
    mapping = create_index_mapping()
    es_client.indices.create(index=index_name, body=mapping)
    print("âœ… Index created with mapping")
    
    # JSONãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
    print("\n4. Loading JSON databases...")
    databases = load_json_databases()
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æº–å‚™
    print("\n5. Preparing documents for indexing...")
    all_documents = []
    
    for db_name, items in databases.items():
        print(f"   Processing {db_name}: {len(items)} items")
        for item in items:
            if "search_name" in item:  # æœ‰åŠ¹ãªã‚¢ã‚¤ãƒ†ãƒ ã®ã¿
                doc = prepare_document(item, db_name)
                all_documents.append(doc)
    
    print(f"âœ… Prepared {len(all_documents)} documents for indexing")
    
    # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    print("\n6. Bulk indexing documents...")
    start_time = time.time()
    bulk_index_documents(es_client, index_name, all_documents)
    end_time = time.time()
    
    print(f"âœ… Indexing completed in {end_time - start_time:.2f} seconds")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆã®è¡¨ç¤º
    print("\n7. Index statistics...")
    stats = es_client.indices.stats(index=index_name)
    doc_count = stats["indices"][index_name]["total"]["docs"]["count"]
    index_size = stats["indices"][index_name]["total"]["store"]["size_in_bytes"]
    
    print(f"   Total documents: {doc_count}")
    print(f"   Index size: {index_size / 1024 / 1024:.2f} MB")
    
    # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢ãƒ†ã‚¹ãƒˆï¼ˆè¦‹å‡ºã—èªåŒ–å¯¾å¿œï¼‰
    print("\n8. Testing sample search...")
    
    # å¾“æ¥ã®æ¤œç´¢
    test_query_standard = {
        "query": {
            "multi_match": {
                "query": "chicken",
                "fields": ["search_name", "search_name.exact"]
            }
        },
        "size": 3
    }
    
    response = es_client.search(index=index_name, body=test_query_standard)
    hits = response["hits"]["hits"]
    
    print(f"   Standard search for 'chicken': {len(hits)} results")
    for hit in hits:
        source = hit["_source"]
        print(f"   - {source['search_name']} ({source['source_db']}) score: {hit['_score']:.2f}")
    
    # è¦‹å‡ºã—èªåŒ–æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\n   Testing lemmatized search...")
    test_query_lemmatized = {
        "query": {
            "multi_match": {
                "query": "tomatoes",
                "fields": ["search_name", "search_name_lemmatized"]
            }
        },
        "size": 5
    }
    
    response_lem = es_client.search(index=index_name, body=test_query_lemmatized)
    hits_lem = response_lem["hits"]["hits"]
    
    print(f"   Lemmatized search for 'tomatoes': {len(hits_lem)} results")
    for hit in hits_lem:
        source = hit["_source"]
        lemmatized = source.get('search_name_lemmatized', 'N/A')
        print(f"   - {source['search_name']} -> {lemmatized} ({source['source_db']}) score: {hit['_score']:.2f}")
    
    print(f"\nğŸ‰ Elasticsearch index '{index_name}' successfully created!")
    print(f"   Ready for high-speed nutrition search")
    
    return True


if __name__ == "__main__":
    success = main()
    if success:
        print("\nâœ… Index creation completed successfully!")
    else:
        print("\nâŒ Index creation failed!") 