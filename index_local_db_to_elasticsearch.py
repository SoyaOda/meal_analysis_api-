#!/usr/bin/env python3
"""
Local DBå…¨ãƒ‡ãƒ¼ã‚¿ã‚’Elasticsearchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä»•æ§˜æ›¸å¯¾å¿œ: å®Ÿéš›ã®Local DBãƒ‡ãƒ¼ã‚¿ã§Elasticsearchã‚’å‹•ä½œã•ã›ã‚‹
äººæ°—åº¦æŒ‡æ¨™ï¼ˆnum_favoritesï¼‰ã‚‚çµ±åˆ
"""
import asyncio
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
import time

from app_v2.elasticsearch.client import es_client
from app_v2.elasticsearch.config import es_config

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

LOCAL_DB_PATH = Path("/Users/odasoya/meal_analysis_api /nutrition_db_experiment/nutrition_db/unified_nutrition_db.json")


def load_popularity_metrics() -> Dict[str, int]:
    """
    processed_newãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰äººæ°—åº¦æŒ‡æ¨™ï¼ˆnum_favoritesï¼‰ã‚’èª­ã¿è¾¼ã¿
    
    Returns:
        é£Ÿå“IDã¨num_favoritesã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸
    """
    popularity_data = {}
    processed_count = 0
    error_count = 0
    
    # raw_nutrition_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
    raw_data_dir = Path("raw_nutrition_data")
    if not raw_data_dir.exists():
        logger.warning(f"raw_nutrition_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {raw_data_dir}")
        return popularity_data
    
    logger.info("äººæ°—åº¦æŒ‡æ¨™ï¼ˆnum_favoritesï¼‰ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    # recipe, food, brandedã‚«ãƒ†ã‚´ãƒªã‚’å‡¦ç†
    for category in ['recipe', 'food', 'branded']:
        category_dir = raw_data_dir / category
        
        if not category_dir.exists():
            continue
        
        for item_dir in category_dir.iterdir():
            if not item_dir.is_dir():
                continue
                
            # processed_newãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            processed_new_dir = item_dir / 'processed_new'
            popularity_file = processed_new_dir / 'data_with_popularity.json'
            
            if popularity_file.exists():
                try:
                    with open(popularity_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # äººæ°—åº¦æŒ‡æ¨™ã‚’æŠ½å‡º
                    popularity_metrics = data.get('popularity_metrics', {})
                    num_favorites = popularity_metrics.get('num_favorites', 0)
                    
                    # é£Ÿå“IDã‚’ç”Ÿæˆï¼ˆLocal DBã®IDã¨ä¸€è‡´ã•ã›ã‚‹ï¼‰
                    food_id = data.get('id')
                    if food_id:
                        popularity_data[food_id] = num_favorites
                        processed_count += 1
                        
                        # é«˜ã„äººæ°—åº¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ­ã‚°
                        if num_favorites > 100:
                            logger.debug(f"é«˜äººæ°—åº¦ã‚¢ã‚¤ãƒ†ãƒ : {data.get('title', 'Unknown')} - {num_favorites} favorites")
                    
                except Exception as e:
                    logger.error(f"äººæ°—åº¦ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {popularity_file}: {e}")
                    error_count += 1
    
    logger.info(f"äººæ°—åº¦æŒ‡æ¨™èª­ã¿è¾¼ã¿å®Œäº†: æˆåŠŸ {processed_count}, ã‚¨ãƒ©ãƒ¼ {error_count}")
    
    # çµ±è¨ˆæƒ…å ±ã‚’ãƒ­ã‚°
    if popularity_data:
        values = list(popularity_data.values())
        avg_favorites = sum(values) / len(values)
        max_favorites = max(values)
        min_favorites = min(values)
        
        logger.info(f"äººæ°—åº¦çµ±è¨ˆ: å¹³å‡ {avg_favorites:.1f}, æœ€å¤§ {max_favorites}, æœ€å° {min_favorites}")
    
    return popularity_data


def convert_local_db_item_to_elasticsearch(
    item: Dict[str, Any], 
    index: int, 
    popularity_data: Dict[str, int]
) -> Dict[str, Any]:
    """
    Local DBã‚¢ã‚¤ãƒ†ãƒ ã‚’Elasticsearchç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ï¼ˆäººæ°—åº¦æŒ‡æ¨™ä»˜ãï¼‰
    
    Args:
        item: Local DBã‚¢ã‚¤ãƒ†ãƒ 
        index: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·
        popularity_data: äººæ°—åº¦æŒ‡æ¨™ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    
    Returns:
        Elasticsearchç”¨ãƒ‡ãƒ¼ã‚¿
    """
    # æ „é¤Šæƒ…å ±ã‚’å¤‰æ›ï¼ˆLocal DB â†’ Elasticsearchå½¢å¼ï¼‰
    nutrition_data = item.get("nutrition", {})
    weight = item.get("weight", 100.0)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100g
    
    # ğŸ¯ é‡è¦ãªä¿®æ­£: æ „é¤Šç´ ã‚’100gã‚ãŸã‚Šã®å€¤ã«æ­£è¦åŒ–
    def normalize_to_100g(value, weight):
        """æ „é¤Šç´ å€¤ã‚’100gã‚ãŸã‚Šã«æ­£è¦åŒ–"""
        if weight <= 0:
            return 0.0
        return (value / weight) * 100.0
    
    # ğŸ¯ ä¿®æ­£ï¼šã‚³ã‚¢4æ „é¤Šç´ ã®ã¿ã‚’ä½¿ç”¨ï¼ˆfiber_total_dietary, sodiumç­‰ã¯å‰Šé™¤ï¼‰
    core_nutrients = {
        "calories": "calories",
        "protein_g": "protein", 
        "fat_total_g": "fat",
        "carbohydrate_by_difference_g": "carbs"
    }
    
    elasticsearch_nutrition = {}
    for es_key, local_key in core_nutrients.items():
        raw_value = nutrition_data.get(local_key, 0.0)
        normalized_value = normalize_to_100g(raw_value, weight)
        elasticsearch_nutrition[es_key] = normalized_value
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æ­£è¦åŒ–
    db_type = item.get("db_type", "unknown")
    if db_type == "dish":
        data_type = "dish"
    elif db_type == "ingredient":
        data_type = "ingredient"
    elif db_type == "branded":
        data_type = "branded"
    else:
        data_type = f"local_{db_type}"
    
    # ğŸ¯ äººæ°—åº¦æŒ‡æ¨™ã‚’å–å¾—
    item_id = str(item.get('id', index))
    num_favorites = popularity_data.get(item_id, 0)
    
    # Elasticsearchç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    elasticsearch_item = {
        "food_id": f"local_{item_id}",
        "fdc_id": item.get('id'),  # Local DBã®IDã‚’ä¿æŒ
        "food_name": item.get("search_name", "Unknown Food"),
        "description": item.get("search_name", "Unknown Food"),
        "brand_name": None,  # Local DBã«ã¯ãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±ãªã—
        "category": data_type,
        "data_type": data_type,  # ğŸ¯ é‡è¦ï¼šdata_typeã‚’æ­£ã—ãè¨­å®š
        "num_favorites": num_favorites,  # ğŸ¯ äººæ°—åº¦æŒ‡æ¨™ã‚’è¿½åŠ 
        "ingredients_text": None,  # Local DBã«ã¯è©³ç´°ãªææ–™ãƒªã‚¹ãƒˆãªã—
        "nutrition": elasticsearch_nutrition,  # ğŸ¯ 100gã‚ãŸã‚Šã«æ­£è¦åŒ–æ¸ˆã¿
        "weight": 100.0,  # ğŸ¯ å¸¸ã«100gã¨ã—ã¦çµ±ä¸€ï¼ˆæ­£è¦åŒ–æ¸ˆã¿ã®ãŸã‚ï¼‰
        # Local DBç‰¹æœ‰ã®æƒ…å ±ã‚’ä¿æŒ
        "local_db_metadata": {
            "original_db_type": db_type,
            "original_id": item.get('id'),
            "original_weight": weight,  # å…ƒã®weightå€¤ã¯ä¿æŒ
            "nutrition_normalized_to_100g": True,  # æ­£è¦åŒ–æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤º
            "num_favorites": num_favorites,  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚‚ä¿æŒ
            "has_popularity_data": num_favorites > 0  # äººæ°—åº¦ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡
        }
    }
    
    return elasticsearch_item


async def index_local_db_to_elasticsearch():
    """Local DBã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’Elasticsearchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"""
    
    print("ğŸš€ Local DBå…¨ãƒ‡ãƒ¼ã‚¿ã®Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–‹å§‹")
    print("=" * 70)
    
    # 1. Local DBãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    print(f"ğŸ“ Local DBãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {LOCAL_DB_PATH}")
    
    if not LOCAL_DB_PATH.exists():
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {LOCAL_DB_PATH}")
        return False
    
    with open(LOCAL_DB_PATH, 'r', encoding='utf-8') as f:
        local_db_data = json.load(f)
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(local_db_data):,}ä»¶")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ã®çµ±è¨ˆ
    db_type_counts = {}
    for item in local_db_data:
        db_type = item.get("db_type", "unknown")
        db_type_counts[db_type] = db_type_counts.get(db_type, 0) + 1
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:")
    for db_type, count in sorted(db_type_counts.items()):
        print(f"   {db_type}: {count:,}ä»¶")
    
    # 1.5. äººæ°—åº¦æŒ‡æ¨™ã®èª­ã¿è¾¼ã¿ï¼ˆä¸€åº¦ã ã‘ï¼‰
    print(f"\nâ­ äººæ°—åº¦æŒ‡æ¨™èª­ã¿è¾¼ã¿...")
    popularity_data = load_popularity_metrics()
    print(f"âœ… äººæ°—åº¦æŒ‡æ¨™èª­ã¿è¾¼ã¿å®Œäº†: {len(popularity_data):,}ä»¶ã®ã‚¢ã‚¤ãƒ†ãƒ ã«äººæ°—åº¦ãƒ‡ãƒ¼ã‚¿ã‚ã‚Š")
    
    # 2. Elasticsearchæ¥ç¶šç¢ºèª
    print(f"\nğŸ”Œ Elasticsearchæ¥ç¶šç¢ºèª...")
    es_healthy = await es_client.health_check()
    if not es_healthy:
        print(f"âŒ Elasticsearchæ¥ç¶šå¤±æ•—")
        return False
    print(f"âœ… Elasticsearchæ¥ç¶šæˆåŠŸ")
    
    # 3. æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‰Šé™¤ãƒ»å†ä½œæˆ
    index_name = es_config.food_nutrition_index
    print(f"\nğŸ—‘ï¸  æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤ãƒ»å†ä½œæˆ: {index_name}")
    
    try:
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤
        if es_client.client.indices.exists(index=index_name):
            es_client.client.indices.delete(index=index_name)
            print(f"   æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤å®Œäº†")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­å®šèª­ã¿è¾¼ã¿
        settings_path = Path("elasticsearch_config/food_nutrition_index_settings.json")
        with open(settings_path, 'r', encoding='utf-8') as f:
            index_settings = json.load(f)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        es_client.client.indices.create(
            index=index_name,
            body=index_settings
        )
        print(f"   æ–°è¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†")
        
    except Exception as e:
        print(f"âŒ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return False
    
    # 4. ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
    print(f"\nğŸ“ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†...")
    
    batch_size = 1000  # ãƒãƒƒãƒã‚µã‚¤ã‚º
    total_items = len(local_db_data)
    successful_indexed = 0
    failed_indexed = 0
    
    start_time = time.time()
    
    for i in range(0, total_items, batch_size):
        batch_data = local_db_data[i:i + batch_size]
        batch_num = (i // batch_size) + 1
        total_batches = (total_items + batch_size - 1) // batch_size
        
        print(f"   ãƒãƒƒãƒ {batch_num}/{total_batches}: {len(batch_data)}ä»¶å‡¦ç†ä¸­...")
        
        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
        bulk_body = []
        
        for j, item in enumerate(batch_data):
            try:
                # ãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆäººæ°—åº¦ãƒ‡ãƒ¼ã‚¿ã‚’å†åˆ©ç”¨ï¼‰
                es_item = convert_local_db_item_to_elasticsearch(item, i + j, popularity_data)
                
                # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
                bulk_body.append({
                    "index": {
                        "_index": index_name,
                        "_id": es_item["food_id"]
                    }
                })
                bulk_body.append(es_item)
                
            except Exception as e:
                logger.error(f"ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚¨ãƒ©ãƒ¼ (index {i + j}): {e}")
                failed_indexed += 1
        
        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®Ÿè¡Œ
        try:
            if bulk_body:
                bulk_result = es_client.client.bulk(
                    body=bulk_body,
                    refresh=False  # æœ€å¾Œã«ã¾ã¨ã‚ã¦refresh
                )
                
                # çµæœå‡¦ç†
                if bulk_result.get("errors"):
                    for item in bulk_result.get("items", []):
                        if "index" in item and "error" in item["index"]:
                            failed_indexed += 1
                            logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {item['index']['error']}")
                        else:
                            successful_indexed += 1
                else:
                    successful_indexed += len(batch_data)
                
                # é€²æ—è¡¨ç¤º
                progress = (i + len(batch_data)) / total_items * 100
                elapsed = time.time() - start_time
                rate = (i + len(batch_data)) / elapsed if elapsed > 0 else 0
                print(f"   é€²æ—: {progress:.1f}% ({successful_indexed:,}ä»¶æˆåŠŸ, {failed_indexed}ä»¶å¤±æ•—) - {rate:.1f}ä»¶/ç§’")
        
        except Exception as e:
            logger.error(f"ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼ (batch {batch_num}): {e}")
            failed_indexed += len(batch_data)
    
    # 5. ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹refresh
    print(f"\nğŸ”„ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹refresh...")
    es_client.client.indices.refresh(index=index_name)
    
    # 6. çµæœç¢ºèª
    total_time = time.time() - start_time
    print(f"\nâœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†å®Œäº†!")
    print(f"ğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
    print(f"   ç·ä»¶æ•°: {total_items:,}ä»¶")
    print(f"   æˆåŠŸ: {successful_indexed:,}ä»¶")
    print(f"   å¤±æ•—: {failed_indexed}ä»¶")
    print(f"   å‡¦ç†æ™‚é–“: {total_time:.2f}ç§’")
    print(f"   å‡¦ç†é€Ÿåº¦: {total_items / total_time:.1f}ä»¶/ç§’")
    
    # äººæ°—åº¦çµ±è¨ˆ
    items_with_popularity = sum(1 for item in local_db_data if str(item.get('id', '')) in popularity_data)
    print(f"   äººæ°—åº¦ãƒ‡ãƒ¼ã‚¿ä»˜ãã‚¢ã‚¤ãƒ†ãƒ : {items_with_popularity:,}ä»¶ ({items_with_popularity/total_items*100:.1f}%)")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»¶æ•°ç¢ºèª
    count_result = es_client.client.count(index=index_name)
    indexed_count = count_result["count"]
    print(f"   Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»¶æ•°: {indexed_count:,}ä»¶")
    
    if indexed_count != successful_indexed:
        print(f"âš ï¸  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»¶æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“!")
    
    return indexed_count > 0


async def test_indexed_data():
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ãƒ†ã‚¹ãƒˆæ¤œç´¢"""
    
    print(f"\nğŸ” ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆæ¤œç´¢...")
    
    # ã‚µãƒ³ãƒ—ãƒ«æ¤œç´¢
    test_queries = ["chicken", "rice", "salad", "ice cream"]
    
    for query in test_queries:
        try:
            result = es_client.client.search(
                index=es_config.food_nutrition_index,
                body={
                    "query": {
                        "multi_match": {
                            "query": query,
                            "fields": ["food_name^3", "description^1.5"],
                            "type": "most_fields"
                        }
                    },
                    "size": 3
                },
                request_timeout=30
            )
            
            hits = result.get("hits", {}).get("hits", [])
            print(f"\n'{query}' æ¤œç´¢çµæœ: {len(hits)}ä»¶")
            
            for hit in hits[:2]:  # ä¸Šä½2ä»¶
                source = hit["_source"]
                score = hit["_score"]
                data_type = source.get("data_type", "unknown")
                calories = source.get("nutrition", {}).get("calories", 0)
                print(f"  {score:.2f}: {source['food_name']} ({data_type}, {calories}cal)")
        
        except Exception as e:
            print(f"âŒ æ¤œç´¢ã‚¨ãƒ©ãƒ¼ '{query}': {e}")


if __name__ == "__main__":
    asyncio.run(index_local_db_to_elasticsearch())
    asyncio.run(test_indexed_data()) 