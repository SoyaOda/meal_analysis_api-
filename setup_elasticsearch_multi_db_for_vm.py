#!/usr/bin/env python3
"""
ElasticsearchVMç”¨ã®3ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ­ãƒ¼ã‚«ãƒ«ã¨åŒæ§˜ã®æ „é¤Šãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚’å®Ÿç¾
"""

import os
import sys
import json
import time
import logging
from typing import Dict, List, Any, Optional
from elasticsearch import Elasticsearch
from elasticsearch.exceptions import RequestError

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è¦‹å‡ºã—èªåŒ–ã®ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
LEMMATIZATION_AVAILABLE = False
try:
    import nltk
    from nltk.stem import WordNetLemmatizer
    from nltk.tokenize import word_tokenize
    LEMMATIZATION_AVAILABLE = True
    logger.info("âœ… NLTK lemmatization available")
    
    # å¿…è¦ãªNLTKãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    try:
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('corpora/wordnet')
    except LookupError:
        logger.warning("âš ï¸ NLTK data missing, downloading...")
        nltk.download('punkt')
        nltk.download('wordnet')
        nltk.download('omw-1.4')
        
except ImportError:
    logger.warning("âš ï¸ NLTK not available, lemmatization disabled")

def lemmatize_term(term: str) -> str:
    """å˜èªã®è¦‹å‡ºã—èªåŒ–"""
    if not LEMMATIZATION_AVAILABLE:
        return term.lower()
    
    try:
        lemmatizer = WordNetLemmatizer()
        tokens = word_tokenize(term.lower())
        lemmatized = [lemmatizer.lemmatize(token) for token in tokens]
        return " ".join(lemmatized)
    except Exception as e:
        logger.warning(f"Lemmatization failed for '{term}': {e}")
        return term.lower()

def create_index_mapping() -> Dict[str, Any]:
    """Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°å®šç¾©"""
    return {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0,
            "analysis": {
                "analyzer": {
                    "food_name_analyzer": {
                        "type": "custom",
                        "tokenizer": "standard",
                        "filter": [
                            "lowercase",
                            "stop",
                            "snowball"
                        ]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "search_name": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "exact": {
                            "type": "keyword"
                        },
                        "suggest": {
                            "type": "completion"
                        }
                    }
                },
                "search_name_lemmatized": {
                    "type": "text",
                    "analyzer": "food_name_analyzer"
                },
                "brand": {
                    "type": "text",
                    "analyzer": "food_name_analyzer",
                    "fields": {
                        "keyword": {
                            "type": "keyword"
                        }
                    }
                },
                "category": {
                    "type": "keyword"
                },
                "source_db": {
                    "type": "keyword"
                },
                "data_type": {
                    "type": "keyword"
                },
                "nutrition": {
                    "properties": {
                        "calories": {"type": "float"},
                        "protein": {"type": "float"},
                        "carbs": {"type": "float"},
                        "fat": {"type": "float"},
                        "fiber": {"type": "float"},
                        "sugar": {"type": "float"},
                        "sodium": {"type": "float"},
                        "cholesterol": {"type": "float"},
                        "saturated_fat": {"type": "float"},
                        "trans_fat": {"type": "float"},
                        "potassium": {"type": "float"},
                        "calcium": {"type": "float"},
                        "iron": {"type": "float"},
                        "vitamin_a": {"type": "float"},
                        "vitamin_c": {"type": "float"}
                    }
                },
                "ingredients": {
                    "type": "text",
                    "analyzer": "food_name_analyzer"
                },
                "preparation": {
                    "type": "text",
                    "analyzer": "food_name_analyzer"
                },
                "serving_size": {
                    "type": "text"
                },
                "serving_unit": {
                    "type": "keyword"
                },
                "created_at": {
                    "type": "date"
                }
            }
        }
    }

def create_sample_nutrition_databases() -> Dict[str, List[Dict[str, Any]]]:
    """ãƒ­ãƒ¼ã‚«ãƒ«ã¨åŒæ§˜ã®3ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    
    # YAZIO ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (1,825é …ç›®ç›¸å½“ã®ã‚µãƒ³ãƒ—ãƒ«)
    yazio_samples = [
        {
            "search_name": "ç™½ç±³",
            "brand": "",
            "category": "ä¸»é£Ÿ",
            "nutrition": {
                "calories": 168,
                "protein": 2.5,
                "carbs": 37.1,
                "fat": 0.3,
                "fiber": 0.3,
                "sugar": 0.1,
                "sodium": 1
            },
            "ingredients": "ç±³",
            "preparation": "ç‚Šé£¯",
            "serving_size": "100g",
            "serving_unit": "g"
        },
        {
            "search_name": "ç„ç±³",
            "brand": "",
            "category": "ä¸»é£Ÿ",
            "nutrition": {
                "calories": 165,
                "protein": 2.8,
                "carbs": 35.6,
                "fat": 1.0,
                "fiber": 1.4,
                "sugar": 0.4,
                "sodium": 1
            },
            "ingredients": "ç„ç±³",
            "preparation": "ç‚Šé£¯",
            "serving_size": "100g",
            "serving_unit": "g"
        },
        {
            "search_name": "é£Ÿãƒ‘ãƒ³",
            "brand": "YAZIO",
            "category": "ãƒ‘ãƒ³é¡",
            "nutrition": {
                "calories": 264,
                "protein": 9.0,
                "carbs": 47.0,
                "fat": 4.4,
                "fiber": 2.3,
                "sugar": 3.0,
                "sodium": 500
            },
            "ingredients": "å°éº¦ç²‰ã€ã‚¤ãƒ¼ã‚¹ãƒˆã€å¡©ã€ç ‚ç³–",
            "preparation": "ç„¼æˆ",
            "serving_size": "100g",
            "serving_unit": "g"
        }
    ]
    
    # MyNetDiary ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (1,142é …ç›®ç›¸å½“ã®ã‚µãƒ³ãƒ—ãƒ«)
    mynetdiary_samples = [
        {
            "search_name": "é¶ã‚€ã­è‚‰",
            "brand": "",
            "category": "è‚‰é¡",
            "nutrition": {
                "calories": 108,
                "protein": 22.3,
                "carbs": 0,
                "fat": 1.5,
                "fiber": 0,
                "sugar": 0,
                "sodium": 39,
                "cholesterol": 58
            },
            "ingredients": "é¶è‚‰",
            "preparation": "ç”Ÿ",
            "serving_size": "100g",
            "serving_unit": "g"
        },
        {
            "search_name": "é¶ã‚‚ã‚‚è‚‰",
            "brand": "",
            "category": "è‚‰é¡",
            "nutrition": {
                "calories": 200,
                "protein": 16.2,
                "carbs": 0,
                "fat": 14.2,
                "fiber": 0,
                "sugar": 0,
                "sodium": 98,
                "cholesterol": 89
            },
            "ingredients": "é¶è‚‰",
            "preparation": "ç”Ÿ",
            "serving_size": "100g",
            "serving_unit": "g"
        },
        {
            "search_name": "è±šãƒ­ãƒ¼ã‚¹",
            "brand": "MyNetDiary",
            "category": "è‚‰é¡",
            "nutrition": {
                "calories": 263,
                "protein": 19.3,
                "carbs": 0.2,
                "fat": 19.2,
                "fiber": 0,
                "sugar": 0,
                "sodium": 59,
                "cholesterol": 69
            },
            "ingredients": "è±šè‚‰",
            "preparation": "ç”Ÿ",
            "serving_size": "100g",
            "serving_unit": "g"
        }
    ]
    
    # EatThisMuch ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ (8,878é …ç›®ç›¸å½“ã®ã‚µãƒ³ãƒ—ãƒ«)
    eatthismuch_samples = [
        {
            "search_name": "ãƒˆãƒãƒˆ",
            "brand": "",
            "category": "é‡èœ",
            "nutrition": {
                "calories": 19,
                "protein": 0.7,
                "carbs": 4.7,
                "fat": 0.1,
                "fiber": 1.0,
                "sugar": 2.6,
                "sodium": 3,
                "potassium": 237,
                "vitamin_c": 21.0,
                "vitamin_a": 449
            },
            "ingredients": "ãƒˆãƒãƒˆ",
            "preparation": "ç”Ÿ",
            "serving_size": "100g",
            "serving_unit": "g"
        },
        {
            "search_name": "ãã‚…ã†ã‚Š",
            "brand": "",
            "category": "é‡èœ",
            "nutrition": {
                "calories": 14,
                "protein": 0.7,
                "carbs": 3.6,
                "fat": 0.1,
                "fiber": 0.5,
                "sugar": 1.7,
                "sodium": 2,
                "potassium": 147,
                "vitamin_c": 2.8
            },
            "ingredients": "ãã‚…ã†ã‚Š",
            "preparation": "ç”Ÿ",
            "serving_size": "100g",
            "serving_unit": "g"
        },
        {
            "search_name": "ãƒãƒŠãƒŠ",
            "brand": "EatThisMuch",
            "category": "æœç‰©",
            "nutrition": {
                "calories": 89,
                "protein": 1.1,
                "carbs": 22.8,
                "fat": 0.3,
                "fiber": 2.6,
                "sugar": 12.2,
                "sodium": 1,
                "potassium": 358,
                "vitamin_c": 8.7,
                "vitamin_a": 64
            },
            "ingredients": "ãƒãƒŠãƒŠ",
            "preparation": "ç”Ÿ",
            "serving_size": "100g",
            "serving_unit": "g"
        },
        {
            "search_name": "ã‚¢ãƒœã‚«ãƒ‰",
            "brand": "",
            "category": "æœç‰©",
            "nutrition": {
                "calories": 160,
                "protein": 2.0,
                "carbs": 8.5,
                "fat": 14.7,
                "fiber": 6.7,
                "sugar": 0.7,
                "sodium": 7,
                "potassium": 485
            },
            "ingredients": "ã‚¢ãƒœã‚«ãƒ‰",
            "preparation": "ç”Ÿ",
            "serving_size": "100g",
            "serving_unit": "g"
        },
        {
            "search_name": "é®­",
            "brand": "",
            "category": "é­šé¡",
            "nutrition": {
                "calories": 208,
                "protein": 25.4,
                "carbs": 0,
                "fat": 12.4,
                "fiber": 0,
                "sugar": 0,
                "sodium": 59,
                "cholesterol": 55
            },
            "ingredients": "é®­",
            "preparation": "ç”Ÿ",
            "serving_size": "100g",
            "serving_unit": "g"
        }
    ]
    
    return {
        "yazio": yazio_samples,
        "mynetdiary": mynetdiary_samples,
        "eatthismuch": eatthismuch_samples
    }

def prepare_document(item: Dict[str, Any], db_name: str) -> Dict[str, Any]:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Elasticsearchç”¨ã«æº–å‚™"""
    doc = {
        "search_name": item["search_name"],
        "search_name_lemmatized": lemmatize_term(item["search_name"]),
        "brand": item.get("brand", ""),
        "category": item.get("category", ""),
        "source_db": db_name,
        "data_type": "food",
        "nutrition": item.get("nutrition", {}),
        "ingredients": item.get("ingredients", ""),
        "preparation": item.get("preparation", ""),
        "serving_size": item.get("serving_size", "100g"),
        "serving_unit": item.get("serving_unit", "g"),
        "created_at": "2024-01-01T00:00:00Z"
    }
    
    # æ¤œç´¢ç”¨ã®ã‚µã‚¸ã‚§ã‚¹ãƒˆè¨­å®š
    if "search_name" in doc:
        doc["search_name"] = {
            "input": doc["search_name"],
            "suggest": doc["search_name"]
        }
    
    return doc

def bulk_index_documents(es_client: Elasticsearch, index_name: str, documents: List[Dict[str, Any]], batch_size: int = 100):
    """ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†"""
    from elasticsearch.helpers import bulk
    
    def doc_generator():
        for i, doc in enumerate(documents):
            yield {
                "_index": index_name,
                "_id": f"{doc['source_db']}_{i}",
                "_source": doc
            }
    
    try:
        success, failed = bulk(es_client, doc_generator(), chunk_size=batch_size)
        logger.info(f"âœ… Successfully indexed: {success} documents")
        if failed:
            logger.warning(f"âš ï¸ Failed to index: {len(failed)} documents")
            for failure in failed[:5]:  # æœ€åˆã®5ã¤ã®å¤±æ•—ã®ã¿è¡¨ç¤º
                logger.warning(f"   Failed: {failure}")
        
        return success
    except Exception as e:
        logger.error(f"âŒ Bulk indexing failed: {e}")
        return 0

def test_multi_database_search(es_client: Elasticsearch, index_name: str):
    """è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ãƒ†ã‚¹ãƒˆ"""
    logger.info("ğŸ” Testing multi-database search...")
    
    test_queries = [
        {"term": "é¶", "description": "é¶è‚‰æ¤œç´¢ (MyNetDiary)"},
        {"term": "ãƒˆãƒãƒˆ", "description": "é‡èœæ¤œç´¢ (EatThisMuch)"},
        {"term": "ç±³", "description": "ä¸»é£Ÿæ¤œç´¢ (YAZIO)"},
        {"term": "ãƒãƒŠãƒŠ", "description": "æœç‰©æ¤œç´¢ (EatThisMuch)"}
    ]
    
    for query_info in test_queries:
        term = query_info["term"]
        desc = query_info["description"]
        
        query = {
            "query": {
                "multi_match": {
                    "query": term,
                    "fields": ["search_name", "search_name_lemmatized"],
                    "type": "best_fields"
                }
            },
            "size": 3
        }
        
        try:
            response = es_client.search(index=index_name, body=query)
            hits = response["hits"]["hits"]
            
            logger.info(f"   {desc}: {len(hits)} results")
            for hit in hits:
                source = hit["_source"]
                logger.info(f"     - {source['search_name']} ({source['source_db']}) score: {hit['_score']:.2f}")
        except Exception as e:
            logger.error(f"   âŒ Search failed for '{term}': {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    logger.info("=== Elasticsearch Multi-Database Index Creation for VM ===")
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰Elasticsearch URLã‚’å–å¾—
    if len(sys.argv) > 1:
        es_host = sys.argv[1]
    else:
        es_host = input("Elasticsearch URL ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ä¾‹: http://10.128.0.2:9200): ")
    
    if not es_host.startswith('http'):
        es_host = f"http://{es_host}"
    
    if not es_host.endswith(':9200'):
        es_host = f"{es_host}:9200"
    
    logger.info(f"ğŸ”— Connecting to Elasticsearch: {es_host}")
    
    try:
        # Elasticsearch ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
        es_client = Elasticsearch([es_host], timeout=30)
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        if not es_client.ping():
            logger.error("âŒ Cannot connect to Elasticsearch")
            return False
        
        logger.info("âœ… Connected to Elasticsearch")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å
        index_name = "nutrition_db"
        
        # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å‰Šé™¤
        if es_client.indices.exists(index=index_name):
            logger.info(f"Deleting existing index '{index_name}'...")
            es_client.indices.delete(index=index_name)
            logger.info("âœ… Deleted existing index")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        logger.info(f"Creating index '{index_name}'...")
        mapping = create_index_mapping()
        es_client.indices.create(index=index_name, body=mapping)
        logger.info("âœ… Index created with mapping")
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆ
        logger.info("Creating sample nutrition databases...")
        databases = create_sample_nutrition_databases()
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæº–å‚™
        logger.info("Preparing documents for indexing...")
        all_documents = []
        
        for db_name, items in databases.items():
            logger.info(f"   Processing {db_name}: {len(items)} items")
            for item in items:
                doc = prepare_document(item, db_name)
                all_documents.append(doc)
        
        logger.info(f"âœ… Prepared {len(all_documents)} documents for indexing")
        
        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        logger.info("Bulk indexing documents...")
        start_time = time.time()
        indexed_count = bulk_index_documents(es_client, index_name, all_documents)
        end_time = time.time()
        
        logger.info(f"âœ… Indexing completed in {end_time - start_time:.2f} seconds")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆ
        logger.info("Index statistics...")
        es_client.indices.refresh(index=index_name)
        stats = es_client.indices.stats(index=index_name)
        doc_count = stats["indices"][index_name]["total"]["docs"]["count"]
        index_size = stats["indices"][index_name]["total"]["store"]["size_in_bytes"]
        
        logger.info(f"   Total documents: {doc_count}")
        logger.info(f"   Index size: {index_size / 1024:.2f} KB")
        
        # è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        test_multi_database_search(es_client, index_name)
        
        logger.info("")
        logger.info("ğŸ‰ Elasticsearch multi-database index successfully created!")
        logger.info("ğŸ“Š Database distribution:")
        logger.info("   - yazio: 3 items (ä¸»é£Ÿã€ãƒ‘ãƒ³é¡)")
        logger.info("   - mynetdiary: 3 items (è‚‰é¡)")
        logger.info("   - eatthismuch: 5 items (é‡èœã€æœç‰©ã€é­šé¡)")
        logger.info("")
        logger.info("ğŸ“ Cloud Runç’°å¢ƒå¤‰æ•°è¨­å®š:")
        logger.info(f"   ELASTIC_HOST={es_host}")
        logger.info("   ELASTIC_INDEX=nutrition_db")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Setup failed: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)